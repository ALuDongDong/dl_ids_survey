Using TensorFlow backend.
2019-12-26 23:46:33,470 [INFO] Initialized logging. log_filename = results_selected_models/selected_nsl_lstm_deep_rep1/run_log.log
2019-12-26 23:46:33,470 [INFO] ================= Running experiment no. 1  ================= 

2019-12-26 23:46:33,470 [INFO] Experiment parameters given below
2019-12-26 23:46:33,470 [INFO] 
{'experiment_num': 1, 'results_dir': 'results_selected_models/selected_nsl_lstm_deep_rep1', 'model': 'lstm', 'model_type': 'classifier', 'normal_label': 'normal', 'lstm_time_steps': 16, 'lstm_layer_units': [64, 32], 'lstm_layer_activations': ['relu', 'relu'], 'lstm_layer_dropout_rates': [0.2, 0.2], 'batch_size': 256, 'epochs': 300, 'early_stop_patience': 50, 'dataset_dir': '../Datasets/small_datasets/nsl_kdd_five_classes', 'description': 'selected_nsl_lstm_deep_rep1'}
2019-12-26 23:46:33,470 [INFO] Created tensorboard log directory: results_selected_models/selected_nsl_lstm_deep_rep1/tf_logs_run_2019_12_26-23_46_33
2019-12-26 23:46:33,470 [INFO] Loading datsets from: ../Datasets/small_datasets/nsl_kdd_five_classes
2019-12-26 23:46:33,478 [INFO] Reading X, y files
2019-12-26 23:46:33,478 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_train.h5
2019-12-26 23:46:33,988 [INFO] Reading complete. time_to_read=0.51 seconds
2019-12-26 23:46:33,988 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_val.h5
2019-12-26 23:46:34,123 [INFO] Reading complete. time_to_read=0.14 seconds
2019-12-26 23:46:34,124 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_test.h5
2019-12-26 23:46:34,241 [INFO] Reading complete. time_to_read=0.12 seconds
2019-12-26 23:46:34,241 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_train.h5
2019-12-26 23:46:34,266 [INFO] Reading complete. time_to_read=0.03 seconds
2019-12-26 23:46:34,266 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_val.h5
2019-12-26 23:46:34,285 [INFO] Reading complete. time_to_read=0.02 seconds
2019-12-26 23:46:34,285 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_test.h5
2019-12-26 23:46:34,302 [INFO] Reading complete. time_to_read=0.02 seconds
2019-12-26 23:46:34,461 [INFO] Preparing flow sequences
2019-12-26 23:46:36,690 [INFO] Extracting flows complete. time_taken = 2.23 sec
2019-12-26 23:46:36,740 [INFO] Initializing model
2019-12-26 23:46:36,740 [WARNING] From /home/hasitha/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

2019-12-26 23:46:36,794 [WARNING] From /home/hasitha/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

2019-12-26 23:46:36,795 [WARNING] From /home/hasitha/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

2019-12-26 23:46:37,263 [WARNING] From /home/hasitha/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

2019-12-26 23:46:37,277 [WARNING] From /home/hasitha/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
2019-12-26 23:46:37,502 [WARNING] From /home/hasitha/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

2019-12-26 23:46:37,514 [WARNING] From /home/hasitha/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.

2019-12-26 23:46:37,517 [WARNING] From /home/hasitha/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2019-12-26 23:46:37,526 [INFO] _________________________________________________________________
2019-12-26 23:46:37,526 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-26 23:46:37,526 [INFO] =================================================================
2019-12-26 23:46:37,526 [INFO] lstm_1 (LSTM)                (None, 16, 64)            47872     
2019-12-26 23:46:37,526 [INFO] _________________________________________________________________
2019-12-26 23:46:37,527 [INFO] batch_normalization_1 (Batch (None, 16, 64)            256       
2019-12-26 23:46:37,527 [INFO] _________________________________________________________________
2019-12-26 23:46:37,527 [INFO] dropout_1 (Dropout)          (None, 16, 64)            0         
2019-12-26 23:46:37,527 [INFO] _________________________________________________________________
2019-12-26 23:46:37,527 [INFO] lstm_2 (LSTM)                (None, 16, 32)            12416     
2019-12-26 23:46:37,527 [INFO] _________________________________________________________________
2019-12-26 23:46:37,527 [INFO] batch_normalization_2 (Batch (None, 16, 32)            128       
2019-12-26 23:46:37,527 [INFO] _________________________________________________________________
2019-12-26 23:46:37,527 [INFO] dropout_2 (Dropout)          (None, 16, 32)            0         
2019-12-26 23:46:37,527 [INFO] _________________________________________________________________
2019-12-26 23:46:37,527 [INFO] time_distributed_1 (TimeDist (None, 16, 5)             165       
2019-12-26 23:46:37,527 [INFO] =================================================================
2019-12-26 23:46:37,528 [INFO] Total params: 60,837
2019-12-26 23:46:37,528 [INFO] Trainable params: 60,645
2019-12-26 23:46:37,528 [INFO] Non-trainable params: 192
2019-12-26 23:46:37,528 [INFO] _________________________________________________________________
2019-12-26 23:46:37,528 [INFO] Training model
2019-12-26 23:46:41.119475: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-12-26 23:46:41.296990: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2893425000 Hz
2019-12-26 23:46:41.297365: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563d59d5d360 executing computations on platform Host. Devices:
2019-12-26 23:46:41.297399: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-12-26 23:46:41.602380: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2019-12-26 23:46:41,609 [WARNING] From /home/hasitha/anaconda3/lib/python3.7/site-packages/keras/callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

2019-12-26 23:46:41,609 [WARNING] From /home/hasitha/anaconda3/lib/python3.7/site-packages/keras/callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

Train on 6298 samples, validate on 1574 samples
Epoch 1/300
 - 2s - loss: 0.4936 - val_loss: 0.3337
 - val_f1: 0.5434
Epoch 2/300
 - 1s - loss: 0.2495 - val_loss: 0.1344
 - val_f1: 0.9189
Epoch 3/300
 - 1s - loss: 0.1036 - val_loss: 0.0752
 - val_f1: 0.9556
Epoch 4/300
 - 1s - loss: 0.0663 - val_loss: 0.0549
 - val_f1: 0.9649
Epoch 5/300
 - 1s - loss: 0.0528 - val_loss: 0.0446
 - val_f1: 0.9686
Epoch 6/300
 - 1s - loss: 0.0446 - val_loss: 0.0380
 - val_f1: 0.9700
Epoch 7/300
 - 1s - loss: 0.0390 - val_loss: 0.0319
 - val_f1: 0.9714
Epoch 8/300
 - 1s - loss: 0.0336 - val_loss: 0.0269
 - val_f1: 0.9745
Epoch 9/300
 - 1s - loss: 0.0286 - val_loss: 0.0233
 - val_f1: 0.9773
Epoch 10/300
 - 1s - loss: 0.0250 - val_loss: 0.0205
 - val_f1: 0.9805
Epoch 11/300
 - 1s - loss: 0.0225 - val_loss: 0.0184
 - val_f1: 0.9821
Epoch 12/300
 - 1s - loss: 0.0205 - val_loss: 0.0170
 - val_f1: 0.9856
Epoch 13/300
 - 1s - loss: 0.0188 - val_loss: 0.0158
 - val_f1: 0.9855
Epoch 14/300
 - 1s - loss: 0.0174 - val_loss: 0.0147
 - val_f1: 0.9856
Epoch 15/300
 - 1s - loss: 0.0163 - val_loss: 0.0140
 - val_f1: 0.9871
Epoch 16/300
 - 1s - loss: 0.0152 - val_loss: 0.0135
 - val_f1: 0.9881
Epoch 17/300
 - 1s - loss: 0.0144 - val_loss: 0.0128
 - val_f1: 0.9891
Epoch 18/300
 - 1s - loss: 0.0135 - val_loss: 0.0124
 - val_f1: 0.9897
Epoch 19/300
 - 1s - loss: 0.0130 - val_loss: 0.0117
 - val_f1: 0.9909
Epoch 20/300
 - 1s - loss: 0.0123 - val_loss: 0.0111
 - val_f1: 0.9914
Epoch 21/300
 - 1s - loss: 0.0117 - val_loss: 0.0108
 - val_f1: 0.9919
Epoch 22/300
 - 1s - loss: 0.0113 - val_loss: 0.0109
 - val_f1: 0.9919
Epoch 23/300
 - 1s - loss: 0.0108 - val_loss: 0.0106
 - val_f1: 0.9917
Epoch 24/300
 - 1s - loss: 0.0105 - val_loss: 0.0103
 - val_f1: 0.9917
Epoch 25/300
 - 1s - loss: 0.0103 - val_loss: 0.0102
 - val_f1: 0.9917
Epoch 26/300
 - 1s - loss: 0.0097 - val_loss: 0.0100
 - val_f1: 0.9920
Epoch 27/300
 - 1s - loss: 0.0101 - val_loss: 0.0098
 - val_f1: 0.9924
Epoch 28/300
 - 1s - loss: 0.0093 - val_loss: 0.0098
 - val_f1: 0.9926
Epoch 29/300
 - 1s - loss: 0.0090 - val_loss: 0.0096
 - val_f1: 0.9926
Epoch 30/300
 - 1s - loss: 0.0088 - val_loss: 0.0094
 - val_f1: 0.9927
Epoch 31/300
 - 1s - loss: 0.0085 - val_loss: 0.0093
/home/hasitha/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2019-12-26 23:47:12,867 [INFO] epoch = 30. Intermediate model saved to results_selected_models/selected_nsl_lstm_deep_rep1/_model_epoch_30.pickle
 - val_f1: 0.9928
Epoch 32/300
 - 1s - loss: 0.0081 - val_loss: 0.0093
 - val_f1: 0.9925
Epoch 33/300
 - 1s - loss: 0.0080 - val_loss: 0.0092
 - val_f1: 0.9930
Epoch 34/300
 - 1s - loss: 0.0080 - val_loss: 0.0089
 - val_f1: 0.9933
Epoch 35/300
 - 1s - loss: 0.0076 - val_loss: 0.0089
 - val_f1: 0.9932
Epoch 36/300
 - 1s - loss: 0.0073 - val_loss: 0.0092
 - val_f1: 0.9931
Epoch 37/300
 - 1s - loss: 0.0073 - val_loss: 0.0088
 - val_f1: 0.9932
Epoch 38/300
 - 1s - loss: 0.0071 - val_loss: 0.0089
 - val_f1: 0.9934
Epoch 39/300
 - 1s - loss: 0.0066 - val_loss: 0.0088
 - val_f1: 0.9931
Epoch 40/300
 - 1s - loss: 0.0066 - val_loss: 0.0087
 - val_f1: 0.9935
Epoch 41/300
 - 1s - loss: 0.0061 - val_loss: 0.0088
 - val_f1: 0.9935
Epoch 42/300
 - 1s - loss: 0.0060 - val_loss: 0.0090
 - val_f1: 0.9935
Epoch 43/300
 - 1s - loss: 0.0062 - val_loss: 0.0091
 - val_f1: 0.9927
Epoch 44/300
 - 1s - loss: 0.0058 - val_loss: 0.0088
 - val_f1: 0.9933
Epoch 45/300
 - 1s - loss: 0.0057 - val_loss: 0.0085
 - val_f1: 0.9936
Epoch 46/300
 - 1s - loss: 0.0056 - val_loss: 0.0084
 - val_f1: 0.9935
Epoch 47/300
 - 1s - loss: 0.0057 - val_loss: 0.0087
 - val_f1: 0.9934
Epoch 48/300
 - 1s - loss: 0.0055 - val_loss: 0.0089
 - val_f1: 0.9934
Epoch 49/300
 - 1s - loss: 0.0054 - val_loss: 0.0089
 - val_f1: 0.9936
Epoch 50/300
 - 1s - loss: 0.0050 - val_loss: 0.0087
 - val_f1: 0.9935
Epoch 51/300
 - 1s - loss: 0.0049 - val_loss: 0.0092
 - val_f1: 0.9935
Epoch 52/300
 - 1s - loss: 0.0048 - val_loss: 0.0091
 - val_f1: 0.9934
Epoch 53/300
 - 1s - loss: 0.0048 - val_loss: 0.0092
 - val_f1: 0.9935
Epoch 54/300
 - 1s - loss: 0.0049 - val_loss: 0.0093
 - val_f1: 0.9935
Epoch 55/300
 - 1s - loss: 0.0045 - val_loss: 0.0090
 - val_f1: 0.9935
Epoch 56/300
 - 1s - loss: 0.0044 - val_loss: 0.0089
 - val_f1: 0.9935
Epoch 57/300
 - 1s - loss: 0.0043 - val_loss: 0.0089
 - val_f1: 0.9936
Epoch 58/300
 - 1s - loss: 0.0042 - val_loss: 0.0089
 - val_f1: 0.9935
Epoch 59/300
 - 1s - loss: 0.0041 - val_loss: 0.0094
 - val_f1: 0.9937
Epoch 60/300
 - 1s - loss: 0.0040 - val_loss: 0.0089
 - val_f1: 0.9934
Epoch 61/300
 - 1s - loss: 0.0040 - val_loss: 0.0091
2019-12-26 23:47:41,088 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_nsl_lstm_deep_rep1/_model_epoch_60.pickle
 - val_f1: 0.9935
Epoch 62/300
 - 1s - loss: 0.0041 - val_loss: 0.0101
 - val_f1: 0.9934
Epoch 63/300
 - 1s - loss: 0.0041 - val_loss: 0.0091
 - val_f1: 0.9937
Epoch 64/300
 - 1s - loss: 0.0041 - val_loss: 0.0092
 - val_f1: 0.9935
Epoch 65/300
 - 1s - loss: 0.0036 - val_loss: 0.0092
 - val_f1: 0.9940
Epoch 66/300
 - 1s - loss: 0.0036 - val_loss: 0.0096
 - val_f1: 0.9935
Epoch 67/300
 - 1s - loss: 0.0035 - val_loss: 0.0099
 - val_f1: 0.9932
Epoch 68/300
 - 1s - loss: 0.0034 - val_loss: 0.0098
 - val_f1: 0.9938
Epoch 69/300
 - 1s - loss: 0.0033 - val_loss: 0.0099
 - val_f1: 0.9933
Epoch 70/300
 - 1s - loss: 0.0034 - val_loss: 0.0101
 - val_f1: 0.9935
Epoch 71/300
 - 1s - loss: 0.0033 - val_loss: 0.0097
 - val_f1: 0.9934
Epoch 72/300
 - 1s - loss: 0.0030 - val_loss: 0.0102
 - val_f1: 0.9933
Epoch 73/300
 - 1s - loss: 0.0032 - val_loss: 0.0104
 - val_f1: 0.9930
Epoch 74/300
 - 1s - loss: 0.0030 - val_loss: 0.0097
 - val_f1: 0.9937
Epoch 75/300
 - 1s - loss: 0.0027 - val_loss: 0.0102
 - val_f1: 0.9934
Epoch 76/300
 - 1s - loss: 0.0028 - val_loss: 0.0102
 - val_f1: 0.9935
Epoch 77/300
 - 1s - loss: 0.0029 - val_loss: 0.0103
 - val_f1: 0.9934
Epoch 78/300
 - 1s - loss: 0.0029 - val_loss: 0.0104
 - val_f1: 0.9935
Epoch 79/300
 - 1s - loss: 0.0027 - val_loss: 0.0101
 - val_f1: 0.9937
Epoch 80/300
 - 1s - loss: 0.0022 - val_loss: 0.0104
 - val_f1: 0.9934
Epoch 81/300
 - 1s - loss: 0.0025 - val_loss: 0.0106
 - val_f1: 0.9937
Epoch 82/300
 - 1s - loss: 0.0024 - val_loss: 0.0107
 - val_f1: 0.9933
Epoch 83/300
 - 1s - loss: 0.0024 - val_loss: 0.0107
 - val_f1: 0.9939
Epoch 84/300
 - 1s - loss: 0.0024 - val_loss: 0.0109
 - val_f1: 0.9937
Epoch 85/300
 - 1s - loss: 0.0025 - val_loss: 0.0109
 - val_f1: 0.9935
Epoch 86/300
 - 1s - loss: 0.0024 - val_loss: 0.0110
 - val_f1: 0.9932
Epoch 87/300
 - 1s - loss: 0.0024 - val_loss: 0.0110
 - val_f1: 0.9935
Epoch 88/300
 - 1s - loss: 0.0022 - val_loss: 0.0109
 - val_f1: 0.9936
Epoch 89/300
 - 1s - loss: 0.0022 - val_loss: 0.0110
 - val_f1: 0.9935
Epoch 90/300
 - 1s - loss: 0.0021 - val_loss: 0.0112
 - val_f1: 0.9933
Epoch 91/300
 - 1s - loss: 0.0023 - val_loss: 0.0109
2019-12-26 23:48:09,335 [INFO] epoch = 90. Intermediate model saved to results_selected_models/selected_nsl_lstm_deep_rep1/_model_epoch_90.pickle
 - val_f1: 0.9936
Epoch 92/300
 - 1s - loss: 0.0023 - val_loss: 0.0112
 - val_f1: 0.9933
Epoch 93/300
 - 1s - loss: 0.0021 - val_loss: 0.0117
 - val_f1: 0.9935
Epoch 94/300
 - 1s - loss: 0.0022 - val_loss: 0.0119
 - val_f1: 0.9934
Epoch 95/300
 - 1s - loss: 0.0021 - val_loss: 0.0115
 - val_f1: 0.9931
Epoch 96/300
 - 1s - loss: 0.0019 - val_loss: 0.0119
2019-12-26 23:48:14,159 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-26 23:48:14,864 [INFO] Last epoch loss evaluation: train_loss = 0.003044, val_loss = 0.008401
2019-12-26 23:48:14,864 [INFO] Training complete. time_to_train = 97.34 sec, 1.62 min
2019-12-26 23:48:14,869 [INFO] Model saved to results_selected_models/selected_nsl_lstm_deep_rep1/best_model.pickle
2019-12-26 23:48:14,889 [INFO] Training history saved to: results_selected_models/selected_nsl_lstm_deep_rep1/training_error_history.csv
2019-12-26 23:48:15,047 [INFO] Plot saved to: results_selected_models/selected_nsl_lstm_deep_rep1/training_error_history.png
2019-12-26 23:48:15,156 [INFO] Plot saved to: results_selected_models/selected_nsl_lstm_deep_rep1/training_f1_history.png
2019-12-26 23:48:15,156 [INFO] Making predictions on training, validation, testing data
2019-12-26 23:48:15,877 [INFO] Evaluating predictions (results)
2019-12-26 23:48:16,207 [INFO] Dataset: Testing. Classification report below
2019-12-26 23:48:16,207 [INFO] 
              precision    recall  f1-score   support

         dos       0.95      0.85      0.89      7458
      normal       0.68      0.93      0.78      9711
       probe       0.74      0.69      0.71      2421
         r2l       0.91      0.09      0.17      2421
         u2r       0.60      0.02      0.03       533

   micro avg       0.77      0.77      0.77     22544
   macro avg       0.77      0.52      0.52     22544
weighted avg       0.80      0.77      0.73     22544

2019-12-26 23:48:16,207 [INFO] Overall accuracy (micro avg): 0.7654808374733854
2019-12-26 23:48:16,540 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.7655         0.7655                       0.7655                0.0586                   0.2345  0.7655
1     Macro avg        0.9062         0.7738                       0.5163                0.0780                   0.4837  0.5191
2  Weighted avg        0.8652         0.7959                       0.7655                0.1555                   0.2345  0.7291
2019-12-26 23:48:16,912 [INFO] Dataset: Validation. Classification report below
2019-12-26 23:48:16,912 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00      9183
      normal       0.99      0.99      0.99     13462
       probe       0.98      0.99      0.98      2330
         r2l       0.93      0.81      0.87       199
         u2r       0.67      0.40      0.50        10

   micro avg       0.99      0.99      0.99     25184
   macro avg       0.91      0.84      0.87     25184
weighted avg       0.99      0.99      0.99     25184

2019-12-26 23:48:16,912 [INFO] Overall accuracy (micro avg): 0.9935673443456162
2019-12-26 23:48:17,290 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9936         0.9936                       0.9936                0.0016                   0.0064  0.9936
1     Macro avg        0.9974         0.9143                       0.8387                0.0022                   0.1613  0.8690
2  Weighted avg        0.9959         0.9935                       0.9936                0.0044                   0.0064  0.9935
2019-12-26 23:48:18,788 [INFO] Dataset: Training. Classification report below
2019-12-26 23:48:18,788 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00     36738
      normal       1.00      1.00      1.00     53871
       probe       1.00      0.99      1.00      9321
         r2l       0.98      0.92      0.95       796
         u2r       0.92      0.57      0.71        42

   micro avg       1.00      1.00      1.00    100768
   macro avg       0.98      0.90      0.93    100768
weighted avg       1.00      1.00      1.00    100768

2019-12-26 23:48:18,788 [INFO] Overall accuracy (micro avg): 0.9979160050809781
2019-12-26 23:48:20,483 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9979         0.9979                       0.9979                0.0005                   0.0021  0.9979
1     Macro avg        0.9992         0.9793                       0.8977                0.0008                   0.1023  0.9301
2  Weighted avg        0.9987         0.9979                       0.9979                0.0017                   0.0021  0.9979
2019-12-26 23:48:20,509 [INFO] Results saved to: results_selected_models/selected_nsl_lstm_deep_rep1/selected_nsl_lstm_deep_rep1_results.xlsx
2019-12-26 23:48:20,509 [INFO] ================= Finished running experiment no. 1 ================= 

2019-12-26 23:48:20,513 [INFO] Created directory: results_selected_models/selected_nsl_lstm_deep_rep2
2019-12-26 23:48:20,513 [INFO] Initialized logging. log_filename = results_selected_models/selected_nsl_lstm_deep_rep2/run_log.log
2019-12-26 23:48:20,513 [INFO] ================= Running experiment no. 2  ================= 

2019-12-26 23:48:20,513 [INFO] Experiment parameters given below
2019-12-26 23:48:20,513 [INFO] 
{'experiment_num': 2, 'results_dir': 'results_selected_models/selected_nsl_lstm_deep_rep2', 'model': 'lstm', 'model_type': 'classifier', 'normal_label': 'normal', 'lstm_time_steps': 17, 'lstm_layer_units': [64, 33], 'lstm_layer_activations': ['relu', 'relu'], 'lstm_layer_dropout_rates': [0.2, 0.3], 'batch_size': 256, 'epochs': 300, 'early_stop_patience': 50, 'dataset_dir': '../Datasets/small_datasets/nsl_kdd_five_classes', 'description': 'selected_nsl_lstm_deep_rep2'}
2019-12-26 23:48:20,513 [INFO] Created tensorboard log directory: results_selected_models/selected_nsl_lstm_deep_rep2/tf_logs_run_2019_12_26-23_48_20
2019-12-26 23:48:20,514 [INFO] Loading datsets from: ../Datasets/small_datasets/nsl_kdd_five_classes
2019-12-26 23:48:20,514 [INFO] Reading X, y files
2019-12-26 23:48:20,514 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_train.h5
2019-12-26 23:48:20,771 [INFO] Reading complete. time_to_read=0.26 seconds
2019-12-26 23:48:20,771 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_val.h5
2019-12-26 23:48:20,834 [INFO] Reading complete. time_to_read=0.06 seconds
2019-12-26 23:48:20,834 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_test.h5
2019-12-26 23:48:20,895 [INFO] Reading complete. time_to_read=0.06 seconds
2019-12-26 23:48:20,895 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_train.h5
2019-12-26 23:48:20,902 [INFO] Reading complete. time_to_read=0.01 seconds
2019-12-26 23:48:20,902 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_val.h5
2019-12-26 23:48:20,907 [INFO] Reading complete. time_to_read=0.00 seconds
2019-12-26 23:48:20,907 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_test.h5
2019-12-26 23:48:20,910 [INFO] Reading complete. time_to_read=0.00 seconds
2019-12-26 23:48:21,068 [INFO] Preparing flow sequences
2019-12-26 23:48:23,185 [INFO] Extracting flows complete. time_taken = 2.12 sec
2019-12-26 23:48:23,235 [INFO] Initializing model
2019-12-26 23:48:23,693 [INFO] _________________________________________________________________
2019-12-26 23:48:23,693 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-26 23:48:23,693 [INFO] =================================================================
2019-12-26 23:48:23,693 [INFO] lstm_3 (LSTM)                (None, 17, 64)            47872     
2019-12-26 23:48:23,693 [INFO] _________________________________________________________________
2019-12-26 23:48:23,693 [INFO] batch_normalization_3 (Batch (None, 17, 64)            256       
2019-12-26 23:48:23,693 [INFO] _________________________________________________________________
2019-12-26 23:48:23,693 [INFO] dropout_3 (Dropout)          (None, 17, 64)            0         
2019-12-26 23:48:23,693 [INFO] _________________________________________________________________
2019-12-26 23:48:23,693 [INFO] lstm_4 (LSTM)                (None, 17, 33)            12936     
2019-12-26 23:48:23,693 [INFO] _________________________________________________________________
2019-12-26 23:48:23,693 [INFO] batch_normalization_4 (Batch (None, 17, 33)            132       
2019-12-26 23:48:23,693 [INFO] _________________________________________________________________
2019-12-26 23:48:23,693 [INFO] dropout_4 (Dropout)          (None, 17, 33)            0         
2019-12-26 23:48:23,693 [INFO] _________________________________________________________________
2019-12-26 23:48:23,694 [INFO] time_distributed_2 (TimeDist (None, 17, 5)             170       
2019-12-26 23:48:23,694 [INFO] =================================================================
2019-12-26 23:48:23,694 [INFO] Total params: 61,366
2019-12-26 23:48:23,694 [INFO] Trainable params: 61,172
2019-12-26 23:48:23,694 [INFO] Non-trainable params: 194
2019-12-26 23:48:23,694 [INFO] _________________________________________________________________
2019-12-26 23:48:23,694 [INFO] Training model
 - val_f1: 0.9933
Epoch 00096: early stopping
Train on 5928 samples, validate on 1482 samples
Epoch 1/300
 - 2s - loss: 0.5211 - val_loss: 0.3840
 - val_f1: 0.4550
Epoch 2/300
 - 1s - loss: 0.2665 - val_loss: 0.1355
 - val_f1: 0.8999
Epoch 3/300
 - 1s - loss: 0.1175 - val_loss: 0.0671
 - val_f1: 0.9548
Epoch 4/300
 - 1s - loss: 0.0747 - val_loss: 0.0518
 - val_f1: 0.9644
Epoch 5/300
 - 1s - loss: 0.0598 - val_loss: 0.0439
 - val_f1: 0.9694
Epoch 6/300
 - 1s - loss: 0.0510 - val_loss: 0.0390
 - val_f1: 0.9702
Epoch 7/300
 - 1s - loss: 0.0446 - val_loss: 0.0347
 - val_f1: 0.9713
Epoch 8/300
 - 1s - loss: 0.0398 - val_loss: 0.0325
 - val_f1: 0.9712
Epoch 9/300
 - 1s - loss: 0.0350 - val_loss: 0.0294
 - val_f1: 0.9718
Epoch 10/300
 - 1s - loss: 0.0307 - val_loss: 0.0271
 - val_f1: 0.9730
Epoch 11/300
 - 1s - loss: 0.0270 - val_loss: 0.0251
 - val_f1: 0.9742
Epoch 12/300
 - 1s - loss: 0.0242 - val_loss: 0.0223
 - val_f1: 0.9748
Epoch 13/300
 - 1s - loss: 0.0219 - val_loss: 0.0223
 - val_f1: 0.9746
Epoch 14/300
 - 1s - loss: 0.0201 - val_loss: 0.0208
 - val_f1: 0.9760
Epoch 15/300
 - 1s - loss: 0.0190 - val_loss: 0.0200
 - val_f1: 0.9773
Epoch 16/300
 - 1s - loss: 0.0178 - val_loss: 0.0171
 - val_f1: 0.9797
Epoch 17/300
 - 1s - loss: 0.0169 - val_loss: 0.0156
 - val_f1: 0.9823
Epoch 18/300
 - 1s - loss: 0.0156 - val_loss: 0.0134
 - val_f1: 0.9865
Epoch 19/300
 - 1s - loss: 0.0149 - val_loss: 0.0130
 - val_f1: 0.9871
Epoch 20/300
 - 1s - loss: 0.0143 - val_loss: 0.0118
 - val_f1: 0.9911
Epoch 21/300
 - 1s - loss: 0.0134 - val_loss: 0.0116
 - val_f1: 0.9903
Epoch 22/300
 - 1s - loss: 0.0132 - val_loss: 0.0109
 - val_f1: 0.9911
Epoch 23/300
 - 1s - loss: 0.0129 - val_loss: 0.0106
 - val_f1: 0.9913
Epoch 24/300
 - 1s - loss: 0.0122 - val_loss: 0.0106
 - val_f1: 0.9916
Epoch 25/300
 - 1s - loss: 0.0120 - val_loss: 0.0106
 - val_f1: 0.9913
Epoch 26/300
 - 1s - loss: 0.0114 - val_loss: 0.0105
 - val_f1: 0.9915
Epoch 27/300
 - 1s - loss: 0.0113 - val_loss: 0.0104
 - val_f1: 0.9919
Epoch 28/300
 - 1s - loss: 0.0110 - val_loss: 0.0102
 - val_f1: 0.9915
Epoch 29/300
 - 1s - loss: 0.0105 - val_loss: 0.0103
 - val_f1: 0.9914
Epoch 30/300
 - 1s - loss: 0.0104 - val_loss: 0.0101
 - val_f1: 0.9922
Epoch 31/300
 - 1s - loss: 0.0105 - val_loss: 0.0103
/home/hasitha/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2019-12-26 23:48:58,080 [INFO] epoch = 30. Intermediate model saved to results_selected_models/selected_nsl_lstm_deep_rep2/_model_epoch_30.pickle
 - val_f1: 0.9921
Epoch 32/300
 - 1s - loss: 0.0100 - val_loss: 0.0104
 - val_f1: 0.9922
Epoch 33/300
 - 1s - loss: 0.0096 - val_loss: 0.0102
 - val_f1: 0.9924
Epoch 34/300
 - 1s - loss: 0.0091 - val_loss: 0.0101
 - val_f1: 0.9924
Epoch 35/300
 - 1s - loss: 0.0089 - val_loss: 0.0098
 - val_f1: 0.9927
Epoch 36/300
 - 1s - loss: 0.0087 - val_loss: 0.0100
 - val_f1: 0.9927
Epoch 37/300
 - 1s - loss: 0.0085 - val_loss: 0.0101
 - val_f1: 0.9925
Epoch 38/300
 - 1s - loss: 0.0082 - val_loss: 0.0100
 - val_f1: 0.9921
Epoch 39/300
 - 1s - loss: 0.0082 - val_loss: 0.0099
 - val_f1: 0.9926
Epoch 40/300
 - 1s - loss: 0.0081 - val_loss: 0.0099
 - val_f1: 0.9924
Epoch 41/300
 - 1s - loss: 0.0077 - val_loss: 0.0100
 - val_f1: 0.9923
Epoch 42/300
 - 1s - loss: 0.0074 - val_loss: 0.0101
 - val_f1: 0.9923
Epoch 43/300
 - 1s - loss: 0.0074 - val_loss: 0.0098
 - val_f1: 0.9928
Epoch 44/300
 - 1s - loss: 0.0075 - val_loss: 0.0099
 - val_f1: 0.9930
Epoch 45/300
 - 1s - loss: 0.0071 - val_loss: 0.0100
 - val_f1: 0.9930
Epoch 46/300
 - 1s - loss: 0.0073 - val_loss: 0.0103
 - val_f1: 0.9932
Epoch 47/300
 - 1s - loss: 0.0068 - val_loss: 0.0101
 - val_f1: 0.9931
Epoch 48/300
 - 1s - loss: 0.0065 - val_loss: 0.0101
 - val_f1: 0.9931
Epoch 49/300
 - 1s - loss: 0.0064 - val_loss: 0.0096
 - val_f1: 0.9936
Epoch 50/300
 - 1s - loss: 0.0065 - val_loss: 0.0101
 - val_f1: 0.9933
Epoch 51/300
 - 1s - loss: 0.0064 - val_loss: 0.0098
 - val_f1: 0.9932
Epoch 52/300
 - 1s - loss: 0.0062 - val_loss: 0.0099
 - val_f1: 0.9930
Epoch 53/300
 - 1s - loss: 0.0059 - val_loss: 0.0106
 - val_f1: 0.9925
Epoch 54/300
 - 1s - loss: 0.0059 - val_loss: 0.0104
 - val_f1: 0.9926
Epoch 55/300
 - 1s - loss: 0.0058 - val_loss: 0.0102
 - val_f1: 0.9928
Epoch 56/300
 - 1s - loss: 0.0057 - val_loss: 0.0103
 - val_f1: 0.9931
Epoch 57/300
 - 1s - loss: 0.0055 - val_loss: 0.0102
 - val_f1: 0.9929
Epoch 58/300
 - 1s - loss: 0.0053 - val_loss: 0.0103
 - val_f1: 0.9925
Epoch 59/300
 - 1s - loss: 0.0054 - val_loss: 0.0103
 - val_f1: 0.9926
Epoch 60/300
 - 1s - loss: 0.0051 - val_loss: 0.0099
 - val_f1: 0.9934
Epoch 61/300
 - 1s - loss: 0.0050 - val_loss: 0.0099
2019-12-26 23:49:26,998 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_nsl_lstm_deep_rep2/_model_epoch_60.pickle
 - val_f1: 0.9933
Epoch 62/300
 - 1s - loss: 0.0050 - val_loss: 0.0101
 - val_f1: 0.9932
Epoch 63/300
 - 1s - loss: 0.0047 - val_loss: 0.0101
 - val_f1: 0.9929
Epoch 64/300
 - 1s - loss: 0.0047 - val_loss: 0.0104
 - val_f1: 0.9928
Epoch 65/300
 - 1s - loss: 0.0048 - val_loss: 0.0109
 - val_f1: 0.9928
Epoch 66/300
 - 1s - loss: 0.0047 - val_loss: 0.0100
 - val_f1: 0.9930
Epoch 67/300
 - 1s - loss: 0.0044 - val_loss: 0.0103
 - val_f1: 0.9931
Epoch 68/300
 - 1s - loss: 0.0044 - val_loss: 0.0104
 - val_f1: 0.9929
Epoch 69/300
 - 1s - loss: 0.0045 - val_loss: 0.0105
 - val_f1: 0.9929
Epoch 70/300
 - 1s - loss: 0.0041 - val_loss: 0.0109
 - val_f1: 0.9929
Epoch 71/300
 - 1s - loss: 0.0040 - val_loss: 0.0107
 - val_f1: 0.9931
Epoch 72/300
 - 1s - loss: 0.0039 - val_loss: 0.0110
 - val_f1: 0.9931
Epoch 73/300
 - 1s - loss: 0.0039 - val_loss: 0.0110
 - val_f1: 0.9932
Epoch 74/300
 - 1s - loss: 0.0039 - val_loss: 0.0111
 - val_f1: 0.9929
Epoch 75/300
 - 1s - loss: 0.0035 - val_loss: 0.0108
 - val_f1: 0.9928
Epoch 76/300
 - 1s - loss: 0.0037 - val_loss: 0.0110
 - val_f1: 0.9929
Epoch 77/300
 - 1s - loss: 0.0035 - val_loss: 0.0113
 - val_f1: 0.9929
Epoch 78/300
 - 1s - loss: 0.0040 - val_loss: 0.0114
 - val_f1: 0.9929
Epoch 79/300
 - 1s - loss: 0.0039 - val_loss: 0.0114
 - val_f1: 0.9926
Epoch 80/300
 - 1s - loss: 0.0039 - val_loss: 0.0111
 - val_f1: 0.9929
Epoch 81/300
 - 1s - loss: 0.0036 - val_loss: 0.0113
 - val_f1: 0.9928
Epoch 82/300
 - 1s - loss: 0.0034 - val_loss: 0.0114
 - val_f1: 0.9928
Epoch 83/300
 - 1s - loss: 0.0034 - val_loss: 0.0117
 - val_f1: 0.9928
Epoch 84/300
 - 1s - loss: 0.0032 - val_loss: 0.0119
 - val_f1: 0.9930
Epoch 85/300
 - 1s - loss: 0.0033 - val_loss: 0.0120
 - val_f1: 0.9931
Epoch 86/300
 - 1s - loss: 0.0031 - val_loss: 0.0113
 - val_f1: 0.9930
Epoch 87/300
 - 1s - loss: 0.0030 - val_loss: 0.0118
 - val_f1: 0.9927
Epoch 88/300
 - 1s - loss: 0.0031 - val_loss: 0.0123
 - val_f1: 0.9928
Epoch 89/300
 - 1s - loss: 0.0033 - val_loss: 0.0119
 - val_f1: 0.9929
Epoch 90/300
 - 1s - loss: 0.0030 - val_loss: 0.0116
 - val_f1: 0.9929
Epoch 91/300
 - 1s - loss: 0.0030 - val_loss: 0.0116
2019-12-26 23:49:55,881 [INFO] epoch = 90. Intermediate model saved to results_selected_models/selected_nsl_lstm_deep_rep2/_model_epoch_90.pickle
 - val_f1: 0.9932
Epoch 92/300
 - 1s - loss: 0.0030 - val_loss: 0.0124
 - val_f1: 0.9929
Epoch 93/300
 - 1s - loss: 0.0031 - val_loss: 0.0121
 - val_f1: 0.9927
Epoch 94/300
 - 1s - loss: 0.0029 - val_loss: 0.0123
 - val_f1: 0.9930
Epoch 95/300
 - 1s - loss: 0.0030 - val_loss: 0.0115
 - val_f1: 0.9932
Epoch 96/300
 - 1s - loss: 0.0028 - val_loss: 0.0112
 - val_f1: 0.9932
Epoch 97/300
 - 1s - loss: 0.0027 - val_loss: 0.0120
 - val_f1: 0.9931
Epoch 98/300
 - 1s - loss: 0.0026 - val_loss: 0.0123
 - val_f1: 0.9928
Epoch 99/300
 - 1s - loss: 0.0027 - val_loss: 0.0124
2019-12-26 23:50:03,710 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-26 23:50:04,514 [INFO] Last epoch loss evaluation: train_loss = 0.003467, val_loss = 0.009647
2019-12-26 23:50:04,514 [INFO] Training complete. time_to_train = 100.82 sec, 1.68 min
2019-12-26 23:50:04,519 [INFO] Model saved to results_selected_models/selected_nsl_lstm_deep_rep2/best_model.pickle
2019-12-26 23:50:04,521 [INFO] Training history saved to: results_selected_models/selected_nsl_lstm_deep_rep2/training_error_history.csv
2019-12-26 23:50:04,646 [INFO] Plot saved to: results_selected_models/selected_nsl_lstm_deep_rep2/training_error_history.png
2019-12-26 23:50:04,762 [INFO] Plot saved to: results_selected_models/selected_nsl_lstm_deep_rep2/training_f1_history.png
2019-12-26 23:50:04,762 [INFO] Making predictions on training, validation, testing data
2019-12-26 23:50:05,451 [INFO] Evaluating predictions (results)
2019-12-26 23:50:05,749 [INFO] Dataset: Testing. Classification report below
2019-12-26 23:50:05,750 [INFO] 
              precision    recall  f1-score   support

         dos       0.94      0.84      0.89      7458
      normal       0.68      0.93      0.79      9710
       probe       0.77      0.75      0.76      2420
         r2l       0.94      0.13      0.23      2421
         u2r       0.78      0.01      0.03       533

   micro avg       0.77      0.77      0.77     22542
   macro avg       0.82      0.53      0.54     22542
weighted avg       0.81      0.77      0.74     22542

2019-12-26 23:50:05,750 [INFO] Overall accuracy (micro avg): 0.7726466152071688
2019-12-26 23:50:06,076 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.7726         0.7726                       0.7726                0.0568                   0.2274  0.7726
1     Macro avg        0.9091         0.8224                       0.5324                0.0758                   0.4676  0.5386
2  Weighted avg        0.8676         0.8072                       0.7726                0.1515                   0.2274  0.7398
2019-12-26 23:50:06,419 [INFO] Dataset: Validation. Classification report below
2019-12-26 23:50:06,419 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00      9186
      normal       0.99      0.99      0.99     13469
       probe       0.99      0.99      0.99      2330
         r2l       0.86      0.81      0.84       199
         u2r       1.00      0.40      0.57        10

   micro avg       0.99      0.99      0.99     25194
   macro avg       0.97      0.84      0.88     25194
weighted avg       0.99      0.99      0.99     25194

2019-12-26 23:50:06,419 [INFO] Overall accuracy (micro avg): 0.9936889735651345
2019-12-26 23:50:06,799 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9937         0.9937                       0.9937                0.0016                   0.0063  0.9937
1     Macro avg        0.9975         0.9678                       0.8391                0.0021                   0.1609  0.8776
2  Weighted avg        0.9960         0.9936                       0.9937                0.0040                   0.0063  0.9936
2019-12-26 23:50:08,301 [INFO] Dataset: Training. Classification report below
2019-12-26 23:50:08,301 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00     36740
      normal       1.00      1.00      1.00     53873
       probe       1.00      0.99      1.00      9325
         r2l       0.96      0.91      0.93       796
         u2r       0.87      0.48      0.62        42

   micro avg       1.00      1.00      1.00    100776
   macro avg       0.97      0.88      0.91    100776
weighted avg       1.00      1.00      1.00    100776

2019-12-26 23:50:08,301 [INFO] Overall accuracy (micro avg): 0.9975887115979996
2019-12-26 23:50:10,000 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9976         0.9976                       0.9976                0.0006                   0.0024  0.9976
1     Macro avg        0.9990         0.9651                       0.8753                0.0009                   0.1247  0.9085
2  Weighted avg        0.9985         0.9975                       0.9976                0.0019                   0.0024  0.9975
2019-12-26 23:50:10,021 [INFO] Results saved to: results_selected_models/selected_nsl_lstm_deep_rep2/selected_nsl_lstm_deep_rep2_results.xlsx
2019-12-26 23:50:10,021 [INFO] ================= Finished running experiment no. 2 ================= 

2019-12-26 23:50:10,025 [INFO] Created directory: results_selected_models/selected_nsl_lstm_deep_rep3
2019-12-26 23:50:10,025 [INFO] Initialized logging. log_filename = results_selected_models/selected_nsl_lstm_deep_rep3/run_log.log
2019-12-26 23:50:10,025 [INFO] ================= Running experiment no. 3  ================= 

2019-12-26 23:50:10,025 [INFO] Experiment parameters given below
2019-12-26 23:50:10,025 [INFO] 
{'experiment_num': 3, 'results_dir': 'results_selected_models/selected_nsl_lstm_deep_rep3', 'model': 'lstm', 'model_type': 'classifier', 'normal_label': 'normal', 'lstm_time_steps': 18, 'lstm_layer_units': [64, 34], 'lstm_layer_activations': ['relu', 'relu'], 'lstm_layer_dropout_rates': [0.2, 0.4], 'batch_size': 256, 'epochs': 300, 'early_stop_patience': 50, 'dataset_dir': '../Datasets/small_datasets/nsl_kdd_five_classes', 'description': 'selected_nsl_lstm_deep_rep3'}
2019-12-26 23:50:10,025 [INFO] Created tensorboard log directory: results_selected_models/selected_nsl_lstm_deep_rep3/tf_logs_run_2019_12_26-23_50_10
2019-12-26 23:50:10,025 [INFO] Loading datsets from: ../Datasets/small_datasets/nsl_kdd_five_classes
2019-12-26 23:50:10,025 [INFO] Reading X, y files
2019-12-26 23:50:10,026 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_train.h5
2019-12-26 23:50:10,282 [INFO] Reading complete. time_to_read=0.26 seconds
2019-12-26 23:50:10,282 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_val.h5
2019-12-26 23:50:10,347 [INFO] Reading complete. time_to_read=0.06 seconds
2019-12-26 23:50:10,347 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_test.h5
2019-12-26 23:50:10,409 [INFO] Reading complete. time_to_read=0.06 seconds
2019-12-26 23:50:10,409 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_train.h5
2019-12-26 23:50:10,417 [INFO] Reading complete. time_to_read=0.01 seconds
2019-12-26 23:50:10,417 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_val.h5
2019-12-26 23:50:10,421 [INFO] Reading complete. time_to_read=0.00 seconds
2019-12-26 23:50:10,421 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_test.h5
2019-12-26 23:50:10,425 [INFO] Reading complete. time_to_read=0.00 seconds
2019-12-26 23:50:10,580 [INFO] Preparing flow sequences
2019-12-26 23:50:12,583 [INFO] Extracting flows complete. time_taken = 2.00 sec
2019-12-26 23:50:12,634 [INFO] Initializing model
2019-12-26 23:50:13,075 [INFO] _________________________________________________________________
2019-12-26 23:50:13,076 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-26 23:50:13,076 [INFO] =================================================================
2019-12-26 23:50:13,076 [INFO] lstm_5 (LSTM)                (None, 18, 64)            47872     
2019-12-26 23:50:13,076 [INFO] _________________________________________________________________
2019-12-26 23:50:13,076 [INFO] batch_normalization_5 (Batch (None, 18, 64)            256       
2019-12-26 23:50:13,076 [INFO] _________________________________________________________________
2019-12-26 23:50:13,076 [INFO] dropout_5 (Dropout)          (None, 18, 64)            0         
2019-12-26 23:50:13,076 [INFO] _________________________________________________________________
2019-12-26 23:50:13,076 [INFO] lstm_6 (LSTM)                (None, 18, 34)            13464     
2019-12-26 23:50:13,076 [INFO] _________________________________________________________________
2019-12-26 23:50:13,076 [INFO] batch_normalization_6 (Batch (None, 18, 34)            136       
2019-12-26 23:50:13,076 [INFO] _________________________________________________________________
2019-12-26 23:50:13,076 [INFO] dropout_6 (Dropout)          (None, 18, 34)            0         
2019-12-26 23:50:13,076 [INFO] _________________________________________________________________
2019-12-26 23:50:13,076 [INFO] time_distributed_3 (TimeDist (None, 18, 5)             175       
2019-12-26 23:50:13,076 [INFO] =================================================================
2019-12-26 23:50:13,077 [INFO] Total params: 61,903
2019-12-26 23:50:13,077 [INFO] Trainable params: 61,707
2019-12-26 23:50:13,077 [INFO] Non-trainable params: 196
2019-12-26 23:50:13,077 [INFO] _________________________________________________________________
2019-12-26 23:50:13,077 [INFO] Training model
 - val_f1: 0.9930
Epoch 00099: early stopping
Train on 5598 samples, validate on 1399 samples
Epoch 1/300
 - 2s - loss: 0.5575 - val_loss: 0.3818
 - val_f1: 0.4249
Epoch 2/300
 - 1s - loss: 0.3579 - val_loss: 0.1799
 - val_f1: 0.8628
Epoch 3/300
 - 1s - loss: 0.1654 - val_loss: 0.0852
 - val_f1: 0.9472
Epoch 4/300
 - 1s - loss: 0.0958 - val_loss: 0.0674
 - val_f1: 0.9585
Epoch 5/300
 - 1s - loss: 0.0731 - val_loss: 0.0553
 - val_f1: 0.9643
Epoch 6/300
 - 1s - loss: 0.0625 - val_loss: 0.0482
 - val_f1: 0.9677
Epoch 7/300
 - 1s - loss: 0.0555 - val_loss: 0.0433
 - val_f1: 0.9697
Epoch 8/300
 - 1s - loss: 0.0503 - val_loss: 0.0389
 - val_f1: 0.9706
Epoch 9/300
 - 1s - loss: 0.0456 - val_loss: 0.0351
 - val_f1: 0.9716
Epoch 10/300
 - 1s - loss: 0.0406 - val_loss: 0.0317
 - val_f1: 0.9721
Epoch 11/300
 - 1s - loss: 0.0361 - val_loss: 0.0290
 - val_f1: 0.9727
Epoch 12/300
 - 1s - loss: 0.0317 - val_loss: 0.0262
 - val_f1: 0.9742
Epoch 13/300
 - 1s - loss: 0.0286 - val_loss: 0.0245
 - val_f1: 0.9753
Epoch 14/300
 - 1s - loss: 0.0261 - val_loss: 0.0222
 - val_f1: 0.9757
Epoch 15/300
 - 1s - loss: 0.0244 - val_loss: 0.0206
 - val_f1: 0.9777
Epoch 16/300
 - 1s - loss: 0.0232 - val_loss: 0.0183
 - val_f1: 0.9808
Epoch 17/300
 - 1s - loss: 0.0221 - val_loss: 0.0173
 - val_f1: 0.9825
Epoch 18/300
 - 1s - loss: 0.0206 - val_loss: 0.0158
 - val_f1: 0.9851
Epoch 19/300
 - 1s - loss: 0.0198 - val_loss: 0.0150
 - val_f1: 0.9869
Epoch 20/300
 - 1s - loss: 0.0186 - val_loss: 0.0141
 - val_f1: 0.9883
Epoch 21/300
 - 1s - loss: 0.0181 - val_loss: 0.0132
 - val_f1: 0.9895
Epoch 22/300
 - 1s - loss: 0.0171 - val_loss: 0.0127
 - val_f1: 0.9901
Epoch 23/300
 - 1s - loss: 0.0158 - val_loss: 0.0121
 - val_f1: 0.9910
Epoch 24/300
 - 1s - loss: 0.0158 - val_loss: 0.0117
 - val_f1: 0.9908
Epoch 25/300
 - 1s - loss: 0.0152 - val_loss: 0.0114
 - val_f1: 0.9905
Epoch 26/300
 - 1s - loss: 0.0146 - val_loss: 0.0114
 - val_f1: 0.9903
Epoch 27/300
 - 1s - loss: 0.0141 - val_loss: 0.0112
 - val_f1: 0.9906
Epoch 28/300
 - 1s - loss: 0.0142 - val_loss: 0.0108
 - val_f1: 0.9912
Epoch 29/300
 - 1s - loss: 0.0136 - val_loss: 0.0108
 - val_f1: 0.9913
Epoch 30/300
 - 1s - loss: 0.0127 - val_loss: 0.0105
 - val_f1: 0.9915
Epoch 31/300
 - 1s - loss: 0.0124 - val_loss: 0.0106
/home/hasitha/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2019-12-26 23:50:48,004 [INFO] epoch = 30. Intermediate model saved to results_selected_models/selected_nsl_lstm_deep_rep3/_model_epoch_30.pickle
 - val_f1: 0.9917
Epoch 32/300
 - 1s - loss: 0.0122 - val_loss: 0.0101
 - val_f1: 0.9919
Epoch 33/300
 - 1s - loss: 0.0121 - val_loss: 0.0103
 - val_f1: 0.9918
Epoch 34/300
 - 1s - loss: 0.0113 - val_loss: 0.0101
 - val_f1: 0.9918
Epoch 35/300
 - 1s - loss: 0.0111 - val_loss: 0.0099
 - val_f1: 0.9921
Epoch 36/300
 - 1s - loss: 0.0109 - val_loss: 0.0099
 - val_f1: 0.9922
Epoch 37/300
 - 1s - loss: 0.0105 - val_loss: 0.0096
 - val_f1: 0.9923
Epoch 38/300
 - 1s - loss: 0.0104 - val_loss: 0.0098
 - val_f1: 0.9925
Epoch 39/300
 - 1s - loss: 0.0103 - val_loss: 0.0098
 - val_f1: 0.9926
Epoch 40/300
 - 1s - loss: 0.0097 - val_loss: 0.0093
 - val_f1: 0.9924
Epoch 41/300
 - 1s - loss: 0.0096 - val_loss: 0.0097
 - val_f1: 0.9927
Epoch 42/300
 - 1s - loss: 0.0095 - val_loss: 0.0098
 - val_f1: 0.9924
Epoch 43/300
 - 1s - loss: 0.0093 - val_loss: 0.0094
 - val_f1: 0.9925
Epoch 44/300
 - 1s - loss: 0.0089 - val_loss: 0.0094
 - val_f1: 0.9925
Epoch 45/300
 - 1s - loss: 0.0088 - val_loss: 0.0098
 - val_f1: 0.9926
Epoch 46/300
 - 1s - loss: 0.0086 - val_loss: 0.0096
 - val_f1: 0.9928
Epoch 47/300
 - 1s - loss: 0.0087 - val_loss: 0.0097
 - val_f1: 0.9925
Epoch 48/300
 - 1s - loss: 0.0084 - val_loss: 0.0093
 - val_f1: 0.9930
Epoch 49/300
 - 1s - loss: 0.0081 - val_loss: 0.0096
 - val_f1: 0.9927
Epoch 50/300
 - 1s - loss: 0.0082 - val_loss: 0.0095
 - val_f1: 0.9929
Epoch 51/300
 - 1s - loss: 0.0078 - val_loss: 0.0096
 - val_f1: 0.9929
Epoch 52/300
 - 1s - loss: 0.0078 - val_loss: 0.0096
 - val_f1: 0.9929
Epoch 53/300
 - 1s - loss: 0.0073 - val_loss: 0.0094
 - val_f1: 0.9928
Epoch 54/300
 - 1s - loss: 0.0072 - val_loss: 0.0096
 - val_f1: 0.9927
Epoch 55/300
 - 1s - loss: 0.0071 - val_loss: 0.0098
 - val_f1: 0.9929
Epoch 56/300
 - 1s - loss: 0.0069 - val_loss: 0.0097
 - val_f1: 0.9928
Epoch 57/300
 - 1s - loss: 0.0069 - val_loss: 0.0097
 - val_f1: 0.9925
Epoch 58/300
 - 1s - loss: 0.0068 - val_loss: 0.0099
 - val_f1: 0.9924
Epoch 59/300
 - 1s - loss: 0.0068 - val_loss: 0.0094
 - val_f1: 0.9929
Epoch 60/300
 - 1s - loss: 0.0066 - val_loss: 0.0094
 - val_f1: 0.9929
Epoch 61/300
 - 1s - loss: 0.0064 - val_loss: 0.0096
2019-12-26 23:51:16,591 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_nsl_lstm_deep_rep3/_model_epoch_60.pickle
 - val_f1: 0.9930
Epoch 62/300
 - 1s - loss: 0.0065 - val_loss: 0.0096
 - val_f1: 0.9927
Epoch 63/300
 - 1s - loss: 0.0061 - val_loss: 0.0097
 - val_f1: 0.9929
Epoch 64/300
 - 1s - loss: 0.0062 - val_loss: 0.0104
 - val_f1: 0.9927
Epoch 65/300
 - 1s - loss: 0.0057 - val_loss: 0.0095
 - val_f1: 0.9931
Epoch 66/300
 - 1s - loss: 0.0058 - val_loss: 0.0098
 - val_f1: 0.9928
Epoch 67/300
 - 1s - loss: 0.0057 - val_loss: 0.0102
 - val_f1: 0.9926
Epoch 68/300
 - 1s - loss: 0.0057 - val_loss: 0.0103
 - val_f1: 0.9928
Epoch 69/300
 - 1s - loss: 0.0055 - val_loss: 0.0099
 - val_f1: 0.9929
Epoch 70/300
 - 1s - loss: 0.0053 - val_loss: 0.0100
 - val_f1: 0.9929
Epoch 71/300
 - 1s - loss: 0.0051 - val_loss: 0.0104
 - val_f1: 0.9929
Epoch 72/300
 - 1s - loss: 0.0053 - val_loss: 0.0100
 - val_f1: 0.9929
Epoch 73/300
 - 1s - loss: 0.0052 - val_loss: 0.0102
 - val_f1: 0.9930
Epoch 74/300
 - 1s - loss: 0.0049 - val_loss: 0.0104
 - val_f1: 0.9933
Epoch 75/300
 - 1s - loss: 0.0049 - val_loss: 0.0101
 - val_f1: 0.9931
Epoch 76/300
 - 1s - loss: 0.0047 - val_loss: 0.0106
 - val_f1: 0.9930
Epoch 77/300
 - 1s - loss: 0.0051 - val_loss: 0.0107
 - val_f1: 0.9928
Epoch 78/300
 - 1s - loss: 0.0050 - val_loss: 0.0103
 - val_f1: 0.9930
Epoch 79/300
 - 1s - loss: 0.0050 - val_loss: 0.0106
 - val_f1: 0.9931
Epoch 80/300
 - 1s - loss: 0.0046 - val_loss: 0.0110
 - val_f1: 0.9930
Epoch 81/300
 - 1s - loss: 0.0043 - val_loss: 0.0109
 - val_f1: 0.9930
Epoch 82/300
 - 1s - loss: 0.0044 - val_loss: 0.0110
 - val_f1: 0.9932
Epoch 83/300
 - 1s - loss: 0.0043 - val_loss: 0.0110
 - val_f1: 0.9931
Epoch 84/300
 - 1s - loss: 0.0040 - val_loss: 0.0109
 - val_f1: 0.9933
Epoch 85/300
 - 1s - loss: 0.0041 - val_loss: 0.0106
 - val_f1: 0.9931
Epoch 86/300
 - 1s - loss: 0.0042 - val_loss: 0.0110
 - val_f1: 0.9932
Epoch 87/300
 - 1s - loss: 0.0040 - val_loss: 0.0111
 - val_f1: 0.9928
Epoch 88/300
 - 1s - loss: 0.0038 - val_loss: 0.0114
 - val_f1: 0.9930
Epoch 89/300
 - 1s - loss: 0.0038 - val_loss: 0.0111
 - val_f1: 0.9932
Epoch 90/300
 - 1s - loss: 0.0042 - val_loss: 0.0112
2019-12-26 23:51:44,369 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-26 23:51:45,281 [INFO] Last epoch loss evaluation: train_loss = 0.005750, val_loss = 0.009254
2019-12-26 23:51:45,282 [INFO] Training complete. time_to_train = 92.20 sec, 1.54 min
2019-12-26 23:51:45,287 [INFO] Model saved to results_selected_models/selected_nsl_lstm_deep_rep3/best_model.pickle
2019-12-26 23:51:45,289 [INFO] Training history saved to: results_selected_models/selected_nsl_lstm_deep_rep3/training_error_history.csv
2019-12-26 23:51:45,411 [INFO] Plot saved to: results_selected_models/selected_nsl_lstm_deep_rep3/training_error_history.png
2019-12-26 23:51:45,526 [INFO] Plot saved to: results_selected_models/selected_nsl_lstm_deep_rep3/training_f1_history.png
2019-12-26 23:51:45,526 [INFO] Making predictions on training, validation, testing data
2019-12-26 23:51:46,213 [INFO] Evaluating predictions (results)
2019-12-26 23:51:46,525 [INFO] Dataset: Testing. Classification report below
2019-12-26 23:51:46,525 [INFO] 
              precision    recall  f1-score   support

         dos       0.95      0.86      0.90      7454
      normal       0.69      0.97      0.81      9708
       probe       0.85      0.70      0.77      2420
         r2l       0.93      0.10      0.18      2421
         u2r       1.00      0.01      0.01       533

   micro avg       0.79      0.79      0.79     22536
   macro avg       0.88      0.53      0.53     22536
weighted avg       0.83      0.79      0.75     22536

2019-12-26 23:51:46,525 [INFO] Overall accuracy (micro avg): 0.7855875044373447
2019-12-26 23:51:46,850 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.7856         0.7856                       0.7856                0.0536                   0.2144  0.7856
1     Macro avg        0.9142         0.8841                       0.5260                0.0728                   0.4740  0.5343
2  Weighted avg        0.8773         0.8270                       0.7856                0.1494                   0.2144  0.7476
/home/hasitha/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2019-12-26 23:51:47,194 [INFO] Dataset: Validation. Classification report below
2019-12-26 23:51:47,194 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00      9183
      normal       0.99      0.99      0.99     13460
       probe       0.98      0.98      0.98      2330
         r2l       0.86      0.81      0.83       199
         u2r       0.00      0.00      0.00        10

   micro avg       0.99      0.99      0.99     25182
   macro avg       0.77      0.76      0.76     25182
weighted avg       0.99      0.99      0.99     25182

2019-12-26 23:51:47,194 [INFO] Overall accuracy (micro avg): 0.9925343499324915
/home/hasitha/sunanda/ids_experiments/utility.py:328: RuntimeWarning: invalid value encountered in true_divide
  PPV = TP / (TP + FP)    # Precision or positive predictive value
/home/hasitha/sunanda/ids_experiments/utility.py:332: RuntimeWarning: invalid value encountered in true_divide
  FDR = FP / (TP + FP)    # False discovery rate
2019-12-26 23:51:47,590 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9925         0.9925                       0.9925                0.0019                   0.0075  0.9925
1     Macro avg        0.9970         0.7657                       0.7566                0.0025                   0.2434  0.7610
2  Weighted avg        0.9954         0.9921                       0.9925                0.0049                   0.0075  0.9923
2019-12-26 23:51:49,102 [INFO] Dataset: Training. Classification report below
2019-12-26 23:51:49,102 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00     36734
      normal       1.00      1.00      1.00     53871
       probe       0.99      0.99      0.99      9321
         r2l       0.92      0.87      0.89       796
         u2r       0.94      0.36      0.52        42

   micro avg       1.00      1.00      1.00    100764
   macro avg       0.97      0.84      0.88    100764
weighted avg       1.00      1.00      1.00    100764

2019-12-26 23:51:49,102 [INFO] Overall accuracy (micro avg): 0.9957226787344686
2019-12-26 23:51:50,807 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9957         0.9957                       0.9957                0.0011                   0.0043  0.9957
1     Macro avg        0.9983         0.9675                       0.8424                0.0015                   0.1576  0.8788
2  Weighted avg        0.9974         0.9957                       0.9957                0.0030                   0.0043  0.9956
2019-12-26 23:51:50,828 [INFO] Results saved to: results_selected_models/selected_nsl_lstm_deep_rep3/selected_nsl_lstm_deep_rep3_results.xlsx
2019-12-26 23:51:50,828 [INFO] ================= Finished running experiment no. 3 ================= 

2019-12-26 23:51:50,832 [INFO] Created directory: results_selected_models/selected_nsl_lstm_deep_rep4
2019-12-26 23:51:50,832 [INFO] Initialized logging. log_filename = results_selected_models/selected_nsl_lstm_deep_rep4/run_log.log
2019-12-26 23:51:50,832 [INFO] ================= Running experiment no. 4  ================= 

2019-12-26 23:51:50,832 [INFO] Experiment parameters given below
2019-12-26 23:51:50,832 [INFO] 
{'experiment_num': 4, 'results_dir': 'results_selected_models/selected_nsl_lstm_deep_rep4', 'model': 'lstm', 'model_type': 'classifier', 'normal_label': 'normal', 'lstm_time_steps': 19, 'lstm_layer_units': [64, 35], 'lstm_layer_activations': ['relu', 'relu'], 'lstm_layer_dropout_rates': [0.2, 0.5], 'batch_size': 256, 'epochs': 300, 'early_stop_patience': 50, 'dataset_dir': '../Datasets/small_datasets/nsl_kdd_five_classes', 'description': 'selected_nsl_lstm_deep_rep4'}
2019-12-26 23:51:50,832 [INFO] Created tensorboard log directory: results_selected_models/selected_nsl_lstm_deep_rep4/tf_logs_run_2019_12_26-23_51_50
2019-12-26 23:51:50,833 [INFO] Loading datsets from: ../Datasets/small_datasets/nsl_kdd_five_classes
2019-12-26 23:51:50,833 [INFO] Reading X, y files
2019-12-26 23:51:50,833 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_train.h5
2019-12-26 23:51:51,095 [INFO] Reading complete. time_to_read=0.26 seconds
2019-12-26 23:51:51,095 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_val.h5
2019-12-26 23:51:51,162 [INFO] Reading complete. time_to_read=0.07 seconds
2019-12-26 23:51:51,163 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_test.h5
2019-12-26 23:51:51,225 [INFO] Reading complete. time_to_read=0.06 seconds
2019-12-26 23:51:51,226 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_train.h5
2019-12-26 23:51:51,233 [INFO] Reading complete. time_to_read=0.01 seconds
2019-12-26 23:51:51,233 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_val.h5
2019-12-26 23:51:51,237 [INFO] Reading complete. time_to_read=0.00 seconds
2019-12-26 23:51:51,238 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_test.h5
2019-12-26 23:51:51,241 [INFO] Reading complete. time_to_read=0.00 seconds
2019-12-26 23:51:51,398 [INFO] Preparing flow sequences
2019-12-26 23:51:53,311 [INFO] Extracting flows complete. time_taken = 1.91 sec
2019-12-26 23:51:53,362 [INFO] Initializing model
2019-12-26 23:51:53,808 [INFO] _________________________________________________________________
2019-12-26 23:51:53,808 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-26 23:51:53,808 [INFO] =================================================================
2019-12-26 23:51:53,808 [INFO] lstm_7 (LSTM)                (None, 19, 64)            47872     
2019-12-26 23:51:53,808 [INFO] _________________________________________________________________
2019-12-26 23:51:53,808 [INFO] batch_normalization_7 (Batch (None, 19, 64)            256       
2019-12-26 23:51:53,809 [INFO] _________________________________________________________________
2019-12-26 23:51:53,809 [INFO] dropout_7 (Dropout)          (None, 19, 64)            0         
2019-12-26 23:51:53,809 [INFO] _________________________________________________________________
2019-12-26 23:51:53,809 [INFO] lstm_8 (LSTM)                (None, 19, 35)            14000     
2019-12-26 23:51:53,809 [INFO] _________________________________________________________________
2019-12-26 23:51:53,809 [INFO] batch_normalization_8 (Batch (None, 19, 35)            140       
2019-12-26 23:51:53,809 [INFO] _________________________________________________________________
2019-12-26 23:51:53,809 [INFO] dropout_8 (Dropout)          (None, 19, 35)            0         
2019-12-26 23:51:53,809 [INFO] _________________________________________________________________
2019-12-26 23:51:53,809 [INFO] time_distributed_4 (TimeDist (None, 19, 5)             180       
2019-12-26 23:51:53,809 [INFO] =================================================================
2019-12-26 23:51:53,809 [INFO] Total params: 62,448
2019-12-26 23:51:53,809 [INFO] Trainable params: 62,250
2019-12-26 23:51:53,809 [INFO] Non-trainable params: 198
2019-12-26 23:51:53,809 [INFO] _________________________________________________________________
2019-12-26 23:51:53,809 [INFO] Training model
 - val_f1: 0.9932
Epoch 00090: early stopping
Train on 5304 samples, validate on 1326 samples
Epoch 1/300
 - 2s - loss: 0.5711 - val_loss: 0.4168
 - val_f1: 0.2511
Epoch 2/300
 - 1s - loss: 0.3966 - val_loss: 0.2562
 - val_f1: 0.7139
Epoch 3/300
 - 1s - loss: 0.2378 - val_loss: 0.1353
 - val_f1: 0.8745
Epoch 4/300
 - 1s - loss: 0.1570 - val_loss: 0.1028
 - val_f1: 0.9239
Epoch 5/300
 - 1s - loss: 0.1204 - val_loss: 0.0806
 - val_f1: 0.9494
Epoch 6/300
 - 1s - loss: 0.0973 - val_loss: 0.0666
 - val_f1: 0.9587
Epoch 7/300
 - 1s - loss: 0.0813 - val_loss: 0.0569
 - val_f1: 0.9640
Epoch 8/300
 - 1s - loss: 0.0706 - val_loss: 0.0507
 - val_f1: 0.9663
Epoch 9/300
 - 1s - loss: 0.0632 - val_loss: 0.0461
 - val_f1: 0.9677
Epoch 10/300
 - 1s - loss: 0.0572 - val_loss: 0.0417
 - val_f1: 0.9687
Epoch 11/300
 - 1s - loss: 0.0523 - val_loss: 0.0387
 - val_f1: 0.9692
Epoch 12/300
 - 1s - loss: 0.0471 - val_loss: 0.0351
 - val_f1: 0.9702
Epoch 13/300
 - 1s - loss: 0.0436 - val_loss: 0.0318
 - val_f1: 0.9707
Epoch 14/300
 - 1s - loss: 0.0392 - val_loss: 0.0289
 - val_f1: 0.9727
Epoch 15/300
 - 1s - loss: 0.0359 - val_loss: 0.0257
 - val_f1: 0.9745
Epoch 16/300
 - 1s - loss: 0.0335 - val_loss: 0.0230
 - val_f1: 0.9774
Epoch 17/300
 - 1s - loss: 0.0308 - val_loss: 0.0212
 - val_f1: 0.9789
Epoch 18/300
 - 1s - loss: 0.0285 - val_loss: 0.0193
 - val_f1: 0.9805
Epoch 19/300
 - 1s - loss: 0.0265 - val_loss: 0.0179
 - val_f1: 0.9824
Epoch 20/300
 - 1s - loss: 0.0250 - val_loss: 0.0166
 - val_f1: 0.9834
Epoch 21/300
 - 1s - loss: 0.0234 - val_loss: 0.0160
 - val_f1: 0.9849
Epoch 22/300
 - 1s - loss: 0.0221 - val_loss: 0.0149
 - val_f1: 0.9869
Epoch 23/300
 - 1s - loss: 0.0212 - val_loss: 0.0144
 - val_f1: 0.9864
Epoch 24/300
 - 1s - loss: 0.0202 - val_loss: 0.0137
 - val_f1: 0.9877
Epoch 25/300
 - 1s - loss: 0.0193 - val_loss: 0.0134
 - val_f1: 0.9883
Epoch 26/300
 - 1s - loss: 0.0185 - val_loss: 0.0133
 - val_f1: 0.9871
Epoch 27/300
 - 1s - loss: 0.0175 - val_loss: 0.0125
 - val_f1: 0.9888
Epoch 28/300
 - 1s - loss: 0.0173 - val_loss: 0.0124
 - val_f1: 0.9887
Epoch 29/300
 - 1s - loss: 0.0165 - val_loss: 0.0118
 - val_f1: 0.9909
Epoch 30/300
 - 1s - loss: 0.0158 - val_loss: 0.0117
 - val_f1: 0.9902
Epoch 31/300
 - 1s - loss: 0.0154 - val_loss: 0.0116
/home/hasitha/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2019-12-26 23:52:29,816 [INFO] epoch = 30. Intermediate model saved to results_selected_models/selected_nsl_lstm_deep_rep4/_model_epoch_30.pickle
 - val_f1: 0.9901
Epoch 32/300
 - 1s - loss: 0.0147 - val_loss: 0.0115
 - val_f1: 0.9911
Epoch 33/300
 - 1s - loss: 0.0143 - val_loss: 0.0112
 - val_f1: 0.9911
Epoch 34/300
 - 1s - loss: 0.0141 - val_loss: 0.0110
 - val_f1: 0.9912
Epoch 35/300
 - 1s - loss: 0.0140 - val_loss: 0.0111
 - val_f1: 0.9911
Epoch 36/300
 - 1s - loss: 0.0134 - val_loss: 0.0108
 - val_f1: 0.9917
Epoch 37/300
 - 1s - loss: 0.0130 - val_loss: 0.0108
 - val_f1: 0.9913
Epoch 38/300
 - 1s - loss: 0.0122 - val_loss: 0.0106
 - val_f1: 0.9918
Epoch 39/300
 - 1s - loss: 0.0125 - val_loss: 0.0108
 - val_f1: 0.9909
Epoch 40/300
 - 1s - loss: 0.0122 - val_loss: 0.0105
 - val_f1: 0.9916
Epoch 41/300
 - 1s - loss: 0.0117 - val_loss: 0.0104
 - val_f1: 0.9920
Epoch 42/300
 - 1s - loss: 0.0115 - val_loss: 0.0103
 - val_f1: 0.9922
Epoch 43/300
 - 1s - loss: 0.0116 - val_loss: 0.0102
 - val_f1: 0.9920
Epoch 44/300
 - 1s - loss: 0.0111 - val_loss: 0.0102
 - val_f1: 0.9921
Epoch 45/300
 - 1s - loss: 0.0111 - val_loss: 0.0102
 - val_f1: 0.9923
Epoch 46/300
 - 1s - loss: 0.0105 - val_loss: 0.0100
 - val_f1: 0.9924
Epoch 47/300
 - 1s - loss: 0.0102 - val_loss: 0.0099
 - val_f1: 0.9925
Epoch 48/300
 - 1s - loss: 0.0099 - val_loss: 0.0100
 - val_f1: 0.9922
Epoch 49/300
 - 1s - loss: 0.0097 - val_loss: 0.0099
 - val_f1: 0.9922
Epoch 50/300
 - 1s - loss: 0.0098 - val_loss: 0.0100
 - val_f1: 0.9925
Epoch 51/300
 - 1s - loss: 0.0095 - val_loss: 0.0098
 - val_f1: 0.9923
Epoch 52/300
 - 1s - loss: 0.0091 - val_loss: 0.0098
 - val_f1: 0.9927
Epoch 53/300
 - 1s - loss: 0.0091 - val_loss: 0.0097
 - val_f1: 0.9927
Epoch 54/300
 - 1s - loss: 0.0090 - val_loss: 0.0097
 - val_f1: 0.9929
Epoch 55/300
 - 1s - loss: 0.0089 - val_loss: 0.0102
 - val_f1: 0.9925
Epoch 56/300
 - 1s - loss: 0.0090 - val_loss: 0.0099
 - val_f1: 0.9923
Epoch 57/300
 - 1s - loss: 0.0085 - val_loss: 0.0098
 - val_f1: 0.9927
Epoch 58/300
 - 1s - loss: 0.0084 - val_loss: 0.0100
 - val_f1: 0.9922
Epoch 59/300
 - 1s - loss: 0.0080 - val_loss: 0.0098
 - val_f1: 0.9921
Epoch 60/300
 - 1s - loss: 0.0080 - val_loss: 0.0097
 - val_f1: 0.9924
Epoch 61/300
 - 1s - loss: 0.0081 - val_loss: 0.0101
2019-12-26 23:52:58,802 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_nsl_lstm_deep_rep4/_model_epoch_60.pickle
 - val_f1: 0.9921
Epoch 62/300
 - 1s - loss: 0.0075 - val_loss: 0.0097
 - val_f1: 0.9927
Epoch 63/300
 - 1s - loss: 0.0076 - val_loss: 0.0097
 - val_f1: 0.9926
Epoch 64/300
 - 1s - loss: 0.0077 - val_loss: 0.0098
 - val_f1: 0.9928
Epoch 65/300
 - 1s - loss: 0.0076 - val_loss: 0.0099
 - val_f1: 0.9926
Epoch 66/300
 - 1s - loss: 0.0073 - val_loss: 0.0098
 - val_f1: 0.9928
Epoch 67/300
 - 1s - loss: 0.0070 - val_loss: 0.0099
 - val_f1: 0.9928
Epoch 68/300
 - 1s - loss: 0.0068 - val_loss: 0.0102
 - val_f1: 0.9926
Epoch 69/300
 - 1s - loss: 0.0073 - val_loss: 0.0097
 - val_f1: 0.9929
Epoch 70/300
 - 1s - loss: 0.0069 - val_loss: 0.0102
 - val_f1: 0.9926
Epoch 71/300
 - 1s - loss: 0.0068 - val_loss: 0.0097
 - val_f1: 0.9930
Epoch 72/300
 - 1s - loss: 0.0064 - val_loss: 0.0099
 - val_f1: 0.9931
Epoch 73/300
 - 1s - loss: 0.0066 - val_loss: 0.0099
 - val_f1: 0.9929
Epoch 74/300
 - 1s - loss: 0.0064 - val_loss: 0.0101
 - val_f1: 0.9929
Epoch 75/300
 - 1s - loss: 0.0063 - val_loss: 0.0102
 - val_f1: 0.9933
Epoch 76/300
 - 1s - loss: 0.0065 - val_loss: 0.0100
 - val_f1: 0.9931
Epoch 77/300
 - 1s - loss: 0.0062 - val_loss: 0.0104
 - val_f1: 0.9930
Epoch 78/300
 - 1s - loss: 0.0060 - val_loss: 0.0105
 - val_f1: 0.9929
Epoch 79/300
 - 1s - loss: 0.0059 - val_loss: 0.0103
 - val_f1: 0.9929
Epoch 80/300
 - 1s - loss: 0.0057 - val_loss: 0.0105
 - val_f1: 0.9929
Epoch 81/300
 - 1s - loss: 0.0056 - val_loss: 0.0106
 - val_f1: 0.9930
Epoch 82/300
 - 1s - loss: 0.0058 - val_loss: 0.0107
 - val_f1: 0.9929
Epoch 83/300
 - 1s - loss: 0.0055 - val_loss: 0.0104
 - val_f1: 0.9928
Epoch 84/300
 - 1s - loss: 0.0054 - val_loss: 0.0104
 - val_f1: 0.9931
Epoch 85/300
 - 1s - loss: 0.0056 - val_loss: 0.0100
 - val_f1: 0.9932
Epoch 86/300
 - 1s - loss: 0.0053 - val_loss: 0.0101
 - val_f1: 0.9932
Epoch 87/300
 - 1s - loss: 0.0050 - val_loss: 0.0108
 - val_f1: 0.9928
Epoch 88/300
 - 1s - loss: 0.0052 - val_loss: 0.0105
 - val_f1: 0.9933
Epoch 89/300
 - 1s - loss: 0.0051 - val_loss: 0.0104
 - val_f1: 0.9930
Epoch 90/300
 - 1s - loss: 0.0051 - val_loss: 0.0107
 - val_f1: 0.9931
Epoch 91/300
 - 1s - loss: 0.0050 - val_loss: 0.0102
2019-12-26 23:53:27,742 [INFO] epoch = 90. Intermediate model saved to results_selected_models/selected_nsl_lstm_deep_rep4/_model_epoch_90.pickle
 - val_f1: 0.9933
Epoch 92/300
 - 1s - loss: 0.0047 - val_loss: 0.0106
 - val_f1: 0.9931
Epoch 93/300
 - 1s - loss: 0.0048 - val_loss: 0.0108
 - val_f1: 0.9932
Epoch 94/300
 - 1s - loss: 0.0047 - val_loss: 0.0109
 - val_f1: 0.9934
Epoch 95/300
 - 1s - loss: 0.0043 - val_loss: 0.0108
 - val_f1: 0.9933
Epoch 96/300
 - 1s - loss: 0.0047 - val_loss: 0.0114
 - val_f1: 0.9932
Epoch 97/300
 - 1s - loss: 0.0048 - val_loss: 0.0109
 - val_f1: 0.9935
Epoch 98/300
 - 1s - loss: 0.0044 - val_loss: 0.0111
 - val_f1: 0.9930
Epoch 99/300
 - 1s - loss: 0.0046 - val_loss: 0.0118
 - val_f1: 0.9931
Epoch 100/300
 - 1s - loss: 0.0042 - val_loss: 0.0112
 - val_f1: 0.9932
Epoch 101/300
 - 1s - loss: 0.0044 - val_loss: 0.0118
 - val_f1: 0.9926
Epoch 102/300
 - 1s - loss: 0.0043 - val_loss: 0.0109
 - val_f1: 0.9935
Epoch 103/300
 - 1s - loss: 0.0041 - val_loss: 0.0109
 - val_f1: 0.9931
Epoch 104/300
 - 1s - loss: 0.0041 - val_loss: 0.0113
 - val_f1: 0.9933
Epoch 105/300
 - 1s - loss: 0.0041 - val_loss: 0.0114
 - val_f1: 0.9930
Epoch 106/300
 - 1s - loss: 0.0042 - val_loss: 0.0110
 - val_f1: 0.9932
Epoch 107/300
 - 1s - loss: 0.0040 - val_loss: 0.0113
 - val_f1: 0.9935
Epoch 108/300
 - 1s - loss: 0.0040 - val_loss: 0.0116
 - val_f1: 0.9931
Epoch 109/300
 - 1s - loss: 0.0041 - val_loss: 0.0112
 - val_f1: 0.9936
Epoch 110/300
 - 1s - loss: 0.0039 - val_loss: 0.0112
 - val_f1: 0.9933
Epoch 111/300
 - 1s - loss: 0.0039 - val_loss: 0.0113
 - val_f1: 0.9934
Epoch 112/300
 - 1s - loss: 0.0034 - val_loss: 0.0111
2019-12-26 23:53:48,128 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-26 23:53:49,125 [INFO] Last epoch loss evaluation: train_loss = 0.003922, val_loss = 0.009665
2019-12-26 23:53:49,125 [INFO] Training complete. time_to_train = 115.32 sec, 1.92 min
2019-12-26 23:53:49,131 [INFO] Model saved to results_selected_models/selected_nsl_lstm_deep_rep4/best_model.pickle
2019-12-26 23:53:49,133 [INFO] Training history saved to: results_selected_models/selected_nsl_lstm_deep_rep4/training_error_history.csv
2019-12-26 23:53:49,263 [INFO] Plot saved to: results_selected_models/selected_nsl_lstm_deep_rep4/training_error_history.png
2019-12-26 23:53:49,392 [INFO] Plot saved to: results_selected_models/selected_nsl_lstm_deep_rep4/training_f1_history.png
2019-12-26 23:53:49,392 [INFO] Making predictions on training, validation, testing data
2019-12-26 23:53:50,095 [INFO] Evaluating predictions (results)
2019-12-26 23:53:50,408 [INFO] Dataset: Testing. Classification report below
2019-12-26 23:53:50,408 [INFO] 
              precision    recall  f1-score   support

         dos       0.95      0.84      0.89      7453
      normal       0.68      0.93      0.79      9707
       probe       0.68      0.68      0.68      2420
         r2l       0.93      0.12      0.21      2421
         u2r       0.67      0.01      0.02       533

   micro avg       0.76      0.76      0.76     22534
   macro avg       0.78      0.52      0.52     22534
weighted avg       0.80      0.76      0.73     22534

2019-12-26 23:53:50,408 [INFO] Overall accuracy (micro avg): 0.763823555516109
2019-12-26 23:53:50,762 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.7638         0.7638                       0.7638                0.0590                   0.2362  0.7638
1     Macro avg        0.9055         0.7820                       0.5152                0.0776                   0.4848  0.5174
2  Weighted avg        0.8662         0.7970                       0.7638                0.1521                   0.2362  0.7297
2019-12-26 23:53:51,110 [INFO] Dataset: Validation. Classification report below
2019-12-26 23:53:51,110 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00      9186
      normal       0.99      0.99      0.99     13469
       probe       0.98      0.98      0.98      2330
         r2l       0.85      0.82      0.84       199
         u2r       1.00      0.20      0.33        10

   micro avg       0.99      0.99      0.99     25194
   macro avg       0.97      0.80      0.83     25194
weighted avg       0.99      0.99      0.99     25194

2019-12-26 23:53:51,110 [INFO] Overall accuracy (micro avg): 0.9928951337620068
2019-12-26 23:53:51,491 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9929         0.9929                       0.9929                0.0018                   0.0071  0.9929
1     Macro avg        0.9972         0.9653                       0.7989                0.0023                   0.2011  0.8287
2  Weighted avg        0.9956         0.9929                       0.9929                0.0046                   0.0071  0.9928
2019-12-26 23:53:52,998 [INFO] Dataset: Training. Classification report below
2019-12-26 23:53:52,998 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00     36740
      normal       1.00      1.00      1.00     53873
       probe       0.99      0.99      0.99      9325
         r2l       0.94      0.90      0.92       796
         u2r       0.92      0.29      0.44        42

   micro avg       1.00      1.00      1.00    100776
   macro avg       0.97      0.84      0.87    100776
weighted avg       1.00      1.00      1.00    100776

2019-12-26 23:53:52,998 [INFO] Overall accuracy (micro avg): 0.9970727157259665
2019-12-26 23:53:54,703 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9971         0.9971                       0.9971                0.0007                   0.0029  0.9971
1     Macro avg        0.9988         0.9712                       0.8355                0.0010                   0.1645  0.8696
2  Weighted avg        0.9982         0.9970                       0.9971                0.0022                   0.0029  0.9970
2019-12-26 23:53:54,723 [INFO] Results saved to: results_selected_models/selected_nsl_lstm_deep_rep4/selected_nsl_lstm_deep_rep4_results.xlsx
2019-12-26 23:53:54,724 [INFO] ================= Finished running experiment no. 4 ================= 

2019-12-26 23:53:54,727 [INFO] Created directory: results_selected_models/selected_nsl_lstm_deep_rep5
2019-12-26 23:53:54,728 [INFO] Initialized logging. log_filename = results_selected_models/selected_nsl_lstm_deep_rep5/run_log.log
2019-12-26 23:53:54,728 [INFO] ================= Running experiment no. 5  ================= 

2019-12-26 23:53:54,728 [INFO] Experiment parameters given below
2019-12-26 23:53:54,728 [INFO] 
{'experiment_num': 5, 'results_dir': 'results_selected_models/selected_nsl_lstm_deep_rep5', 'model': 'lstm', 'model_type': 'classifier', 'normal_label': 'normal', 'lstm_time_steps': 20, 'lstm_layer_units': [64, 36], 'lstm_layer_activations': ['relu', 'relu'], 'lstm_layer_dropout_rates': [0.2, 0.6], 'batch_size': 256, 'epochs': 300, 'early_stop_patience': 50, 'dataset_dir': '../Datasets/small_datasets/nsl_kdd_five_classes', 'description': 'selected_nsl_lstm_deep_rep5'}
2019-12-26 23:53:54,728 [INFO] Created tensorboard log directory: results_selected_models/selected_nsl_lstm_deep_rep5/tf_logs_run_2019_12_26-23_53_54
2019-12-26 23:53:54,728 [INFO] Loading datsets from: ../Datasets/small_datasets/nsl_kdd_five_classes
2019-12-26 23:53:54,728 [INFO] Reading X, y files
2019-12-26 23:53:54,728 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_train.h5
2019-12-26 23:53:54,992 [INFO] Reading complete. time_to_read=0.26 seconds
2019-12-26 23:53:54,992 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_val.h5
2019-12-26 23:53:55,061 [INFO] Reading complete. time_to_read=0.07 seconds
2019-12-26 23:53:55,061 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_test.h5
2019-12-26 23:53:55,124 [INFO] Reading complete. time_to_read=0.06 seconds
2019-12-26 23:53:55,124 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_train.h5
2019-12-26 23:53:55,132 [INFO] Reading complete. time_to_read=0.01 seconds
2019-12-26 23:53:55,132 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_val.h5
2019-12-26 23:53:55,136 [INFO] Reading complete. time_to_read=0.00 seconds
2019-12-26 23:53:55,136 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_test.h5
2019-12-26 23:53:55,140 [INFO] Reading complete. time_to_read=0.00 seconds
2019-12-26 23:53:55,296 [INFO] Preparing flow sequences
2019-12-26 23:53:57,102 [INFO] Extracting flows complete. time_taken = 1.81 sec
2019-12-26 23:53:57,151 [INFO] Initializing model
2019-12-26 23:53:57,552 [WARNING] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.
2019-12-26 23:53:57,594 [INFO] _________________________________________________________________
2019-12-26 23:53:57,594 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-26 23:53:57,594 [INFO] =================================================================
2019-12-26 23:53:57,595 [INFO] lstm_9 (LSTM)                (None, 20, 64)            47872     
2019-12-26 23:53:57,595 [INFO] _________________________________________________________________
2019-12-26 23:53:57,595 [INFO] batch_normalization_9 (Batch (None, 20, 64)            256       
2019-12-26 23:53:57,595 [INFO] _________________________________________________________________
2019-12-26 23:53:57,595 [INFO] dropout_9 (Dropout)          (None, 20, 64)            0         
2019-12-26 23:53:57,595 [INFO] _________________________________________________________________
2019-12-26 23:53:57,595 [INFO] lstm_10 (LSTM)               (None, 20, 36)            14544     
2019-12-26 23:53:57,595 [INFO] _________________________________________________________________
2019-12-26 23:53:57,595 [INFO] batch_normalization_10 (Batc (None, 20, 36)            144       
2019-12-26 23:53:57,595 [INFO] _________________________________________________________________
2019-12-26 23:53:57,595 [INFO] dropout_10 (Dropout)         (None, 20, 36)            0         
2019-12-26 23:53:57,595 [INFO] _________________________________________________________________
2019-12-26 23:53:57,595 [INFO] time_distributed_5 (TimeDist (None, 20, 5)             185       
2019-12-26 23:53:57,595 [INFO] =================================================================
2019-12-26 23:53:57,595 [INFO] Total params: 63,001
2019-12-26 23:53:57,595 [INFO] Trainable params: 62,801
2019-12-26 23:53:57,596 [INFO] Non-trainable params: 200
2019-12-26 23:53:57,596 [INFO] _________________________________________________________________
2019-12-26 23:53:57,596 [INFO] Training model
 - val_f1: 0.9933
Epoch 00112: early stopping
Train on 5038 samples, validate on 1259 samples
Epoch 1/300
 - 2s - loss: 0.5849 - val_loss: 0.4277
 - val_f1: 0.3828
Epoch 2/300
 - 1s - loss: 0.4093 - val_loss: 0.2828
 - val_f1: 0.6825
Epoch 3/300
 - 1s - loss: 0.2608 - val_loss: 0.1446
 - val_f1: 0.8877
Epoch 4/300
 - 1s - loss: 0.1684 - val_loss: 0.0907
 - val_f1: 0.9430
Epoch 5/300
 - 1s - loss: 0.1204 - val_loss: 0.0708
 - val_f1: 0.9555
Epoch 6/300
 - 1s - loss: 0.0970 - val_loss: 0.0601
 - val_f1: 0.9610
Epoch 7/300
 - 1s - loss: 0.0829 - val_loss: 0.0536
 - val_f1: 0.9643
Epoch 8/300
 - 1s - loss: 0.0743 - val_loss: 0.0497
 - val_f1: 0.9664
Epoch 9/300
 - 1s - loss: 0.0680 - val_loss: 0.0463
 - val_f1: 0.9678
Epoch 10/300
 - 1s - loss: 0.0631 - val_loss: 0.0431
 - val_f1: 0.9688
Epoch 11/300
 - 1s - loss: 0.0579 - val_loss: 0.0404
 - val_f1: 0.9694
Epoch 12/300
 - 1s - loss: 0.0544 - val_loss: 0.0371
 - val_f1: 0.9701
Epoch 13/300
 - 1s - loss: 0.0500 - val_loss: 0.0345
 - val_f1: 0.9704
Epoch 14/300
 - 1s - loss: 0.0469 - val_loss: 0.0316
 - val_f1: 0.9708
Epoch 15/300
 - 1s - loss: 0.0439 - val_loss: 0.0291
 - val_f1: 0.9715
Epoch 16/300
 - 1s - loss: 0.0403 - val_loss: 0.0266
 - val_f1: 0.9730
Epoch 17/300
 - 1s - loss: 0.0373 - val_loss: 0.0243
 - val_f1: 0.9741
Epoch 18/300
 - 1s - loss: 0.0348 - val_loss: 0.0222
 - val_f1: 0.9759
Epoch 19/300
 - 1s - loss: 0.0328 - val_loss: 0.0206
 - val_f1: 0.9780
Epoch 20/300
 - 1s - loss: 0.0306 - val_loss: 0.0193
 - val_f1: 0.9796
Epoch 21/300
 - 1s - loss: 0.0290 - val_loss: 0.0181
 - val_f1: 0.9807
Epoch 22/300
 - 1s - loss: 0.0280 - val_loss: 0.0172
 - val_f1: 0.9814
Epoch 23/300
 - 1s - loss: 0.0258 - val_loss: 0.0164
 - val_f1: 0.9821
Epoch 24/300
 - 1s - loss: 0.0248 - val_loss: 0.0154
 - val_f1: 0.9841
Epoch 25/300
 - 1s - loss: 0.0234 - val_loss: 0.0150
 - val_f1: 0.9852
Epoch 26/300
 - 1s - loss: 0.0223 - val_loss: 0.0145
 - val_f1: 0.9857
Epoch 27/300
 - 1s - loss: 0.0219 - val_loss: 0.0140
 - val_f1: 0.9871
Epoch 28/300
 - 1s - loss: 0.0210 - val_loss: 0.0133
 - val_f1: 0.9887
Epoch 29/300
 - 1s - loss: 0.0202 - val_loss: 0.0130
 - val_f1: 0.9888
Epoch 30/300
 - 1s - loss: 0.0194 - val_loss: 0.0125
 - val_f1: 0.9897
Epoch 31/300
 - 1s - loss: 0.0187 - val_loss: 0.0119
/home/hasitha/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2019-12-26 23:54:34,740 [INFO] epoch = 30. Intermediate model saved to results_selected_models/selected_nsl_lstm_deep_rep5/_model_epoch_30.pickle
 - val_f1: 0.9911
Epoch 32/300
 - 1s - loss: 0.0183 - val_loss: 0.0116
 - val_f1: 0.9918
Epoch 33/300
 - 1s - loss: 0.0174 - val_loss: 0.0117
 - val_f1: 0.9912
Epoch 34/300
 - 1s - loss: 0.0175 - val_loss: 0.0111
 - val_f1: 0.9917
Epoch 35/300
 - 1s - loss: 0.0166 - val_loss: 0.0110
 - val_f1: 0.9919
Epoch 36/300
 - 1s - loss: 0.0160 - val_loss: 0.0108
 - val_f1: 0.9919
Epoch 37/300
 - 1s - loss: 0.0158 - val_loss: 0.0109
 - val_f1: 0.9917
Epoch 38/300
 - 1s - loss: 0.0152 - val_loss: 0.0107
 - val_f1: 0.9919
Epoch 39/300
 - 1s - loss: 0.0147 - val_loss: 0.0104
 - val_f1: 0.9919
Epoch 40/300
 - 1s - loss: 0.0146 - val_loss: 0.0104
 - val_f1: 0.9923
Epoch 41/300
 - 1s - loss: 0.0140 - val_loss: 0.0103
 - val_f1: 0.9921
Epoch 42/300
 - 1s - loss: 0.0138 - val_loss: 0.0104
 - val_f1: 0.9921
Epoch 43/300
 - 1s - loss: 0.0135 - val_loss: 0.0105
 - val_f1: 0.9923
Epoch 44/300
 - 1s - loss: 0.0132 - val_loss: 0.0108
 - val_f1: 0.9921
Epoch 45/300
 - 1s - loss: 0.0131 - val_loss: 0.0108
 - val_f1: 0.9918
Epoch 46/300
 - 1s - loss: 0.0129 - val_loss: 0.0104
 - val_f1: 0.9922
Epoch 47/300
 - 1s - loss: 0.0122 - val_loss: 0.0107
 - val_f1: 0.9923
Epoch 48/300
 - 1s - loss: 0.0128 - val_loss: 0.0105
 - val_f1: 0.9919
Epoch 49/300
 - 1s - loss: 0.0117 - val_loss: 0.0103
 - val_f1: 0.9920
Epoch 50/300
 - 1s - loss: 0.0117 - val_loss: 0.0100
 - val_f1: 0.9926
Epoch 51/300
 - 1s - loss: 0.0116 - val_loss: 0.0101
 - val_f1: 0.9922
Epoch 52/300
 - 1s - loss: 0.0113 - val_loss: 0.0101
 - val_f1: 0.9923
Epoch 53/300
 - 1s - loss: 0.0113 - val_loss: 0.0101
 - val_f1: 0.9927
Epoch 54/300
 - 1s - loss: 0.0109 - val_loss: 0.0102
 - val_f1: 0.9924
Epoch 55/300
 - 1s - loss: 0.0103 - val_loss: 0.0102
 - val_f1: 0.9927
Epoch 56/300
 - 1s - loss: 0.0105 - val_loss: 0.0103
 - val_f1: 0.9928
Epoch 57/300
 - 1s - loss: 0.0102 - val_loss: 0.0105
 - val_f1: 0.9926
Epoch 58/300
 - 1s - loss: 0.0101 - val_loss: 0.0106
 - val_f1: 0.9923
Epoch 59/300
 - 1s - loss: 0.0102 - val_loss: 0.0108
 - val_f1: 0.9924
Epoch 60/300
 - 1s - loss: 0.0096 - val_loss: 0.0106
 - val_f1: 0.9928
Epoch 61/300
 - 1s - loss: 0.0096 - val_loss: 0.0105
2019-12-26 23:55:03,701 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_nsl_lstm_deep_rep5/_model_epoch_60.pickle
 - val_f1: 0.9925
Epoch 62/300
 - 1s - loss: 0.0095 - val_loss: 0.0103
 - val_f1: 0.9929
Epoch 63/300
 - 1s - loss: 0.0091 - val_loss: 0.0104
 - val_f1: 0.9931
Epoch 64/300
 - 1s - loss: 0.0089 - val_loss: 0.0108
 - val_f1: 0.9928
Epoch 65/300
 - 1s - loss: 0.0090 - val_loss: 0.0108
 - val_f1: 0.9925
Epoch 66/300
 - 1s - loss: 0.0089 - val_loss: 0.0108
 - val_f1: 0.9925
Epoch 67/300
 - 1s - loss: 0.0088 - val_loss: 0.0110
 - val_f1: 0.9923
Epoch 68/300
 - 1s - loss: 0.0085 - val_loss: 0.0110
 - val_f1: 0.9927
Epoch 69/300
 - 1s - loss: 0.0086 - val_loss: 0.0110
 - val_f1: 0.9925
Epoch 70/300
 - 1s - loss: 0.0087 - val_loss: 0.0110
 - val_f1: 0.9927
Epoch 71/300
 - 1s - loss: 0.0085 - val_loss: 0.0111
 - val_f1: 0.9925
Epoch 72/300
 - 1s - loss: 0.0083 - val_loss: 0.0111
 - val_f1: 0.9927
Epoch 73/300
 - 1s - loss: 0.0079 - val_loss: 0.0111
 - val_f1: 0.9925
Epoch 74/300
 - 1s - loss: 0.0076 - val_loss: 0.0107
 - val_f1: 0.9930
Epoch 75/300
 - 1s - loss: 0.0077 - val_loss: 0.0110
 - val_f1: 0.9929
Epoch 76/300
 - 1s - loss: 0.0076 - val_loss: 0.0111
 - val_f1: 0.9927
Epoch 77/300
 - 1s - loss: 0.0074 - val_loss: 0.0110
 - val_f1: 0.9928
Epoch 78/300
 - 1s - loss: 0.0073 - val_loss: 0.0111
 - val_f1: 0.9928
Epoch 79/300
 - 1s - loss: 0.0075 - val_loss: 0.0115
 - val_f1: 0.9929
Epoch 80/300
 - 1s - loss: 0.0074 - val_loss: 0.0115
 - val_f1: 0.9928
Epoch 81/300
 - 1s - loss: 0.0072 - val_loss: 0.0120
 - val_f1: 0.9924
Epoch 82/300
 - 1s - loss: 0.0071 - val_loss: 0.0117
 - val_f1: 0.9926
Epoch 83/300
 - 1s - loss: 0.0072 - val_loss: 0.0116
 - val_f1: 0.9926
Epoch 84/300
 - 1s - loss: 0.0069 - val_loss: 0.0114
 - val_f1: 0.9929
Epoch 85/300
 - 1s - loss: 0.0070 - val_loss: 0.0114
 - val_f1: 0.9927
Epoch 86/300
 - 1s - loss: 0.0067 - val_loss: 0.0116
 - val_f1: 0.9928
Epoch 87/300
 - 1s - loss: 0.0066 - val_loss: 0.0113
 - val_f1: 0.9929
Epoch 88/300
 - 1s - loss: 0.0064 - val_loss: 0.0118
 - val_f1: 0.9925
Epoch 89/300
 - 1s - loss: 0.0062 - val_loss: 0.0122
 - val_f1: 0.9926
Epoch 90/300
 - 1s - loss: 0.0064 - val_loss: 0.0120
 - val_f1: 0.9927
Epoch 91/300
 - 1s - loss: 0.0061 - val_loss: 0.0121
2019-12-26 23:55:32,648 [INFO] epoch = 90. Intermediate model saved to results_selected_models/selected_nsl_lstm_deep_rep5/_model_epoch_90.pickle
 - val_f1: 0.9931
Epoch 92/300
 - 1s - loss: 0.0062 - val_loss: 0.0125
 - val_f1: 0.9927
Epoch 93/300
 - 1s - loss: 0.0059 - val_loss: 0.0126
 - val_f1: 0.9927
Epoch 94/300
 - 1s - loss: 0.0059 - val_loss: 0.0124
 - val_f1: 0.9923
Epoch 95/300
 - 1s - loss: 0.0062 - val_loss: 0.0122
 - val_f1: 0.9924
Epoch 96/300
 - 1s - loss: 0.0059 - val_loss: 0.0125
 - val_f1: 0.9925
Epoch 97/300
 - 1s - loss: 0.0056 - val_loss: 0.0122
 - val_f1: 0.9928
Epoch 98/300
 - 1s - loss: 0.0055 - val_loss: 0.0124
 - val_f1: 0.9927
Epoch 99/300
 - 1s - loss: 0.0056 - val_loss: 0.0126
 - val_f1: 0.9925
Epoch 100/300
 - 1s - loss: 0.0056 - val_loss: 0.0122
2019-12-26 23:55:41,458 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-26 23:55:42,562 [INFO] Last epoch loss evaluation: train_loss = 0.006089, val_loss = 0.009995
2019-12-26 23:55:42,562 [INFO] Training complete. time_to_train = 104.97 sec, 1.75 min
2019-12-26 23:55:42,568 [INFO] Model saved to results_selected_models/selected_nsl_lstm_deep_rep5/best_model.pickle
2019-12-26 23:55:42,570 [INFO] Training history saved to: results_selected_models/selected_nsl_lstm_deep_rep5/training_error_history.csv
2019-12-26 23:55:42,702 [INFO] Plot saved to: results_selected_models/selected_nsl_lstm_deep_rep5/training_error_history.png
2019-12-26 23:55:42,825 [INFO] Plot saved to: results_selected_models/selected_nsl_lstm_deep_rep5/training_f1_history.png
2019-12-26 23:55:42,825 [INFO] Making predictions on training, validation, testing data
2019-12-26 23:55:43,535 [INFO] Evaluating predictions (results)
2019-12-26 23:55:43,829 [INFO] Dataset: Testing. Classification report below
2019-12-26 23:55:43,829 [INFO] 
              precision    recall  f1-score   support

         dos       0.95      0.85      0.90      7457
      normal       0.69      0.93      0.79      9709
       probe       0.75      0.77      0.76      2420
         r2l       0.94      0.10      0.18      2421
         u2r       0.00      0.00      0.00       533

   micro avg       0.78      0.78      0.78     22540
   macro avg       0.66      0.53      0.53     22540
weighted avg       0.79      0.78      0.74     22540

2019-12-26 23:55:43,829 [INFO] Overall accuracy (micro avg): 0.7754658385093167
/home/hasitha/sunanda/ids_experiments/utility.py:334: RuntimeWarning: invalid value encountered in true_divide
  F1 = (2 * PPV * TPR) / (PPV + TPR)  # F1 score
2019-12-26 23:55:44,157 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.7755         0.7755                       0.7755                0.0561                   0.2245  0.7755
1     Macro avg        0.9102         0.6648                       0.5305                0.0745                   0.4695  0.5263
2  Weighted avg        0.8711         0.7928                       0.7755                0.1479                   0.2245  0.7391
/home/hasitha/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2019-12-26 23:55:44,499 [INFO] Dataset: Validation. Classification report below
2019-12-26 23:55:44,499 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00      9181
      normal       0.99      0.99      0.99     13460
       probe       0.98      0.98      0.98      2330
         r2l       0.90      0.77      0.83       199
         u2r       0.00      0.00      0.00        10

   micro avg       0.99      0.99      0.99     25180
   macro avg       0.77      0.75      0.76     25180
weighted avg       0.99      0.99      0.99     25180

2019-12-26 23:55:44,499 [INFO] Overall accuracy (micro avg): 0.9928514694201748
/home/hasitha/sunanda/ids_experiments/utility.py:328: RuntimeWarning: invalid value encountered in true_divide
  PPV = TP / (TP + FP)    # Precision or positive predictive value
/home/hasitha/sunanda/ids_experiments/utility.py:332: RuntimeWarning: invalid value encountered in true_divide
  FDR = FP / (TP + FP)    # False discovery rate
2019-12-26 23:55:44,878 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9929         0.9929                       0.9929                0.0018                   0.0071  0.9929
1     Macro avg        0.9971         0.7739                       0.7497                0.0025                   0.2503  0.7609
2  Weighted avg        0.9956         0.9923                       0.9929                0.0052                   0.0071  0.9926
2019-12-26 23:55:46,376 [INFO] Dataset: Training. Classification report below
2019-12-26 23:55:46,376 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00     36733
      normal       0.99      1.00      1.00     53868
       probe       0.99      0.99      0.99      9321
         r2l       0.93      0.82      0.87       796
         u2r       0.00      0.00      0.00        42

   micro avg       1.00      1.00      1.00    100760
   macro avg       0.78      0.76      0.77    100760
weighted avg       0.99      1.00      1.00    100760

2019-12-26 23:55:46,376 [INFO] Overall accuracy (micro avg): 0.9954247717348154
2019-12-26 23:55:48,070 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9954         0.9954                       0.9954                0.0011                   0.0046  0.9954
1     Macro avg        0.9982         0.7831                       0.7613                0.0017                   0.2387  0.7716
2  Weighted avg        0.9973         0.9949                       0.9954                0.0037                   0.0046  0.9952
2019-12-26 23:55:48,090 [INFO] Results saved to: results_selected_models/selected_nsl_lstm_deep_rep5/selected_nsl_lstm_deep_rep5_results.xlsx
2019-12-26 23:55:48,091 [INFO] ================= Finished running experiment no. 5 ================= 

2019-12-26 23:55:48,118 [INFO] Created directory: results_selected_models/selected_ids17_lstm_deep_rep1
2019-12-26 23:55:48,118 [INFO] Initialized logging. log_filename = results_selected_models/selected_ids17_lstm_deep_rep1/run_log.log
2019-12-26 23:55:48,118 [INFO] ================= Running experiment no. 1  ================= 

2019-12-26 23:55:48,118 [INFO] Experiment parameters given below
2019-12-26 23:55:48,118 [INFO] 
{'experiment_num': 1, 'results_dir': 'results_selected_models/selected_ids17_lstm_deep_rep1', 'model': 'lstm', 'model_type': 'classifier', 'normal_label': 'BENIGN', 'lstm_time_steps': 32, 'lstm_layer_units': [64, 32], 'lstm_layer_activations': ['relu', 'relu'], 'lstm_layer_dropout_rates': [0.2, 0.2], 'batch_size': 256, 'epochs': 300, 'early_stop_patience': 50, 'dataset_dir': '../Datasets/full_datasets/ids2017', 'description': 'selected_ids17_lstm_deep_rep1'}
2019-12-26 23:55:48,118 [INFO] Created tensorboard log directory: results_selected_models/selected_ids17_lstm_deep_rep1/tf_logs_run_2019_12_26-23_55_48
2019-12-26 23:55:48,118 [INFO] Loading datsets from: ../Datasets/full_datasets/ids2017
2019-12-26 23:55:48,126 [INFO] Reading X, y files
2019-12-26 23:55:48,126 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_train.h5
2019-12-26 23:55:53,851 [INFO] Reading complete. time_to_read=5.72 seconds
2019-12-26 23:55:53,851 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_val.h5
2019-12-26 23:55:55,437 [INFO] Reading complete. time_to_read=1.59 seconds
2019-12-26 23:55:55,437 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_test.h5
2019-12-26 23:55:57,014 [INFO] Reading complete. time_to_read=1.58 seconds
2019-12-26 23:55:57,014 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_train.h5
2019-12-26 23:55:57,454 [INFO] Reading complete. time_to_read=0.44 seconds
2019-12-26 23:55:57,455 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_val.h5
2019-12-26 23:55:57,618 [INFO] Reading complete. time_to_read=0.16 seconds
2019-12-26 23:55:57,619 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_test.h5
2019-12-26 23:55:57,782 [INFO] Reading complete. time_to_read=0.16 seconds
2019-12-26 23:56:00,193 [INFO] Preparing flow sequences
2019-12-26 23:56:21,911 [INFO] Extracting flows complete. time_taken = 21.72 sec
2019-12-26 23:56:23,054 [INFO] Initializing model
2019-12-26 23:56:23,499 [INFO] _________________________________________________________________
2019-12-26 23:56:23,499 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-26 23:56:23,499 [INFO] =================================================================
2019-12-26 23:56:23,499 [INFO] lstm_11 (LSTM)               (None, 32, 64)            36608     
2019-12-26 23:56:23,499 [INFO] _________________________________________________________________
2019-12-26 23:56:23,499 [INFO] batch_normalization_11 (Batc (None, 32, 64)            256       
2019-12-26 23:56:23,499 [INFO] _________________________________________________________________
2019-12-26 23:56:23,499 [INFO] dropout_11 (Dropout)         (None, 32, 64)            0         
2019-12-26 23:56:23,499 [INFO] _________________________________________________________________
2019-12-26 23:56:23,499 [INFO] lstm_12 (LSTM)               (None, 32, 32)            12416     
2019-12-26 23:56:23,500 [INFO] _________________________________________________________________
2019-12-26 23:56:23,500 [INFO] batch_normalization_12 (Batc (None, 32, 32)            128       
2019-12-26 23:56:23,500 [INFO] _________________________________________________________________
2019-12-26 23:56:23,500 [INFO] dropout_12 (Dropout)         (None, 32, 32)            0         
2019-12-26 23:56:23,500 [INFO] _________________________________________________________________
2019-12-26 23:56:23,500 [INFO] time_distributed_6 (TimeDist (None, 32, 12)            396       
2019-12-26 23:56:23,500 [INFO] =================================================================
2019-12-26 23:56:23,500 [INFO] Total params: 49,804
2019-12-26 23:56:23,500 [INFO] Trainable params: 49,612
2019-12-26 23:56:23,500 [INFO] Non-trainable params: 192
2019-12-26 23:56:23,500 [INFO] _________________________________________________________________
2019-12-26 23:56:23,500 [INFO] Training model
 - val_f1: 0.9927
Epoch 00100: early stopping
Train on 53021 samples, validate on 17673 samples
Epoch 1/300
 - 14s - loss: 0.1436 - val_loss: 0.0464
 - val_f1: 0.8646
Epoch 2/300
 - 13s - loss: 0.0321 - val_loss: 0.0311
 - val_f1: 0.9236
Epoch 3/300
 - 13s - loss: 0.0232 - val_loss: 0.0215
 - val_f1: 0.9387
Epoch 4/300
 - 13s - loss: 0.0191 - val_loss: 0.0170
 - val_f1: 0.9490
Epoch 5/300
 - 13s - loss: 0.0164 - val_loss: 0.0139
 - val_f1: 0.9653
Epoch 6/300
 - 13s - loss: 0.0146 - val_loss: 0.0155
 - val_f1: 0.9543
Epoch 7/300
 - 13s - loss: 0.0132 - val_loss: 0.0111
 - val_f1: 0.9724
Epoch 8/300
 - 13s - loss: 0.0123 - val_loss: 0.0109
 - val_f1: 0.9741
Epoch 9/300
 - 13s - loss: 0.0114 - val_loss: 0.0095
 - val_f1: 0.9776
Epoch 10/300
 - 13s - loss: 0.0103 - val_loss: 0.0097
 - val_f1: 0.9796
Epoch 11/300
 - 13s - loss: 0.0089 - val_loss: 0.0071
 - val_f1: 0.9840
Epoch 12/300
 - 13s - loss: 0.0084 - val_loss: 0.0094
 - val_f1: 0.9799
Epoch 13/300
 - 13s - loss: 0.0075 - val_loss: 0.0071
 - val_f1: 0.9843
Epoch 14/300
 - 13s - loss: 0.0073 - val_loss: 0.0057
 - val_f1: 0.9871
Epoch 15/300
 - 13s - loss: 0.0093 - val_loss: 0.0114
 - val_f1: 0.9649
Epoch 16/300
 - 13s - loss: 0.0086 - val_loss: 0.0084
 - val_f1: 0.9806
Epoch 17/300
 - 13s - loss: 0.0076 - val_loss: 0.0081
 - val_f1: 0.9745
Epoch 18/300
 - 13s - loss: 0.0062 - val_loss: 0.0067
 - val_f1: 0.9864
Epoch 19/300
 - 13s - loss: 0.0057 - val_loss: 0.0075
 - val_f1: 0.9798
Epoch 20/300
 - 13s - loss: 0.0055 - val_loss: 0.0039
 - val_f1: 0.9919
Epoch 21/300
 - 13s - loss: 0.0052 - val_loss: 0.0038
 - val_f1: 0.9928
Epoch 22/300
 - 13s - loss: 0.0052 - val_loss: 0.0042
 - val_f1: 0.9907
Epoch 23/300
 - 13s - loss: 0.0050 - val_loss: 0.0050
 - val_f1: 0.9891
Epoch 24/300
 - 13s - loss: 0.0047 - val_loss: 0.0034
 - val_f1: 0.9928
Epoch 25/300
 - 13s - loss: 0.0054 - val_loss: 0.0068
 - val_f1: 0.9814
Epoch 26/300
 - 13s - loss: 0.0049 - val_loss: 0.0043
 - val_f1: 0.9900
Epoch 27/300
 - 13s - loss: 0.0046 - val_loss: 0.0044
 - val_f1: 0.9900
Epoch 28/300
 - 13s - loss: 0.0045 - val_loss: 0.0034
 - val_f1: 0.9933
Epoch 29/300
 - 13s - loss: 0.0045 - val_loss: 0.0045
 - val_f1: 0.9900
Epoch 30/300
 - 13s - loss: 0.0043 - val_loss: 0.0034
 - val_f1: 0.9931
Epoch 31/300
 - 13s - loss: 0.0042 - val_loss: 0.0045
/home/hasitha/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2019-12-27 00:04:36,264 [INFO] epoch = 30. Intermediate model saved to results_selected_models/selected_ids17_lstm_deep_rep1/_model_epoch_30.pickle
 - val_f1: 0.9896
Epoch 32/300
 - 13s - loss: 0.0043 - val_loss: 0.0036
 - val_f1: 0.9914
Epoch 33/300
 - 13s - loss: 0.0040 - val_loss: 0.0050
 - val_f1: 0.9893
Epoch 34/300
 - 13s - loss: 0.0040 - val_loss: 0.0035
 - val_f1: 0.9921
Epoch 35/300
 - 13s - loss: 0.0043 - val_loss: 0.0212
 - val_f1: 0.9612
Epoch 36/300
 - 13s - loss: 0.0044 - val_loss: 0.0054
 - val_f1: 0.9889
Epoch 37/300
 - 13s - loss: 0.0038 - val_loss: 0.0036
 - val_f1: 0.9909
Epoch 38/300
 - 13s - loss: 0.0038 - val_loss: 0.0030
 - val_f1: 0.9929
Epoch 39/300
 - 13s - loss: 0.0037 - val_loss: 0.0032
 - val_f1: 0.9937
Epoch 40/300
 - 13s - loss: 0.0036 - val_loss: 0.0030
 - val_f1: 0.9930
Epoch 41/300
 - 13s - loss: 0.0038 - val_loss: 0.0031
 - val_f1: 0.9930
Epoch 42/300
 - 13s - loss: 0.0036 - val_loss: 0.0032
 - val_f1: 0.9919
Epoch 43/300
 - 13s - loss: 0.0036 - val_loss: 0.0030
 - val_f1: 0.9922
Epoch 44/300
 - 13s - loss: 0.0035 - val_loss: 0.0030
 - val_f1: 0.9922
Epoch 45/300
 - 13s - loss: 0.0035 - val_loss: 0.0030
 - val_f1: 0.9923
Epoch 46/300
 - 13s - loss: 0.0034 - val_loss: 0.0034
 - val_f1: 0.9921
Epoch 47/300
 - 13s - loss: 0.0040 - val_loss: 0.0035
 - val_f1: 0.9920
Epoch 48/300
 - 13s - loss: 0.0034 - val_loss: 0.0034
 - val_f1: 0.9927
Epoch 49/300
 - 13s - loss: 0.0054 - val_loss: 0.0068
 - val_f1: 0.9813
Epoch 50/300
 - 13s - loss: 0.0057 - val_loss: 0.0040
 - val_f1: 0.9922
Epoch 51/300
 - 13s - loss: 0.0043 - val_loss: 0.0051
 - val_f1: 0.9860
Epoch 52/300
 - 13s - loss: 0.0037 - val_loss: 0.0051
 - val_f1: 0.9900
Epoch 53/300
 - 13s - loss: 0.0036 - val_loss: 0.0046
 - val_f1: 0.9909
Epoch 54/300
 - 13s - loss: 0.0034 - val_loss: 0.0157
 - val_f1: 0.9612
Epoch 55/300
 - 13s - loss: 0.0034 - val_loss: 0.0051
 - val_f1: 0.9858
Epoch 56/300
 - 13s - loss: 0.0032 - val_loss: 0.0029
 - val_f1: 0.9931
Epoch 57/300
 - 13s - loss: 0.0032 - val_loss: 0.0027
 - val_f1: 0.9936
Epoch 58/300
 - 13s - loss: 0.0032 - val_loss: 0.0027
 - val_f1: 0.9945
Epoch 59/300
 - 13s - loss: 0.0032 - val_loss: 0.0027
 - val_f1: 0.9945
Epoch 60/300
 - 13s - loss: 0.0032 - val_loss: 0.0033
 - val_f1: 0.9918
Epoch 61/300
 - 13s - loss: 0.0032 - val_loss: 0.0027
2019-12-27 00:12:28,951 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_ids17_lstm_deep_rep1/_model_epoch_60.pickle
 - val_f1: 0.9935
Epoch 62/300
 - 13s - loss: 0.0031 - val_loss: 0.0033
 - val_f1: 0.9930
Epoch 63/300
 - 13s - loss: 0.0034 - val_loss: 0.0035
 - val_f1: 0.9917
Epoch 64/300
 - 13s - loss: 0.0031 - val_loss: 0.0039
 - val_f1: 0.9914
Epoch 65/300
 - 13s - loss: 0.0031 - val_loss: 0.0025
 - val_f1: 0.9945
Epoch 66/300
 - 13s - loss: 0.0030 - val_loss: 0.0034
 - val_f1: 0.9921
Epoch 67/300
 - 13s - loss: 0.0030 - val_loss: 0.0027
 - val_f1: 0.9928
Epoch 68/300
 - 13s - loss: 0.0030 - val_loss: 0.0027
 - val_f1: 0.9926
Epoch 69/300
 - 13s - loss: 0.0030 - val_loss: 0.0027
 - val_f1: 0.9932
Epoch 70/300
 - 13s - loss: 0.0030 - val_loss: 0.0026
 - val_f1: 0.9941
Epoch 71/300
 - 13s - loss: 0.0031 - val_loss: 0.0024
 - val_f1: 0.9948
Epoch 72/300
 - 13s - loss: 0.0029 - val_loss: 0.0029
 - val_f1: 0.9945
Epoch 73/300
 - 13s - loss: 0.0029 - val_loss: 0.0024
 - val_f1: 0.9948
Epoch 74/300
 - 13s - loss: 0.0029 - val_loss: 0.0033
 - val_f1: 0.9922
Epoch 75/300
 - 13s - loss: 0.0029 - val_loss: 0.0026
 - val_f1: 0.9938
Epoch 76/300
 - 13s - loss: 0.0028 - val_loss: 0.0027
 - val_f1: 0.9937
Epoch 77/300
 - 13s - loss: 0.0029 - val_loss: 0.0028
 - val_f1: 0.9925
Epoch 78/300
 - 13s - loss: 0.0029 - val_loss: 0.0028
 - val_f1: 0.9937
Epoch 79/300
 - 13s - loss: 0.0028 - val_loss: 0.0024
 - val_f1: 0.9946
Epoch 80/300
 - 13s - loss: 0.0031 - val_loss: 0.0025
 - val_f1: 0.9942
Epoch 81/300
 - 13s - loss: 0.0029 - val_loss: 0.0025
 - val_f1: 0.9949
Epoch 82/300
 - 13s - loss: 0.0028 - val_loss: 0.0031
 - val_f1: 0.9924
Epoch 83/300
 - 13s - loss: 0.0028 - val_loss: 0.0027
 - val_f1: 0.9926
Epoch 84/300
 - 13s - loss: 0.0042 - val_loss: 0.0305
 - val_f1: 0.9157
Epoch 85/300
 - 13s - loss: 0.0032 - val_loss: 0.0025
 - val_f1: 0.9949
Epoch 86/300
 - 13s - loss: 0.0030 - val_loss: 0.0037
 - val_f1: 0.9926
Epoch 87/300
 - 13s - loss: 0.0032 - val_loss: 0.0056
 - val_f1: 0.9868
Epoch 88/300
 - 13s - loss: 0.0030 - val_loss: 0.0025
 - val_f1: 0.9947
Epoch 89/300
 - 13s - loss: 0.0030 - val_loss: 0.0026
 - val_f1: 0.9941
Epoch 90/300
 - 13s - loss: 0.0030 - val_loss: 0.0031
 - val_f1: 0.9922
Epoch 91/300
 - 13s - loss: 0.0030 - val_loss: 0.0043
2019-12-27 00:20:20,542 [INFO] epoch = 90. Intermediate model saved to results_selected_models/selected_ids17_lstm_deep_rep1/_model_epoch_90.pickle
 - val_f1: 0.9901
Epoch 92/300
 - 13s - loss: 0.0030 - val_loss: 0.0030
 - val_f1: 0.9924
Epoch 93/300
 - 13s - loss: 0.0029 - val_loss: 0.0024
 - val_f1: 0.9943
Epoch 94/300
 - 13s - loss: 0.0029 - val_loss: 0.0024
 - val_f1: 0.9939
Epoch 95/300
 - 13s - loss: 0.0029 - val_loss: 0.0024
 - val_f1: 0.9945
Epoch 96/300
 - 13s - loss: 0.0029 - val_loss: 0.0027
 - val_f1: 0.9945
Epoch 97/300
 - 13s - loss: 0.0030 - val_loss: 0.0028
 - val_f1: 0.9927
Epoch 98/300
 - 13s - loss: 0.0028 - val_loss: 0.0025
 - val_f1: 0.9931
Epoch 99/300
 - 13s - loss: 0.0027 - val_loss: 0.0041
 - val_f1: 0.9919
Epoch 100/300
 - 13s - loss: 0.0028 - val_loss: 0.0087
 - val_f1: 0.9867
Epoch 101/300
 - 13s - loss: 0.0029 - val_loss: 0.0084
 - val_f1: 0.9726
Epoch 102/300
 - 13s - loss: 0.0028 - val_loss: 0.0045
 - val_f1: 0.9891
Epoch 103/300
 - 13s - loss: 0.0028 - val_loss: 0.0038
 - val_f1: 0.9902
Epoch 104/300
 - 13s - loss: 0.0027 - val_loss: 0.0023
 - val_f1: 0.9947
Epoch 105/300
 - 13s - loss: 0.0027 - val_loss: 0.0023
 - val_f1: 0.9949
Epoch 106/300
 - 13s - loss: 0.0029 - val_loss: 0.0023
 - val_f1: 0.9949
Epoch 107/300
 - 13s - loss: 0.0028 - val_loss: 0.0023
 - val_f1: 0.9946
Epoch 108/300
 - 13s - loss: 0.0027 - val_loss: 0.0023
 - val_f1: 0.9949
Epoch 109/300
 - 13s - loss: 0.0027 - val_loss: 0.0024
 - val_f1: 0.9935
Epoch 110/300
 - 13s - loss: 0.0027 - val_loss: 0.0052
 - val_f1: 0.9885
Epoch 111/300
 - 13s - loss: 0.0027 - val_loss: 0.0025
 - val_f1: 0.9931
Epoch 112/300
 - 13s - loss: 0.0028 - val_loss: 0.0227
 - val_f1: 0.9618
Epoch 113/300
 - 13s - loss: 0.0026 - val_loss: 0.0026
 - val_f1: 0.9946
Epoch 114/300
 - 13s - loss: 0.0026 - val_loss: 0.0023
 - val_f1: 0.9937
Epoch 115/300
 - 13s - loss: 0.0026 - val_loss: 0.0022
 - val_f1: 0.9955
Epoch 116/300
 - 13s - loss: 0.0027 - val_loss: 0.0022
 - val_f1: 0.9951
Epoch 117/300
 - 13s - loss: 0.0026 - val_loss: 0.0023
 - val_f1: 0.9949
Epoch 118/300
 - 13s - loss: 0.0028 - val_loss: 0.0328
 - val_f1: 0.9604
Epoch 119/300
 - 13s - loss: 0.0027 - val_loss: 0.0022
 - val_f1: 0.9949
Epoch 120/300
 - 13s - loss: 0.0027 - val_loss: 0.0027
 - val_f1: 0.9936
Epoch 121/300
 - 13s - loss: 0.0027 - val_loss: 0.0024
2019-12-27 00:28:12,582 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_ids17_lstm_deep_rep1/_model_epoch_120.pickle
 - val_f1: 0.9946
Epoch 122/300
 - 13s - loss: 0.0026 - val_loss: 0.0023
 - val_f1: 0.9949
Epoch 123/300
 - 13s - loss: 0.0027 - val_loss: 0.0023
 - val_f1: 0.9941
Epoch 124/300
 - 13s - loss: 0.0026 - val_loss: 0.0044
 - val_f1: 0.9924
Epoch 125/300
 - 13s - loss: 0.0026 - val_loss: 0.0024
 - val_f1: 0.9945
Epoch 126/300
 - 13s - loss: 0.0026 - val_loss: 0.0024
 - val_f1: 0.9933
Epoch 127/300
 - 13s - loss: 0.0026 - val_loss: 0.0026
 - val_f1: 0.9939
Epoch 128/300
 - 13s - loss: 0.0026 - val_loss: 0.0025
 - val_f1: 0.9948
Epoch 129/300
 - 13s - loss: 0.0026 - val_loss: 0.0023
 - val_f1: 0.9943
Epoch 130/300
 - 13s - loss: 0.0026 - val_loss: 0.0024
 - val_f1: 0.9950
Epoch 131/300
 - 13s - loss: 0.0025 - val_loss: 0.0023
 - val_f1: 0.9951
Epoch 132/300
 - 13s - loss: 0.0025 - val_loss: 0.0026
 - val_f1: 0.9931
Epoch 133/300
 - 13s - loss: 0.0026 - val_loss: 0.0024
 - val_f1: 0.9937
Epoch 134/300
 - 13s - loss: 0.0025 - val_loss: 0.0024
 - val_f1: 0.9947
Epoch 135/300
 - 13s - loss: 0.0027 - val_loss: 0.0030
 - val_f1: 0.9922
Epoch 136/300
 - 13s - loss: 0.0027 - val_loss: 0.0025
 - val_f1: 0.9944
Epoch 137/300
 - 13s - loss: 0.0026 - val_loss: 0.0023
 - val_f1: 0.9951
Epoch 138/300
 - 13s - loss: 0.0026 - val_loss: 0.0098
 - val_f1: 0.9834
Epoch 139/300
 - 13s - loss: 0.0026 - val_loss: 0.0030
 - val_f1: 0.9940
Epoch 140/300
 - 13s - loss: 0.0025 - val_loss: 0.0022
 - val_f1: 0.9951
Epoch 141/300
 - 13s - loss: 0.0025 - val_loss: 0.0026
 - val_f1: 0.9935
Epoch 142/300
 - 13s - loss: 0.0024 - val_loss: 0.0034
 - val_f1: 0.9920
Epoch 143/300
 - 13s - loss: 0.0026 - val_loss: 0.0026
 - val_f1: 0.9946
Epoch 144/300
 - 13s - loss: 0.0025 - val_loss: 0.0023
 - val_f1: 0.9949
Epoch 145/300
 - 13s - loss: 0.0025 - val_loss: 0.0026
 - val_f1: 0.9941
Epoch 146/300
 - 13s - loss: 0.0026 - val_loss: 0.0022
 - val_f1: 0.9950
Epoch 147/300
 - 13s - loss: 0.0024 - val_loss: 0.0025
 - val_f1: 0.9943
Epoch 148/300
 - 13s - loss: 0.0024 - val_loss: 0.0022
 - val_f1: 0.9954
Epoch 149/300
 - 13s - loss: 0.0024 - val_loss: 0.0022
 - val_f1: 0.9953
Epoch 150/300
 - 13s - loss: 0.0024 - val_loss: 0.0025
 - val_f1: 0.9947
Epoch 151/300
 - 13s - loss: 0.0025 - val_loss: 0.0022
2019-12-27 00:36:04,620 [INFO] epoch = 150. Intermediate model saved to results_selected_models/selected_ids17_lstm_deep_rep1/_model_epoch_150.pickle
 - val_f1: 0.9951
Epoch 152/300
 - 13s - loss: 0.0025 - val_loss: 0.0028
 - val_f1: 0.9942
Epoch 153/300
 - 13s - loss: 0.0024 - val_loss: 0.0021
 - val_f1: 0.9953
Epoch 154/300
 - 13s - loss: 0.0024 - val_loss: 0.0023
 - val_f1: 0.9948
Epoch 155/300
 - 13s - loss: 0.0024 - val_loss: 0.0079
 - val_f1: 0.9817
Epoch 156/300
 - 13s - loss: 0.0025 - val_loss: 0.0022
 - val_f1: 0.9948
Epoch 157/300
 - 13s - loss: 0.0024 - val_loss: 0.0023
 - val_f1: 0.9949
Epoch 158/300
 - 13s - loss: 0.0024 - val_loss: 0.0026
 - val_f1: 0.9945
Epoch 159/300
 - 13s - loss: 0.0025 - val_loss: 0.0023
 - val_f1: 0.9953
Epoch 160/300
 - 13s - loss: 0.0026 - val_loss: 0.0149
 - val_f1: 0.9679
Epoch 161/300
 - 13s - loss: 0.0025 - val_loss: 0.0023
 - val_f1: 0.9951
Epoch 162/300
 - 13s - loss: 0.0024 - val_loss: 0.0021
 - val_f1: 0.9950
Epoch 163/300
 - 13s - loss: 0.0024 - val_loss: 0.0023
 - val_f1: 0.9952
Epoch 164/300
 - 13s - loss: 0.0024 - val_loss: 0.0027
 - val_f1: 0.9941
Epoch 165/300
 - 13s - loss: 0.0024 - val_loss: 0.0023
 - val_f1: 0.9953
Epoch 166/300
 - 13s - loss: 0.0024 - val_loss: 0.0026
 - val_f1: 0.9931
Epoch 167/300
 - 13s - loss: 0.0024 - val_loss: 0.0026
 - val_f1: 0.9946
Epoch 168/300
 - 13s - loss: 0.0025 - val_loss: 0.0029
 - val_f1: 0.9935
Epoch 169/300
 - 13s - loss: 0.0023 - val_loss: 0.0489
 - val_f1: 0.9443
Epoch 170/300
 - 13s - loss: 0.0023 - val_loss: 0.0042
 - val_f1: 0.9930
Epoch 171/300
 - 13s - loss: 0.0024 - val_loss: 0.0022
 - val_f1: 0.9949
Epoch 172/300
 - 13s - loss: 0.0027 - val_loss: 0.0024
 - val_f1: 0.9936
Epoch 173/300
 - 13s - loss: 0.0026 - val_loss: 0.0024
 - val_f1: 0.9945
Epoch 174/300
 - 13s - loss: 0.0026 - val_loss: 0.0033
 - val_f1: 0.9921
Epoch 175/300
 - 13s - loss: 0.0025 - val_loss: 0.0024
 - val_f1: 0.9948
Epoch 176/300
 - 13s - loss: 0.0025 - val_loss: 0.0024
 - val_f1: 0.9935
Epoch 177/300
 - 13s - loss: 0.0025 - val_loss: 0.0022
 - val_f1: 0.9948
Epoch 178/300
 - 13s - loss: 0.0028 - val_loss: 0.0024
 - val_f1: 0.9937
Epoch 179/300
 - 13s - loss: 0.0024 - val_loss: 0.0025
 - val_f1: 0.9944
Epoch 180/300
 - 13s - loss: 0.0024 - val_loss: 0.0025
 - val_f1: 0.9947
Epoch 181/300
 - 13s - loss: 0.0024 - val_loss: 0.0025
2019-12-27 00:43:56,078 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_ids17_lstm_deep_rep1/_model_epoch_180.pickle
 - val_f1: 0.9945
Epoch 182/300
 - 13s - loss: 0.0025 - val_loss: 0.0068
 - val_f1: 0.9893
Epoch 183/300
 - 13s - loss: 0.0027 - val_loss: 0.0023
 - val_f1: 0.9947
Epoch 184/300
 - 13s - loss: 0.0024 - val_loss: 0.0027
 - val_f1: 0.9935
Epoch 185/300
 - 13s - loss: 0.0024 - val_loss: 0.0022
 - val_f1: 0.9942
Epoch 186/300
 - 13s - loss: 0.0024 - val_loss: 0.0033
 - val_f1: 0.9929
Epoch 187/300
 - 13s - loss: 0.0024 - val_loss: 0.0028
 - val_f1: 0.9927
Epoch 188/300
 - 13s - loss: 0.0023 - val_loss: 0.0024
 - val_f1: 0.9946
Epoch 189/300
 - 13s - loss: 0.0023 - val_loss: 0.0021
 - val_f1: 0.9949
Epoch 190/300
 - 13s - loss: 0.0024 - val_loss: 0.0022
 - val_f1: 0.9950
Epoch 191/300
 - 13s - loss: 0.0023 - val_loss: 0.0023
 - val_f1: 0.9951
Epoch 192/300
 - 13s - loss: 0.0023 - val_loss: 0.0021
 - val_f1: 0.9957
Epoch 193/300
 - 13s - loss: 0.0024 - val_loss: 0.0026
 - val_f1: 0.9947
Epoch 194/300
 - 13s - loss: 0.0023 - val_loss: 0.0026
 - val_f1: 0.9931
Epoch 195/300
 - 13s - loss: 0.0023 - val_loss: 0.0021
 - val_f1: 0.9955
Epoch 196/300
 - 13s - loss: 0.0023 - val_loss: 0.0022
 - val_f1: 0.9954
Epoch 197/300
 - 13s - loss: 0.0022 - val_loss: 0.0043
 - val_f1: 0.9916
Epoch 198/300
 - 13s - loss: 0.0024 - val_loss: 0.0021
 - val_f1: 0.9950
Epoch 199/300
 - 13s - loss: 0.0023 - val_loss: 0.0052
 - val_f1: 0.9870
Epoch 200/300
 - 13s - loss: 0.0023 - val_loss: 0.0021
 - val_f1: 0.9951
Epoch 201/300
 - 13s - loss: 0.0023 - val_loss: 0.0022
 - val_f1: 0.9948
Epoch 202/300
 - 13s - loss: 0.0023 - val_loss: 0.0065
 - val_f1: 0.9923
Epoch 203/300
 - 13s - loss: 0.0023 - val_loss: 0.0022
 - val_f1: 0.9951
Epoch 204/300
 - 13s - loss: 0.0023 - val_loss: 0.0021
 - val_f1: 0.9947
Epoch 205/300
 - 13s - loss: 0.0023 - val_loss: 0.0021
 - val_f1: 0.9950
Epoch 206/300
 - 13s - loss: 0.0023 - val_loss: 0.0026
 - val_f1: 0.9945
Epoch 207/300
 - 13s - loss: 0.0023 - val_loss: 0.0025
 - val_f1: 0.9949
Epoch 208/300
 - 13s - loss: 0.0022 - val_loss: 0.0023
 - val_f1: 0.9948
Epoch 209/300
 - 13s - loss: 0.0023 - val_loss: 0.0027
 - val_f1: 0.9929
Epoch 210/300
 - 13s - loss: 0.0023 - val_loss: 0.0023
 - val_f1: 0.9951
Epoch 211/300
 - 13s - loss: 0.0022 - val_loss: 0.0022
2019-12-27 00:51:47,520 [INFO] epoch = 210. Intermediate model saved to results_selected_models/selected_ids17_lstm_deep_rep1/_model_epoch_210.pickle
 - val_f1: 0.9955
Epoch 212/300
 - 13s - loss: 0.0022 - val_loss: 0.0021
 - val_f1: 0.9958
Epoch 213/300
 - 13s - loss: 0.0023 - val_loss: 0.0021
 - val_f1: 0.9955
Epoch 214/300
 - 13s - loss: 0.0022 - val_loss: 0.0025
 - val_f1: 0.9931
Epoch 215/300
 - 13s - loss: 0.0022 - val_loss: 0.0022
 - val_f1: 0.9953
Epoch 216/300
 - 13s - loss: 0.0023 - val_loss: 0.0030
 - val_f1: 0.9932
Epoch 217/300
 - 13s - loss: 0.0023 - val_loss: 0.0040
 - val_f1: 0.9921
Epoch 218/300
 - 13s - loss: 0.0023 - val_loss: 0.0020
 - val_f1: 0.9957
Epoch 219/300
 - 13s - loss: 0.0022 - val_loss: 0.0021
 - val_f1: 0.9957
Epoch 220/300
 - 13s - loss: 0.0022 - val_loss: 0.0023
 - val_f1: 0.9948
Epoch 221/300
 - 13s - loss: 0.0022 - val_loss: 0.0038
 - val_f1: 0.9909
Epoch 222/300
 - 13s - loss: 0.0023 - val_loss: 0.0040
 - val_f1: 0.9916
Epoch 223/300
 - 13s - loss: 0.0023 - val_loss: 0.0083
 - val_f1: 0.9836
Epoch 224/300
 - 13s - loss: 0.0023 - val_loss: 0.0024
 - val_f1: 0.9950
Epoch 225/300
 - 13s - loss: 0.0024 - val_loss: 0.0021
 - val_f1: 0.9955
Epoch 226/300
 - 13s - loss: 0.0022 - val_loss: 0.0021
 - val_f1: 0.9953
Epoch 227/300
 - 13s - loss: 0.0022 - val_loss: 0.0022
 - val_f1: 0.9955
Epoch 228/300
 - 13s - loss: 0.0022 - val_loss: 0.0020
 - val_f1: 0.9957
Epoch 229/300
 - 13s - loss: 0.0023 - val_loss: 0.0021
 - val_f1: 0.9955
Epoch 230/300
 - 13s - loss: 0.0022 - val_loss: 0.0021
 - val_f1: 0.9952
Epoch 231/300
 - 13s - loss: 0.0024 - val_loss: 0.0022
 - val_f1: 0.9955
Epoch 232/300
 - 13s - loss: 0.0022 - val_loss: 0.0019
 - val_f1: 0.9960
Epoch 233/300
 - 13s - loss: 0.0022 - val_loss: 0.0022
 - val_f1: 0.9940
Epoch 234/300
 - 13s - loss: 0.0022 - val_loss: 0.0020
 - val_f1: 0.9953
Epoch 235/300
 - 13s - loss: 0.0022 - val_loss: 0.0025
 - val_f1: 0.9936
Epoch 236/300
 - 13s - loss: 0.0022 - val_loss: 0.0036
 - val_f1: 0.9925
Epoch 237/300
 - 13s - loss: 0.0024 - val_loss: 0.0023
 - val_f1: 0.9951
Epoch 238/300
 - 13s - loss: 0.0022 - val_loss: 0.0024
 - val_f1: 0.9949
Epoch 239/300
 - 13s - loss: 0.0023 - val_loss: 0.0019
 - val_f1: 0.9960
Epoch 240/300
 - 13s - loss: 0.0035 - val_loss: 0.0022
 - val_f1: 0.9954
Epoch 241/300
 - 13s - loss: 0.0025 - val_loss: 0.0353
2019-12-27 00:59:39,181 [INFO] epoch = 240. Intermediate model saved to results_selected_models/selected_ids17_lstm_deep_rep1/_model_epoch_240.pickle
 - val_f1: 0.9548
Epoch 242/300
 - 13s - loss: 0.0024 - val_loss: 0.0096
 - val_f1: 0.9721
Epoch 243/300
 - 13s - loss: 0.0024 - val_loss: 0.0029
 - val_f1: 0.9931
Epoch 244/300
 - 13s - loss: 0.0023 - val_loss: 0.0021
 - val_f1: 0.9956
Epoch 245/300
 - 13s - loss: 0.0022 - val_loss: 0.0024
 - val_f1: 0.9954
Epoch 246/300
 - 13s - loss: 0.0021 - val_loss: 0.0025
 - val_f1: 0.9950
Epoch 247/300
 - 13s - loss: 0.0022 - val_loss: 0.0023
 - val_f1: 0.9949
Epoch 248/300
 - 13s - loss: 0.0021 - val_loss: 0.0021
 - val_f1: 0.9956
Epoch 249/300
 - 13s - loss: 0.0021 - val_loss: 0.0022
 - val_f1: 0.9954
Epoch 250/300
 - 13s - loss: 0.0021 - val_loss: 0.0037
 - val_f1: 0.9928
Epoch 251/300
 - 13s - loss: 0.0022 - val_loss: 0.0024
 - val_f1: 0.9948
Epoch 252/300
 - 13s - loss: 0.0021 - val_loss: 0.0022
 - val_f1: 0.9955
Epoch 253/300
 - 13s - loss: 0.0021 - val_loss: 0.0021
 - val_f1: 0.9955
Epoch 254/300
 - 13s - loss: 0.0021 - val_loss: 0.0025
 - val_f1: 0.9935
Epoch 255/300
 - 13s - loss: 0.0021 - val_loss: 0.0049
 - val_f1: 0.9875
Epoch 256/300
 - 13s - loss: 0.0021 - val_loss: 0.0021
 - val_f1: 0.9955
Epoch 257/300
 - 13s - loss: 0.0021 - val_loss: 0.0020
 - val_f1: 0.9960
Epoch 258/300
 - 13s - loss: 0.0022 - val_loss: 0.0033
 - val_f1: 0.9932
Epoch 259/300
 - 13s - loss: 0.0021 - val_loss: 0.0020
 - val_f1: 0.9956
Epoch 260/300
 - 13s - loss: 0.0021 - val_loss: 0.0019
 - val_f1: 0.9961
Epoch 261/300
 - 13s - loss: 0.0021 - val_loss: 0.0027
 - val_f1: 0.9949
Epoch 262/300
 - 13s - loss: 0.0022 - val_loss: 0.0022
 - val_f1: 0.9952
Epoch 263/300
 - 13s - loss: 0.0021 - val_loss: 0.0021
 - val_f1: 0.9952
Epoch 264/300
 - 13s - loss: 0.0023 - val_loss: 0.0031
 - val_f1: 0.9937
Epoch 265/300
 - 13s - loss: 0.0021 - val_loss: 0.0021
 - val_f1: 0.9957
Epoch 266/300
 - 13s - loss: 0.0024 - val_loss: 0.0021
 - val_f1: 0.9940
Epoch 267/300
 - 13s - loss: 0.0021 - val_loss: 0.0027
 - val_f1: 0.9943
Epoch 268/300
 - 13s - loss: 0.0021 - val_loss: 0.0021
 - val_f1: 0.9947
Epoch 269/300
 - 13s - loss: 0.0021 - val_loss: 0.0020
 - val_f1: 0.9955
Epoch 270/300
 - 13s - loss: 0.0021 - val_loss: 0.0022
 - val_f1: 0.9951
Epoch 271/300
 - 13s - loss: 0.0021 - val_loss: 0.0026
2019-12-27 01:07:30,641 [INFO] epoch = 270. Intermediate model saved to results_selected_models/selected_ids17_lstm_deep_rep1/_model_epoch_270.pickle
 - val_f1: 0.9947
Epoch 272/300
 - 13s - loss: 0.0022 - val_loss: 0.0023
 - val_f1: 0.9952
Epoch 273/300
 - 13s - loss: 0.0021 - val_loss: 0.0020
 - val_f1: 0.9958
Epoch 274/300
 - 13s - loss: 0.0021 - val_loss: 0.0024
 - val_f1: 0.9940
Epoch 275/300
 - 13s - loss: 0.0020 - val_loss: 0.0021
 - val_f1: 0.9956
Epoch 276/300
 - 13s - loss: 0.0021 - val_loss: 0.0025
 - val_f1: 0.9947
Epoch 277/300
 - 13s - loss: 0.0022 - val_loss: 0.0033
 - val_f1: 0.9928
Epoch 278/300
 - 13s - loss: 0.0021 - val_loss: 0.0025
 - val_f1: 0.9956
Epoch 279/300
 - 13s - loss: 0.0023 - val_loss: 0.0021
 - val_f1: 0.9956
Epoch 280/300
 - 13s - loss: 0.0021 - val_loss: 0.0038
 - val_f1: 0.9927
Epoch 281/300
 - 13s - loss: 0.0022 - val_loss: 0.0020
 - val_f1: 0.9957
Epoch 282/300
 - 13s - loss: 0.0020 - val_loss: 0.0021
 - val_f1: 0.9956
Epoch 283/300
 - 13s - loss: 0.0028 - val_loss: 0.0130
 - val_f1: 0.9703
Epoch 284/300
 - 13s - loss: 0.0023 - val_loss: 0.0023
 - val_f1: 0.9951
Epoch 285/300
 - 13s - loss: 0.0022 - val_loss: 0.0021
 - val_f1: 0.9954
Epoch 286/300
 - 13s - loss: 0.0022 - val_loss: 0.0021
 - val_f1: 0.9954
Epoch 287/300
 - 13s - loss: 0.0022 - val_loss: 0.0020
 - val_f1: 0.9958
Epoch 288/300
 - 13s - loss: 0.0022 - val_loss: 0.0023
 - val_f1: 0.9950
Epoch 289/300
 - 13s - loss: 0.0021 - val_loss: 0.0021
 - val_f1: 0.9959
Epoch 290/300
 - 13s - loss: 0.0022 - val_loss: 0.0025
 - val_f1: 0.9947
Epoch 291/300
 - 13s - loss: 0.0021 - val_loss: 0.0024
 - val_f1: 0.9951
Epoch 292/300
 - 13s - loss: 0.0022 - val_loss: 0.0025
 - val_f1: 0.9943
Epoch 293/300
 - 13s - loss: 0.0022 - val_loss: 0.0027
 - val_f1: 0.9934
Epoch 294/300
 - 13s - loss: 0.0023 - val_loss: 0.0028
 - val_f1: 0.9935
Epoch 295/300
 - 13s - loss: 0.0022 - val_loss: 0.0023
 - val_f1: 0.9951
Epoch 296/300
 - 13s - loss: 0.0021 - val_loss: 0.0033
 - val_f1: 0.9945
Epoch 297/300
 - 13s - loss: 0.0023 - val_loss: 0.0029
 - val_f1: 0.9941
Epoch 298/300
 - 13s - loss: 0.0023 - val_loss: 0.0027
 - val_f1: 0.9949
Epoch 299/300
 - 13s - loss: 0.0021 - val_loss: 0.0024
 - val_f1: 0.9950
Epoch 300/300
 - 13s - loss: 0.0021 - val_loss: 0.0022
2019-12-27 01:15:09,378 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-27 01:15:20,080 [INFO] Last epoch loss evaluation: train_loss = 0.001560, val_loss = 0.001910
2019-12-27 01:15:20,080 [INFO] Training complete. time_to_train = 4736.58 sec, 78.94 min
2019-12-27 01:15:20,085 [INFO] Model saved to results_selected_models/selected_ids17_lstm_deep_rep1/best_model.pickle
2019-12-27 01:15:20,088 [INFO] Training history saved to: results_selected_models/selected_ids17_lstm_deep_rep1/training_error_history.csv
2019-12-27 01:15:20,228 [INFO] Plot saved to: results_selected_models/selected_ids17_lstm_deep_rep1/training_error_history.png
2019-12-27 01:15:20,359 [INFO] Plot saved to: results_selected_models/selected_ids17_lstm_deep_rep1/training_f1_history.png
2019-12-27 01:15:20,359 [INFO] Making predictions on training, validation, testing data
2019-12-27 01:15:33,077 [INFO] Evaluating predictions (results)
2019-12-27 01:15:43,462 [INFO] Dataset: Testing. Classification report below
2019-12-27 01:15:43,462 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       1.00      1.00      1.00    454243
                   Bot       0.93      0.38      0.54       391
                  DDoS       1.00      1.00      1.00     25604
         DoS GoldenEye       0.99      0.99      0.99      2058
              DoS Hulk       0.98      0.99      0.99     46023
      DoS Slowhttptest       0.92      0.98      0.95      1100
         DoS slowloris       0.99      0.99      0.99      1159
           FTP-Patator       1.00      1.00      1.00      1586
              PortScan       0.99      1.00      1.00     31761
           SSH-Patator       0.95      0.97      0.96      1179
Web Attack Brute Force       0.65      0.78      0.71       302
        Web Attack XSS       0.00      0.00      0.00       130

             micro avg       1.00      1.00      1.00    565536
             macro avg       0.87      0.84      0.84    565536
          weighted avg       1.00      1.00      1.00    565536

2019-12-27 01:15:43,462 [INFO] Overall accuracy (micro avg): 0.9962371979856278
/home/hasitha/sunanda/ids_experiments/utility.py:334: RuntimeWarning: invalid value encountered in true_divide
  F1 = (2 * PPV * TPR) / (PPV + TPR)  # F1 score
2019-12-27 01:15:55,259 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9962         0.9962                       0.9962                0.0003                   0.0038  0.9962
1     Macro avg        0.9994         0.8667                       0.8400                0.0008                   0.1600  0.8432
2  Weighted avg        0.9971         0.9960                       0.9962                0.0061                   0.0038  0.9961
2019-12-27 01:16:05,657 [INFO] Dataset: Validation. Classification report below
2019-12-27 01:16:05,657 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       1.00      1.00      1.00    454243
                   Bot       0.92      0.34      0.49       391
                  DDoS       1.00      1.00      1.00     25605
         DoS GoldenEye       0.99      0.99      0.99      2059
              DoS Hulk       0.98      0.99      0.99     46023
      DoS Slowhttptest       0.91      0.98      0.95      1099
         DoS slowloris       0.99      0.98      0.99      1159
           FTP-Patator       1.00      1.00      1.00      1587
              PortScan       0.99      1.00      1.00     31758
           SSH-Patator       0.95      0.97      0.96      1180
Web Attack Brute Force       0.63      0.77      0.70       301
        Web Attack XSS       0.80      0.03      0.06       131

             micro avg       1.00      1.00      1.00    565536
             macro avg       0.93      0.84      0.84    565536
          weighted avg       1.00      1.00      1.00    565536

2019-12-27 01:16:05,657 [INFO] Overall accuracy (micro avg): 0.9963503649635036
2019-12-27 01:16:17,485 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9964         0.9964                       0.9964                0.0003                   0.0036  0.9964
1     Macro avg        0.9994         0.9313                       0.8371                0.0008                   0.1629  0.8424
2  Weighted avg        0.9972         0.9963                       0.9964                0.0056                   0.0036  0.9962
2019-12-27 01:16:51,700 [INFO] Dataset: Training. Classification report below
2019-12-27 01:16:51,700 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       1.00      1.00      1.00   1362781
                   Bot       0.98      0.41      0.57      1174
                  DDoS       1.00      1.00      1.00     76815
         DoS GoldenEye       0.99      0.99      0.99      6176
              DoS Hulk       0.99      0.99      0.99    138073
      DoS Slowhttptest       0.95      0.99      0.97      3300
         DoS slowloris       0.99      0.99      0.99      3478
           FTP-Patator       1.00      1.00      1.00      4761
              PortScan       0.99      1.00      1.00     95281
           SSH-Patator       0.97      0.99      0.98      3538
Web Attack Brute Force       0.71      0.87      0.78       904
        Web Attack XSS       0.94      0.04      0.08       391

             micro avg       1.00      1.00      1.00   1696672
             macro avg       0.96      0.86      0.86   1696672
          weighted avg       1.00      1.00      1.00   1696672

2019-12-27 01:16:51,700 [INFO] Overall accuracy (micro avg): 0.9969510901340978
2019-12-27 01:17:30,569 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9970         0.9970                       0.9970                0.0003                   0.0030  0.9970
1     Macro avg        0.9995         0.9589                       0.8551                0.0006                   0.1449  0.8620
2  Weighted avg        0.9976         0.9970                       0.9970                0.0047                   0.0030  0.9968
2019-12-27 01:17:30,594 [INFO] Results saved to: results_selected_models/selected_ids17_lstm_deep_rep1/selected_ids17_lstm_deep_rep1_results.xlsx
2019-12-27 01:17:30,600 [INFO] ================= Finished running experiment no. 1 ================= 

2019-12-27 01:17:30,672 [INFO] Created directory: results_selected_models/selected_ids17_lstm_deep_rep2
2019-12-27 01:17:30,673 [INFO] Initialized logging. log_filename = results_selected_models/selected_ids17_lstm_deep_rep2/run_log.log
2019-12-27 01:17:30,673 [INFO] ================= Running experiment no. 2  ================= 

2019-12-27 01:17:30,673 [INFO] Experiment parameters given below
2019-12-27 01:17:30,673 [INFO] 
{'experiment_num': 2, 'results_dir': 'results_selected_models/selected_ids17_lstm_deep_rep2', 'model': 'lstm', 'model_type': 'classifier', 'normal_label': 'BENIGN', 'lstm_time_steps': 32, 'lstm_layer_units': [64, 32], 'lstm_layer_activations': ['relu', 'relu'], 'lstm_layer_dropout_rates': [0.2, 0.2], 'batch_size': 256, 'epochs': 300, 'early_stop_patience': 50, 'dataset_dir': '../Datasets/full_datasets/ids2017', 'description': 'selected_ids17_lstm_deep_rep2'}
2019-12-27 01:17:30,673 [INFO] Created tensorboard log directory: results_selected_models/selected_ids17_lstm_deep_rep2/tf_logs_run_2019_12_27-01_17_30
2019-12-27 01:17:30,673 [INFO] Loading datsets from: ../Datasets/full_datasets/ids2017
2019-12-27 01:17:30,673 [INFO] Reading X, y files
2019-12-27 01:17:30,673 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_train.h5
2019-12-27 01:17:34,860 [INFO] Reading complete. time_to_read=4.19 seconds
2019-12-27 01:17:34,861 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_val.h5
2019-12-27 01:17:36,291 [INFO] Reading complete. time_to_read=1.43 seconds
2019-12-27 01:17:36,291 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_test.h5
2019-12-27 01:17:37,721 [INFO] Reading complete. time_to_read=1.43 seconds
2019-12-27 01:17:37,721 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_train.h5
2019-12-27 01:17:37,957 [INFO] Reading complete. time_to_read=0.24 seconds
2019-12-27 01:17:37,958 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_val.h5
2019-12-27 01:17:38,032 [INFO] Reading complete. time_to_read=0.07 seconds
2019-12-27 01:17:38,032 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_test.h5
2019-12-27 01:17:38,107 [INFO] Reading complete. time_to_read=0.07 seconds
2019-12-27 01:17:40,616 [INFO] Preparing flow sequences
2019-12-27 01:18:02,619 [INFO] Extracting flows complete. time_taken = 22.00 sec
2019-12-27 01:18:03,761 [INFO] Initializing model
2019-12-27 01:18:04,209 [INFO] _________________________________________________________________
2019-12-27 01:18:04,209 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-27 01:18:04,209 [INFO] =================================================================
2019-12-27 01:18:04,209 [INFO] lstm_13 (LSTM)               (None, 32, 64)            36608     
2019-12-27 01:18:04,209 [INFO] _________________________________________________________________
2019-12-27 01:18:04,209 [INFO] batch_normalization_13 (Batc (None, 32, 64)            256       
2019-12-27 01:18:04,209 [INFO] _________________________________________________________________
2019-12-27 01:18:04,209 [INFO] dropout_13 (Dropout)         (None, 32, 64)            0         
2019-12-27 01:18:04,209 [INFO] _________________________________________________________________
2019-12-27 01:18:04,209 [INFO] lstm_14 (LSTM)               (None, 32, 32)            12416     
2019-12-27 01:18:04,210 [INFO] _________________________________________________________________
2019-12-27 01:18:04,210 [INFO] batch_normalization_14 (Batc (None, 32, 32)            128       
2019-12-27 01:18:04,210 [INFO] _________________________________________________________________
2019-12-27 01:18:04,210 [INFO] dropout_14 (Dropout)         (None, 32, 32)            0         
2019-12-27 01:18:04,210 [INFO] _________________________________________________________________
2019-12-27 01:18:04,210 [INFO] time_distributed_7 (TimeDist (None, 32, 12)            396       
2019-12-27 01:18:04,210 [INFO] =================================================================
2019-12-27 01:18:04,210 [INFO] Total params: 49,804
2019-12-27 01:18:04,210 [INFO] Trainable params: 49,612
2019-12-27 01:18:04,210 [INFO] Non-trainable params: 192
2019-12-27 01:18:04,210 [INFO] _________________________________________________________________
2019-12-27 01:18:04,210 [INFO] Training model
 - val_f1: 0.9958
Train on 53021 samples, validate on 17673 samples
Epoch 1/300
 - 14s - loss: 0.1826 - val_loss: 0.0855
 - val_f1: 0.8275
Epoch 2/300
 - 13s - loss: 0.0776 - val_loss: 0.0769
 - val_f1: 0.8259
Epoch 3/300
 - 13s - loss: 0.0730 - val_loss: 0.0695
 - val_f1: 0.8270
Epoch 4/300
 - 13s - loss: 0.0633 - val_loss: 0.0610
 - val_f1: 0.8342
Epoch 5/300
 - 13s - loss: 0.0565 - val_loss: 0.0535
 - val_f1: 0.8440
Epoch 6/300
 - 13s - loss: 0.0515 - val_loss: 0.0497
 - val_f1: 0.8506
Epoch 7/300
 - 13s - loss: 0.0470 - val_loss: 0.0466
 - val_f1: 0.8562
Epoch 8/300
 - 13s - loss: 0.0432 - val_loss: 0.0433
 - val_f1: 0.8648
Epoch 9/300
 - 13s - loss: 0.0407 - val_loss: 0.0368
 - val_f1: 0.8999
Epoch 10/300
 - 13s - loss: 0.0378 - val_loss: 0.0340
 - val_f1: 0.9120
Epoch 11/300
 - 13s - loss: 0.0352 - val_loss: 0.0299
 - val_f1: 0.9243
Epoch 12/300
 - 13s - loss: 0.0331 - val_loss: 0.0269
 - val_f1: 0.9362
Epoch 13/300
 - 13s - loss: 0.0313 - val_loss: 0.0249
 - val_f1: 0.9429
Epoch 14/300
 - 13s - loss: 0.0294 - val_loss: 0.0235
 - val_f1: 0.9467
Epoch 15/300
 - 13s - loss: 0.0278 - val_loss: 0.0223
 - val_f1: 0.9483
Epoch 16/300
 - 13s - loss: 0.0263 - val_loss: 0.0210
 - val_f1: 0.9527
Epoch 17/300
 - 13s - loss: 0.0250 - val_loss: 0.0200
 - val_f1: 0.9553
Epoch 18/300
 - 13s - loss: 0.0239 - val_loss: 0.0188
 - val_f1: 0.9584
Epoch 19/300
 - 13s - loss: 0.0227 - val_loss: 0.0175
 - val_f1: 0.9606
Epoch 20/300
 - 13s - loss: 0.0214 - val_loss: 0.0165
 - val_f1: 0.9621
Epoch 21/300
 - 13s - loss: 0.0205 - val_loss: 0.0158
 - val_f1: 0.9628
Epoch 22/300
 - 13s - loss: 0.0196 - val_loss: 0.0152
 - val_f1: 0.9633
Epoch 23/300
 - 13s - loss: 0.0188 - val_loss: 0.0154
 - val_f1: 0.9623
Epoch 24/300
 - 13s - loss: 0.0181 - val_loss: 0.0158
 - val_f1: 0.9618
Epoch 25/300
 - 13s - loss: 0.0174 - val_loss: 0.0141
 - val_f1: 0.9640
Epoch 26/300
 - 13s - loss: 0.0168 - val_loss: 0.0133
 - val_f1: 0.9654
Epoch 27/300
 - 13s - loss: 0.0163 - val_loss: 0.0156
 - val_f1: 0.9617
Epoch 28/300
 - 13s - loss: 0.0158 - val_loss: 0.0132
 - val_f1: 0.9640
Epoch 29/300
 - 13s - loss: 0.0151 - val_loss: 0.0121
 - val_f1: 0.9665
Epoch 30/300
 - 13s - loss: 0.0148 - val_loss: 0.0115
 - val_f1: 0.9691
Epoch 31/300
 - 13s - loss: 0.0142 - val_loss: 0.0110
/home/hasitha/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2019-12-27 01:26:18,277 [INFO] epoch = 30. Intermediate model saved to results_selected_models/selected_ids17_lstm_deep_rep2/_model_epoch_30.pickle
 - val_f1: 0.9713
Epoch 32/300
 - 13s - loss: 0.0137 - val_loss: 0.0106
 - val_f1: 0.9716
Epoch 33/300
 - 13s - loss: 0.0133 - val_loss: 0.0110
 - val_f1: 0.9714
Epoch 34/300
 - 13s - loss: 0.0130 - val_loss: 0.0100
 - val_f1: 0.9734
Epoch 35/300
 - 13s - loss: 0.0126 - val_loss: 0.0098
 - val_f1: 0.9737
Epoch 36/300
 - 13s - loss: 0.0121 - val_loss: 0.0097
 - val_f1: 0.9738
Epoch 37/300
 - 13s - loss: 0.0118 - val_loss: 0.0091
 - val_f1: 0.9764
Epoch 38/300
 - 13s - loss: 0.0115 - val_loss: 0.0094
 - val_f1: 0.9753
Epoch 39/300
 - 13s - loss: 0.0113 - val_loss: 0.0087
 - val_f1: 0.9771
Epoch 40/300
 - 13s - loss: 0.0110 - val_loss: 0.0098
 - val_f1: 0.9743
Epoch 41/300
 - 13s - loss: 0.0108 - val_loss: 0.0087
 - val_f1: 0.9771
Epoch 42/300
 - 13s - loss: 0.0106 - val_loss: 0.0084
 - val_f1: 0.9781
Epoch 43/300
 - 13s - loss: 0.0104 - val_loss: 0.0081
 - val_f1: 0.9779
Epoch 44/300
 - 13s - loss: 0.0102 - val_loss: 0.0083
 - val_f1: 0.9785
Epoch 45/300
 - 13s - loss: 0.0100 - val_loss: 0.0082
 - val_f1: 0.9777
Epoch 46/300
 - 13s - loss: 0.0097 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 47/300
 - 13s - loss: 0.0095 - val_loss: 0.0081
 - val_f1: 0.9783
Epoch 48/300
 - 13s - loss: 0.0095 - val_loss: 0.0077
 - val_f1: 0.9783
Epoch 49/300
 - 13s - loss: 0.0093 - val_loss: 0.0075
 - val_f1: 0.9795
Epoch 50/300
 - 13s - loss: 0.0091 - val_loss: 0.0082
 - val_f1: 0.9779
Epoch 51/300
 - 13s - loss: 0.0088 - val_loss: 0.0072
 - val_f1: 0.9804
Epoch 52/300
 - 13s - loss: 0.0085 - val_loss: 0.0064
 - val_f1: 0.9827
Epoch 53/300
 - 13s - loss: 0.0077 - val_loss: 0.0074
 - val_f1: 0.9792
Epoch 54/300
 - 13s - loss: 0.0069 - val_loss: 0.0063
 - val_f1: 0.9826
Epoch 55/300
 - 13s - loss: 0.0063 - val_loss: 0.0050
 - val_f1: 0.9881
Epoch 56/300
 - 13s - loss: 0.0062 - val_loss: 0.0045
 - val_f1: 0.9897
Epoch 57/300
 - 13s - loss: 0.0062 - val_loss: 0.0135
 - val_f1: 0.9787
Epoch 58/300
 - 13s - loss: 0.0082 - val_loss: 0.0072
 - val_f1: 0.9802
Epoch 59/300
 - 13s - loss: 0.0078 - val_loss: 0.0060
 - val_f1: 0.9834
Epoch 60/300
 - 13s - loss: 0.0072 - val_loss: 0.0061
 - val_f1: 0.9828
Epoch 61/300
 - 13s - loss: 0.0063 - val_loss: 0.0043
2019-12-27 01:34:11,084 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_ids17_lstm_deep_rep2/_model_epoch_60.pickle
 - val_f1: 0.9900
Epoch 62/300
 - 13s - loss: 0.0082 - val_loss: 0.0065
 - val_f1: 0.9815
Epoch 63/300
 - 13s - loss: 0.0076 - val_loss: 0.0063
 - val_f1: 0.9838
Epoch 64/300
 - 13s - loss: 0.0074 - val_loss: 0.0060
 - val_f1: 0.9841
Epoch 65/300
 - 13s - loss: 0.0070 - val_loss: 0.0065
 - val_f1: 0.9854
Epoch 66/300
 - 13s - loss: 0.0063 - val_loss: 0.0050
 - val_f1: 0.9885
Epoch 67/300
 - 13s - loss: 0.0052 - val_loss: 0.0035
 - val_f1: 0.9926
Epoch 68/300
 - 13s - loss: 0.0047 - val_loss: 0.0037
 - val_f1: 0.9915
Epoch 69/300
 - 13s - loss: 0.0049 - val_loss: 0.0042
 - val_f1: 0.9900
Epoch 70/300
 - 13s - loss: 0.0051 - val_loss: 0.0036
 - val_f1: 0.9918
Epoch 71/300
 - 13s - loss: 0.0049 - val_loss: 0.0036
 - val_f1: 0.9915
Epoch 72/300
 - 13s - loss: 0.0048 - val_loss: 0.0035
 - val_f1: 0.9926
Epoch 73/300
 - 13s - loss: 0.0048 - val_loss: 0.0038
 - val_f1: 0.9904
Epoch 74/300
 - 13s - loss: 0.0047 - val_loss: 0.0048
 - val_f1: 0.9892
Epoch 75/300
 - 13s - loss: 0.0048 - val_loss: 0.0094
 - val_f1: 0.9762
Epoch 76/300
 - 13s - loss: 0.0070 - val_loss: 0.0068
 - val_f1: 0.9824
Epoch 77/300
 - 13s - loss: 0.0070 - val_loss: 0.0082
 - val_f1: 0.9786
Epoch 78/300
 - 13s - loss: 0.0068 - val_loss: 0.0085
 - val_f1: 0.9739
Epoch 79/300
 - 13s - loss: 0.0068 - val_loss: 0.0084
 - val_f1: 0.9739
Epoch 80/300
 - 13s - loss: 0.0048 - val_loss: 0.0036
 - val_f1: 0.9927
Epoch 81/300
 - 13s - loss: 0.0043 - val_loss: 0.0039
 - val_f1: 0.9915
Epoch 82/300
 - 13s - loss: 0.0043 - val_loss: 0.0032
 - val_f1: 0.9921
Epoch 83/300
 - 13s - loss: 0.0044 - val_loss: 0.0045
 - val_f1: 0.9905
Epoch 84/300
 - 13s - loss: 0.0043 - val_loss: 0.0033
 - val_f1: 0.9925
Epoch 85/300
 - 13s - loss: 0.0042 - val_loss: 0.0034
 - val_f1: 0.9925
Epoch 86/300
 - 13s - loss: 0.0046 - val_loss: 0.0034
 - val_f1: 0.9920
Epoch 87/300
 - 13s - loss: 0.0041 - val_loss: 0.0030
 - val_f1: 0.9942
Epoch 88/300
 - 13s - loss: 0.0040 - val_loss: 0.0028
 - val_f1: 0.9942
Epoch 89/300
 - 13s - loss: 0.0040 - val_loss: 0.0055
 - val_f1: 0.9879
Epoch 90/300
 - 13s - loss: 0.0040 - val_loss: 0.0038
 - val_f1: 0.9917
Epoch 91/300
 - 13s - loss: 0.0039 - val_loss: 0.0043
2019-12-27 01:42:03,707 [INFO] epoch = 90. Intermediate model saved to results_selected_models/selected_ids17_lstm_deep_rep2/_model_epoch_90.pickle
 - val_f1: 0.9892
Epoch 92/300
 - 13s - loss: 0.0039 - val_loss: 0.0036
 - val_f1: 0.9917
Epoch 93/300
 - 13s - loss: 0.0039 - val_loss: 0.0050
 - val_f1: 0.9900
Epoch 94/300
 - 13s - loss: 0.0039 - val_loss: 0.0035
 - val_f1: 0.9918
Epoch 95/300
 - 13s - loss: 0.0038 - val_loss: 0.0036
 - val_f1: 0.9921
Epoch 96/300
 - 13s - loss: 0.0037 - val_loss: 0.0045
 - val_f1: 0.9904
Epoch 97/300
 - 13s - loss: 0.0036 - val_loss: 0.0030
 - val_f1: 0.9938
Epoch 98/300
 - 13s - loss: 0.0036 - val_loss: 0.0040
 - val_f1: 0.9914
Epoch 99/300
 - 13s - loss: 0.0035 - val_loss: 0.0032
 - val_f1: 0.9935
Epoch 100/300
 - 13s - loss: 0.0034 - val_loss: 0.0036
 - val_f1: 0.9921
Epoch 101/300
 - 13s - loss: 0.0035 - val_loss: 0.0031
 - val_f1: 0.9927
Epoch 102/300
 - 13s - loss: 0.0048 - val_loss: 0.0028
 - val_f1: 0.9943
Epoch 103/300
 - 13s - loss: 0.0039 - val_loss: 0.0031
 - val_f1: 0.9926
Epoch 104/300
 - 13s - loss: 0.0038 - val_loss: 0.0027
 - val_f1: 0.9943
Epoch 105/300
 - 13s - loss: 0.0037 - val_loss: 0.0061
 - val_f1: 0.9808
Epoch 106/300
 - 13s - loss: 0.0037 - val_loss: 0.0032
 - val_f1: 0.9937
Epoch 107/300
 - 13s - loss: 0.0035 - val_loss: 0.0030
 - val_f1: 0.9933
Epoch 108/300
 - 13s - loss: 0.0034 - val_loss: 0.0027
 - val_f1: 0.9936
Epoch 109/300
 - 13s - loss: 0.0033 - val_loss: 0.0027
 - val_f1: 0.9937
Epoch 110/300
 - 13s - loss: 0.0033 - val_loss: 0.0025
 - val_f1: 0.9945
Epoch 111/300
 - 13s - loss: 0.0033 - val_loss: 0.0025
 - val_f1: 0.9944
Epoch 112/300
 - 13s - loss: 0.0033 - val_loss: 0.0027
 - val_f1: 0.9931
Epoch 113/300
 - 13s - loss: 0.0033 - val_loss: 0.0034
 - val_f1: 0.9922
Epoch 114/300
 - 13s - loss: 0.0032 - val_loss: 0.0027
 - val_f1: 0.9941
Epoch 115/300
 - 13s - loss: 0.0045 - val_loss: 0.0030
 - val_f1: 0.9943
Epoch 116/300
 - 13s - loss: 0.0036 - val_loss: 0.0026
 - val_f1: 0.9945
Epoch 117/300
 - 13s - loss: 0.0034 - val_loss: 0.0026
 - val_f1: 0.9945
Epoch 118/300
 - 13s - loss: 0.0033 - val_loss: 0.0035
 - val_f1: 0.9926
Epoch 119/300
 - 13s - loss: 0.0033 - val_loss: 0.0027
 - val_f1: 0.9941
Epoch 120/300
 - 13s - loss: 0.0032 - val_loss: 0.0026
 - val_f1: 0.9941
Epoch 121/300
 - 13s - loss: 0.0032 - val_loss: 0.0037
2019-12-27 01:49:56,490 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_ids17_lstm_deep_rep2/_model_epoch_120.pickle
 - val_f1: 0.9915
Epoch 122/300
 - 13s - loss: 0.0035 - val_loss: 0.0042
 - val_f1: 0.9915
Epoch 123/300
 - 13s - loss: 0.0033 - val_loss: 0.0027
 - val_f1: 0.9942
Epoch 124/300
 - 13s - loss: 0.0031 - val_loss: 0.0025
 - val_f1: 0.9944
Epoch 125/300
 - 13s - loss: 0.0031 - val_loss: 0.0029
 - val_f1: 0.9938
Epoch 126/300
 - 13s - loss: 0.0031 - val_loss: 0.0034
 - val_f1: 0.9930
Epoch 127/300
 - 13s - loss: 0.0031 - val_loss: 0.0024
 - val_f1: 0.9946
Epoch 128/300
 - 13s - loss: 0.0030 - val_loss: 0.0025
 - val_f1: 0.9931
Epoch 129/300
 - 13s - loss: 0.0030 - val_loss: 0.0026
 - val_f1: 0.9945
Epoch 130/300
 - 13s - loss: 0.0036 - val_loss: 0.0025
 - val_f1: 0.9948
Epoch 131/300
 - 13s - loss: 0.0030 - val_loss: 0.0024
 - val_f1: 0.9951
Epoch 132/300
 - 13s - loss: 0.0029 - val_loss: 0.0044
 - val_f1: 0.9907
Epoch 133/300
 - 13s - loss: 0.0029 - val_loss: 0.0027
 - val_f1: 0.9942
Epoch 134/300
 - 13s - loss: 0.0029 - val_loss: 0.0029
 - val_f1: 0.9935
Epoch 135/300
 - 13s - loss: 0.0029 - val_loss: 0.0024
 - val_f1: 0.9948
Epoch 136/300
 - 13s - loss: 0.0029 - val_loss: 0.0023
 - val_f1: 0.9949
Epoch 137/300
 - 13s - loss: 0.0028 - val_loss: 0.0025
 - val_f1: 0.9947
Epoch 138/300
 - 13s - loss: 0.0029 - val_loss: 0.0023
 - val_f1: 0.9950
Epoch 139/300
 - 13s - loss: 0.0028 - val_loss: 0.0023
 - val_f1: 0.9949
Epoch 140/300
 - 13s - loss: 0.0029 - val_loss: 0.0024
 - val_f1: 0.9947
Epoch 141/300
 - 13s - loss: 0.0028 - val_loss: 0.0023
 - val_f1: 0.9951
Epoch 142/300
 - 13s - loss: 0.0028 - val_loss: 0.0033
 - val_f1: 0.9924
Epoch 143/300
 - 13s - loss: 0.0033 - val_loss: 0.0023
 - val_f1: 0.9950
Epoch 144/300
 - 13s - loss: 0.0029 - val_loss: 0.0025
 - val_f1: 0.9936
Epoch 145/300
 - 13s - loss: 0.0028 - val_loss: 0.0027
 - val_f1: 0.9928
Epoch 146/300
 - 13s - loss: 0.0028 - val_loss: 0.0024
 - val_f1: 0.9947
Epoch 147/300
 - 13s - loss: 0.0029 - val_loss: 0.0046
 - val_f1: 0.9906
Epoch 148/300
 - 13s - loss: 0.0028 - val_loss: 0.0023
 - val_f1: 0.9951
Epoch 149/300
 - 13s - loss: 0.0033 - val_loss: 0.0099
 - val_f1: 0.9761
Epoch 150/300
 - 13s - loss: 0.0035 - val_loss: 0.0034
 - val_f1: 0.9940
Epoch 151/300
 - 13s - loss: 0.0027 - val_loss: 0.0035
2019-12-27 01:57:49,353 [INFO] epoch = 150. Intermediate model saved to results_selected_models/selected_ids17_lstm_deep_rep2/_model_epoch_150.pickle
 - val_f1: 0.9932
Epoch 152/300
 - 13s - loss: 0.0027 - val_loss: 0.0025
 - val_f1: 0.9949
Epoch 153/300
 - 13s - loss: 0.0026 - val_loss: 0.0025
 - val_f1: 0.9946
Epoch 154/300
 - 13s - loss: 0.0026 - val_loss: 0.0022
 - val_f1: 0.9951
Epoch 155/300
 - 13s - loss: 0.0026 - val_loss: 0.0023
 - val_f1: 0.9950
Epoch 156/300
 - 13s - loss: 0.0026 - val_loss: 0.0025
 - val_f1: 0.9937
Epoch 157/300
 - 13s - loss: 0.0026 - val_loss: 0.0023
 - val_f1: 0.9949
Epoch 158/300
 - 13s - loss: 0.0026 - val_loss: 0.0027
 - val_f1: 0.9943
Epoch 159/300
 - 13s - loss: 0.0026 - val_loss: 0.0023
 - val_f1: 0.9948
Epoch 160/300
 - 13s - loss: 0.0027 - val_loss: 0.0030
 - val_f1: 0.9944
Epoch 161/300
 - 13s - loss: 0.0029 - val_loss: 0.0035
 - val_f1: 0.9929
Epoch 162/300
 - 13s - loss: 0.0029 - val_loss: 0.0024
 - val_f1: 0.9946
Epoch 163/300
 - 13s - loss: 0.0028 - val_loss: 0.0043
 - val_f1: 0.9920
Epoch 164/300
 - 13s - loss: 0.0028 - val_loss: 0.0125
 - val_f1: 0.9831
Epoch 165/300
 - 13s - loss: 0.0032 - val_loss: 0.0029
 - val_f1: 0.9927
Epoch 166/300
 - 13s - loss: 0.0027 - val_loss: 0.0023
 - val_f1: 0.9952
Epoch 167/300
 - 13s - loss: 0.0027 - val_loss: 0.0026
 - val_f1: 0.9932
Epoch 168/300
 - 13s - loss: 0.0027 - val_loss: 0.0025
 - val_f1: 0.9935
Epoch 169/300
 - 13s - loss: 0.0027 - val_loss: 0.0031
 - val_f1: 0.9925
Epoch 170/300
 - 13s - loss: 0.0061 - val_loss: 0.0052
 - val_f1: 0.9866
Epoch 171/300
 - 13s - loss: 0.0036 - val_loss: 0.0028
 - val_f1: 0.9931
Epoch 172/300
 - 13s - loss: 0.0029 - val_loss: 0.0027
 - val_f1: 0.9943
Epoch 173/300
 - 13s - loss: 0.0028 - val_loss: 0.0024
 - val_f1: 0.9946
Epoch 174/300
 - 13s - loss: 0.0027 - val_loss: 0.0024
 - val_f1: 0.9950
Epoch 175/300
 - 13s - loss: 0.0028 - val_loss: 0.0043
 - val_f1: 0.9885
Epoch 176/300
 - 13s - loss: 0.0027 - val_loss: 0.0039
 - val_f1: 0.9922
Epoch 177/300
 - 13s - loss: 0.0027 - val_loss: 0.0024
 - val_f1: 0.9950
Epoch 178/300
 - 13s - loss: 0.0026 - val_loss: 0.0026
 - val_f1: 0.9941
Epoch 179/300
 - 13s - loss: 0.0026 - val_loss: 0.0026
 - val_f1: 0.9945
Epoch 180/300
 - 13s - loss: 0.0026 - val_loss: 0.0023
 - val_f1: 0.9951
Epoch 181/300
 - 13s - loss: 0.0026 - val_loss: 0.0026
2019-12-27 02:05:42,433 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_ids17_lstm_deep_rep2/_model_epoch_180.pickle
 - val_f1: 0.9933
Epoch 182/300
 - 13s - loss: 0.0026 - val_loss: 0.0024
 - val_f1: 0.9950
Epoch 183/300
 - 13s - loss: 0.0026 - val_loss: 0.0023
 - val_f1: 0.9949
Epoch 184/300
 - 13s - loss: 0.0025 - val_loss: 0.0024
 - val_f1: 0.9942
Epoch 185/300
 - 13s - loss: 0.0025 - val_loss: 0.0028
 - val_f1: 0.9940
Epoch 186/300
 - 13s - loss: 0.0027 - val_loss: 0.0089
 - val_f1: 0.9725
Epoch 187/300
 - 13s - loss: 0.0026 - val_loss: 0.0025
 - val_f1: 0.9947
Epoch 188/300
 - 13s - loss: 0.0028 - val_loss: 0.0028
 - val_f1: 0.9943
Epoch 189/300
 - 13s - loss: 0.0026 - val_loss: 0.0023
 - val_f1: 0.9948
Epoch 190/300
 - 13s - loss: 0.0025 - val_loss: 0.0026
 - val_f1: 0.9947
Epoch 191/300
 - 13s - loss: 0.0025 - val_loss: 0.0028
 - val_f1: 0.9941
Epoch 192/300
 - 13s - loss: 0.0026 - val_loss: 0.0024
 - val_f1: 0.9937
Epoch 193/300
 - 13s - loss: 0.0025 - val_loss: 0.0038
 - val_f1: 0.9922
Epoch 194/300
 - 13s - loss: 0.0025 - val_loss: 0.0025
 - val_f1: 0.9945
Epoch 195/300
 - 13s - loss: 0.0025 - val_loss: 0.0024
 - val_f1: 0.9948
Epoch 196/300
 - 13s - loss: 0.0025 - val_loss: 0.0023
 - val_f1: 0.9949
Epoch 197/300
 - 13s - loss: 0.0025 - val_loss: 0.0022
 - val_f1: 0.9952
Epoch 198/300
 - 13s - loss: 0.0025 - val_loss: 0.0024
 - val_f1: 0.9948
Epoch 199/300
 - 13s - loss: 0.0026 - val_loss: 0.0026
 - val_f1: 0.9932
Epoch 200/300
 - 13s - loss: 0.0024 - val_loss: 0.0022
 - val_f1: 0.9953
Epoch 201/300
 - 13s - loss: 0.0025 - val_loss: 0.0025
 - val_f1: 0.9941
Epoch 202/300
 - 13s - loss: 0.0024 - val_loss: 0.0028
 - val_f1: 0.9937
Epoch 203/300
 - 13s - loss: 0.0024 - val_loss: 0.0028
 - val_f1: 0.9945
Epoch 204/300
 - 13s - loss: 0.0028 - val_loss: 0.0037
 - val_f1: 0.9924
Epoch 205/300
 - 13s - loss: 0.0026 - val_loss: 0.0023
 - val_f1: 0.9951
Epoch 206/300
 - 13s - loss: 0.0025 - val_loss: 0.0030
 - val_f1: 0.9929
Epoch 207/300
 - 13s - loss: 0.0025 - val_loss: 0.0034
 - val_f1: 0.9930
Epoch 208/300
 - 13s - loss: 0.0025 - val_loss: 0.0029
 - val_f1: 0.9945
Epoch 209/300
 - 13s - loss: 0.0025 - val_loss: 0.0027
 - val_f1: 0.9943
Epoch 210/300
 - 13s - loss: 0.0025 - val_loss: 0.0023
 - val_f1: 0.9950
Epoch 211/300
 - 13s - loss: 0.0025 - val_loss: 0.0026
2019-12-27 02:13:36,303 [INFO] epoch = 210. Intermediate model saved to results_selected_models/selected_ids17_lstm_deep_rep2/_model_epoch_210.pickle
 - val_f1: 0.9945
Epoch 212/300
 - 13s - loss: 0.0025 - val_loss: 0.0024
 - val_f1: 0.9950
Epoch 213/300
 - 13s - loss: 0.0024 - val_loss: 0.0022
 - val_f1: 0.9952
Epoch 214/300
 - 13s - loss: 0.0025 - val_loss: 0.0023
 - val_f1: 0.9951
Epoch 215/300
 - 13s - loss: 0.0025 - val_loss: 0.0023
 - val_f1: 0.9949
Epoch 216/300
 - 13s - loss: 0.0024 - val_loss: 0.0026
 - val_f1: 0.9945
Epoch 217/300
 - 13s - loss: 0.0024 - val_loss: 0.0025
 - val_f1: 0.9933
Epoch 218/300
 - 13s - loss: 0.0024 - val_loss: 0.0026
 - val_f1: 0.9933
Epoch 219/300
 - 13s - loss: 0.0025 - val_loss: 0.0027
 - val_f1: 0.9946
Epoch 220/300
 - 13s - loss: 0.0025 - val_loss: 0.0023
 - val_f1: 0.9950
Epoch 221/300
 - 13s - loss: 0.0024 - val_loss: 0.0024
 - val_f1: 0.9940
Epoch 222/300
 - 13s - loss: 0.0025 - val_loss: 0.0025
 - val_f1: 0.9941
Epoch 223/300
 - 13s - loss: 0.0024 - val_loss: 0.0022
 - val_f1: 0.9951
Epoch 224/300
 - 13s - loss: 0.0024 - val_loss: 0.0026
 - val_f1: 0.9938
Epoch 225/300
 - 13s - loss: 0.0025 - val_loss: 0.0027
 - val_f1: 0.9930
Epoch 226/300
 - 13s - loss: 0.0024 - val_loss: 0.0025
 - val_f1: 0.9947
Epoch 227/300
 - 13s - loss: 0.0024 - val_loss: 0.0022
 - val_f1: 0.9953
Epoch 228/300
 - 13s - loss: 0.0024 - val_loss: 0.0022
 - val_f1: 0.9952
Epoch 229/300
 - 13s - loss: 0.0024 - val_loss: 0.0024
 - val_f1: 0.9948
Epoch 230/300
 - 13s - loss: 0.0024 - val_loss: 0.0022
 - val_f1: 0.9950
Epoch 231/300
 - 13s - loss: 0.0024 - val_loss: 0.0024
 - val_f1: 0.9935
Epoch 232/300
 - 13s - loss: 0.0024 - val_loss: 0.0028
 - val_f1: 0.9942
Epoch 233/300
 - 13s - loss: 0.0023 - val_loss: 0.0024
 - val_f1: 0.9944
Epoch 234/300
 - 13s - loss: 0.0024 - val_loss: 0.0023
 - val_f1: 0.9949
Epoch 235/300
 - 13s - loss: 0.0023 - val_loss: 0.0026
 - val_f1: 0.9946
Epoch 236/300
 - 13s - loss: 0.0024 - val_loss: 0.0023
 - val_f1: 0.9950
Epoch 237/300
 - 13s - loss: 0.0025 - val_loss: 0.0036
 - val_f1: 0.9924
Epoch 238/300
 - 13s - loss: 0.0023 - val_loss: 0.0023
 - val_f1: 0.9943
Epoch 239/300
 - 13s - loss: 0.0023 - val_loss: 0.0024
 - val_f1: 0.9949
Epoch 240/300
 - 13s - loss: 0.0023 - val_loss: 0.0024
 - val_f1: 0.9936
Epoch 241/300
 - 13s - loss: 0.0023 - val_loss: 0.0022
2019-12-27 02:21:30,090 [INFO] epoch = 240. Intermediate model saved to results_selected_models/selected_ids17_lstm_deep_rep2/_model_epoch_240.pickle
 - val_f1: 0.9952
Epoch 242/300
 - 13s - loss: 0.0024 - val_loss: 0.0022
 - val_f1: 0.9953
Epoch 243/300
 - 13s - loss: 0.0024 - val_loss: 0.0026
 - val_f1: 0.9933
Epoch 244/300
 - 13s - loss: 0.0023 - val_loss: 0.0027
 - val_f1: 0.9938
Epoch 245/300
 - 13s - loss: 0.0024 - val_loss: 0.0023
 - val_f1: 0.9948
Epoch 246/300
 - 13s - loss: 0.0023 - val_loss: 0.0023
 - val_f1: 0.9951
Epoch 247/300
 - 13s - loss: 0.0023 - val_loss: 0.0024
 - val_f1: 0.9948
Epoch 248/300
 - 13s - loss: 0.0023 - val_loss: 0.0023
 - val_f1: 0.9952
Epoch 249/300
 - 13s - loss: 0.0023 - val_loss: 0.0023
 - val_f1: 0.9951
Epoch 250/300
 - 13s - loss: 0.0024 - val_loss: 0.0023
 - val_f1: 0.9948
Epoch 251/300
 - 13s - loss: 0.0023 - val_loss: 0.0022
 - val_f1: 0.9952
Epoch 252/300
 - 13s - loss: 0.0023 - val_loss: 0.0022
 - val_f1: 0.9952
Epoch 253/300
 - 13s - loss: 0.0023 - val_loss: 0.0023
 - val_f1: 0.9949
Epoch 254/300
 - 13s - loss: 0.0023 - val_loss: 0.0025
 - val_f1: 0.9949
Epoch 255/300
 - 13s - loss: 0.0023 - val_loss: 0.0023
 - val_f1: 0.9951
Epoch 256/300
 - 13s - loss: 0.0023 - val_loss: 0.0023
 - val_f1: 0.9950
Epoch 257/300
 - 13s - loss: 0.0023 - val_loss: 0.0049
 - val_f1: 0.9916
Epoch 258/300
 - 13s - loss: 0.0023 - val_loss: 0.0024
 - val_f1: 0.9941
Epoch 259/300
 - 13s - loss: 0.0023 - val_loss: 0.0024
 - val_f1: 0.9947
Epoch 260/300
 - 13s - loss: 0.0023 - val_loss: 0.0024
 - val_f1: 0.9948
Epoch 261/300
 - 13s - loss: 0.0023 - val_loss: 0.0024
 - val_f1: 0.9948
Epoch 262/300
 - 13s - loss: 0.0024 - val_loss: 0.0032
 - val_f1: 0.9927
Epoch 263/300
 - 13s - loss: 0.0022 - val_loss: 0.0022
 - val_f1: 0.9952
Epoch 264/300
 - 13s - loss: 0.0023 - val_loss: 0.0024
 - val_f1: 0.9948
Epoch 265/300
 - 13s - loss: 0.0023 - val_loss: 0.0024
 - val_f1: 0.9948
Epoch 266/300
 - 13s - loss: 0.0023 - val_loss: 0.0025
 - val_f1: 0.9948
Epoch 267/300
 - 13s - loss: 0.0023 - val_loss: 0.0035
 - val_f1: 0.9929
Epoch 268/300
 - 13s - loss: 0.0022 - val_loss: 0.0022
 - val_f1: 0.9954
Epoch 269/300
 - 13s - loss: 0.0023 - val_loss: 0.0036
 - val_f1: 0.9922
Epoch 270/300
 - 13s - loss: 0.0023 - val_loss: 0.0024
 - val_f1: 0.9948
Epoch 271/300
 - 13s - loss: 0.0024 - val_loss: 0.0029
2019-12-27 02:29:23,700 [INFO] epoch = 270. Intermediate model saved to results_selected_models/selected_ids17_lstm_deep_rep2/_model_epoch_270.pickle
 - val_f1: 0.9925
Epoch 272/300
 - 13s - loss: 0.0023 - val_loss: 0.0025
 - val_f1: 0.9937
Epoch 273/300
 - 13s - loss: 0.0023 - val_loss: 0.0026
 - val_f1: 0.9946
Epoch 274/300
 - 13s - loss: 0.0023 - val_loss: 0.0030
 - val_f1: 0.9934
Epoch 275/300
 - 13s - loss: 0.0023 - val_loss: 0.0023
 - val_f1: 0.9952
Epoch 276/300
 - 13s - loss: 0.0023 - val_loss: 0.0026
 - val_f1: 0.9945
Epoch 277/300
 - 13s - loss: 0.0023 - val_loss: 0.0024
 - val_f1: 0.9946
Epoch 278/300
 - 13s - loss: 0.0022 - val_loss: 0.0032
 - val_f1: 0.9929
Epoch 279/300
 - 13s - loss: 0.0024 - val_loss: 0.0030
 - val_f1: 0.9927
Epoch 280/300
 - 13s - loss: 0.0022 - val_loss: 0.0022
 - val_f1: 0.9952
Epoch 281/300
 - 13s - loss: 0.0022 - val_loss: 0.0023
 - val_f1: 0.9949
Epoch 282/300
 - 13s - loss: 0.0022 - val_loss: 0.0022
 - val_f1: 0.9952
Epoch 283/300
 - 13s - loss: 0.0022 - val_loss: 0.0023
 - val_f1: 0.9951
Epoch 284/300
 - 13s - loss: 0.0025 - val_loss: 0.0126
 - val_f1: 0.9834
Epoch 285/300
 - 13s - loss: 0.0030 - val_loss: 0.0032
 - val_f1: 0.9928
Epoch 286/300
 - 13s - loss: 0.0023 - val_loss: 0.0025
 - val_f1: 0.9944
Epoch 287/300
 - 13s - loss: 0.0022 - val_loss: 0.0024
 - val_f1: 0.9949
Epoch 288/300
 - 13s - loss: 0.0022 - val_loss: 0.0025
 - val_f1: 0.9946
Epoch 289/300
 - 13s - loss: 0.0022 - val_loss: 0.0024
 - val_f1: 0.9947
Epoch 290/300
 - 13s - loss: 0.0022 - val_loss: 0.0024
 - val_f1: 0.9950
Epoch 291/300
 - 13s - loss: 0.0022 - val_loss: 0.0024
 - val_f1: 0.9949
Epoch 292/300
 - 13s - loss: 0.0022 - val_loss: 0.0023
 - val_f1: 0.9951
Epoch 293/300
 - 13s - loss: 0.0022 - val_loss: 0.0024
 - val_f1: 0.9945
Epoch 294/300
 - 13s - loss: 0.0022 - val_loss: 0.0026
 - val_f1: 0.9936
Epoch 295/300
 - 13s - loss: 0.0022 - val_loss: 0.0023
 - val_f1: 0.9952
Epoch 296/300
 - 13s - loss: 0.0022 - val_loss: 0.0027
 - val_f1: 0.9942
Epoch 297/300
 - 13s - loss: 0.0022 - val_loss: 0.0023
 - val_f1: 0.9950
Epoch 298/300
 - 13s - loss: 0.0023 - val_loss: 0.0023
 - val_f1: 0.9949
Epoch 299/300
 - 13s - loss: 0.0022 - val_loss: 0.0027
 - val_f1: 0.9942
Epoch 300/300
 - 13s - loss: 0.0023 - val_loss: 0.0109
2019-12-27 02:37:04,482 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-27 02:37:15,338 [INFO] Last epoch loss evaluation: train_loss = 0.001801, val_loss = 0.002183
2019-12-27 02:37:15,338 [INFO] Training complete. time_to_train = 4751.13 sec, 79.19 min
2019-12-27 02:37:15,343 [INFO] Model saved to results_selected_models/selected_ids17_lstm_deep_rep2/best_model.pickle
2019-12-27 02:37:15,347 [INFO] Training history saved to: results_selected_models/selected_ids17_lstm_deep_rep2/training_error_history.csv
2019-12-27 02:37:15,488 [INFO] Plot saved to: results_selected_models/selected_ids17_lstm_deep_rep2/training_error_history.png
2019-12-27 02:37:15,619 [INFO] Plot saved to: results_selected_models/selected_ids17_lstm_deep_rep2/training_f1_history.png
2019-12-27 02:37:15,619 [INFO] Making predictions on training, validation, testing data
2019-12-27 02:37:28,533 [INFO] Evaluating predictions (results)
2019-12-27 02:37:38,905 [INFO] Dataset: Testing. Classification report below
2019-12-27 02:37:38,905 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       1.00      1.00      1.00    454243
                   Bot       0.97      0.37      0.54       391
                  DDoS       1.00      1.00      1.00     25604
         DoS GoldenEye       0.99      0.98      0.99      2058
              DoS Hulk       0.98      0.99      0.99     46023
      DoS Slowhttptest       0.89      0.99      0.94      1100
         DoS slowloris       0.99      0.98      0.98      1159
           FTP-Patator       1.00      1.00      1.00      1586
              PortScan       0.99      1.00      1.00     31761
           SSH-Patator       0.94      0.98      0.96      1179
Web Attack Brute Force       0.83      0.11      0.20       302
        Web Attack XSS       0.00      0.00      0.00       130

             micro avg       1.00      1.00      1.00    565536
             macro avg       0.88      0.78      0.80    565536
          weighted avg       1.00      1.00      1.00    565536

2019-12-27 02:37:38,905 [INFO] Overall accuracy (micro avg): 0.995588256096871
/home/hasitha/sunanda/ids_experiments/utility.py:334: RuntimeWarning: invalid value encountered in true_divide
  F1 = (2 * PPV * TPR) / (PPV + TPR)  # F1 score
2019-12-27 02:37:50,688 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9956         0.9956                       0.9956                0.0004                   0.0044  0.9956
1     Macro avg        0.9993         0.8821                       0.7831                0.0010                   0.2169  0.7984
2  Weighted avg        0.9964         0.9953                       0.9956                0.0079                   0.0044  0.9952
2019-12-27 02:38:01,068 [INFO] Dataset: Validation. Classification report below
2019-12-27 02:38:01,068 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       1.00      1.00      1.00    454243
                   Bot       0.98      0.33      0.50       391
                  DDoS       1.00      1.00      1.00     25605
         DoS GoldenEye       0.99      0.98      0.99      2059
              DoS Hulk       0.98      0.99      0.99     46023
      DoS Slowhttptest       0.89      0.99      0.94      1099
         DoS slowloris       0.99      0.98      0.98      1159
           FTP-Patator       0.99      0.99      0.99      1587
              PortScan       0.99      1.00      1.00     31758
           SSH-Patator       0.94      0.98      0.96      1180
Web Attack Brute Force       0.71      0.08      0.15       301
        Web Attack XSS       1.00      0.03      0.06       131

             micro avg       1.00      1.00      1.00    565536
             macro avg       0.96      0.78      0.80    565536
          weighted avg       1.00      1.00      1.00    565536

2019-12-27 02:38:01,068 [INFO] Overall accuracy (micro avg): 0.9956077066711934
2019-12-27 02:38:12,873 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9956         0.9956                       0.9956                0.0004                   0.0044  0.9956
1     Macro avg        0.9993         0.9555                       0.7791                0.0010                   0.2209  0.7950
2  Weighted avg        0.9964         0.9955                       0.9956                0.0076                   0.0044  0.9952
2019-12-27 02:38:47,044 [INFO] Dataset: Training. Classification report below
2019-12-27 02:38:47,045 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       1.00      1.00      1.00   1362781
                   Bot       0.99      0.38      0.55      1174
                  DDoS       1.00      1.00      1.00     76815
         DoS GoldenEye       1.00      0.99      0.99      6176
              DoS Hulk       0.98      0.99      0.99    138073
      DoS Slowhttptest       0.92      0.99      0.95      3300
         DoS slowloris       0.99      0.99      0.99      3478
           FTP-Patator       1.00      1.00      1.00      4761
              PortScan       0.99      1.00      1.00     95281
           SSH-Patator       0.96      0.98      0.97      3538
Web Attack Brute Force       0.93      0.12      0.22       904
        Web Attack XSS       0.92      0.03      0.06       391

             micro avg       1.00      1.00      1.00   1696672
             macro avg       0.97      0.79      0.81   1696672
          weighted avg       1.00      1.00      1.00   1696672

2019-12-27 02:38:47,045 [INFO] Overall accuracy (micro avg): 0.9961477527772015
2019-12-27 02:39:25,859 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9961         0.9961                       0.9961                0.0004                   0.0039  0.9961
1     Macro avg        0.9994         0.9727                       0.7892                0.0009                   0.2108  0.8091
2  Weighted avg        0.9968         0.9961                       0.9961                0.0069                   0.0039  0.9958
2019-12-27 02:39:25,883 [INFO] Results saved to: results_selected_models/selected_ids17_lstm_deep_rep2/selected_ids17_lstm_deep_rep2_results.xlsx
2019-12-27 02:39:25,887 [INFO] ================= Finished running experiment no. 2 ================= 

2019-12-27 02:39:25,958 [INFO] Created directory: results_selected_models/selected_ids17_lstm_deep_rep3
2019-12-27 02:39:25,958 [INFO] Initialized logging. log_filename = results_selected_models/selected_ids17_lstm_deep_rep3/run_log.log
2019-12-27 02:39:25,959 [INFO] ================= Running experiment no. 3  ================= 

2019-12-27 02:39:25,959 [INFO] Experiment parameters given below
2019-12-27 02:39:25,959 [INFO] 
{'experiment_num': 3, 'results_dir': 'results_selected_models/selected_ids17_lstm_deep_rep3', 'model': 'lstm', 'model_type': 'classifier', 'normal_label': 'BENIGN', 'lstm_time_steps': 32, 'lstm_layer_units': [64, 32], 'lstm_layer_activations': ['relu', 'relu'], 'lstm_layer_dropout_rates': [0.2, 0.2], 'batch_size': 256, 'epochs': 300, 'early_stop_patience': 50, 'dataset_dir': '../Datasets/full_datasets/ids2017', 'description': 'selected_ids17_lstm_deep_rep3'}
2019-12-27 02:39:25,959 [INFO] Created tensorboard log directory: results_selected_models/selected_ids17_lstm_deep_rep3/tf_logs_run_2019_12_27-02_39_25
2019-12-27 02:39:25,959 [INFO] Loading datsets from: ../Datasets/full_datasets/ids2017
2019-12-27 02:39:25,959 [INFO] Reading X, y files
2019-12-27 02:39:25,959 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_train.h5
2019-12-27 02:39:30,125 [INFO] Reading complete. time_to_read=4.17 seconds
2019-12-27 02:39:30,125 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_val.h5
2019-12-27 02:39:31,564 [INFO] Reading complete. time_to_read=1.44 seconds
2019-12-27 02:39:31,564 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_test.h5
2019-12-27 02:39:33,000 [INFO] Reading complete. time_to_read=1.44 seconds
2019-12-27 02:39:33,000 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_train.h5
2019-12-27 02:39:33,219 [INFO] Reading complete. time_to_read=0.22 seconds
2019-12-27 02:39:33,219 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_val.h5
2019-12-27 02:39:33,294 [INFO] Reading complete. time_to_read=0.07 seconds
2019-12-27 02:39:33,294 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_test.h5
2019-12-27 02:39:33,369 [INFO] Reading complete. time_to_read=0.07 seconds
2019-12-27 02:39:35,799 [INFO] Preparing flow sequences
2019-12-27 02:39:57,641 [INFO] Extracting flows complete. time_taken = 21.84 sec
2019-12-27 02:39:58,779 [INFO] Initializing model
2019-12-27 02:39:59,226 [INFO] _________________________________________________________________
2019-12-27 02:39:59,226 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-27 02:39:59,226 [INFO] =================================================================
2019-12-27 02:39:59,226 [INFO] lstm_15 (LSTM)               (None, 32, 64)            36608     
2019-12-27 02:39:59,226 [INFO] _________________________________________________________________
2019-12-27 02:39:59,226 [INFO] batch_normalization_15 (Batc (None, 32, 64)            256       
2019-12-27 02:39:59,226 [INFO] _________________________________________________________________
2019-12-27 02:39:59,226 [INFO] dropout_15 (Dropout)         (None, 32, 64)            0         
2019-12-27 02:39:59,226 [INFO] _________________________________________________________________
2019-12-27 02:39:59,226 [INFO] lstm_16 (LSTM)               (None, 32, 32)            12416     
2019-12-27 02:39:59,226 [INFO] _________________________________________________________________
2019-12-27 02:39:59,226 [INFO] batch_normalization_16 (Batc (None, 32, 32)            128       
2019-12-27 02:39:59,226 [INFO] _________________________________________________________________
2019-12-27 02:39:59,226 [INFO] dropout_16 (Dropout)         (None, 32, 32)            0         
2019-12-27 02:39:59,226 [INFO] _________________________________________________________________
2019-12-27 02:39:59,227 [INFO] time_distributed_8 (TimeDist (None, 32, 12)            396       
2019-12-27 02:39:59,227 [INFO] =================================================================
2019-12-27 02:39:59,227 [INFO] Total params: 49,804
2019-12-27 02:39:59,227 [INFO] Trainable params: 49,612
2019-12-27 02:39:59,227 [INFO] Non-trainable params: 192
2019-12-27 02:39:59,227 [INFO] _________________________________________________________________
2019-12-27 02:39:59,227 [INFO] Training model
 - val_f1: 0.9817
Train on 53021 samples, validate on 17673 samples
Epoch 1/300
 - 14s - loss: 0.1777 - val_loss: 0.0828
 - val_f1: 0.8116
Epoch 2/300
 - 13s - loss: 0.0637 - val_loss: 0.0532
 - val_f1: 0.8403
Epoch 3/300
 - 13s - loss: 0.0458 - val_loss: 0.0397
 - val_f1: 0.8908
Epoch 4/300
 - 13s - loss: 0.0398 - val_loss: 0.0440
 - val_f1: 0.8520
Epoch 5/300
 - 13s - loss: 0.0349 - val_loss: 0.0363
 - val_f1: 0.8985
Epoch 6/300
 - 13s - loss: 0.0314 - val_loss: 0.0272
 - val_f1: 0.9279
Epoch 7/300
 - 13s - loss: 0.0282 - val_loss: 0.0236
 - val_f1: 0.9421
Epoch 8/300
 - 13s - loss: 0.0255 - val_loss: 0.0219
 - val_f1: 0.9467
Epoch 9/300
 - 13s - loss: 0.0234 - val_loss: 0.0198
 - val_f1: 0.9516
Epoch 10/300
 - 13s - loss: 0.0216 - val_loss: 0.0185
 - val_f1: 0.9541
Epoch 11/300
 - 13s - loss: 0.0202 - val_loss: 0.0178
 - val_f1: 0.9555
Epoch 12/300
 - 13s - loss: 0.0191 - val_loss: 0.0171
 - val_f1: 0.9574
Epoch 13/300
 - 13s - loss: 0.0181 - val_loss: 0.0156
 - val_f1: 0.9593
Epoch 14/300
 - 13s - loss: 0.0171 - val_loss: 0.0154
 - val_f1: 0.9592
Epoch 15/300
 - 13s - loss: 0.0162 - val_loss: 0.0154
 - val_f1: 0.9598
Epoch 16/300
 - 13s - loss: 0.0154 - val_loss: 0.0149
 - val_f1: 0.9611
Epoch 17/300
 - 13s - loss: 0.0147 - val_loss: 0.0135
 - val_f1: 0.9630
Epoch 18/300
 - 13s - loss: 0.0140 - val_loss: 0.0131
 - val_f1: 0.9664
Epoch 19/300
 - 13s - loss: 0.0133 - val_loss: 0.0127
 - val_f1: 0.9689
Epoch 20/300
 - 13s - loss: 0.0126 - val_loss: 0.0110
 - val_f1: 0.9704
Epoch 21/300
 - 13s - loss: 0.0119 - val_loss: 0.0111
 - val_f1: 0.9697
Epoch 22/300
 - 13s - loss: 0.0114 - val_loss: 0.0119
 - val_f1: 0.9694
Epoch 23/300
 - 13s - loss: 0.0104 - val_loss: 0.0089
 - val_f1: 0.9775
Epoch 24/300
 - 13s - loss: 0.0092 - val_loss: 0.0086
 - val_f1: 0.9803
Epoch 25/300
 - 13s - loss: 0.0083 - val_loss: 0.0060
 - val_f1: 0.9890
Epoch 26/300
 - 13s - loss: 0.0078 - val_loss: 0.0054
 - val_f1: 0.9883
Epoch 27/300
 - 13s - loss: 0.0071 - val_loss: 0.0049
 - val_f1: 0.9900
Epoch 28/300
 - 13s - loss: 0.0079 - val_loss: 0.0051
 - val_f1: 0.9894
Epoch 29/300
 - 13s - loss: 0.0065 - val_loss: 0.0055
 - val_f1: 0.9879
Epoch 30/300
 - 13s - loss: 0.0063 - val_loss: 0.0044
 - val_f1: 0.9911
Epoch 31/300
 - 13s - loss: 0.0065 - val_loss: 0.0050
/home/hasitha/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2019-12-27 02:48:15,305 [INFO] epoch = 30. Intermediate model saved to results_selected_models/selected_ids17_lstm_deep_rep3/_model_epoch_30.pickle
 - val_f1: 0.9898
Epoch 32/300
 - 13s - loss: 0.0100 - val_loss: 0.0083
 - val_f1: 0.9821
Epoch 33/300
 - 13s - loss: 0.0097 - val_loss: 0.0065
 - val_f1: 0.9845
Epoch 34/300
 - 13s - loss: 0.0083 - val_loss: 0.0061
 - val_f1: 0.9855
Epoch 35/300
 - 13s - loss: 0.0075 - val_loss: 0.0049
 - val_f1: 0.9893
Epoch 36/300
 - 13s - loss: 0.0071 - val_loss: 0.0044
 - val_f1: 0.9909
Epoch 37/300
 - 13s - loss: 0.0067 - val_loss: 0.0048
 - val_f1: 0.9898
Epoch 38/300
 - 13s - loss: 0.0064 - val_loss: 0.0042
 - val_f1: 0.9910
Epoch 39/300
 - 13s - loss: 0.0061 - val_loss: 0.0039
 - val_f1: 0.9912
Epoch 40/300
 - 13s - loss: 0.0060 - val_loss: 0.0043
 - val_f1: 0.9909
Epoch 41/300
 - 13s - loss: 0.0064 - val_loss: 0.0038
 - val_f1: 0.9919
Epoch 42/300
 - 13s - loss: 0.0057 - val_loss: 0.0041
 - val_f1: 0.9904
Epoch 43/300
 - 13s - loss: 0.0055 - val_loss: 0.0037
 - val_f1: 0.9922
Epoch 44/300
 - 13s - loss: 0.0055 - val_loss: 0.0036
 - val_f1: 0.9922
Epoch 45/300
 - 13s - loss: 0.0054 - val_loss: 0.0041
 - val_f1: 0.9911
Epoch 46/300
 - 13s - loss: 0.0053 - val_loss: 0.0035
 - val_f1: 0.9929
Epoch 47/300
 - 13s - loss: 0.0051 - val_loss: 0.0035
 - val_f1: 0.9930
Epoch 48/300
 - 13s - loss: 0.0094 - val_loss: 0.0075
 - val_f1: 0.9803
Epoch 49/300
 - 13s - loss: 0.0081 - val_loss: 0.0070
 - val_f1: 0.9838
Epoch 50/300
 - 13s - loss: 0.0071 - val_loss: 0.0071
 - val_f1: 0.9718
Epoch 51/300
 - 13s - loss: 0.0056 - val_loss: 0.0036
 - val_f1: 0.9931
Epoch 52/300
 - 13s - loss: 0.0054 - val_loss: 0.0050
 - val_f1: 0.9904
Epoch 53/300
 - 13s - loss: 0.0087 - val_loss: 0.0067
 - val_f1: 0.9841
Epoch 54/300
 - 13s - loss: 0.0065 - val_loss: 0.0052
 - val_f1: 0.9903
Epoch 55/300
 - 13s - loss: 0.0072 - val_loss: 0.0076
 - val_f1: 0.9819
Epoch 56/300
 - 13s - loss: 0.0077 - val_loss: 0.0075
 - val_f1: 0.9752
Epoch 57/300
 - 13s - loss: 0.0067 - val_loss: 0.0104
 - val_f1: 0.9594
Epoch 58/300
 - 13s - loss: 0.0055 - val_loss: 0.0047
 - val_f1: 0.9927
Epoch 59/300
 - 13s - loss: 0.0053 - val_loss: 0.0067
 - val_f1: 0.9841
Epoch 60/300
 - 13s - loss: 0.0052 - val_loss: 0.0038
 - val_f1: 0.9921
Epoch 61/300
 - 13s - loss: 0.0050 - val_loss: 0.0045
2019-12-27 02:56:08,381 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_ids17_lstm_deep_rep3/_model_epoch_60.pickle
 - val_f1: 0.9903
Epoch 62/300
 - 13s - loss: 0.0049 - val_loss: 0.0035
 - val_f1: 0.9938
Epoch 63/300
 - 13s - loss: 0.0046 - val_loss: 0.0033
 - val_f1: 0.9929
Epoch 64/300
 - 13s - loss: 0.0043 - val_loss: 0.0050
 - val_f1: 0.9868
Epoch 65/300
 - 13s - loss: 0.0042 - val_loss: 0.0043
 - val_f1: 0.9921
Epoch 66/300
 - 13s - loss: 0.0045 - val_loss: 0.0141
 - val_f1: 0.9620
Epoch 67/300
 - 13s - loss: 0.0041 - val_loss: 0.0175
 - val_f1: 0.9626
Epoch 68/300
 - 13s - loss: 0.0040 - val_loss: 0.0096
 - val_f1: 0.9664
Epoch 69/300
 - 13s - loss: 0.0039 - val_loss: 0.0052
 - val_f1: 0.9862
Epoch 70/300
 - 13s - loss: 0.0040 - val_loss: 0.0049
 - val_f1: 0.9886
Epoch 71/300
 - 13s - loss: 0.0038 - val_loss: 0.0032
 - val_f1: 0.9945
Epoch 72/300
 - 13s - loss: 0.0037 - val_loss: 0.0030
 - val_f1: 0.9941
Epoch 73/300
 - 13s - loss: 0.0037 - val_loss: 0.0030
 - val_f1: 0.9937
Epoch 74/300
 - 13s - loss: 0.0036 - val_loss: 0.0034
 - val_f1: 0.9942
Epoch 75/300
 - 13s - loss: 0.0037 - val_loss: 0.0078
 - val_f1: 0.9687
Epoch 76/300
 - 13s - loss: 0.0058 - val_loss: 0.0106
 - val_f1: 0.9601
Epoch 77/300
 - 13s - loss: 0.0047 - val_loss: 0.0051
 - val_f1: 0.9900
Epoch 78/300
 - 13s - loss: 0.0044 - val_loss: 0.0048
 - val_f1: 0.9899
Epoch 79/300
 - 13s - loss: 0.0041 - val_loss: 0.0055
 - val_f1: 0.9874
Epoch 80/300
 - 13s - loss: 0.0049 - val_loss: 0.0182
 - val_f1: 0.9799
Epoch 81/300
 - 13s - loss: 0.0048 - val_loss: 0.0200
 - val_f1: 0.9604
Epoch 82/300
 - 13s - loss: 0.0039 - val_loss: 0.0085
 - val_f1: 0.9666
Epoch 83/300
 - 13s - loss: 0.0038 - val_loss: 0.0054
 - val_f1: 0.9901
Epoch 84/300
 - 13s - loss: 0.0038 - val_loss: 0.0033
 - val_f1: 0.9922
Epoch 85/300
 - 13s - loss: 0.0037 - val_loss: 0.0034
 - val_f1: 0.9929
Epoch 86/300
 - 13s - loss: 0.0036 - val_loss: 0.0028
 - val_f1: 0.9943
Epoch 87/300
 - 13s - loss: 0.0054 - val_loss: 0.0043
 - val_f1: 0.9903
Epoch 88/300
 - 13s - loss: 0.0046 - val_loss: 0.0057
 - val_f1: 0.9837
Epoch 89/300
 - 13s - loss: 0.0041 - val_loss: 0.0086
 - val_f1: 0.9637
Epoch 90/300
 - 13s - loss: 0.0037 - val_loss: 0.0148
 - val_f1: 0.9565
Epoch 91/300
 - 13s - loss: 0.0037 - val_loss: 0.0143
2019-12-27 03:04:01,829 [INFO] epoch = 90. Intermediate model saved to results_selected_models/selected_ids17_lstm_deep_rep3/_model_epoch_90.pickle
 - val_f1: 0.9633
Epoch 92/300
 - 13s - loss: 0.0034 - val_loss: 0.0101
 - val_f1: 0.9647
Epoch 93/300
 - 13s - loss: 0.0037 - val_loss: 0.0095
 - val_f1: 0.9647
Epoch 94/300
 - 13s - loss: 0.0034 - val_loss: 0.0204
 - val_f1: 0.9629
Epoch 95/300
 - 13s - loss: 0.0033 - val_loss: 0.0120
 - val_f1: 0.9641
Epoch 96/300
 - 13s - loss: 0.0033 - val_loss: 0.0034
 - val_f1: 0.9936
Epoch 97/300
 - 13s - loss: 0.0033 - val_loss: 0.0180
 - val_f1: 0.9629
Epoch 98/300
 - 13s - loss: 0.0032 - val_loss: 0.0035
 - val_f1: 0.9931
Epoch 99/300
 - 13s - loss: 0.0033 - val_loss: 0.0051
 - val_f1: 0.9916
Epoch 100/300
 - 13s - loss: 0.0032 - val_loss: 0.0031
 - val_f1: 0.9942
Epoch 101/300
 - 13s - loss: 0.0032 - val_loss: 0.0093
 - val_f1: 0.9669
Epoch 102/300
 - 13s - loss: 0.0032 - val_loss: 0.0034
 - val_f1: 0.9925
Epoch 103/300
 - 13s - loss: 0.0031 - val_loss: 0.0033
 - val_f1: 0.9933
Epoch 104/300
 - 13s - loss: 0.0033 - val_loss: 0.0028
 - val_f1: 0.9942
Epoch 105/300
 - 13s - loss: 0.0031 - val_loss: 0.0046
 - val_f1: 0.9870
Epoch 106/300
 - 13s - loss: 0.0035 - val_loss: 0.0028
 - val_f1: 0.9939
Epoch 107/300
 - 13s - loss: 0.0031 - val_loss: 0.0205
 - val_f1: 0.9634
Epoch 108/300
 - 13s - loss: 0.0032 - val_loss: 0.0031
 - val_f1: 0.9944
Epoch 109/300
 - 13s - loss: 0.0030 - val_loss: 0.0025
 - val_f1: 0.9950
Epoch 110/300
 - 13s - loss: 0.0029 - val_loss: 0.0145
 - val_f1: 0.9658
Epoch 111/300
 - 13s - loss: 0.0030 - val_loss: 0.0028
 - val_f1: 0.9930
Epoch 112/300
 - 13s - loss: 0.0030 - val_loss: 0.0043
 - val_f1: 0.9922
Epoch 113/300
 - 13s - loss: 0.0032 - val_loss: 0.0290
 - val_f1: 0.9633
Epoch 114/300
 - 13s - loss: 0.0030 - val_loss: 0.0083
 - val_f1: 0.9699
Epoch 115/300
 - 13s - loss: 0.0030 - val_loss: 0.0044
 - val_f1: 0.9906
Epoch 116/300
 - 13s - loss: 0.0031 - val_loss: 0.0043
 - val_f1: 0.9906
Epoch 117/300
 - 13s - loss: 0.0029 - val_loss: 0.0061
 - val_f1: 0.9774
Epoch 118/300
 - 13s - loss: 0.0029 - val_loss: 0.0032
 - val_f1: 0.9932
Epoch 119/300
 - 13s - loss: 0.0031 - val_loss: 0.0090
 - val_f1: 0.9840
Epoch 120/300
 - 13s - loss: 0.0033 - val_loss: 0.0109
 - val_f1: 0.9646
Epoch 121/300
 - 13s - loss: 0.0030 - val_loss: 0.0170
2019-12-27 03:11:55,016 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_ids17_lstm_deep_rep3/_model_epoch_120.pickle
 - val_f1: 0.9631
Epoch 122/300
 - 13s - loss: 0.0030 - val_loss: 0.0053
 - val_f1: 0.9827
Epoch 123/300
 - 13s - loss: 0.0029 - val_loss: 0.0149
 - val_f1: 0.9618
Epoch 124/300
 - 13s - loss: 0.0029 - val_loss: 0.0077
 - val_f1: 0.9734
Epoch 125/300
 - 13s - loss: 0.0028 - val_loss: 0.0027
 - val_f1: 0.9943
Epoch 126/300
 - 13s - loss: 0.0029 - val_loss: 0.0028
 - val_f1: 0.9941
Epoch 127/300
 - 13s - loss: 0.0029 - val_loss: 0.0025
 - val_f1: 0.9946
Epoch 128/300
 - 13s - loss: 0.0028 - val_loss: 0.0035
 - val_f1: 0.9934
Epoch 129/300
 - 13s - loss: 0.0029 - val_loss: 0.0024
 - val_f1: 0.9951
Epoch 130/300
 - 13s - loss: 0.0028 - val_loss: 0.0025
 - val_f1: 0.9950
Epoch 131/300
 - 13s - loss: 0.0029 - val_loss: 0.0037
 - val_f1: 0.9920
Epoch 132/300
 - 13s - loss: 0.0028 - val_loss: 0.0030
 - val_f1: 0.9926
Epoch 133/300
 - 13s - loss: 0.0029 - val_loss: 0.0027
 - val_f1: 0.9944
Epoch 134/300
 - 13s - loss: 0.0028 - val_loss: 0.0028
 - val_f1: 0.9945
Epoch 135/300
 - 13s - loss: 0.0029 - val_loss: 0.0090
 - val_f1: 0.9677
Epoch 136/300
 - 13s - loss: 0.0028 - val_loss: 0.0032
 - val_f1: 0.9937
Epoch 137/300
 - 13s - loss: 0.0029 - val_loss: 0.0026
 - val_f1: 0.9948
Epoch 138/300
 - 13s - loss: 0.0028 - val_loss: 0.0032
 - val_f1: 0.9935
Epoch 139/300
 - 13s - loss: 0.0028 - val_loss: 0.0076
 - val_f1: 0.9704
Epoch 140/300
 - 13s - loss: 0.0028 - val_loss: 0.0026
 - val_f1: 0.9947
Epoch 141/300
 - 13s - loss: 0.0028 - val_loss: 0.0046
 - val_f1: 0.9885
Epoch 142/300
 - 13s - loss: 0.0027 - val_loss: 0.0031
 - val_f1: 0.9948
Epoch 143/300
 - 13s - loss: 0.0028 - val_loss: 0.0030
 - val_f1: 0.9934
Epoch 144/300
 - 13s - loss: 0.0028 - val_loss: 0.0030
 - val_f1: 0.9939
Epoch 145/300
 - 13s - loss: 0.0027 - val_loss: 0.0038
 - val_f1: 0.9923
Epoch 146/300
 - 13s - loss: 0.0027 - val_loss: 0.0168
 - val_f1: 0.9622
Epoch 147/300
 - 13s - loss: 0.0030 - val_loss: 0.0101
 - val_f1: 0.9640
Epoch 148/300
 - 13s - loss: 0.0029 - val_loss: 0.0119
 - val_f1: 0.9630
Epoch 149/300
 - 13s - loss: 0.0032 - val_loss: 0.0094
 - val_f1: 0.9673
Epoch 150/300
 - 13s - loss: 0.0030 - val_loss: 0.0068
 - val_f1: 0.9754
Epoch 151/300
 - 13s - loss: 0.0029 - val_loss: 0.0034
2019-12-27 03:19:49,116 [INFO] epoch = 150. Intermediate model saved to results_selected_models/selected_ids17_lstm_deep_rep3/_model_epoch_150.pickle
 - val_f1: 0.9926
Epoch 152/300
 - 13s - loss: 0.0029 - val_loss: 0.0032
 - val_f1: 0.9921
Epoch 153/300
 - 13s - loss: 0.0028 - val_loss: 0.0028
 - val_f1: 0.9939
Epoch 154/300
 - 13s - loss: 0.0029 - val_loss: 0.0032
 - val_f1: 0.9941
Epoch 155/300
 - 13s - loss: 0.0029 - val_loss: 0.0036
 - val_f1: 0.9927
Epoch 156/300
 - 13s - loss: 0.0029 - val_loss: 0.0031
 - val_f1: 0.9924
Epoch 157/300
 - 13s - loss: 0.0028 - val_loss: 0.0109
 - val_f1: 0.9827
Epoch 158/300
 - 13s - loss: 0.0031 - val_loss: 0.0027
 - val_f1: 0.9945
Epoch 159/300
 - 13s - loss: 0.0028 - val_loss: 0.0044
 - val_f1: 0.9881
Epoch 160/300
 - 13s - loss: 0.0028 - val_loss: 0.0043
 - val_f1: 0.9920
Epoch 161/300
 - 13s - loss: 0.0029 - val_loss: 0.0025
 - val_f1: 0.9937
Epoch 162/300
 - 13s - loss: 0.0031 - val_loss: 0.0069
 - val_f1: 0.9754
Epoch 163/300
 - 13s - loss: 0.0029 - val_loss: 0.0025
 - val_f1: 0.9946
Epoch 164/300
 - 13s - loss: 0.0028 - val_loss: 0.0026
 - val_f1: 0.9945
Epoch 165/300
 - 13s - loss: 0.0028 - val_loss: 0.0032
 - val_f1: 0.9926
Epoch 166/300
 - 13s - loss: 0.0028 - val_loss: 0.0029
 - val_f1: 0.9941
Epoch 167/300
 - 13s - loss: 0.0027 - val_loss: 0.0024
 - val_f1: 0.9946
Epoch 168/300
 - 13s - loss: 0.0028 - val_loss: 0.0080
 - val_f1: 0.9734
Epoch 169/300
 - 13s - loss: 0.0029 - val_loss: 0.0114
 - val_f1: 0.9651
Epoch 170/300
 - 13s - loss: 0.0028 - val_loss: 0.0027
 - val_f1: 0.9929
Epoch 171/300
 - 13s - loss: 0.0027 - val_loss: 0.0026
 - val_f1: 0.9939
Epoch 172/300
 - 13s - loss: 0.0029 - val_loss: 0.0031
 - val_f1: 0.9923
Epoch 173/300
 - 13s - loss: 0.0028 - val_loss: 0.0031
 - val_f1: 0.9937
Epoch 174/300
 - 13s - loss: 0.0027 - val_loss: 0.0032
 - val_f1: 0.9932
Epoch 175/300
 - 13s - loss: 0.0027 - val_loss: 0.0027
 - val_f1: 0.9948
Epoch 176/300
 - 13s - loss: 0.0027 - val_loss: 0.0027
 - val_f1: 0.9943
Epoch 177/300
 - 13s - loss: 0.0027 - val_loss: 0.0026
 - val_f1: 0.9943
Epoch 178/300
 - 13s - loss: 0.0027 - val_loss: 0.0028
 - val_f1: 0.9943
Epoch 179/300
 - 13s - loss: 0.0027 - val_loss: 0.0025
2019-12-27 03:27:14,629 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-27 03:27:25,817 [INFO] Last epoch loss evaluation: train_loss = 0.002255, val_loss = 0.002427
2019-12-27 03:27:25,817 [INFO] Training complete. time_to_train = 2846.59 sec, 47.44 min
2019-12-27 03:27:25,823 [INFO] Model saved to results_selected_models/selected_ids17_lstm_deep_rep3/best_model.pickle
2019-12-27 03:27:25,825 [INFO] Training history saved to: results_selected_models/selected_ids17_lstm_deep_rep3/training_error_history.csv
2019-12-27 03:27:25,968 [INFO] Plot saved to: results_selected_models/selected_ids17_lstm_deep_rep3/training_error_history.png
2019-12-27 03:27:26,102 [INFO] Plot saved to: results_selected_models/selected_ids17_lstm_deep_rep3/training_f1_history.png
2019-12-27 03:27:26,102 [INFO] Making predictions on training, validation, testing data
2019-12-27 03:27:39,151 [INFO] Evaluating predictions (results)
/home/hasitha/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2019-12-27 03:27:49,566 [INFO] Dataset: Testing. Classification report below
2019-12-27 03:27:49,566 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       1.00      1.00      1.00    454243
                   Bot       0.97      0.37      0.53       391
                  DDoS       1.00      1.00      1.00     25604
         DoS GoldenEye       1.00      0.97      0.98      2058
              DoS Hulk       0.98      0.99      0.99     46023
      DoS Slowhttptest       0.89      0.99      0.93      1100
         DoS slowloris       0.99      0.97      0.98      1159
           FTP-Patator       0.99      1.00      0.99      1586
              PortScan       0.99      1.00      1.00     31761
           SSH-Patator       0.96      0.97      0.96      1179
Web Attack Brute Force       1.00      0.09      0.16       302
        Web Attack XSS       0.00      0.00      0.00       130

             micro avg       1.00      1.00      1.00    565536
             macro avg       0.90      0.78      0.79    565536
          weighted avg       1.00      1.00      1.00    565536

2019-12-27 03:27:49,566 [INFO] Overall accuracy (micro avg): 0.9954627114807899
/home/hasitha/sunanda/ids_experiments/utility.py:328: RuntimeWarning: invalid value encountered in true_divide
  PPV = TP / (TP + FP)    # Precision or positive predictive value
/home/hasitha/sunanda/ids_experiments/utility.py:332: RuntimeWarning: invalid value encountered in true_divide
  FDR = FP / (TP + FP)    # False discovery rate
2019-12-27 03:28:01,385 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9955         0.9955                       0.9955                0.0004                   0.0045  0.9955
1     Macro avg        0.9992         0.8966                       0.7779                0.0012                   0.2221  0.7940
2  Weighted avg        0.9962         0.9953                       0.9955                0.0095                   0.0045  0.9951
2019-12-27 03:28:11,774 [INFO] Dataset: Validation. Classification report below
2019-12-27 03:28:11,774 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       1.00      1.00      1.00    454243
                   Bot       0.94      0.33      0.49       391
                  DDoS       1.00      1.00      1.00     25605
         DoS GoldenEye       0.99      0.97      0.98      2059
              DoS Hulk       0.98      0.99      0.99     46023
      DoS Slowhttptest       0.89      0.98      0.93      1099
         DoS slowloris       0.99      0.97      0.98      1159
           FTP-Patator       0.99      1.00      0.99      1587
              PortScan       0.99      1.00      1.00     31758
           SSH-Patator       0.97      0.97      0.97      1180
Web Attack Brute Force       0.93      0.05      0.09       301
        Web Attack XSS       0.67      0.02      0.03       131

             micro avg       1.00      1.00      1.00    565536
             macro avg       0.95      0.77      0.79    565536
          weighted avg       1.00      1.00      1.00    565536

2019-12-27 03:28:11,775 [INFO] Overall accuracy (micro avg): 0.9955281361398743
2019-12-27 03:28:23,587 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9955         0.9955                       0.9955                0.0004                   0.0045  0.9955
1     Macro avg        0.9993         0.9454                       0.7719                0.0012                   0.2281  0.7868
2  Weighted avg        0.9963         0.9954                       0.9955                0.0094                   0.0045  0.9951
2019-12-27 03:28:57,785 [INFO] Dataset: Training. Classification report below
2019-12-27 03:28:57,785 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       1.00      1.00      1.00   1362781
                   Bot       0.97      0.37      0.54      1174
                  DDoS       1.00      1.00      1.00     76815
         DoS GoldenEye       0.99      0.98      0.99      6176
              DoS Hulk       0.98      0.99      0.99    138073
      DoS Slowhttptest       0.90      0.99      0.94      3300
         DoS slowloris       0.99      0.98      0.99      3478
           FTP-Patator       1.00      1.00      1.00      4761
              PortScan       0.99      1.00      1.00     95281
           SSH-Patator       0.97      0.97      0.97      3538
Web Attack Brute Force       0.95      0.08      0.15       904
        Web Attack XSS       1.00      0.02      0.04       391

             micro avg       1.00      1.00      1.00   1696672
             macro avg       0.98      0.78      0.80   1696672
          weighted avg       1.00      1.00      1.00   1696672

2019-12-27 03:28:57,785 [INFO] Overall accuracy (micro avg): 0.9958035495369759
2019-12-27 03:29:36,636 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9958         0.9958                       0.9958                0.0004                   0.0042  0.9958
1     Macro avg        0.9993         0.9790                       0.7809                0.0011                   0.2191  0.7991
2  Weighted avg        0.9965         0.9958                       0.9958                0.0088                   0.0042  0.9954
2019-12-27 03:29:36,660 [INFO] Results saved to: results_selected_models/selected_ids17_lstm_deep_rep3/selected_ids17_lstm_deep_rep3_results.xlsx
2019-12-27 03:29:36,666 [INFO] ================= Finished running experiment no. 3 ================= 

2019-12-27 03:29:36,736 [INFO] Created directory: results_selected_models/selected_ids17_lstm_deep_rep4
2019-12-27 03:29:36,736 [INFO] Initialized logging. log_filename = results_selected_models/selected_ids17_lstm_deep_rep4/run_log.log
2019-12-27 03:29:36,736 [INFO] ================= Running experiment no. 4  ================= 

2019-12-27 03:29:36,736 [INFO] Experiment parameters given below
2019-12-27 03:29:36,736 [INFO] 
{'experiment_num': 4, 'results_dir': 'results_selected_models/selected_ids17_lstm_deep_rep4', 'model': 'lstm', 'model_type': 'classifier', 'normal_label': 'BENIGN', 'lstm_time_steps': 32, 'lstm_layer_units': [64, 32], 'lstm_layer_activations': ['relu', 'relu'], 'lstm_layer_dropout_rates': [0.2, 0.2], 'batch_size': 256, 'epochs': 300, 'early_stop_patience': 50, 'dataset_dir': '../Datasets/full_datasets/ids2017', 'description': 'selected_ids17_lstm_deep_rep4'}
2019-12-27 03:29:36,736 [INFO] Created tensorboard log directory: results_selected_models/selected_ids17_lstm_deep_rep4/tf_logs_run_2019_12_27-03_29_36
2019-12-27 03:29:36,736 [INFO] Loading datsets from: ../Datasets/full_datasets/ids2017
2019-12-27 03:29:36,736 [INFO] Reading X, y files
2019-12-27 03:29:36,737 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_train.h5
2019-12-27 03:29:40,931 [INFO] Reading complete. time_to_read=4.19 seconds
2019-12-27 03:29:40,931 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_val.h5
2019-12-27 03:29:42,372 [INFO] Reading complete. time_to_read=1.44 seconds
2019-12-27 03:29:42,372 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_test.h5
2019-12-27 03:29:43,816 [INFO] Reading complete. time_to_read=1.44 seconds
2019-12-27 03:29:43,816 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_train.h5
2019-12-27 03:29:44,040 [INFO] Reading complete. time_to_read=0.22 seconds
2019-12-27 03:29:44,040 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_val.h5
2019-12-27 03:29:44,114 [INFO] Reading complete. time_to_read=0.07 seconds
2019-12-27 03:29:44,114 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_test.h5
2019-12-27 03:29:44,189 [INFO] Reading complete. time_to_read=0.07 seconds
2019-12-27 03:29:46,611 [INFO] Preparing flow sequences
2019-12-27 03:30:08,529 [INFO] Extracting flows complete. time_taken = 21.92 sec
2019-12-27 03:30:09,680 [INFO] Initializing model
2019-12-27 03:30:10,128 [INFO] _________________________________________________________________
2019-12-27 03:30:10,128 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-27 03:30:10,128 [INFO] =================================================================
2019-12-27 03:30:10,128 [INFO] lstm_17 (LSTM)               (None, 32, 64)            36608     
2019-12-27 03:30:10,128 [INFO] _________________________________________________________________
2019-12-27 03:30:10,128 [INFO] batch_normalization_17 (Batc (None, 32, 64)            256       
2019-12-27 03:30:10,128 [INFO] _________________________________________________________________
2019-12-27 03:30:10,128 [INFO] dropout_17 (Dropout)         (None, 32, 64)            0         
2019-12-27 03:30:10,128 [INFO] _________________________________________________________________
2019-12-27 03:30:10,129 [INFO] lstm_18 (LSTM)               (None, 32, 32)            12416     
2019-12-27 03:30:10,129 [INFO] _________________________________________________________________
2019-12-27 03:30:10,129 [INFO] batch_normalization_18 (Batc (None, 32, 32)            128       
2019-12-27 03:30:10,129 [INFO] _________________________________________________________________
2019-12-27 03:30:10,129 [INFO] dropout_18 (Dropout)         (None, 32, 32)            0         
2019-12-27 03:30:10,129 [INFO] _________________________________________________________________
2019-12-27 03:30:10,129 [INFO] time_distributed_9 (TimeDist (None, 32, 12)            396       
2019-12-27 03:30:10,129 [INFO] =================================================================
2019-12-27 03:30:10,129 [INFO] Total params: 49,804
2019-12-27 03:30:10,129 [INFO] Trainable params: 49,612
2019-12-27 03:30:10,129 [INFO] Non-trainable params: 192
2019-12-27 03:30:10,129 [INFO] _________________________________________________________________
2019-12-27 03:30:10,129 [INFO] Training model
 - val_f1: 0.9942
Epoch 00179: early stopping
Train on 53021 samples, validate on 17673 samples
Epoch 1/300
 - 14s - loss: 0.1393 - val_loss: 0.0451
 - val_f1: 0.8958
Epoch 2/300
 - 13s - loss: 0.0396 - val_loss: 0.0308
 - val_f1: 0.9224
Epoch 3/300
 - 13s - loss: 0.0292 - val_loss: 0.0240
 - val_f1: 0.9372
Epoch 4/300
 - 13s - loss: 0.0242 - val_loss: 0.0189
 - val_f1: 0.9487
Epoch 5/300
 - 13s - loss: 0.0211 - val_loss: 0.0168
 - val_f1: 0.9508
Epoch 6/300
 - 13s - loss: 0.0188 - val_loss: 0.0147
 - val_f1: 0.9635
Epoch 7/300
 - 13s - loss: 0.0169 - val_loss: 0.0167
 - val_f1: 0.9444
Epoch 8/300
 - 13s - loss: 0.0155 - val_loss: 0.0130
 - val_f1: 0.9665
Epoch 9/300
 - 13s - loss: 0.0145 - val_loss: 0.0124
 - val_f1: 0.9661
Epoch 10/300
 - 13s - loss: 0.0134 - val_loss: 0.0110
 - val_f1: 0.9676
Epoch 11/300
 - 13s - loss: 0.0123 - val_loss: 0.0194
 - val_f1: 0.9479
Epoch 12/300
 - 13s - loss: 0.0111 - val_loss: 0.0080
 - val_f1: 0.9821
Epoch 13/300
 - 13s - loss: 0.0100 - val_loss: 0.0081
 - val_f1: 0.9797
Epoch 14/300
 - 13s - loss: 0.0090 - val_loss: 0.0078
 - val_f1: 0.9814
Epoch 15/300
 - 13s - loss: 0.0086 - val_loss: 0.0070
 - val_f1: 0.9865
Epoch 16/300
 - 13s - loss: 0.0081 - val_loss: 0.0087
 - val_f1: 0.9721
Epoch 17/300
 - 13s - loss: 0.0077 - val_loss: 0.0085
 - val_f1: 0.9790
Epoch 18/300
 - 13s - loss: 0.0073 - val_loss: 0.0055
 - val_f1: 0.9879
Epoch 19/300
 - 13s - loss: 0.0068 - val_loss: 0.0056
 - val_f1: 0.9864
Epoch 20/300
 - 13s - loss: 0.0097 - val_loss: 0.0074
 - val_f1: 0.9818
Epoch 21/300
 - 13s - loss: 0.0090 - val_loss: 0.0072
 - val_f1: 0.9828
Epoch 22/300
 - 13s - loss: 0.0088 - val_loss: 0.0085
 - val_f1: 0.9757
Epoch 23/300
 - 13s - loss: 0.0084 - val_loss: 0.0074
 - val_f1: 0.9814
Epoch 24/300
 - 13s - loss: 0.0082 - val_loss: 0.0074
 - val_f1: 0.9776
Epoch 25/300
 - 13s - loss: 0.0079 - val_loss: 0.0067
 - val_f1: 0.9811
Epoch 26/300
 - 13s - loss: 0.0074 - val_loss: 0.0074
 - val_f1: 0.9757
Epoch 27/300
 - 13s - loss: 0.0067 - val_loss: 0.0060
 - val_f1: 0.9834
Epoch 28/300
 - 13s - loss: 0.0061 - val_loss: 0.0041
 - val_f1: 0.9923
Epoch 29/300
 - 13s - loss: 0.0055 - val_loss: 0.0037
 - val_f1: 0.9920
Epoch 30/300
 - 13s - loss: 0.0057 - val_loss: 0.0036
 - val_f1: 0.9926
Epoch 31/300
 - 13s - loss: 0.0049 - val_loss: 0.0039
/home/hasitha/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2019-12-27 03:38:29,062 [INFO] epoch = 30. Intermediate model saved to results_selected_models/selected_ids17_lstm_deep_rep4/_model_epoch_30.pickle
 - val_f1: 0.9914
Epoch 32/300
 - 13s - loss: 0.0048 - val_loss: 0.0086
 - val_f1: 0.9788
Epoch 33/300
 - 13s - loss: 0.0053 - val_loss: 0.0032
 - val_f1: 0.9938
Epoch 34/300
 - 13s - loss: 0.0044 - val_loss: 0.0037
 - val_f1: 0.9922
Epoch 35/300
 - 13s - loss: 0.0044 - val_loss: 0.0042
 - val_f1: 0.9902
Epoch 36/300
 - 13s - loss: 0.0043 - val_loss: 0.0034
 - val_f1: 0.9921
Epoch 37/300
 - 13s - loss: 0.0043 - val_loss: 0.0035
 - val_f1: 0.9928
Epoch 38/300
 - 13s - loss: 0.0049 - val_loss: 0.0030
 - val_f1: 0.9939
Epoch 39/300
 - 13s - loss: 0.0040 - val_loss: 0.0031
 - val_f1: 0.9932
Epoch 40/300
 - 13s - loss: 0.0041 - val_loss: 0.0040
 - val_f1: 0.9916
Epoch 41/300
 - 13s - loss: 0.0038 - val_loss: 0.0037
 - val_f1: 0.9914
Epoch 42/300
 - 13s - loss: 0.0037 - val_loss: 0.0031
 - val_f1: 0.9930
Epoch 43/300
 - 13s - loss: 0.0048 - val_loss: 0.0032
 - val_f1: 0.9933
Epoch 44/300
 - 13s - loss: 0.0038 - val_loss: 0.0031
 - val_f1: 0.9933
Epoch 45/300
 - 13s - loss: 0.0036 - val_loss: 0.0044
 - val_f1: 0.9906
Epoch 46/300
 - 13s - loss: 0.0037 - val_loss: 0.0033
 - val_f1: 0.9921
Epoch 47/300
 - 13s - loss: 0.0036 - val_loss: 0.0032
 - val_f1: 0.9921
Epoch 48/300
 - 13s - loss: 0.0036 - val_loss: 0.0028
 - val_f1: 0.9942
Epoch 49/300
 - 13s - loss: 0.0035 - val_loss: 0.0029
 - val_f1: 0.9931
Epoch 50/300
 - 13s - loss: 0.0035 - val_loss: 0.0038
 - val_f1: 0.9914
Epoch 51/300
 - 13s - loss: 0.0034 - val_loss: 0.0027
 - val_f1: 0.9945
Epoch 52/300
 - 13s - loss: 0.0034 - val_loss: 0.0031
 - val_f1: 0.9930
Epoch 53/300
 - 13s - loss: 0.0034 - val_loss: 0.0025
 - val_f1: 0.9947
Epoch 54/300
 - 13s - loss: 0.0036 - val_loss: 0.0031
 - val_f1: 0.9937
Epoch 55/300
 - 13s - loss: 0.0065 - val_loss: 0.0046
 - val_f1: 0.9896
Epoch 56/300
 - 13s - loss: 0.0051 - val_loss: 0.0126
 - val_f1: 0.9833
Epoch 57/300
 - 13s - loss: 0.0045 - val_loss: 0.0048
 - val_f1: 0.9917
Epoch 58/300
 - 13s - loss: 0.0042 - val_loss: 0.0030
 - val_f1: 0.9939
Epoch 59/300
 - 13s - loss: 0.0040 - val_loss: 0.0027
 - val_f1: 0.9942
Epoch 60/300
 - 13s - loss: 0.0038 - val_loss: 0.0033
 - val_f1: 0.9918
Epoch 61/300
 - 13s - loss: 0.0037 - val_loss: 0.0075
2019-12-27 03:46:24,849 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_ids17_lstm_deep_rep4/_model_epoch_60.pickle
 - val_f1: 0.9890
Epoch 62/300
 - 13s - loss: 0.0038 - val_loss: 0.0030
 - val_f1: 0.9926
Epoch 63/300
 - 13s - loss: 0.0036 - val_loss: 0.0028
 - val_f1: 0.9936
Epoch 64/300
 - 13s - loss: 0.0035 - val_loss: 0.0028
 - val_f1: 0.9942
Epoch 65/300
 - 13s - loss: 0.0034 - val_loss: 0.0028
 - val_f1: 0.9929
Epoch 66/300
 - 13s - loss: 0.0034 - val_loss: 0.0029
 - val_f1: 0.9945
Epoch 67/300
 - 13s - loss: 0.0033 - val_loss: 0.0030
 - val_f1: 0.9930
Epoch 68/300
 - 13s - loss: 0.0033 - val_loss: 0.0058
 - val_f1: 0.9894
Epoch 69/300
 - 13s - loss: 0.0033 - val_loss: 0.0028
 - val_f1: 0.9938
Epoch 70/300
 - 13s - loss: 0.0032 - val_loss: 0.0035
 - val_f1: 0.9923
Epoch 71/300
 - 13s - loss: 0.0032 - val_loss: 0.0029
 - val_f1: 0.9933
Epoch 72/300
 - 13s - loss: 0.0032 - val_loss: 0.0027
 - val_f1: 0.9926
Epoch 73/300
 - 13s - loss: 0.0032 - val_loss: 0.0028
 - val_f1: 0.9934
Epoch 74/300
 - 13s - loss: 0.0031 - val_loss: 0.0027
 - val_f1: 0.9937
Epoch 75/300
 - 13s - loss: 0.0032 - val_loss: 0.0026
 - val_f1: 0.9944
Epoch 76/300
 - 13s - loss: 0.0030 - val_loss: 0.0028
 - val_f1: 0.9939
Epoch 77/300
 - 13s - loss: 0.0030 - val_loss: 0.0025
 - val_f1: 0.9940
Epoch 78/300
 - 13s - loss: 0.0031 - val_loss: 0.0025
 - val_f1: 0.9944
Epoch 79/300
 - 13s - loss: 0.0030 - val_loss: 0.0034
 - val_f1: 0.9935
Epoch 80/300
 - 13s - loss: 0.0032 - val_loss: 0.0051
 - val_f1: 0.9909
Epoch 81/300
 - 13s - loss: 0.0031 - val_loss: 0.0025
 - val_f1: 0.9946
Epoch 82/300
 - 13s - loss: 0.0038 - val_loss: 0.0167
 - val_f1: 0.9787
Epoch 83/300
 - 13s - loss: 0.0038 - val_loss: 0.0050
 - val_f1: 0.9881
Epoch 84/300
 - 13s - loss: 0.0031 - val_loss: 0.0060
 - val_f1: 0.9771
Epoch 85/300
 - 13s - loss: 0.0031 - val_loss: 0.0048
 - val_f1: 0.9909
Epoch 86/300
 - 13s - loss: 0.0030 - val_loss: 0.0053
 - val_f1: 0.9917
Epoch 87/300
 - 13s - loss: 0.0032 - val_loss: 0.0038
 - val_f1: 0.9921
Epoch 88/300
 - 13s - loss: 0.0030 - val_loss: 0.0040
 - val_f1: 0.9911
Epoch 89/300
 - 13s - loss: 0.0029 - val_loss: 0.0060
 - val_f1: 0.9775
Epoch 90/300
 - 13s - loss: 0.0029 - val_loss: 0.0029
 - val_f1: 0.9937
Epoch 91/300
 - 13s - loss: 0.0029 - val_loss: 0.0030
2019-12-27 03:54:20,244 [INFO] epoch = 90. Intermediate model saved to results_selected_models/selected_ids17_lstm_deep_rep4/_model_epoch_90.pickle
 - val_f1: 0.9945
Epoch 92/300
 - 13s - loss: 0.0028 - val_loss: 0.0025
 - val_f1: 0.9942
Epoch 93/300
 - 13s - loss: 0.0029 - val_loss: 0.0030
 - val_f1: 0.9925
Epoch 94/300
 - 13s - loss: 0.0030 - val_loss: 0.0029
 - val_f1: 0.9934
Epoch 95/300
 - 13s - loss: 0.0028 - val_loss: 0.0024
 - val_f1: 0.9946
Epoch 96/300
 - 13s - loss: 0.0028 - val_loss: 0.0024
 - val_f1: 0.9943
Epoch 97/300
 - 13s - loss: 0.0029 - val_loss: 0.0026
 - val_f1: 0.9942
Epoch 98/300
 - 13s - loss: 0.0028 - val_loss: 0.0025
 - val_f1: 0.9944
Epoch 99/300
 - 13s - loss: 0.0027 - val_loss: 0.0026
 - val_f1: 0.9941
Epoch 100/300
 - 13s - loss: 0.0027 - val_loss: 0.0028
 - val_f1: 0.9929
Epoch 101/300
 - 13s - loss: 0.0028 - val_loss: 0.0024
 - val_f1: 0.9945
Epoch 102/300
 - 13s - loss: 0.0028 - val_loss: 0.0024
 - val_f1: 0.9947
Epoch 103/300
 - 13s - loss: 0.0028 - val_loss: 0.0025
 - val_f1: 0.9943
Epoch 104/300
 - 13s - loss: 0.0027 - val_loss: 0.0024
 - val_f1: 0.9949
Epoch 105/300
 - 13s - loss: 0.0029 - val_loss: 0.0024
 - val_f1: 0.9947
Epoch 106/300
 - 13s - loss: 0.0029 - val_loss: 0.0033
 - val_f1: 0.9924
Epoch 107/300
 - 13s - loss: 0.0029 - val_loss: 0.0034
 - val_f1: 0.9917
Epoch 108/300
 - 13s - loss: 0.0028 - val_loss: 0.0036
 - val_f1: 0.9923
Epoch 109/300
 - 13s - loss: 0.0031 - val_loss: 0.0026
 - val_f1: 0.9943
Epoch 110/300
 - 13s - loss: 0.0027 - val_loss: 0.0047
 - val_f1: 0.9869
Epoch 111/300
 - 13s - loss: 0.0028 - val_loss: 0.0032
 - val_f1: 0.9931
Epoch 112/300
 - 13s - loss: 0.0027 - val_loss: 0.0024
 - val_f1: 0.9947
Epoch 113/300
 - 13s - loss: 0.0027 - val_loss: 0.0024
 - val_f1: 0.9947
Epoch 114/300
 - 13s - loss: 0.0027 - val_loss: 0.0024
 - val_f1: 0.9947
Epoch 115/300
 - 13s - loss: 0.0026 - val_loss: 0.0033
 - val_f1: 0.9936
Epoch 116/300
 - 13s - loss: 0.0027 - val_loss: 0.0028
 - val_f1: 0.9942
Epoch 117/300
 - 13s - loss: 0.0026 - val_loss: 0.0023
 - val_f1: 0.9951
Epoch 118/300
 - 13s - loss: 0.0027 - val_loss: 0.0023
 - val_f1: 0.9947
Epoch 119/300
 - 13s - loss: 0.0026 - val_loss: 0.0035
 - val_f1: 0.9927
Epoch 120/300
 - 13s - loss: 0.0026 - val_loss: 0.0023
 - val_f1: 0.9948
Epoch 121/300
 - 13s - loss: 0.0026 - val_loss: 0.0023
2019-12-27 04:02:16,225 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_ids17_lstm_deep_rep4/_model_epoch_120.pickle
 - val_f1: 0.9949
Epoch 122/300
 - 13s - loss: 0.0026 - val_loss: 0.0026
 - val_f1: 0.9945
Epoch 123/300
 - 13s - loss: 0.0029 - val_loss: 0.0110
 - val_f1: 0.9840
Epoch 124/300
 - 13s - loss: 0.0027 - val_loss: 0.0041
 - val_f1: 0.9900
Epoch 125/300
 - 13s - loss: 0.0027 - val_loss: 0.0025
 - val_f1: 0.9944
Epoch 126/300
 - 13s - loss: 0.0026 - val_loss: 0.0024
 - val_f1: 0.9949
Epoch 127/300
 - 13s - loss: 0.0026 - val_loss: 0.0056
 - val_f1: 0.9914
Epoch 128/300
 - 13s - loss: 0.0026 - val_loss: 0.0023
 - val_f1: 0.9948
Epoch 129/300
 - 13s - loss: 0.0026 - val_loss: 0.0030
 - val_f1: 0.9929
Epoch 130/300
 - 13s - loss: 0.0027 - val_loss: 0.0024
 - val_f1: 0.9948
Epoch 131/300
 - 13s - loss: 0.0026 - val_loss: 0.0024
 - val_f1: 0.9949
Epoch 132/300
 - 13s - loss: 0.0028 - val_loss: 0.0024
 - val_f1: 0.9946
Epoch 133/300
 - 13s - loss: 0.0027 - val_loss: 0.0028
 - val_f1: 0.9929
Epoch 134/300
 - 13s - loss: 0.0026 - val_loss: 0.0034
 - val_f1: 0.9925
Epoch 135/300
 - 13s - loss: 0.0026 - val_loss: 0.0026
 - val_f1: 0.9942
Epoch 136/300
 - 13s - loss: 0.0026 - val_loss: 0.0023
 - val_f1: 0.9950
Epoch 137/300
 - 13s - loss: 0.0025 - val_loss: 0.0022
 - val_f1: 0.9950
Epoch 138/300
 - 13s - loss: 0.0025 - val_loss: 0.0023
 - val_f1: 0.9951
Epoch 139/300
 - 13s - loss: 0.0025 - val_loss: 0.0023
 - val_f1: 0.9950
Epoch 140/300
 - 13s - loss: 0.0026 - val_loss: 0.0028
 - val_f1: 0.9940
Epoch 141/300
 - 13s - loss: 0.0027 - val_loss: 0.0026
 - val_f1: 0.9932
Epoch 142/300
 - 13s - loss: 0.0025 - val_loss: 0.0027
 - val_f1: 0.9941
Epoch 143/300
 - 13s - loss: 0.0025 - val_loss: 0.0035
 - val_f1: 0.9927
Epoch 144/300
 - 13s - loss: 0.0025 - val_loss: 0.0025
 - val_f1: 0.9947
Epoch 145/300
 - 13s - loss: 0.0025 - val_loss: 0.0026
 - val_f1: 0.9945
Epoch 146/300
 - 13s - loss: 0.0025 - val_loss: 0.0052
 - val_f1: 0.9845
Epoch 147/300
 - 13s - loss: 0.0025 - val_loss: 0.0025
 - val_f1: 0.9946
Epoch 148/300
 - 13s - loss: 0.0025 - val_loss: 0.0023
 - val_f1: 0.9947
Epoch 149/300
 - 13s - loss: 0.0025 - val_loss: 0.0103
 - val_f1: 0.9852
Epoch 150/300
 - 13s - loss: 0.0026 - val_loss: 0.0023
 - val_f1: 0.9950
Epoch 151/300
 - 13s - loss: 0.0040 - val_loss: 0.0120
2019-12-27 04:10:11,378 [INFO] epoch = 150. Intermediate model saved to results_selected_models/selected_ids17_lstm_deep_rep4/_model_epoch_150.pickle
 - val_f1: 0.9639
Epoch 152/300
 - 13s - loss: 0.0029 - val_loss: 0.0032
 - val_f1: 0.9924
Epoch 153/300
 - 13s - loss: 0.0028 - val_loss: 0.0026
 - val_f1: 0.9943
Epoch 154/300
 - 13s - loss: 0.0027 - val_loss: 0.0028
 - val_f1: 0.9946
Epoch 155/300
 - 13s - loss: 0.0027 - val_loss: 0.0024
 - val_f1: 0.9941
Epoch 156/300
 - 13s - loss: 0.0027 - val_loss: 0.0024
 - val_f1: 0.9950
Epoch 157/300
 - 13s - loss: 0.0027 - val_loss: 0.0024
 - val_f1: 0.9950
Epoch 158/300
 - 13s - loss: 0.0026 - val_loss: 0.0030
 - val_f1: 0.9941
Epoch 159/300
 - 13s - loss: 0.0026 - val_loss: 0.0023
 - val_f1: 0.9951
Epoch 160/300
 - 13s - loss: 0.0026 - val_loss: 0.0023
 - val_f1: 0.9951
Epoch 161/300
 - 13s - loss: 0.0026 - val_loss: 0.0023
 - val_f1: 0.9949
Epoch 162/300
 - 13s - loss: 0.0025 - val_loss: 0.0027
 - val_f1: 0.9944
Epoch 163/300
 - 13s - loss: 0.0024 - val_loss: 0.0026
 - val_f1: 0.9946
Epoch 164/300
 - 13s - loss: 0.0027 - val_loss: 0.0025
 - val_f1: 0.9950
Epoch 165/300
 - 13s - loss: 0.0025 - val_loss: 0.0022
 - val_f1: 0.9951
Epoch 166/300
 - 13s - loss: 0.0024 - val_loss: 0.0052
 - val_f1: 0.9918
Epoch 167/300
 - 13s - loss: 0.0025 - val_loss: 0.0024
 - val_f1: 0.9946
Epoch 168/300
 - 13s - loss: 0.0024 - val_loss: 0.0022
 - val_f1: 0.9951
Epoch 169/300
 - 13s - loss: 0.0024 - val_loss: 0.0023
 - val_f1: 0.9947
Epoch 170/300
 - 13s - loss: 0.0024 - val_loss: 0.0023
 - val_f1: 0.9947
Epoch 171/300
 - 13s - loss: 0.0024 - val_loss: 0.0023
 - val_f1: 0.9951
Epoch 172/300
 - 13s - loss: 0.0024 - val_loss: 0.0024
 - val_f1: 0.9948
Epoch 173/300
 - 13s - loss: 0.0024 - val_loss: 0.0023
 - val_f1: 0.9949
Epoch 174/300
 - 13s - loss: 0.0024 - val_loss: 0.0025
 - val_f1: 0.9946
Epoch 175/300
 - 13s - loss: 0.0023 - val_loss: 0.0023
 - val_f1: 0.9949
Epoch 176/300
 - 13s - loss: 0.0025 - val_loss: 0.0024
 - val_f1: 0.9951
Epoch 177/300
 - 13s - loss: 0.0025 - val_loss: 0.0022
 - val_f1: 0.9953
Epoch 178/300
 - 13s - loss: 0.0024 - val_loss: 0.0025
 - val_f1: 0.9948
Epoch 179/300
 - 13s - loss: 0.0024 - val_loss: 0.0025
 - val_f1: 0.9947
Epoch 180/300
 - 13s - loss: 0.0024 - val_loss: 0.0026
 - val_f1: 0.9944
Epoch 181/300
 - 13s - loss: 0.0024 - val_loss: 0.0064
2019-12-27 04:18:06,765 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_ids17_lstm_deep_rep4/_model_epoch_180.pickle
 - val_f1: 0.9912
Epoch 182/300
 - 13s - loss: 0.0025 - val_loss: 0.0033
 - val_f1: 0.9928
Epoch 183/300
 - 13s - loss: 0.0024 - val_loss: 0.0025
 - val_f1: 0.9940
Epoch 184/300
 - 13s - loss: 0.0023 - val_loss: 0.0022
 - val_f1: 0.9954
Epoch 185/300
 - 13s - loss: 0.0024 - val_loss: 0.0023
 - val_f1: 0.9950
Epoch 186/300
 - 13s - loss: 0.0023 - val_loss: 0.0024
 - val_f1: 0.9949
Epoch 187/300
 - 13s - loss: 0.0024 - val_loss: 0.0023
 - val_f1: 0.9950
Epoch 188/300
 - 13s - loss: 0.0024 - val_loss: 0.0023
 - val_f1: 0.9948
Epoch 189/300
 - 13s - loss: 0.0023 - val_loss: 0.0026
 - val_f1: 0.9946
Epoch 190/300
 - 13s - loss: 0.0026 - val_loss: 0.0227
 - val_f1: 0.9697
Epoch 191/300
 - 13s - loss: 0.0039 - val_loss: 0.0027
 - val_f1: 0.9946
Epoch 192/300
 - 13s - loss: 0.0028 - val_loss: 0.0026
 - val_f1: 0.9943
Epoch 193/300
 - 13s - loss: 0.0026 - val_loss: 0.0032
 - val_f1: 0.9941
Epoch 194/300
 - 13s - loss: 0.0026 - val_loss: 0.0022
 - val_f1: 0.9951
Epoch 195/300
 - 13s - loss: 0.0026 - val_loss: 0.0024
 - val_f1: 0.9951
Epoch 196/300
 - 13s - loss: 0.0025 - val_loss: 0.0025
 - val_f1: 0.9949
Epoch 197/300
 - 13s - loss: 0.0025 - val_loss: 0.0023
 - val_f1: 0.9950
Epoch 198/300
 - 13s - loss: 0.0025 - val_loss: 0.0022
 - val_f1: 0.9952
Epoch 199/300
 - 13s - loss: 0.0025 - val_loss: 0.0022
 - val_f1: 0.9952
Epoch 200/300
 - 13s - loss: 0.0024 - val_loss: 0.0023
 - val_f1: 0.9950
Epoch 201/300
 - 13s - loss: 0.0025 - val_loss: 0.0039
 - val_f1: 0.9924
Epoch 202/300
 - 13s - loss: 0.0025 - val_loss: 0.0034
 - val_f1: 0.9925
Epoch 203/300
 - 13s - loss: 0.0024 - val_loss: 0.0023
 - val_f1: 0.9952
Epoch 204/300
 - 13s - loss: 0.0026 - val_loss: 0.0025
 - val_f1: 0.9944
Epoch 205/300
 - 13s - loss: 0.0024 - val_loss: 0.0025
 - val_f1: 0.9952
Epoch 206/300
 - 13s - loss: 0.0024 - val_loss: 0.0021
 - val_f1: 0.9956
Epoch 207/300
 - 13s - loss: 0.0024 - val_loss: 0.0024
 - val_f1: 0.9952
Epoch 208/300
 - 13s - loss: 0.0026 - val_loss: 0.0032
 - val_f1: 0.9928
Epoch 209/300
 - 13s - loss: 0.0025 - val_loss: 0.0023
 - val_f1: 0.9950
Epoch 210/300
 - 13s - loss: 0.0025 - val_loss: 0.0025
 - val_f1: 0.9944
Epoch 211/300
 - 13s - loss: 0.0024 - val_loss: 0.0026
2019-12-27 04:26:02,989 [INFO] epoch = 210. Intermediate model saved to results_selected_models/selected_ids17_lstm_deep_rep4/_model_epoch_210.pickle
 - val_f1: 0.9950
Epoch 212/300
 - 13s - loss: 0.0025 - val_loss: 0.0025
 - val_f1: 0.9947
Epoch 213/300
 - 13s - loss: 0.0024 - val_loss: 0.0025
 - val_f1: 0.9948
Epoch 214/300
 - 13s - loss: 0.0024 - val_loss: 0.0023
 - val_f1: 0.9950
Epoch 215/300
 - 13s - loss: 0.0024 - val_loss: 0.0024
 - val_f1: 0.9949
Epoch 216/300
 - 13s - loss: 0.0024 - val_loss: 0.0024
 - val_f1: 0.9947
Epoch 217/300
 - 13s - loss: 0.0026 - val_loss: 0.0026
 - val_f1: 0.9941
Epoch 218/300
 - 13s - loss: 0.0026 - val_loss: 0.0028
 - val_f1: 0.9937
Epoch 219/300
 - 13s - loss: 0.0024 - val_loss: 0.0025
 - val_f1: 0.9940
Epoch 220/300
 - 13s - loss: 0.0023 - val_loss: 0.0029
 - val_f1: 0.9926
Epoch 221/300
 - 13s - loss: 0.0023 - val_loss: 0.0025
 - val_f1: 0.9944
Epoch 222/300
 - 13s - loss: 0.0023 - val_loss: 0.0024
 - val_f1: 0.9947
Epoch 223/300
 - 13s - loss: 0.0023 - val_loss: 0.0024
 - val_f1: 0.9943
Epoch 224/300
 - 13s - loss: 0.0023 - val_loss: 0.0022
 - val_f1: 0.9950
Epoch 225/300
 - 13s - loss: 0.0023 - val_loss: 0.0024
 - val_f1: 0.9950
Epoch 226/300
 - 13s - loss: 0.0023 - val_loss: 0.0028
 - val_f1: 0.9948
Epoch 227/300
 - 13s - loss: 0.0023 - val_loss: 0.0024
 - val_f1: 0.9945
Epoch 228/300
 - 13s - loss: 0.0022 - val_loss: 0.0023
 - val_f1: 0.9949
Epoch 229/300
 - 13s - loss: 0.0022 - val_loss: 0.0023
 - val_f1: 0.9951
Epoch 230/300
 - 13s - loss: 0.0023 - val_loss: 0.0025
 - val_f1: 0.9939
Epoch 231/300
 - 13s - loss: 0.0022 - val_loss: 0.0021
 - val_f1: 0.9949
Epoch 232/300
 - 13s - loss: 0.0022 - val_loss: 0.0022
 - val_f1: 0.9952
Epoch 233/300
 - 13s - loss: 0.0022 - val_loss: 0.0025
 - val_f1: 0.9946
Epoch 234/300
 - 13s - loss: 0.0023 - val_loss: 0.0021
 - val_f1: 0.9958
Epoch 235/300
 - 13s - loss: 0.0022 - val_loss: 0.0036
 - val_f1: 0.9921
Epoch 236/300
 - 13s - loss: 0.0024 - val_loss: 0.0023
 - val_f1: 0.9952
Epoch 237/300
 - 13s - loss: 0.0033 - val_loss: 0.0028
 - val_f1: 0.9930
Epoch 238/300
 - 13s - loss: 0.0033 - val_loss: 0.0031
 - val_f1: 0.9935
Epoch 239/300
 - 13s - loss: 0.0027 - val_loss: 0.0040
 - val_f1: 0.9928
Epoch 240/300
 - 13s - loss: 0.0023 - val_loss: 0.0023
 - val_f1: 0.9946
Epoch 241/300
 - 13s - loss: 0.0023 - val_loss: 0.0020
2019-12-27 04:33:58,557 [INFO] epoch = 240. Intermediate model saved to results_selected_models/selected_ids17_lstm_deep_rep4/_model_epoch_240.pickle
 - val_f1: 0.9959
Epoch 242/300
 - 13s - loss: 0.0022 - val_loss: 0.0022
 - val_f1: 0.9955
Epoch 243/300
 - 13s - loss: 0.0022 - val_loss: 0.0021
 - val_f1: 0.9955
Epoch 244/300
 - 13s - loss: 0.0022 - val_loss: 0.0022
 - val_f1: 0.9953
Epoch 245/300
 - 13s - loss: 0.0022 - val_loss: 0.0023
 - val_f1: 0.9953
Epoch 246/300
 - 13s - loss: 0.0023 - val_loss: 0.0021
 - val_f1: 0.9949
Epoch 247/300
 - 13s - loss: 0.0022 - val_loss: 0.0022
 - val_f1: 0.9953
Epoch 248/300
 - 13s - loss: 0.0023 - val_loss: 0.0019
 - val_f1: 0.9959
Epoch 249/300
 - 13s - loss: 0.0022 - val_loss: 0.0021
 - val_f1: 0.9954
Epoch 250/300
 - 13s - loss: 0.0022 - val_loss: 0.0024
 - val_f1: 0.9949
Epoch 251/300
 - 13s - loss: 0.0022 - val_loss: 0.0021
 - val_f1: 0.9954
Epoch 252/300
 - 13s - loss: 0.0022 - val_loss: 0.0021
 - val_f1: 0.9952
Epoch 253/300
 - 13s - loss: 0.0022 - val_loss: 0.0023
 - val_f1: 0.9945
Epoch 254/300
 - 13s - loss: 0.0022 - val_loss: 0.0024
 - val_f1: 0.9946
Epoch 255/300
 - 13s - loss: 0.0021 - val_loss: 0.0030
 - val_f1: 0.9929
Epoch 256/300
 - 13s - loss: 0.0021 - val_loss: 0.0022
 - val_f1: 0.9950
Epoch 257/300
 - 13s - loss: 0.0021 - val_loss: 0.0024
 - val_f1: 0.9954
Epoch 258/300
 - 13s - loss: 0.0021 - val_loss: 0.0021
 - val_f1: 0.9957
Epoch 259/300
 - 13s - loss: 0.0021 - val_loss: 0.0023
 - val_f1: 0.9952
Epoch 260/300
 - 13s - loss: 0.0022 - val_loss: 0.0044
 - val_f1: 0.9906
Epoch 261/300
 - 13s - loss: 0.0023 - val_loss: 0.0024
 - val_f1: 0.9950
Epoch 262/300
 - 13s - loss: 0.0022 - val_loss: 0.0038
 - val_f1: 0.9916
Epoch 263/300
 - 13s - loss: 0.0022 - val_loss: 0.0026
 - val_f1: 0.9943
Epoch 264/300
 - 13s - loss: 0.0021 - val_loss: 0.0022
 - val_f1: 0.9949
Epoch 265/300
 - 13s - loss: 0.0022 - val_loss: 0.0020
 - val_f1: 0.9958
Epoch 266/300
 - 13s - loss: 0.0021 - val_loss: 0.0023
 - val_f1: 0.9942
Epoch 267/300
 - 13s - loss: 0.0021 - val_loss: 0.0025
 - val_f1: 0.9932
Epoch 268/300
 - 13s - loss: 0.0021 - val_loss: 0.0022
 - val_f1: 0.9953
Epoch 269/300
 - 13s - loss: 0.0021 - val_loss: 0.0020
 - val_f1: 0.9959
Epoch 270/300
 - 13s - loss: 0.0021 - val_loss: 0.0020
 - val_f1: 0.9961
Epoch 271/300
 - 13s - loss: 0.0021 - val_loss: 0.0021
2019-12-27 04:41:54,436 [INFO] epoch = 270. Intermediate model saved to results_selected_models/selected_ids17_lstm_deep_rep4/_model_epoch_270.pickle
 - val_f1: 0.9952
Epoch 272/300
 - 13s - loss: 0.0025 - val_loss: 0.0021
 - val_f1: 0.9954
Epoch 273/300
 - 13s - loss: 0.0021 - val_loss: 0.0023
 - val_f1: 0.9947
Epoch 274/300
 - 13s - loss: 0.0021 - val_loss: 0.0021
 - val_f1: 0.9953
Epoch 275/300
 - 13s - loss: 0.0021 - val_loss: 0.0023
 - val_f1: 0.9945
Epoch 276/300
 - 13s - loss: 0.0020 - val_loss: 0.0023
 - val_f1: 0.9943
Epoch 277/300
 - 13s - loss: 0.0021 - val_loss: 0.0024
 - val_f1: 0.9936
Epoch 278/300
 - 13s - loss: 0.0021 - val_loss: 0.0022
 - val_f1: 0.9953
Epoch 279/300
 - 13s - loss: 0.0020 - val_loss: 0.0023
 - val_f1: 0.9947
Epoch 280/300
 - 13s - loss: 0.0021 - val_loss: 0.0021
 - val_f1: 0.9953
Epoch 281/300
 - 13s - loss: 0.0020 - val_loss: 0.0027
 - val_f1: 0.9935
Epoch 282/300
 - 13s - loss: 0.0020 - val_loss: 0.0020
 - val_f1: 0.9958
Epoch 283/300
 - 13s - loss: 0.0021 - val_loss: 0.0021
 - val_f1: 0.9956
Epoch 284/300
 - 13s - loss: 0.0020 - val_loss: 0.0024
 - val_f1: 0.9945
Epoch 285/300
 - 13s - loss: 0.0020 - val_loss: 0.0022
 - val_f1: 0.9955
Epoch 286/300
 - 13s - loss: 0.0020 - val_loss: 0.0030
 - val_f1: 0.9930
Epoch 287/300
 - 13s - loss: 0.0020 - val_loss: 0.0023
 - val_f1: 0.9950
Epoch 288/300
 - 13s - loss: 0.0020 - val_loss: 0.0025
 - val_f1: 0.9950
Epoch 289/300
 - 13s - loss: 0.0020 - val_loss: 0.0021
 - val_f1: 0.9955
Epoch 290/300
 - 13s - loss: 0.0021 - val_loss: 0.0021
 - val_f1: 0.9958
Epoch 291/300
 - 13s - loss: 0.0020 - val_loss: 0.0025
 - val_f1: 0.9948
Epoch 292/300
 - 13s - loss: 0.0020 - val_loss: 0.0021
 - val_f1: 0.9954
Epoch 293/300
 - 13s - loss: 0.0020 - val_loss: 0.0026
 - val_f1: 0.9930
Epoch 294/300
 - 13s - loss: 0.0020 - val_loss: 0.0023
 - val_f1: 0.9953
Epoch 295/300
 - 13s - loss: 0.0023 - val_loss: 0.0023
 - val_f1: 0.9949
Epoch 296/300
 - 13s - loss: 0.0021 - val_loss: 0.0025
 - val_f1: 0.9944
Epoch 297/300
 - 13s - loss: 0.0020 - val_loss: 0.0023
 - val_f1: 0.9947
Epoch 298/300
 - 13s - loss: 0.0020 - val_loss: 0.0024
2019-12-27 04:49:05,364 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-27 04:49:16,507 [INFO] Last epoch loss evaluation: train_loss = 0.001630, val_loss = 0.001936
2019-12-27 04:49:16,507 [INFO] Training complete. time_to_train = 4746.38 sec, 79.11 min
2019-12-27 04:49:16,513 [INFO] Model saved to results_selected_models/selected_ids17_lstm_deep_rep4/best_model.pickle
2019-12-27 04:49:16,516 [INFO] Training history saved to: results_selected_models/selected_ids17_lstm_deep_rep4/training_error_history.csv
2019-12-27 04:49:16,656 [INFO] Plot saved to: results_selected_models/selected_ids17_lstm_deep_rep4/training_error_history.png
2019-12-27 04:49:16,780 [INFO] Plot saved to: results_selected_models/selected_ids17_lstm_deep_rep4/training_f1_history.png
2019-12-27 04:49:16,780 [INFO] Making predictions on training, validation, testing data
2019-12-27 04:49:29,831 [INFO] Evaluating predictions (results)
/home/hasitha/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2019-12-27 04:49:40,195 [INFO] Dataset: Testing. Classification report below
2019-12-27 04:49:40,196 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       1.00      1.00      1.00    454243
                   Bot       0.96      0.40      0.56       391
                  DDoS       1.00      1.00      1.00     25604
         DoS GoldenEye       0.99      0.98      0.99      2058
              DoS Hulk       0.98      0.99      0.99     46023
      DoS Slowhttptest       0.89      0.99      0.94      1100
         DoS slowloris       0.99      0.98      0.99      1159
           FTP-Patator       1.00      1.00      1.00      1586
              PortScan       0.99      1.00      1.00     31761
           SSH-Patator       0.96      0.97      0.96      1179
Web Attack Brute Force       0.64      0.43      0.52       302
        Web Attack XSS       0.00      0.00      0.00       130

             micro avg       1.00      1.00      1.00    565536
             macro avg       0.87      0.81      0.83    565536
          weighted avg       1.00      1.00      1.00    565536

2019-12-27 04:49:40,196 [INFO] Overall accuracy (micro avg): 0.9960851298591071
/home/hasitha/sunanda/ids_experiments/utility.py:328: RuntimeWarning: invalid value encountered in true_divide
  PPV = TP / (TP + FP)    # Precision or positive predictive value
/home/hasitha/sunanda/ids_experiments/utility.py:332: RuntimeWarning: invalid value encountered in true_divide
  FDR = FP / (TP + FP)    # False discovery rate
2019-12-27 04:49:51,973 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9961         0.9961                       0.9961                0.0004                   0.0039  0.9961
1     Macro avg        0.9993         0.8660                       0.8113                0.0009                   0.1887  0.8272
2  Weighted avg        0.9969         0.9958                       0.9961                0.0068                   0.0039  0.9959
2019-12-27 04:50:02,342 [INFO] Dataset: Validation. Classification report below
2019-12-27 04:50:02,342 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       1.00      1.00      1.00    454243
                   Bot       0.95      0.36      0.53       391
                  DDoS       1.00      1.00      1.00     25605
         DoS GoldenEye       0.99      0.98      0.99      2059
              DoS Hulk       0.98      0.99      0.99     46023
      DoS Slowhttptest       0.89      0.98      0.93      1099
         DoS slowloris       0.99      0.98      0.98      1159
           FTP-Patator       0.99      0.99      0.99      1587
              PortScan       0.99      1.00      1.00     31758
           SSH-Patator       0.96      0.97      0.96      1180
Web Attack Brute Force       0.64      0.35      0.45       301
        Web Attack XSS       0.75      0.02      0.04       131

             micro avg       1.00      1.00      1.00    565536
             macro avg       0.93      0.80      0.82    565536
          weighted avg       1.00      1.00      1.00    565536

2019-12-27 04:50:02,342 [INFO] Overall accuracy (micro avg): 0.9961841509647484
2019-12-27 04:50:14,139 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9962         0.9962                       0.9962                0.0003                   0.0038  0.9962
1     Macro avg        0.9994         0.9288                       0.8024                0.0009                   0.1976  0.8224
2  Weighted avg        0.9969         0.9961                       0.9962                0.0067                   0.0038  0.9959
2019-12-27 04:50:48,402 [INFO] Dataset: Training. Classification report below
2019-12-27 04:50:48,402 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       1.00      1.00      1.00   1362781
                   Bot       0.98      0.41      0.57      1174
                  DDoS       1.00      1.00      1.00     76815
         DoS GoldenEye       1.00      0.99      0.99      6176
              DoS Hulk       0.99      1.00      0.99    138073
      DoS Slowhttptest       0.91      0.99      0.95      3300
         DoS slowloris       0.99      0.99      0.99      3478
           FTP-Patator       1.00      1.00      1.00      4761
              PortScan       0.99      1.00      1.00     95281
           SSH-Patator       0.97      0.98      0.98      3538
Web Attack Brute Force       0.70      0.52      0.60       904
        Web Attack XSS       1.00      0.03      0.06       391

             micro avg       1.00      1.00      1.00   1696672
             macro avg       0.96      0.82      0.84   1696672
          weighted avg       1.00      1.00      1.00   1696672

2019-12-27 04:50:48,402 [INFO] Overall accuracy (micro avg): 0.9967666113426755
2019-12-27 04:51:27,321 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9968         0.9968                       0.9968                0.0003                   0.0032  0.9968
1     Macro avg        0.9995         0.9601                       0.8242                0.0007                   0.1758  0.8434
2  Weighted avg        0.9974         0.9967                       0.9968                0.0054                   0.0032  0.9966
2019-12-27 04:51:27,345 [INFO] Results saved to: results_selected_models/selected_ids17_lstm_deep_rep4/selected_ids17_lstm_deep_rep4_results.xlsx
2019-12-27 04:51:27,349 [INFO] ================= Finished running experiment no. 4 ================= 

2019-12-27 04:51:27,420 [INFO] Created directory: results_selected_models/selected_ids17_lstm_deep_rep5
2019-12-27 04:51:27,420 [INFO] Initialized logging. log_filename = results_selected_models/selected_ids17_lstm_deep_rep5/run_log.log
2019-12-27 04:51:27,420 [INFO] ================= Running experiment no. 5  ================= 

2019-12-27 04:51:27,420 [INFO] Experiment parameters given below
2019-12-27 04:51:27,420 [INFO] 
{'experiment_num': 5, 'results_dir': 'results_selected_models/selected_ids17_lstm_deep_rep5', 'model': 'lstm', 'model_type': 'classifier', 'normal_label': 'BENIGN', 'lstm_time_steps': 32, 'lstm_layer_units': [64, 32], 'lstm_layer_activations': ['relu', 'relu'], 'lstm_layer_dropout_rates': [0.2, 0.2], 'batch_size': 256, 'epochs': 300, 'early_stop_patience': 50, 'dataset_dir': '../Datasets/full_datasets/ids2017', 'description': 'selected_ids17_lstm_deep_rep5'}
2019-12-27 04:51:27,420 [INFO] Created tensorboard log directory: results_selected_models/selected_ids17_lstm_deep_rep5/tf_logs_run_2019_12_27-04_51_27
2019-12-27 04:51:27,420 [INFO] Loading datsets from: ../Datasets/full_datasets/ids2017
2019-12-27 04:51:27,421 [INFO] Reading X, y files
2019-12-27 04:51:27,421 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_train.h5
2019-12-27 04:51:31,667 [INFO] Reading complete. time_to_read=4.25 seconds
2019-12-27 04:51:31,667 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_val.h5
2019-12-27 04:51:33,106 [INFO] Reading complete. time_to_read=1.44 seconds
2019-12-27 04:51:33,107 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_test.h5
2019-12-27 04:51:34,544 [INFO] Reading complete. time_to_read=1.44 seconds
2019-12-27 04:51:34,544 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_train.h5
2019-12-27 04:51:34,763 [INFO] Reading complete. time_to_read=0.22 seconds
2019-12-27 04:51:34,763 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_val.h5
2019-12-27 04:51:34,838 [INFO] Reading complete. time_to_read=0.07 seconds
2019-12-27 04:51:34,838 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_test.h5
2019-12-27 04:51:34,913 [INFO] Reading complete. time_to_read=0.07 seconds
2019-12-27 04:51:37,342 [INFO] Preparing flow sequences
2019-12-27 04:51:59,177 [INFO] Extracting flows complete. time_taken = 21.83 sec
2019-12-27 04:52:00,319 [INFO] Initializing model
2019-12-27 04:52:00,766 [INFO] _________________________________________________________________
2019-12-27 04:52:00,766 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-27 04:52:00,766 [INFO] =================================================================
2019-12-27 04:52:00,767 [INFO] lstm_19 (LSTM)               (None, 32, 64)            36608     
2019-12-27 04:52:00,767 [INFO] _________________________________________________________________
2019-12-27 04:52:00,767 [INFO] batch_normalization_19 (Batc (None, 32, 64)            256       
2019-12-27 04:52:00,767 [INFO] _________________________________________________________________
2019-12-27 04:52:00,767 [INFO] dropout_19 (Dropout)         (None, 32, 64)            0         
2019-12-27 04:52:00,767 [INFO] _________________________________________________________________
2019-12-27 04:52:00,767 [INFO] lstm_20 (LSTM)               (None, 32, 32)            12416     
2019-12-27 04:52:00,767 [INFO] _________________________________________________________________
2019-12-27 04:52:00,767 [INFO] batch_normalization_20 (Batc (None, 32, 32)            128       
2019-12-27 04:52:00,767 [INFO] _________________________________________________________________
2019-12-27 04:52:00,767 [INFO] dropout_20 (Dropout)         (None, 32, 32)            0         
2019-12-27 04:52:00,767 [INFO] _________________________________________________________________
2019-12-27 04:52:00,767 [INFO] time_distributed_10 (TimeDis (None, 32, 12)            396       
2019-12-27 04:52:00,767 [INFO] =================================================================
2019-12-27 04:52:00,767 [INFO] Total params: 49,804
2019-12-27 04:52:00,768 [INFO] Trainable params: 49,612
2019-12-27 04:52:00,768 [INFO] Non-trainable params: 192
2019-12-27 04:52:00,768 [INFO] _________________________________________________________________
2019-12-27 04:52:00,768 [INFO] Training model
 - val_f1: 0.9941
Epoch 00298: early stopping
Train on 53021 samples, validate on 17673 samples
Epoch 1/300
 - 14s - loss: 0.1508 - val_loss: 0.0625
 - val_f1: 0.8357
Epoch 2/300
 - 13s - loss: 0.0453 - val_loss: 0.0507
 - val_f1: 0.8483
Epoch 3/300
 - 13s - loss: 0.0349 - val_loss: 0.0343
 - val_f1: 0.9034
Epoch 4/300
 - 13s - loss: 0.0309 - val_loss: 0.0324
 - val_f1: 0.9083
Epoch 5/300
 - 13s - loss: 0.0282 - val_loss: 0.0270
 - val_f1: 0.9308
Epoch 6/300
 - 13s - loss: 0.0263 - val_loss: 0.0278
 - val_f1: 0.9307
Epoch 7/300
 - 13s - loss: 0.0247 - val_loss: 0.0219
 - val_f1: 0.9403
Epoch 8/300
 - 13s - loss: 0.0233 - val_loss: 0.0218
 - val_f1: 0.9417
Epoch 9/300
 - 13s - loss: 0.0220 - val_loss: 0.0215
 - val_f1: 0.9394
Epoch 10/300
 - 13s - loss: 0.0209 - val_loss: 0.0172
 - val_f1: 0.9595
Epoch 11/300
 - 13s - loss: 0.0199 - val_loss: 0.0173
 - val_f1: 0.9596
Epoch 12/300
 - 13s - loss: 0.0192 - val_loss: 0.0188
 - val_f1: 0.9428
Epoch 13/300
 - 13s - loss: 0.0185 - val_loss: 0.0156
 - val_f1: 0.9627
Epoch 14/300
 - 13s - loss: 0.0178 - val_loss: 0.0173
 - val_f1: 0.9463
Epoch 15/300
 - 13s - loss: 0.0170 - val_loss: 0.0154
 - val_f1: 0.9650
Epoch 16/300
 - 13s - loss: 0.0163 - val_loss: 0.0158
 - val_f1: 0.9527
Epoch 17/300
 - 13s - loss: 0.0156 - val_loss: 0.0147
 - val_f1: 0.9636
Epoch 18/300
 - 13s - loss: 0.0148 - val_loss: 0.0136
 - val_f1: 0.9637
Epoch 19/300
 - 13s - loss: 0.0141 - val_loss: 0.0118
 - val_f1: 0.9709
Epoch 20/300
 - 13s - loss: 0.0132 - val_loss: 0.0110
 - val_f1: 0.9753
Epoch 21/300
 - 13s - loss: 0.0121 - val_loss: 0.0106
 - val_f1: 0.9765
Epoch 22/300
 - 13s - loss: 0.0114 - val_loss: 0.0091
 - val_f1: 0.9785
Epoch 23/300
 - 13s - loss: 0.0108 - val_loss: 0.0085
 - val_f1: 0.9805
Epoch 24/300
 - 13s - loss: 0.0101 - val_loss: 0.0086
 - val_f1: 0.9807
Epoch 25/300
 - 13s - loss: 0.0096 - val_loss: 0.0079
 - val_f1: 0.9815
Epoch 26/300
 - 13s - loss: 0.0094 - val_loss: 0.0075
 - val_f1: 0.9817
Epoch 27/300
 - 13s - loss: 0.0095 - val_loss: 0.0070
 - val_f1: 0.9838
Epoch 28/300
 - 13s - loss: 0.0084 - val_loss: 0.0065
 - val_f1: 0.9838
Epoch 29/300
 - 13s - loss: 0.0081 - val_loss: 0.0064
 - val_f1: 0.9861
Epoch 30/300
 - 13s - loss: 0.0083 - val_loss: 0.0062
 - val_f1: 0.9844
Epoch 31/300
 - 13s - loss: 0.0076 - val_loss: 0.0060
/home/hasitha/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2019-12-27 05:00:19,107 [INFO] epoch = 30. Intermediate model saved to results_selected_models/selected_ids17_lstm_deep_rep5/_model_epoch_30.pickle
 - val_f1: 0.9834
Epoch 32/300
 - 13s - loss: 0.0078 - val_loss: 0.0066
 - val_f1: 0.9831
Epoch 33/300
 - 13s - loss: 0.0075 - val_loss: 0.0060
 - val_f1: 0.9837
Epoch 34/300
 - 13s - loss: 0.0071 - val_loss: 0.0050
 - val_f1: 0.9897
Epoch 35/300
 - 13s - loss: 0.0103 - val_loss: 0.0087
 - val_f1: 0.9827
Epoch 36/300
 - 13s - loss: 0.0090 - val_loss: 0.0073
 - val_f1: 0.9817
Epoch 37/300
 - 13s - loss: 0.0082 - val_loss: 0.0072
 - val_f1: 0.9798
Epoch 38/300
 - 13s - loss: 0.0075 - val_loss: 0.0080
 - val_f1: 0.9762
Epoch 39/300
 - 13s - loss: 0.0073 - val_loss: 0.0060
 - val_f1: 0.9865
Epoch 40/300
 - 13s - loss: 0.0066 - val_loss: 0.0078
 - val_f1: 0.9831
Epoch 41/300
 - 13s - loss: 0.0058 - val_loss: 0.0043
 - val_f1: 0.9905
Epoch 42/300
 - 13s - loss: 0.0075 - val_loss: 0.0080
 - val_f1: 0.9803
Epoch 43/300
 - 13s - loss: 0.0066 - val_loss: 0.0040
 - val_f1: 0.9903
Epoch 44/300
 - 13s - loss: 0.0055 - val_loss: 0.0038
 - val_f1: 0.9915
Epoch 45/300
 - 13s - loss: 0.0053 - val_loss: 0.0039
 - val_f1: 0.9918
Epoch 46/300
 - 13s - loss: 0.0052 - val_loss: 0.0044
 - val_f1: 0.9896
Epoch 47/300
 - 13s - loss: 0.0063 - val_loss: 0.0048
 - val_f1: 0.9886
Epoch 48/300
 - 13s - loss: 0.0051 - val_loss: 0.0038
 - val_f1: 0.9917
Epoch 49/300
 - 13s - loss: 0.0059 - val_loss: 0.0095
 - val_f1: 0.9807
Epoch 50/300
 - 13s - loss: 0.0057 - val_loss: 0.0062
 - val_f1: 0.9870
Epoch 51/300
 - 13s - loss: 0.0049 - val_loss: 0.0063
 - val_f1: 0.9858
Epoch 52/300
 - 13s - loss: 0.0046 - val_loss: 0.0037
 - val_f1: 0.9919
Epoch 53/300
 - 13s - loss: 0.0046 - val_loss: 0.0049
 - val_f1: 0.9886
Epoch 54/300
 - 13s - loss: 0.0062 - val_loss: 0.0086
 - val_f1: 0.9785
Epoch 55/300
 - 13s - loss: 0.0080 - val_loss: 0.0108
 - val_f1: 0.9759
Epoch 56/300
 - 13s - loss: 0.0069 - val_loss: 0.0054
 - val_f1: 0.9899
Epoch 57/300
 - 13s - loss: 0.0063 - val_loss: 0.0043
 - val_f1: 0.9928
Epoch 58/300
 - 13s - loss: 0.0054 - val_loss: 0.0047
 - val_f1: 0.9915
Epoch 59/300
 - 13s - loss: 0.0054 - val_loss: 0.0095
 - val_f1: 0.9766
Epoch 60/300
 - 13s - loss: 0.0054 - val_loss: 0.0039
 - val_f1: 0.9922
Epoch 61/300
 - 13s - loss: 0.0063 - val_loss: 0.0063
2019-12-27 05:08:14,347 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_ids17_lstm_deep_rep5/_model_epoch_60.pickle
 - val_f1: 0.9848
Epoch 62/300
 - 13s - loss: 0.0065 - val_loss: 0.0151
 - val_f1: 0.9576
Epoch 63/300
 - 13s - loss: 0.0052 - val_loss: 0.0080
 - val_f1: 0.9697
Epoch 64/300
 - 13s - loss: 0.0048 - val_loss: 0.0142
 - val_f1: 0.9605
Epoch 65/300
 - 13s - loss: 0.0044 - val_loss: 0.0096
 - val_f1: 0.9636
Epoch 66/300
 - 13s - loss: 0.0046 - val_loss: 0.0060
 - val_f1: 0.9809
Epoch 67/300
 - 13s - loss: 0.0041 - val_loss: 0.0103
 - val_f1: 0.9644
Epoch 68/300
 - 13s - loss: 0.0040 - val_loss: 0.0061
 - val_f1: 0.9806
Epoch 69/300
 - 13s - loss: 0.0042 - val_loss: 0.0052
 - val_f1: 0.9883
Epoch 70/300
 - 13s - loss: 0.0039 - val_loss: 0.0069
 - val_f1: 0.9741
Epoch 71/300
 - 13s - loss: 0.0050 - val_loss: 0.0082
 - val_f1: 0.9825
Epoch 72/300
 - 13s - loss: 0.0060 - val_loss: 0.0191
 - val_f1: 0.9608
Epoch 73/300
 - 13s - loss: 0.0044 - val_loss: 0.0186
 - val_f1: 0.9626
Epoch 74/300
 - 13s - loss: 0.0040 - val_loss: 0.0046
 - val_f1: 0.9903
Epoch 75/300
 - 13s - loss: 0.0042 - val_loss: 0.0052
 - val_f1: 0.9875
Epoch 76/300
 - 13s - loss: 0.0039 - val_loss: 0.0048
 - val_f1: 0.9880
Epoch 77/300
 - 13s - loss: 0.0038 - val_loss: 0.0055
 - val_f1: 0.9845
Epoch 78/300
 - 13s - loss: 0.0042 - val_loss: 0.0137
 - val_f1: 0.9629
Epoch 79/300
 - 13s - loss: 0.0038 - val_loss: 0.0031
 - val_f1: 0.9936
Epoch 80/300
 - 13s - loss: 0.0037 - val_loss: 0.0074
 - val_f1: 0.9707
Epoch 81/300
 - 13s - loss: 0.0042 - val_loss: 0.0111
 - val_f1: 0.9793
Epoch 82/300
 - 13s - loss: 0.0051 - val_loss: 0.0047
 - val_f1: 0.9914
Epoch 83/300
 - 13s - loss: 0.0050 - val_loss: 0.0049
 - val_f1: 0.9886
Epoch 84/300
 - 13s - loss: 0.0047 - val_loss: 0.0034
 - val_f1: 0.9932
Epoch 85/300
 - 13s - loss: 0.0045 - val_loss: 0.0101
 - val_f1: 0.9609
Epoch 86/300
 - 13s - loss: 0.0039 - val_loss: 0.0047
 - val_f1: 0.9895
Epoch 87/300
 - 13s - loss: 0.0037 - val_loss: 0.0132
 - val_f1: 0.9631
Epoch 88/300
 - 13s - loss: 0.0035 - val_loss: 0.0083
 - val_f1: 0.9684
Epoch 89/300
 - 13s - loss: 0.0036 - val_loss: 0.0098
 - val_f1: 0.9650
Epoch 90/300
 - 13s - loss: 0.0035 - val_loss: 0.0076
 - val_f1: 0.9730
Epoch 91/300
 - 13s - loss: 0.0036 - val_loss: 0.0097
2019-12-27 05:16:09,972 [INFO] epoch = 90. Intermediate model saved to results_selected_models/selected_ids17_lstm_deep_rep5/_model_epoch_90.pickle
 - val_f1: 0.9710
Epoch 92/300
 - 13s - loss: 0.0034 - val_loss: 0.0053
 - val_f1: 0.9897
Epoch 93/300
 - 13s - loss: 0.0059 - val_loss: 0.0114
 - val_f1: 0.9594
Epoch 94/300
 - 13s - loss: 0.0045 - val_loss: 0.0125
 - val_f1: 0.9621
Epoch 95/300
 - 13s - loss: 0.0046 - val_loss: 0.0118
 - val_f1: 0.9633
Epoch 96/300
 - 13s - loss: 0.0042 - val_loss: 0.0124
 - val_f1: 0.9606
Epoch 97/300
 - 13s - loss: 0.0043 - val_loss: 0.0155
 - val_f1: 0.9596
Epoch 98/300
 - 13s - loss: 0.0041 - val_loss: 0.0156
 - val_f1: 0.9530
Epoch 99/300
 - 13s - loss: 0.0039 - val_loss: 0.0224
 - val_f1: 0.9626
Epoch 100/300
 - 13s - loss: 0.0038 - val_loss: 0.0122
 - val_f1: 0.9613
Epoch 101/300
 - 13s - loss: 0.0034 - val_loss: 0.0039
 - val_f1: 0.9919
Epoch 102/300
 - 13s - loss: 0.0032 - val_loss: 0.0109
 - val_f1: 0.9636
Epoch 103/300
 - 13s - loss: 0.0033 - val_loss: 0.0100
 - val_f1: 0.9739
Epoch 104/300
 - 13s - loss: 0.0032 - val_loss: 0.0310
 - val_f1: 0.9623
Epoch 105/300
 - 13s - loss: 0.0032 - val_loss: 0.0157
 - val_f1: 0.9640
Epoch 106/300
 - 13s - loss: 0.0031 - val_loss: 0.0157
 - val_f1: 0.9629
Epoch 107/300
 - 13s - loss: 0.0031 - val_loss: 0.0099
 - val_f1: 0.9676
Epoch 108/300
 - 13s - loss: 0.0038 - val_loss: 0.0032
 - val_f1: 0.9941
Epoch 109/300
 - 13s - loss: 0.0037 - val_loss: 0.0070
 - val_f1: 0.9718
Epoch 110/300
 - 13s - loss: 0.0033 - val_loss: 0.0046
 - val_f1: 0.9891
Epoch 111/300
 - 13s - loss: 0.0033 - val_loss: 0.0151
 - val_f1: 0.9633
Epoch 112/300
 - 13s - loss: 0.0033 - val_loss: 0.0032
 - val_f1: 0.9943
Epoch 113/300
 - 13s - loss: 0.0038 - val_loss: 0.0080
 - val_f1: 0.9745
Epoch 114/300
 - 13s - loss: 0.0033 - val_loss: 0.0079
 - val_f1: 0.9689
Epoch 115/300
 - 13s - loss: 0.0031 - val_loss: 0.0045
 - val_f1: 0.9908
Epoch 116/300
 - 13s - loss: 0.0031 - val_loss: 0.0048
 - val_f1: 0.9865
Epoch 117/300
 - 13s - loss: 0.0030 - val_loss: 0.0065
 - val_f1: 0.9892
Epoch 118/300
 - 13s - loss: 0.0030 - val_loss: 0.0035
 - val_f1: 0.9925
Epoch 119/300
 - 13s - loss: 0.0030 - val_loss: 0.0030
 - val_f1: 0.9946
Epoch 120/300
 - 13s - loss: 0.0030 - val_loss: 0.0177
 - val_f1: 0.9633
Epoch 121/300
 - 13s - loss: 0.0030 - val_loss: 0.0133
2019-12-27 05:24:05,340 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_ids17_lstm_deep_rep5/_model_epoch_120.pickle
 - val_f1: 0.9641
Epoch 122/300
 - 13s - loss: 0.0030 - val_loss: 0.0037
 - val_f1: 0.9928
Epoch 123/300
 - 13s - loss: 0.0029 - val_loss: 0.0030
 - val_f1: 0.9939
Epoch 124/300
 - 13s - loss: 0.0029 - val_loss: 0.0027
 - val_f1: 0.9943
Epoch 125/300
 - 13s - loss: 0.0033 - val_loss: 0.0142
 - val_f1: 0.9629
Epoch 126/300
 - 13s - loss: 0.0029 - val_loss: 0.0107
 - val_f1: 0.9663
Epoch 127/300
 - 13s - loss: 0.0029 - val_loss: 0.0026
 - val_f1: 0.9946
Epoch 128/300
 - 13s - loss: 0.0028 - val_loss: 0.0056
 - val_f1: 0.9909
Epoch 129/300
 - 13s - loss: 0.0028 - val_loss: 0.0029
 - val_f1: 0.9946
Epoch 130/300
 - 13s - loss: 0.0029 - val_loss: 0.0116
 - val_f1: 0.9852
Epoch 131/300
 - 13s - loss: 0.0028 - val_loss: 0.0049
 - val_f1: 0.9887
Epoch 132/300
 - 13s - loss: 0.0028 - val_loss: 0.0082
 - val_f1: 0.9876
Epoch 133/300
 - 13s - loss: 0.0029 - val_loss: 0.0069
 - val_f1: 0.9876
Epoch 134/300
 - 13s - loss: 0.0029 - val_loss: 0.0053
 - val_f1: 0.9912
Epoch 135/300
 - 13s - loss: 0.0027 - val_loss: 0.0029
 - val_f1: 0.9950
Epoch 136/300
 - 13s - loss: 0.0028 - val_loss: 0.0029
 - val_f1: 0.9946
Epoch 137/300
 - 13s - loss: 0.0027 - val_loss: 0.0024
 - val_f1: 0.9948
Epoch 138/300
 - 13s - loss: 0.0027 - val_loss: 0.0025
 - val_f1: 0.9946
Epoch 139/300
 - 13s - loss: 0.0028 - val_loss: 0.0042
 - val_f1: 0.9922
Epoch 140/300
 - 13s - loss: 0.0028 - val_loss: 0.0037
 - val_f1: 0.9943
Epoch 141/300
 - 13s - loss: 0.0027 - val_loss: 0.0031
 - val_f1: 0.9943
Epoch 142/300
 - 13s - loss: 0.0026 - val_loss: 0.0062
 - val_f1: 0.9878
Epoch 143/300
 - 13s - loss: 0.0027 - val_loss: 0.0028
 - val_f1: 0.9944
Epoch 144/300
 - 13s - loss: 0.0027 - val_loss: 0.0025
 - val_f1: 0.9949
Epoch 145/300
 - 13s - loss: 0.0027 - val_loss: 0.0024
 - val_f1: 0.9949
Epoch 146/300
 - 13s - loss: 0.0026 - val_loss: 0.0030
 - val_f1: 0.9946
Epoch 147/300
 - 13s - loss: 0.0027 - val_loss: 0.0026
 - val_f1: 0.9933
Epoch 148/300
 - 13s - loss: 0.0026 - val_loss: 0.0084
 - val_f1: 0.9718
Epoch 149/300
 - 13s - loss: 0.0038 - val_loss: 0.0061
 - val_f1: 0.9857
Epoch 150/300
 - 13s - loss: 0.0034 - val_loss: 0.0031
 - val_f1: 0.9934
Epoch 151/300
 - 13s - loss: 0.0029 - val_loss: 0.0024
2019-12-27 05:32:00,325 [INFO] epoch = 150. Intermediate model saved to results_selected_models/selected_ids17_lstm_deep_rep5/_model_epoch_150.pickle
 - val_f1: 0.9948
Epoch 152/300
 - 13s - loss: 0.0029 - val_loss: 0.0025
 - val_f1: 0.9947
Epoch 153/300
 - 13s - loss: 0.0031 - val_loss: 0.0031
 - val_f1: 0.9945
Epoch 154/300
 - 13s - loss: 0.0028 - val_loss: 0.0035
 - val_f1: 0.9921
Epoch 155/300
 - 13s - loss: 0.0028 - val_loss: 0.0114
 - val_f1: 0.9860
Epoch 156/300
 - 13s - loss: 0.0027 - val_loss: 0.0030
 - val_f1: 0.9939
Epoch 157/300
 - 13s - loss: 0.0027 - val_loss: 0.0029
 - val_f1: 0.9938
Epoch 158/300
 - 13s - loss: 0.0030 - val_loss: 0.0062
 - val_f1: 0.9890
Epoch 159/300
 - 13s - loss: 0.0027 - val_loss: 0.0257
 - val_f1: 0.9608
Epoch 160/300
 - 13s - loss: 0.0038 - val_loss: 0.0065
 - val_f1: 0.9862
Epoch 161/300
 - 13s - loss: 0.0032 - val_loss: 0.0093
 - val_f1: 0.9665
Epoch 162/300
 - 13s - loss: 0.0028 - val_loss: 0.0060
 - val_f1: 0.9778
Epoch 163/300
 - 13s - loss: 0.0027 - val_loss: 0.0063
 - val_f1: 0.9786
Epoch 164/300
 - 13s - loss: 0.0027 - val_loss: 0.0085
 - val_f1: 0.9876
Epoch 165/300
 - 13s - loss: 0.0030 - val_loss: 0.0219
 - val_f1: 0.9595
Epoch 166/300
 - 13s - loss: 0.0026 - val_loss: 0.0027
 - val_f1: 0.9935
Epoch 167/300
 - 13s - loss: 0.0026 - val_loss: 0.0027
 - val_f1: 0.9944
Epoch 168/300
 - 13s - loss: 0.0028 - val_loss: 0.0035
 - val_f1: 0.9936
Epoch 169/300
 - 13s - loss: 0.0026 - val_loss: 0.0069
 - val_f1: 0.9746
Epoch 170/300
 - 13s - loss: 0.0026 - val_loss: 0.0031
 - val_f1: 0.9930
Epoch 171/300
 - 13s - loss: 0.0026 - val_loss: 0.0041
 - val_f1: 0.9913
Epoch 172/300
 - 13s - loss: 0.0026 - val_loss: 0.0057
 - val_f1: 0.9914
Epoch 173/300
 - 13s - loss: 0.0025 - val_loss: 0.0211
 - val_f1: 0.9665
Epoch 174/300
 - 13s - loss: 0.0026 - val_loss: 0.0111
 - val_f1: 0.9865
Epoch 175/300
 - 13s - loss: 0.0026 - val_loss: 0.0044
 - val_f1: 0.9923
Epoch 176/300
 - 13s - loss: 0.0026 - val_loss: 0.0030
 - val_f1: 0.9945
Epoch 177/300
 - 13s - loss: 0.0026 - val_loss: 0.0093
 - val_f1: 0.9674
Epoch 178/300
 - 13s - loss: 0.0026 - val_loss: 0.0185
 - val_f1: 0.9597
Epoch 179/300
 - 13s - loss: 0.0026 - val_loss: 0.0030
 - val_f1: 0.9938
Epoch 180/300
 - 13s - loss: 0.0025 - val_loss: 0.0068
 - val_f1: 0.9747
Epoch 181/300
 - 13s - loss: 0.0026 - val_loss: 0.0097
2019-12-27 05:39:55,979 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_ids17_lstm_deep_rep5/_model_epoch_180.pickle
 - val_f1: 0.9667
Epoch 182/300
 - 13s - loss: 0.0025 - val_loss: 0.0051
 - val_f1: 0.9889
Epoch 183/300
 - 13s - loss: 0.0025 - val_loss: 0.0082
 - val_f1: 0.9687
Epoch 184/300
 - 13s - loss: 0.0026 - val_loss: 0.0025
 - val_f1: 0.9952
Epoch 185/300
 - 13s - loss: 0.0027 - val_loss: 0.0046
 - val_f1: 0.9920
Epoch 186/300
 - 13s - loss: 0.0028 - val_loss: 0.0032
 - val_f1: 0.9935
Epoch 187/300
 - 13s - loss: 0.0027 - val_loss: 0.0057
2019-12-27 05:41:34,255 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-27 05:41:45,641 [INFO] Last epoch loss evaluation: train_loss = 0.002198, val_loss = 0.002379
2019-12-27 05:41:45,641 [INFO] Training complete. time_to_train = 2984.87 sec, 49.75 min
2019-12-27 05:41:45,647 [INFO] Model saved to results_selected_models/selected_ids17_lstm_deep_rep5/best_model.pickle
2019-12-27 05:41:45,649 [INFO] Training history saved to: results_selected_models/selected_ids17_lstm_deep_rep5/training_error_history.csv
2019-12-27 05:41:45,794 [INFO] Plot saved to: results_selected_models/selected_ids17_lstm_deep_rep5/training_error_history.png
2019-12-27 05:41:45,933 [INFO] Plot saved to: results_selected_models/selected_ids17_lstm_deep_rep5/training_f1_history.png
2019-12-27 05:41:45,933 [INFO] Making predictions on training, validation, testing data
2019-12-27 05:41:58,942 [INFO] Evaluating predictions (results)
/home/hasitha/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2019-12-27 05:42:09,346 [INFO] Dataset: Testing. Classification report below
2019-12-27 05:42:09,346 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       1.00      1.00      1.00    454243
                   Bot       0.97      0.36      0.52       391
                  DDoS       1.00      1.00      1.00     25604
         DoS GoldenEye       0.99      0.98      0.98      2058
              DoS Hulk       0.98      0.99      0.98     46023
      DoS Slowhttptest       0.88      0.99      0.93      1100
         DoS slowloris       0.98      0.97      0.98      1159
           FTP-Patator       0.99      1.00      0.99      1586
              PortScan       0.99      1.00      1.00     31761
           SSH-Patator       0.96      0.97      0.96      1179
Web Attack Brute Force       1.00      0.10      0.18       302
        Web Attack XSS       0.00      0.00      0.00       130

             micro avg       1.00      1.00      1.00    565536
             macro avg       0.90      0.78      0.79    565536
          weighted avg       1.00      1.00      0.99    565536

2019-12-27 05:42:09,346 [INFO] Overall accuracy (micro avg): 0.9952363775250382
/home/hasitha/sunanda/ids_experiments/utility.py:328: RuntimeWarning: invalid value encountered in true_divide
  PPV = TP / (TP + FP)    # Precision or positive predictive value
/home/hasitha/sunanda/ids_experiments/utility.py:332: RuntimeWarning: invalid value encountered in true_divide
  FDR = FP / (TP + FP)    # False discovery rate
2019-12-27 05:42:21,136 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9952         0.9952                       0.9952                0.0004                   0.0048  0.9952
1     Macro avg        0.9992         0.8951                       0.7782                0.0011                   0.2218  0.7939
2  Weighted avg        0.9961         0.9950                       0.9952                0.0090                   0.0048  0.9948
2019-12-27 05:42:31,509 [INFO] Dataset: Validation. Classification report below
2019-12-27 05:42:31,510 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       1.00      1.00      1.00    454243
                   Bot       0.96      0.32      0.48       391
                  DDoS       1.00      1.00      1.00     25605
         DoS GoldenEye       0.99      0.98      0.98      2059
              DoS Hulk       0.98      0.99      0.99     46023
      DoS Slowhttptest       0.89      0.99      0.93      1099
         DoS slowloris       0.98      0.96      0.97      1159
           FTP-Patator       0.97      1.00      0.99      1587
              PortScan       0.99      1.00      1.00     31758
           SSH-Patator       0.97      0.97      0.97      1180
Web Attack Brute Force       0.88      0.08      0.14       301
        Web Attack XSS       0.00      0.00      0.00       131

             micro avg       1.00      1.00      1.00    565536
             macro avg       0.88      0.77      0.79    565536
          weighted avg       1.00      1.00      0.99    565536

2019-12-27 05:42:31,510 [INFO] Overall accuracy (micro avg): 0.9952664375035365
2019-12-27 05:42:43,308 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9953         0.9953                       0.9953                0.0004                   0.0047  0.9953
1     Macro avg        0.9992         0.8848                       0.7719                0.0011                   0.2281  0.7863
2  Weighted avg        0.9961         0.9950                       0.9953                0.0086                   0.0047  0.9948
2019-12-27 05:43:17,516 [INFO] Dataset: Training. Classification report below
2019-12-27 05:43:17,516 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       1.00      1.00      1.00   1362781
                   Bot       0.98      0.36      0.53      1174
                  DDoS       1.00      1.00      1.00     76815
         DoS GoldenEye       0.99      0.98      0.99      6176
              DoS Hulk       0.98      0.99      0.99    138073
      DoS Slowhttptest       0.90      0.99      0.94      3300
         DoS slowloris       0.98      0.97      0.98      3478
           FTP-Patator       0.98      1.00      0.99      4761
              PortScan       0.99      1.00      1.00     95281
           SSH-Patator       0.97      0.97      0.97      3538
Web Attack Brute Force       0.96      0.09      0.16       904
        Web Attack XSS       0.00      0.00      0.00       391

             micro avg       1.00      1.00      1.00   1696672
             macro avg       0.90      0.78      0.79   1696672
          weighted avg       1.00      1.00      1.00   1696672

2019-12-27 05:43:17,516 [INFO] Overall accuracy (micro avg): 0.9955642575583259
2019-12-27 05:43:56,370 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9956         0.9956                       0.9956                0.0004                   0.0044  0.9956
1     Macro avg        0.9993         0.8952                       0.7782                0.0011                   0.2218  0.7940
2  Weighted avg        0.9963         0.9953                       0.9956                0.0083                   0.0044  0.9952
2019-12-27 05:43:56,407 [INFO] Results saved to: results_selected_models/selected_ids17_lstm_deep_rep5/selected_ids17_lstm_deep_rep5_results.xlsx
2019-12-27 05:43:56,412 [INFO] ================= Finished running experiment no. 5 ================= 

2019-12-27 05:43:56,481 [INFO] Created directory: results_selected_models/selected_ids18_subset_lstm_deep_rep1
2019-12-27 05:43:56,482 [INFO] Initialized logging. log_filename = results_selected_models/selected_ids18_subset_lstm_deep_rep1/run_log.log
2019-12-27 05:43:56,482 [INFO] ================= Running experiment no. 1  ================= 

2019-12-27 05:43:56,482 [INFO] Experiment parameters given below
2019-12-27 05:43:56,482 [INFO] 
{'experiment_num': 1, 'results_dir': 'results_selected_models/selected_ids18_subset_lstm_deep_rep1', 'model': 'lstm', 'model_type': 'classifier', 'normal_label': 'BENIGN', 'lstm_time_steps': 32, 'lstm_layer_units': [64, 32], 'lstm_layer_activations': ['relu', 'relu'], 'lstm_layer_dropout_rates': [0.2, 0.2], 'batch_size': 256, 'epochs': 300, 'early_stop_patience': 50, 'dataset_dir': '../Datasets/small_datasets/ids2018', 'description': 'selected_ids18_subset_lstm_deep_rep1'}
2019-12-27 05:43:56,482 [INFO] Created tensorboard log directory: results_selected_models/selected_ids18_subset_lstm_deep_rep1/tf_logs_run_2019_12_27-05_43_56
2019-12-27 05:43:56,482 [INFO] Loading datsets from: ../Datasets/small_datasets/ids2018
2019-12-27 05:43:56,482 [INFO] Reading X, y files
2019-12-27 05:43:56,482 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_train.h5
2019-12-27 05:44:01,225 [INFO] Reading complete. time_to_read=4.74 seconds
2019-12-27 05:44:01,225 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_val.h5
2019-12-27 05:44:02,894 [INFO] Reading complete. time_to_read=1.67 seconds
2019-12-27 05:44:02,895 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_test.h5
2019-12-27 05:44:04,554 [INFO] Reading complete. time_to_read=1.66 seconds
2019-12-27 05:44:04,554 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_train.h5
2019-12-27 05:44:05,016 [INFO] Reading complete. time_to_read=0.46 seconds
2019-12-27 05:44:05,016 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_val.h5
2019-12-27 05:44:05,192 [INFO] Reading complete. time_to_read=0.18 seconds
2019-12-27 05:44:05,192 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_test.h5
2019-12-27 05:44:05,369 [INFO] Reading complete. time_to_read=0.18 seconds
2019-12-27 05:44:08,125 [INFO] Preparing flow sequences
2019-12-27 05:44:32,936 [INFO] Extracting flows complete. time_taken = 24.81 sec
2019-12-27 05:44:34,279 [INFO] Initializing model
2019-12-27 05:44:34,773 [INFO] _________________________________________________________________
2019-12-27 05:44:34,773 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-27 05:44:34,773 [INFO] =================================================================
2019-12-27 05:44:34,773 [INFO] lstm_21 (LSTM)               (None, 32, 64)            36352     
2019-12-27 05:44:34,773 [INFO] _________________________________________________________________
2019-12-27 05:44:34,774 [INFO] batch_normalization_21 (Batc (None, 32, 64)            256       
2019-12-27 05:44:34,774 [INFO] _________________________________________________________________
2019-12-27 05:44:34,774 [INFO] dropout_21 (Dropout)         (None, 32, 64)            0         
2019-12-27 05:44:34,774 [INFO] _________________________________________________________________
2019-12-27 05:44:34,774 [INFO] lstm_22 (LSTM)               (None, 32, 32)            12416     
2019-12-27 05:44:34,774 [INFO] _________________________________________________________________
2019-12-27 05:44:34,774 [INFO] batch_normalization_22 (Batc (None, 32, 32)            128       
2019-12-27 05:44:34,774 [INFO] _________________________________________________________________
2019-12-27 05:44:34,774 [INFO] dropout_22 (Dropout)         (None, 32, 32)            0         
2019-12-27 05:44:34,774 [INFO] _________________________________________________________________
2019-12-27 05:44:34,774 [INFO] time_distributed_11 (TimeDis (None, 32, 15)            495       
2019-12-27 05:44:34,774 [INFO] =================================================================
2019-12-27 05:44:34,774 [INFO] Total params: 49,647
2019-12-27 05:44:34,774 [INFO] Trainable params: 49,455
2019-12-27 05:44:34,774 [INFO] Non-trainable params: 192
2019-12-27 05:44:34,774 [INFO] _________________________________________________________________
2019-12-27 05:44:34,774 [INFO] Training model
 - val_f1: 0.9901
Epoch 00187: early stopping
Train on 60514 samples, validate on 20171 samples
Epoch 1/300
 - 16s - loss: 0.1651 - val_loss: 0.0857
 - val_f1: 0.7526
Epoch 2/300
 - 15s - loss: 0.0861 - val_loss: 0.0806
 - val_f1: 0.7520
Epoch 3/300
 - 15s - loss: 0.0798 - val_loss: 0.0763
 - val_f1: 0.7528
Epoch 4/300
 - 15s - loss: 0.0757 - val_loss: 0.0737
 - val_f1: 0.7549
Epoch 5/300
 - 15s - loss: 0.0726 - val_loss: 0.0702
 - val_f1: 0.7562
Epoch 6/300
 - 15s - loss: 0.0695 - val_loss: 0.0680
 - val_f1: 0.7575
Epoch 7/300
 - 15s - loss: 0.0665 - val_loss: 0.0659
 - val_f1: 0.7588
Epoch 8/300
 - 15s - loss: 0.0634 - val_loss: 0.0645
 - val_f1: 0.7618
Epoch 9/300
 - 15s - loss: 0.0602 - val_loss: 0.0625
 - val_f1: 0.7645
Epoch 10/300
 - 15s - loss: 0.0573 - val_loss: 0.0585
 - val_f1: 0.7716
Epoch 11/300
 - 15s - loss: 0.0543 - val_loss: 0.0556
 - val_f1: 0.7744
Epoch 12/300
 - 15s - loss: 0.0511 - val_loss: 0.0550
 - val_f1: 0.7771
Epoch 13/300
 - 15s - loss: 0.0485 - val_loss: 0.0508
 - val_f1: 0.7842
Epoch 14/300
 - 15s - loss: 0.0465 - val_loss: 0.0438
 - val_f1: 0.8048
Epoch 15/300
 - 15s - loss: 0.0443 - val_loss: 0.0433
 - val_f1: 0.8043
Epoch 16/300
 - 15s - loss: 0.0422 - val_loss: 0.0391
 - val_f1: 0.8384
Epoch 17/300
 - 15s - loss: 0.0388 - val_loss: 0.0389
 - val_f1: 0.8369
Epoch 18/300
 - 15s - loss: 0.0358 - val_loss: 0.0396
 - val_f1: 0.8227
Epoch 19/300
 - 15s - loss: 0.0330 - val_loss: 0.0345
 - val_f1: 0.8605
Epoch 20/300
 - 15s - loss: 0.0306 - val_loss: 0.0338
 - val_f1: 0.8563
Epoch 21/300
 - 15s - loss: 0.0286 - val_loss: 0.0302
 - val_f1: 0.8853
Epoch 22/300
 - 15s - loss: 0.0269 - val_loss: 0.0251
 - val_f1: 0.9243
Epoch 23/300
 - 15s - loss: 0.0252 - val_loss: 0.0228
 - val_f1: 0.9393
Epoch 24/300
 - 15s - loss: 0.0237 - val_loss: 0.0197
 - val_f1: 0.9486
Epoch 25/300
 - 15s - loss: 0.0222 - val_loss: 0.0176
 - val_f1: 0.9529
Epoch 26/300
 - 15s - loss: 0.0205 - val_loss: 0.0154
 - val_f1: 0.9561
Epoch 27/300
 - 15s - loss: 0.0197 - val_loss: 0.0145
 - val_f1: 0.9604
Epoch 28/300
 - 15s - loss: 0.0183 - val_loss: 0.0135
 - val_f1: 0.9641
Epoch 29/300
 - 15s - loss: 0.0180 - val_loss: 0.0133
 - val_f1: 0.9654
Epoch 30/300
 - 15s - loss: 0.0171 - val_loss: 0.0131
 - val_f1: 0.9683
Epoch 31/300
 - 15s - loss: 0.0163 - val_loss: 0.0124
/home/hasitha/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2019-12-27 05:54:13,770 [INFO] epoch = 30. Intermediate model saved to results_selected_models/selected_ids18_subset_lstm_deep_rep1/_model_epoch_30.pickle
 - val_f1: 0.9678
Epoch 32/300
 - 15s - loss: 0.0158 - val_loss: 0.0122
 - val_f1: 0.9687
Epoch 33/300
 - 15s - loss: 0.0152 - val_loss: 0.0119
 - val_f1: 0.9694
Epoch 34/300
 - 15s - loss: 0.0147 - val_loss: 0.0115
 - val_f1: 0.9706
Epoch 35/300
 - 15s - loss: 0.0141 - val_loss: 0.0112
 - val_f1: 0.9704
Epoch 36/300
 - 15s - loss: 0.0137 - val_loss: 0.0108
 - val_f1: 0.9710
Epoch 37/300
 - 15s - loss: 0.0131 - val_loss: 0.0104
 - val_f1: 0.9739
Epoch 38/300
 - 15s - loss: 0.0125 - val_loss: 0.0102
 - val_f1: 0.9745
Epoch 39/300
 - 15s - loss: 0.0121 - val_loss: 0.0101
 - val_f1: 0.9740
Epoch 40/300
 - 15s - loss: 0.0118 - val_loss: 0.0101
 - val_f1: 0.9734
Epoch 41/300
 - 15s - loss: 0.0115 - val_loss: 0.0100
 - val_f1: 0.9742
Epoch 42/300
 - 15s - loss: 0.0112 - val_loss: 0.0095
 - val_f1: 0.9758
Epoch 43/300
 - 15s - loss: 0.0109 - val_loss: 0.0099
 - val_f1: 0.9746
Epoch 44/300
 - 15s - loss: 0.0107 - val_loss: 0.0099
 - val_f1: 0.9751
Epoch 45/300
 - 15s - loss: 0.0105 - val_loss: 0.0092
 - val_f1: 0.9761
Epoch 46/300
 - 15s - loss: 0.0103 - val_loss: 0.0093
 - val_f1: 0.9757
Epoch 47/300
 - 15s - loss: 0.0102 - val_loss: 0.0092
 - val_f1: 0.9756
Epoch 48/300
 - 15s - loss: 0.0100 - val_loss: 0.0089
 - val_f1: 0.9766
Epoch 49/300
 - 15s - loss: 0.0098 - val_loss: 0.0088
 - val_f1: 0.9770
Epoch 50/300
 - 15s - loss: 0.0098 - val_loss: 0.0088
 - val_f1: 0.9759
Epoch 51/300
 - 15s - loss: 0.0096 - val_loss: 0.0087
 - val_f1: 0.9768
Epoch 52/300
 - 15s - loss: 0.0095 - val_loss: 0.0087
 - val_f1: 0.9764
Epoch 53/300
 - 15s - loss: 0.0094 - val_loss: 0.0087
 - val_f1: 0.9766
Epoch 54/300
 - 15s - loss: 0.0093 - val_loss: 0.0086
 - val_f1: 0.9768
Epoch 55/300
 - 15s - loss: 0.0092 - val_loss: 0.0084
 - val_f1: 0.9773
Epoch 56/300
 - 15s - loss: 0.0091 - val_loss: 0.0086
 - val_f1: 0.9768
Epoch 57/300
 - 15s - loss: 0.0091 - val_loss: 0.0084
 - val_f1: 0.9772
Epoch 58/300
 - 15s - loss: 0.0089 - val_loss: 0.0083
 - val_f1: 0.9774
Epoch 59/300
 - 15s - loss: 0.0089 - val_loss: 0.0084
 - val_f1: 0.9773
Epoch 60/300
 - 15s - loss: 0.0088 - val_loss: 0.0084
 - val_f1: 0.9769
Epoch 61/300
 - 15s - loss: 0.0087 - val_loss: 0.0082
2019-12-27 06:03:25,125 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_ids18_subset_lstm_deep_rep1/_model_epoch_60.pickle
 - val_f1: 0.9774
Epoch 62/300
 - 15s - loss: 0.0086 - val_loss: 0.0082
 - val_f1: 0.9777
Epoch 63/300
 - 15s - loss: 0.0086 - val_loss: 0.0082
 - val_f1: 0.9775
Epoch 64/300
 - 15s - loss: 0.0086 - val_loss: 0.0082
 - val_f1: 0.9776
Epoch 65/300
 - 15s - loss: 0.0085 - val_loss: 0.0082
 - val_f1: 0.9776
Epoch 66/300
 - 15s - loss: 0.0085 - val_loss: 0.0081
 - val_f1: 0.9777
Epoch 67/300
 - 15s - loss: 0.0084 - val_loss: 0.0083
 - val_f1: 0.9770
Epoch 68/300
 - 15s - loss: 0.0084 - val_loss: 0.0080
 - val_f1: 0.9778
Epoch 69/300
 - 15s - loss: 0.0084 - val_loss: 0.0080
 - val_f1: 0.9780
Epoch 70/300
 - 15s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9778
Epoch 71/300
 - 15s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9776
Epoch 72/300
 - 15s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9770
Epoch 73/300
 - 15s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9777
Epoch 74/300
 - 15s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 75/300
 - 15s - loss: 0.0082 - val_loss: 0.0083
 - val_f1: 0.9770
Epoch 76/300
 - 15s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9772
Epoch 77/300
 - 15s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 78/300
 - 15s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9780
Epoch 79/300
 - 15s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9777
Epoch 80/300
 - 15s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 81/300
 - 15s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9783
Epoch 82/300
 - 15s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 83/300
 - 15s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 84/300
 - 15s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 85/300
 - 15s - loss: 0.0080 - val_loss: 0.0081
 - val_f1: 0.9776
Epoch 86/300
 - 15s - loss: 0.0080 - val_loss: 0.0087
 - val_f1: 0.9758
Epoch 87/300
 - 15s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9780
Epoch 88/300
 - 15s - loss: 0.0080 - val_loss: 0.0082
 - val_f1: 0.9774
Epoch 89/300
 - 15s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9781
Epoch 90/300
 - 15s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9780
Epoch 91/300
 - 15s - loss: 0.0080 - val_loss: 0.0080
2019-12-27 06:12:36,525 [INFO] epoch = 90. Intermediate model saved to results_selected_models/selected_ids18_subset_lstm_deep_rep1/_model_epoch_90.pickle
 - val_f1: 0.9782
Epoch 92/300
 - 15s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9784
Epoch 93/300
 - 15s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 94/300
 - 15s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9778
Epoch 95/300
 - 15s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 96/300
 - 15s - loss: 0.0079 - val_loss: 0.0081
 - val_f1: 0.9777
Epoch 97/300
 - 15s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9784
Epoch 98/300
 - 15s - loss: 0.0079 - val_loss: 0.0083
 - val_f1: 0.9772
Epoch 99/300
 - 15s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 100/300
 - 15s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 101/300
 - 15s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 102/300
 - 15s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 103/300
 - 15s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9784
Epoch 104/300
 - 15s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9784
Epoch 105/300
 - 15s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 106/300
 - 15s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 107/300
 - 15s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9780
Epoch 108/300
 - 15s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 109/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 110/300
 - 15s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 111/300
 - 15s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9785
Epoch 112/300
 - 15s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9784
Epoch 113/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9784
Epoch 114/300
 - 15s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 115/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 116/300
 - 15s - loss: 0.0078 - val_loss: 0.0082
 - val_f1: 0.9776
Epoch 117/300
 - 15s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 118/300
 - 15s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 119/300
 - 15s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 120/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9784
Epoch 121/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
2019-12-27 06:21:48,300 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_ids18_subset_lstm_deep_rep1/_model_epoch_120.pickle
 - val_f1: 0.9781
Epoch 122/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9785
Epoch 123/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 124/300
 - 15s - loss: 0.0078 - val_loss: 0.0080
 - val_f1: 0.9781
Epoch 125/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 126/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9778
Epoch 127/300
 - 15s - loss: 0.0078 - val_loss: 0.0080
 - val_f1: 0.9782
Epoch 128/300
 - 15s - loss: 0.0078 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 129/300
 - 15s - loss: 0.0077 - val_loss: 0.0080
 - val_f1: 0.9782
Epoch 130/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9784
Epoch 131/300
 - 15s - loss: 0.0077 - val_loss: 0.0080
 - val_f1: 0.9781
Epoch 132/300
 - 15s - loss: 0.0077 - val_loss: 0.0080
 - val_f1: 0.9782
Epoch 133/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 134/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 135/300
 - 15s - loss: 0.0077 - val_loss: 0.0080
 - val_f1: 0.9769
Epoch 136/300
 - 15s - loss: 0.0077 - val_loss: 0.0080
 - val_f1: 0.9781
Epoch 137/300
 - 15s - loss: 0.0077 - val_loss: 0.0080
 - val_f1: 0.9780
Epoch 138/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 139/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 140/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 141/300
 - 15s - loss: 0.0077 - val_loss: 0.0081
 - val_f1: 0.9772
Epoch 142/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 143/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 144/300
 - 15s - loss: 0.0077 - val_loss: 0.0080
 - val_f1: 0.9778
Epoch 145/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9785
Epoch 146/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 147/300
 - 15s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 148/300
 - 15s - loss: 0.0078 - val_loss: 0.0082
 - val_f1: 0.9774
Epoch 149/300
 - 15s - loss: 0.0077 - val_loss: 0.0080
 - val_f1: 0.9781
Epoch 150/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 151/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
2019-12-27 06:30:59,732 [INFO] epoch = 150. Intermediate model saved to results_selected_models/selected_ids18_subset_lstm_deep_rep1/_model_epoch_150.pickle
 - val_f1: 0.9784
Epoch 152/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9784
Epoch 153/300
 - 15s - loss: 0.0077 - val_loss: 0.0080
 - val_f1: 0.9780
Epoch 154/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9785
Epoch 155/300
 - 15s - loss: 0.0077 - val_loss: 0.0080
 - val_f1: 0.9780
Epoch 156/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9784
Epoch 157/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 158/300
 - 15s - loss: 0.0077 - val_loss: 0.0084
 - val_f1: 0.9771
Epoch 159/300
 - 15s - loss: 0.0077 - val_loss: 0.0081
 - val_f1: 0.9781
Epoch 160/300
 - 15s - loss: 0.0077 - val_loss: 0.0082
 - val_f1: 0.9776
Epoch 161/300
 - 15s - loss: 0.0077 - val_loss: 0.0081
 - val_f1: 0.9776
Epoch 162/300
 - 15s - loss: 0.0077 - val_loss: 0.0080
 - val_f1: 0.9782
Epoch 163/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 164/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 165/300
 - 15s - loss: 0.0076 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 166/300
 - 15s - loss: 0.0076 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 167/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 168/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
 - val_f1: 0.9782
Epoch 169/300
 - 15s - loss: 0.0076 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 170/300
 - 15s - loss: 0.0076 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 171/300
 - 15s - loss: 0.0077 - val_loss: 0.0082
 - val_f1: 0.9775
Epoch 172/300
 - 15s - loss: 0.0077 - val_loss: 0.0083
 - val_f1: 0.9773
Epoch 173/300
 - 15s - loss: 0.0077 - val_loss: 0.0081
 - val_f1: 0.9777
Epoch 174/300
 - 15s - loss: 0.0076 - val_loss: 0.0085
 - val_f1: 0.9773
Epoch 175/300
 - 15s - loss: 0.0076 - val_loss: 0.0081
 - val_f1: 0.9779
Epoch 176/300
 - 15s - loss: 0.0077 - val_loss: 0.0081
 - val_f1: 0.9783
Epoch 177/300
 - 15s - loss: 0.0076 - val_loss: 0.0081
 - val_f1: 0.9780
Epoch 178/300
 - 15s - loss: 0.0076 - val_loss: 0.0091
 - val_f1: 0.9724
Epoch 179/300
 - 15s - loss: 0.0076 - val_loss: 0.0094
 - val_f1: 0.9717
Epoch 180/300
 - 15s - loss: 0.0076 - val_loss: 0.0084
 - val_f1: 0.9770
Epoch 181/300
 - 15s - loss: 0.0076 - val_loss: 0.0079
2019-12-27 06:40:11,657 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_ids18_subset_lstm_deep_rep1/_model_epoch_180.pickle
 - val_f1: 0.9783
Epoch 182/300
 - 15s - loss: 0.0076 - val_loss: 0.0095
 - val_f1: 0.9723
Epoch 183/300
 - 15s - loss: 0.0076 - val_loss: 0.0085
 - val_f1: 0.9772
Epoch 184/300
 - 15s - loss: 0.0077 - val_loss: 0.0096
 - val_f1: 0.9712
Epoch 185/300
 - 15s - loss: 0.0076 - val_loss: 0.0079
 - val_f1: 0.9777
Epoch 186/300
 - 15s - loss: 0.0076 - val_loss: 0.0082
 - val_f1: 0.9774
Epoch 187/300
 - 15s - loss: 0.0076 - val_loss: 0.0082
 - val_f1: 0.9775
Epoch 188/300
 - 15s - loss: 0.0076 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 189/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
 - val_f1: 0.9780
Epoch 190/300
 - 15s - loss: 0.0077 - val_loss: 0.0085
 - val_f1: 0.9771
Epoch 191/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
 - val_f1: 0.9782
Epoch 192/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
 - val_f1: 0.9781
Epoch 193/300
 - 15s - loss: 0.0076 - val_loss: 0.0081
 - val_f1: 0.9779
Epoch 194/300
 - 15s - loss: 0.0077 - val_loss: 0.0084
 - val_f1: 0.9770
Epoch 195/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
 - val_f1: 0.9778
Epoch 196/300
 - 15s - loss: 0.0076 - val_loss: 0.0082
 - val_f1: 0.9777
Epoch 197/300
 - 15s - loss: 0.0076 - val_loss: 0.0081
 - val_f1: 0.9780
Epoch 198/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
 - val_f1: 0.9782
Epoch 199/300
 - 15s - loss: 0.0076 - val_loss: 0.0082
 - val_f1: 0.9779
Epoch 200/300
 - 15s - loss: 0.0076 - val_loss: 0.0083
 - val_f1: 0.9774
Epoch 201/300
 - 15s - loss: 0.0076 - val_loss: 0.0081
 - val_f1: 0.9781
Epoch 202/300
 - 15s - loss: 0.0076 - val_loss: 0.0083
 - val_f1: 0.9771
Epoch 203/300
 - 15s - loss: 0.0076 - val_loss: 0.0084
 - val_f1: 0.9773
Epoch 204/300
 - 15s - loss: 0.0076 - val_loss: 0.0081
 - val_f1: 0.9779
Epoch 205/300
 - 15s - loss: 0.0076 - val_loss: 0.0082
 - val_f1: 0.9779
Epoch 206/300
 - 15s - loss: 0.0076 - val_loss: 0.0084
 - val_f1: 0.9770
Epoch 207/300
 - 15s - loss: 0.0076 - val_loss: 0.0082
 - val_f1: 0.9776
Epoch 208/300
 - 15s - loss: 0.0076 - val_loss: 0.0081
 - val_f1: 0.9781
Epoch 209/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
 - val_f1: 0.9783
Epoch 210/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
 - val_f1: 0.9781
Epoch 211/300
 - 15s - loss: 0.0076 - val_loss: 0.0083
2019-12-27 06:49:24,753 [INFO] epoch = 210. Intermediate model saved to results_selected_models/selected_ids18_subset_lstm_deep_rep1/_model_epoch_210.pickle
 - val_f1: 0.9776
Epoch 212/300
 - 15s - loss: 0.0076 - val_loss: 0.0083
 - val_f1: 0.9776
Epoch 213/300
 - 15s - loss: 0.0076 - val_loss: 0.0082
 - val_f1: 0.9777
Epoch 214/300
 - 15s - loss: 0.0076 - val_loss: 0.0083
 - val_f1: 0.9775
Epoch 215/300
 - 15s - loss: 0.0076 - val_loss: 0.0085
2019-12-27 06:50:42,137 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-27 06:50:55,425 [INFO] Last epoch loss evaluation: train_loss = 0.007538, val_loss = 0.007810
2019-12-27 06:50:55,425 [INFO] Training complete. time_to_train = 3980.65 sec, 66.34 min
2019-12-27 06:50:55,431 [INFO] Model saved to results_selected_models/selected_ids18_subset_lstm_deep_rep1/best_model.pickle
2019-12-27 06:50:55,472 [INFO] Training history saved to: results_selected_models/selected_ids18_subset_lstm_deep_rep1/training_error_history.csv
/home/hasitha/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py:514: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  max_open_warning, RuntimeWarning)
2019-12-27 06:50:55,686 [INFO] Plot saved to: results_selected_models/selected_ids18_subset_lstm_deep_rep1/training_error_history.png
2019-12-27 06:50:55,799 [INFO] Plot saved to: results_selected_models/selected_ids18_subset_lstm_deep_rep1/training_f1_history.png
2019-12-27 06:50:55,799 [INFO] Making predictions on training, validation, testing data
2019-12-27 06:51:10,904 [INFO] Evaluating predictions (results)
/home/hasitha/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2019-12-27 06:51:22,939 [INFO] Dataset: Testing. Classification report below
2019-12-27 06:51:22,939 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99    535639
                     Bot       1.00      1.00      1.00     11465
        Brute Force -Web       0.60      0.12      0.21        24
        Brute Force -XSS       1.00      0.22      0.36         9
        DDOS attack-HOIC       1.00      1.00      1.00     27446
    DDOS attack-LOIC-UDP       0.69      0.88      0.77        67
  DDoS attacks-LOIC-HTTP       0.99      0.99      0.99     23008
   DoS attacks-GoldenEye       0.99      1.00      1.00      1651
        DoS attacks-Hulk       1.00      1.00      1.00     18477
DoS attacks-SlowHTTPTest       0.75      0.51      0.61      5596
   DoS attacks-Slowloris       0.96      0.92      0.94       440
          FTP-BruteForce       0.71      0.88      0.79      7718
           Infilteration       0.58      0.00      0.01      6403
           SQL Injection       0.00      0.00      0.00         4
          SSH-Bruteforce       1.00      1.00      1.00      7525

               micro avg       0.98      0.98      0.98    645472
               macro avg       0.82      0.70      0.71    645472
            weighted avg       0.98      0.98      0.98    645472

2019-12-27 06:51:22,939 [INFO] Overall accuracy (micro avg): 0.9835732611174458
/home/hasitha/sunanda/ids_experiments/utility.py:328: RuntimeWarning: invalid value encountered in true_divide
  PPV = TP / (TP + FP)    # Precision or positive predictive value
/home/hasitha/sunanda/ids_experiments/utility.py:332: RuntimeWarning: invalid value encountered in true_divide
  FDR = FP / (TP + FP)    # False discovery rate
2019-12-27 06:51:36,653 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9836         0.9836                       0.9836                0.0012                   0.0164  0.9836
1     Macro avg        0.9978         0.8172                       0.7019                0.0044                   0.2981  0.7107
2  Weighted avg        0.9909         0.9797                       0.9836                0.0499                   0.0164  0.9785
2019-12-27 06:51:49,046 [INFO] Dataset: Validation. Classification report below
2019-12-27 06:51:49,046 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99    535636
                     Bot       1.00      1.00      1.00     11464
        Brute Force -Web       0.80      0.16      0.27        25
        Brute Force -XSS       1.00      0.56      0.71         9
        DDOS attack-HOIC       1.00      1.00      1.00     27447
    DDOS attack-LOIC-UDP       0.71      0.79      0.75        68
  DDoS attacks-LOIC-HTTP       0.99      0.99      0.99     23009
   DoS attacks-GoldenEye       0.99      1.00      1.00      1651
        DoS attacks-Hulk       1.00      1.00      1.00     18478
DoS attacks-SlowHTTPTest       0.76      0.51      0.61      5596
   DoS attacks-Slowloris       0.96      0.93      0.94       439
          FTP-BruteForce       0.71      0.88      0.79      7718
           Infilteration       0.47      0.00      0.01      6403
           SQL Injection       0.00      0.00      0.00         4
          SSH-Bruteforce       1.00      1.00      1.00      7525

               micro avg       0.98      0.98      0.98    645472
               macro avg       0.82      0.72      0.74    645472
            weighted avg       0.98      0.98      0.98    645472

2019-12-27 06:51:49,046 [INFO] Overall accuracy (micro avg): 0.9836243864954638
2019-12-27 06:52:03,095 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9836         0.9836                       0.9836                0.0012                   0.0164  0.9836
1     Macro avg        0.9978         0.8249                       0.7213                0.0044                   0.2787  0.7370
2  Weighted avg        0.9909         0.9786                       0.9836                0.0497                   0.0164  0.9785
2019-12-27 06:52:42,796 [INFO] Dataset: Training. Classification report below
2019-12-27 06:52:42,796 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99   1606935
                     Bot       1.00      1.00      1.00     34396
        Brute Force -Web       0.90      0.12      0.22        73
        Brute Force -XSS       1.00      0.50      0.67        26
        DDOS attack-HOIC       1.00      1.00      1.00     82341
    DDOS attack-LOIC-UDP       0.77      0.92      0.84       203
  DDoS attacks-LOIC-HTTP       0.99      1.00      0.99     69029
   DoS attacks-GoldenEye       1.00      1.00      1.00      4954
        DoS attacks-Hulk       1.00      1.00      1.00     55435
DoS attacks-SlowHTTPTest       0.76      0.51      0.61     16787
   DoS attacks-Slowloris       0.98      0.98      0.98      1318
          FTP-BruteForce       0.71      0.88      0.79     23153
           Infilteration       0.74      0.00      0.01     19210
           SQL Injection       0.00      0.00      0.00        12
          SSH-Bruteforce       1.00      1.00      1.00     22576

               micro avg       0.98      0.98      0.98   1936448
               macro avg       0.86      0.73      0.74   1936448
            weighted avg       0.98      0.98      0.98   1936448

2019-12-27 06:52:42,796 [INFO] Overall accuracy (micro avg): 0.9838560085269524
2019-12-27 06:53:27,879 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9839         0.9839                       0.9839                0.0012                   0.0161  0.9839
1     Macro avg        0.9978         0.8559                       0.7274                0.0044                   0.2726  0.7395
2  Weighted avg        0.9911         0.9816                       0.9839                0.0493                   0.0161  0.9787
2019-12-27 06:53:27,928 [INFO] Results saved to: results_selected_models/selected_ids18_subset_lstm_deep_rep1/selected_ids18_subset_lstm_deep_rep1_results.xlsx
2019-12-27 06:53:27,933 [INFO] ================= Finished running experiment no. 1 ================= 

2019-12-27 06:53:28,017 [INFO] Created directory: results_selected_models/selected_ids18_subset_lstm_deep_rep2
2019-12-27 06:53:28,017 [INFO] Initialized logging. log_filename = results_selected_models/selected_ids18_subset_lstm_deep_rep2/run_log.log
2019-12-27 06:53:28,017 [INFO] ================= Running experiment no. 2  ================= 

2019-12-27 06:53:28,017 [INFO] Experiment parameters given below
2019-12-27 06:53:28,018 [INFO] 
{'experiment_num': 2, 'results_dir': 'results_selected_models/selected_ids18_subset_lstm_deep_rep2', 'model': 'lstm', 'model_type': 'classifier', 'normal_label': 'BENIGN', 'lstm_time_steps': 32, 'lstm_layer_units': [64, 32], 'lstm_layer_activations': ['relu', 'relu'], 'lstm_layer_dropout_rates': [0.2, 0.2], 'batch_size': 256, 'epochs': 300, 'early_stop_patience': 50, 'dataset_dir': '../Datasets/small_datasets/ids2018', 'description': 'selected_ids18_subset_lstm_deep_rep2'}
2019-12-27 06:53:28,018 [INFO] Created tensorboard log directory: results_selected_models/selected_ids18_subset_lstm_deep_rep2/tf_logs_run_2019_12_27-06_53_28
2019-12-27 06:53:28,018 [INFO] Loading datsets from: ../Datasets/small_datasets/ids2018
2019-12-27 06:53:28,018 [INFO] Reading X, y files
2019-12-27 06:53:28,018 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_train.h5
2019-12-27 06:53:33,969 [INFO] Reading complete. time_to_read=5.95 seconds
2019-12-27 06:53:33,970 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_val.h5
2019-12-27 06:53:35,647 [INFO] Reading complete. time_to_read=1.68 seconds
2019-12-27 06:53:35,647 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_test.h5
2019-12-27 06:53:37,449 [INFO] Reading complete. time_to_read=1.80 seconds
2019-12-27 06:53:37,449 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_train.h5
2019-12-27 06:53:37,746 [INFO] Reading complete. time_to_read=0.30 seconds
2019-12-27 06:53:37,746 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_val.h5
2019-12-27 06:53:37,852 [INFO] Reading complete. time_to_read=0.11 seconds
2019-12-27 06:53:37,853 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_test.h5
2019-12-27 06:53:37,971 [INFO] Reading complete. time_to_read=0.12 seconds
2019-12-27 06:53:40,726 [INFO] Preparing flow sequences
2019-12-27 06:54:05,641 [INFO] Extracting flows complete. time_taken = 24.91 sec
2019-12-27 06:54:06,971 [INFO] Initializing model
2019-12-27 06:54:07,421 [INFO] _________________________________________________________________
2019-12-27 06:54:07,421 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-27 06:54:07,421 [INFO] =================================================================
2019-12-27 06:54:07,421 [INFO] lstm_23 (LSTM)               (None, 32, 64)            36352     
2019-12-27 06:54:07,421 [INFO] _________________________________________________________________
2019-12-27 06:54:07,421 [INFO] batch_normalization_23 (Batc (None, 32, 64)            256       
2019-12-27 06:54:07,421 [INFO] _________________________________________________________________
2019-12-27 06:54:07,421 [INFO] dropout_23 (Dropout)         (None, 32, 64)            0         
2019-12-27 06:54:07,421 [INFO] _________________________________________________________________
2019-12-27 06:54:07,421 [INFO] lstm_24 (LSTM)               (None, 32, 32)            12416     
2019-12-27 06:54:07,421 [INFO] _________________________________________________________________
2019-12-27 06:54:07,422 [INFO] batch_normalization_24 (Batc (None, 32, 32)            128       
2019-12-27 06:54:07,422 [INFO] _________________________________________________________________
2019-12-27 06:54:07,422 [INFO] dropout_24 (Dropout)         (None, 32, 32)            0         
2019-12-27 06:54:07,422 [INFO] _________________________________________________________________
2019-12-27 06:54:07,422 [INFO] time_distributed_12 (TimeDis (None, 32, 15)            495       
2019-12-27 06:54:07,422 [INFO] =================================================================
2019-12-27 06:54:07,422 [INFO] Total params: 49,647
2019-12-27 06:54:07,422 [INFO] Trainable params: 49,455
2019-12-27 06:54:07,422 [INFO] Non-trainable params: 192
2019-12-27 06:54:07,422 [INFO] _________________________________________________________________
2019-12-27 06:54:07,422 [INFO] Training model
 - val_f1: 0.9770
Epoch 00215: early stopping
Train on 60514 samples, validate on 20171 samples
Epoch 1/300
 - 17s - loss: 0.1658 - val_loss: 0.0832
 - val_f1: 0.7619
Epoch 2/300
 - 15s - loss: 0.0795 - val_loss: 0.0675
 - val_f1: 0.7674
Epoch 3/300
 - 15s - loss: 0.0788 - val_loss: 0.0833
 - val_f1: 0.7541
Epoch 4/300
 - 15s - loss: 0.0798 - val_loss: 0.0817
 - val_f1: 0.7533
Epoch 5/300
 - 15s - loss: 0.0767 - val_loss: 0.0807
 - val_f1: 0.7542
Epoch 6/300
 - 15s - loss: 0.0752 - val_loss: 0.0843
 - val_f1: 0.7405
Epoch 7/300
 - 15s - loss: 0.0776 - val_loss: 0.0791
 - val_f1: 0.7568
Epoch 8/300
 - 15s - loss: 0.0771 - val_loss: 0.0772
 - val_f1: 0.7584
Epoch 9/300
 - 15s - loss: 0.0760 - val_loss: 0.0757
 - val_f1: 0.7593
Epoch 10/300
 - 15s - loss: 0.0746 - val_loss: 0.0709
 - val_f1: 0.7641
Epoch 11/300
 - 15s - loss: 0.0710 - val_loss: 0.0711
 - val_f1: 0.7666
Epoch 12/300
 - 15s - loss: 0.0705 - val_loss: 0.0637
 - val_f1: 0.7783
Epoch 13/300
 - 15s - loss: 0.0658 - val_loss: 0.0588
 - val_f1: 0.7834
Epoch 14/300
 - 15s - loss: 0.0578 - val_loss: 0.0634
 - val_f1: 0.7637
Epoch 15/300
 - 15s - loss: 0.0516 - val_loss: 0.0560
 - val_f1: 0.7748
Epoch 16/300
 - 15s - loss: 0.0477 - val_loss: 0.0469
 - val_f1: 0.7951
Epoch 17/300
 - 15s - loss: 0.0437 - val_loss: 0.0369
 - val_f1: 0.8406
Epoch 18/300
 - 15s - loss: 0.0406 - val_loss: 0.0318
 - val_f1: 0.8884
Epoch 19/300
 - 15s - loss: 0.0374 - val_loss: 0.0267
 - val_f1: 0.9200
Epoch 20/300
 - 15s - loss: 0.0342 - val_loss: 0.0237
 - val_f1: 0.9271
Epoch 21/300
 - 15s - loss: 0.0311 - val_loss: 0.0212
 - val_f1: 0.9337
Epoch 22/300
 - 15s - loss: 0.0283 - val_loss: 0.0183
 - val_f1: 0.9453
Epoch 23/300
 - 15s - loss: 0.0258 - val_loss: 0.0168
 - val_f1: 0.9537
Epoch 24/300
 - 15s - loss: 0.0237 - val_loss: 0.0195
 - val_f1: 0.9403
Epoch 25/300
 - 15s - loss: 0.0219 - val_loss: 0.0179
 - val_f1: 0.9472
Epoch 26/300
 - 15s - loss: 0.0203 - val_loss: 0.0186
 - val_f1: 0.9450
Epoch 27/300
 - 15s - loss: 0.0190 - val_loss: 0.0165
 - val_f1: 0.9518
Epoch 28/300
 - 15s - loss: 0.0180 - val_loss: 0.0163
 - val_f1: 0.9527
Epoch 29/300
 - 15s - loss: 0.0170 - val_loss: 0.0157
 - val_f1: 0.9523
Epoch 30/300
 - 15s - loss: 0.0162 - val_loss: 0.0145
 - val_f1: 0.9573
Epoch 31/300
 - 15s - loss: 0.0154 - val_loss: 0.0139
/home/hasitha/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2019-12-27 07:03:47,429 [INFO] epoch = 30. Intermediate model saved to results_selected_models/selected_ids18_subset_lstm_deep_rep2/_model_epoch_30.pickle
 - val_f1: 0.9596
Epoch 32/300
 - 15s - loss: 0.0149 - val_loss: 0.0142
 - val_f1: 0.9582
Epoch 33/300
 - 15s - loss: 0.0143 - val_loss: 0.0128
 - val_f1: 0.9652
Epoch 34/300
 - 15s - loss: 0.0138 - val_loss: 0.0124
 - val_f1: 0.9670
Epoch 35/300
 - 15s - loss: 0.0134 - val_loss: 0.0122
 - val_f1: 0.9674
Epoch 36/300
 - 15s - loss: 0.0131 - val_loss: 0.0118
 - val_f1: 0.9696
Epoch 37/300
 - 15s - loss: 0.0127 - val_loss: 0.0113
 - val_f1: 0.9706
Epoch 38/300
 - 15s - loss: 0.0124 - val_loss: 0.0114
 - val_f1: 0.9699
Epoch 39/300
 - 15s - loss: 0.0122 - val_loss: 0.0114
 - val_f1: 0.9687
Epoch 40/300
 - 15s - loss: 0.0119 - val_loss: 0.0110
 - val_f1: 0.9710
Epoch 41/300
 - 15s - loss: 0.0116 - val_loss: 0.0107
 - val_f1: 0.9717
Epoch 42/300
 - 15s - loss: 0.0114 - val_loss: 0.0101
 - val_f1: 0.9733
Epoch 43/300
 - 15s - loss: 0.0111 - val_loss: 0.0105
 - val_f1: 0.9730
Epoch 44/300
 - 15s - loss: 0.0109 - val_loss: 0.0107
 - val_f1: 0.9702
Epoch 45/300
 - 15s - loss: 0.0107 - val_loss: 0.0097
 - val_f1: 0.9738
Epoch 46/300
 - 15s - loss: 0.0105 - val_loss: 0.0094
 - val_f1: 0.9754
Epoch 47/300
 - 15s - loss: 0.0103 - val_loss: 0.0093
 - val_f1: 0.9743
Epoch 48/300
 - 15s - loss: 0.0101 - val_loss: 0.0096
 - val_f1: 0.9740
Epoch 49/300
 - 15s - loss: 0.0100 - val_loss: 0.0092
 - val_f1: 0.9750
Epoch 50/300
 - 15s - loss: 0.0098 - val_loss: 0.0094
 - val_f1: 0.9751
Epoch 51/300
 - 15s - loss: 0.0097 - val_loss: 0.0089
 - val_f1: 0.9767
Epoch 52/300
 - 15s - loss: 0.0095 - val_loss: 0.0087
 - val_f1: 0.9761
Epoch 53/300
 - 15s - loss: 0.0094 - val_loss: 0.0089
 - val_f1: 0.9769
Epoch 54/300
 - 15s - loss: 0.0094 - val_loss: 0.0091
 - val_f1: 0.9768
Epoch 55/300
 - 15s - loss: 0.0093 - val_loss: 0.0089
 - val_f1: 0.9759
Epoch 56/300
 - 15s - loss: 0.0092 - val_loss: 0.0087
 - val_f1: 0.9763
Epoch 57/300
 - 15s - loss: 0.0091 - val_loss: 0.0089
 - val_f1: 0.9768
Epoch 58/300
 - 15s - loss: 0.0090 - val_loss: 0.0087
 - val_f1: 0.9774
Epoch 59/300
 - 15s - loss: 0.0089 - val_loss: 0.0087
 - val_f1: 0.9771
Epoch 60/300
 - 15s - loss: 0.0089 - val_loss: 0.0086
 - val_f1: 0.9775
Epoch 61/300
 - 15s - loss: 0.0088 - val_loss: 0.0086
2019-12-27 07:13:00,125 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_ids18_subset_lstm_deep_rep2/_model_epoch_60.pickle
 - val_f1: 0.9768
Epoch 62/300
 - 15s - loss: 0.0087 - val_loss: 0.0084
 - val_f1: 0.9776
Epoch 63/300
 - 15s - loss: 0.0087 - val_loss: 0.0084
 - val_f1: 0.9771
Epoch 64/300
 - 15s - loss: 0.0088 - val_loss: 0.0090
 - val_f1: 0.9767
Epoch 65/300
 - 15s - loss: 0.0089 - val_loss: 0.0089
 - val_f1: 0.9763
Epoch 66/300
 - 15s - loss: 0.0088 - val_loss: 0.0087
 - val_f1: 0.9769
Epoch 67/300
 - 15s - loss: 0.0087 - val_loss: 0.0085
 - val_f1: 0.9774
Epoch 68/300
 - 15s - loss: 0.0085 - val_loss: 0.0084
 - val_f1: 0.9775
Epoch 69/300
 - 15s - loss: 0.0084 - val_loss: 0.0082
 - val_f1: 0.9776
Epoch 70/300
 - 15s - loss: 0.0084 - val_loss: 0.0082
 - val_f1: 0.9775
Epoch 71/300
 - 15s - loss: 0.0084 - val_loss: 0.0081
 - val_f1: 0.9775
Epoch 72/300
 - 15s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9777
Epoch 73/300
 - 15s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9776
Epoch 74/300
 - 15s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9776
Epoch 75/300
 - 15s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9778
Epoch 76/300
 - 15s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9777
Epoch 77/300
 - 15s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9776
Epoch 78/300
 - 15s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9777
Epoch 79/300
 - 15s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9777
Epoch 80/300
 - 15s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9778
Epoch 81/300
 - 15s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9777
Epoch 82/300
 - 15s - loss: 0.0081 - val_loss: 0.0081
 - val_f1: 0.9775
Epoch 83/300
 - 15s - loss: 0.0081 - val_loss: 0.0081
 - val_f1: 0.9773
Epoch 84/300
 - 15s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9776
Epoch 85/300
 - 15s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 86/300
 - 15s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9778
Epoch 87/300
 - 15s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9778
Epoch 88/300
 - 15s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9777
Epoch 89/300
 - 15s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9778
Epoch 90/300
 - 15s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9778
Epoch 91/300
 - 15s - loss: 0.0080 - val_loss: 0.0079
2019-12-27 07:22:12,787 [INFO] epoch = 90. Intermediate model saved to results_selected_models/selected_ids18_subset_lstm_deep_rep2/_model_epoch_90.pickle
 - val_f1: 0.9778
Epoch 92/300
 - 15s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9778
Epoch 93/300
 - 15s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9778
Epoch 94/300
 - 15s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9777
Epoch 95/300
 - 15s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9777
Epoch 96/300
 - 15s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 97/300
 - 15s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9775
Epoch 98/300
 - 15s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 99/300
 - 15s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9776
Epoch 100/300
 - 15s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 101/300
 - 15s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9779
Epoch 102/300
 - 15s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9780
Epoch 103/300
 - 15s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9778
Epoch 104/300
 - 15s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 105/300
 - 15s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 106/300
 - 15s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 107/300
 - 15s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 108/300
 - 15s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 109/300
 - 15s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9774
Epoch 110/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 111/300
 - 15s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9780
Epoch 112/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 113/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 114/300
 - 15s - loss: 0.0078 - val_loss: 0.0080
 - val_f1: 0.9772
Epoch 115/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 116/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 117/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 118/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9768
Epoch 119/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 120/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 121/300
 - 15s - loss: 0.0078 - val_loss: 0.0078
2019-12-27 07:31:25,551 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_ids18_subset_lstm_deep_rep2/_model_epoch_120.pickle
 - val_f1: 0.9781
Epoch 122/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 123/300
 - 15s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 124/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9778
Epoch 125/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 126/300
 - 15s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 127/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 128/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 129/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 130/300
 - 15s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 131/300
 - 15s - loss: 0.0078 - val_loss: 0.0080
 - val_f1: 0.9780
Epoch 132/300
 - 15s - loss: 0.0078 - val_loss: 0.0082
 - val_f1: 0.9773
Epoch 133/300
 - 15s - loss: 0.0077 - val_loss: 0.0081
 - val_f1: 0.9779
Epoch 134/300
 - 15s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 135/300
 - 15s - loss: 0.0077 - val_loss: 0.0081
 - val_f1: 0.9773
Epoch 136/300
 - 15s - loss: 0.0077 - val_loss: 0.0080
 - val_f1: 0.9778
Epoch 137/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 138/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 139/300
 - 15s - loss: 0.0077 - val_loss: 0.0080
 - val_f1: 0.9780
Epoch 140/300
 - 15s - loss: 0.0077 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 141/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 142/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9784
Epoch 143/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 144/300
 - 15s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9780
Epoch 145/300
 - 15s - loss: 0.0077 - val_loss: 0.0080
 - val_f1: 0.9774
Epoch 146/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 147/300
 - 15s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 148/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9784
Epoch 149/300
 - 15s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 150/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 151/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
2019-12-27 07:40:38,985 [INFO] epoch = 150. Intermediate model saved to results_selected_models/selected_ids18_subset_lstm_deep_rep2/_model_epoch_150.pickle
 - val_f1: 0.9783
Epoch 152/300
 - 15s - loss: 0.0077 - val_loss: 0.0080
 - val_f1: 0.9776
Epoch 153/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 154/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
 - val_f1: 0.9774
Epoch 155/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 156/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 157/300
 - 15s - loss: 0.0077 - val_loss: 0.0080
 - val_f1: 0.9778
Epoch 158/300
 - 15s - loss: 0.0077 - val_loss: 0.0082
 - val_f1: 0.9772
Epoch 159/300
 - 15s - loss: 0.0077 - val_loss: 0.0080
 - val_f1: 0.9776
Epoch 160/300
 - 15s - loss: 0.0076 - val_loss: 0.0081
 - val_f1: 0.9775
Epoch 161/300
 - 15s - loss: 0.0077 - val_loss: 0.0081
 - val_f1: 0.9777
Epoch 162/300
 - 15s - loss: 0.0076 - val_loss: 0.0083
 - val_f1: 0.9769
Epoch 163/300
 - 15s - loss: 0.0077 - val_loss: 0.0080
 - val_f1: 0.9778
Epoch 164/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 165/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
 - val_f1: 0.9778
Epoch 166/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
 - val_f1: 0.9776
Epoch 167/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
 - val_f1: 0.9780
Epoch 168/300
 - 15s - loss: 0.0076 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 169/300
 - 15s - loss: 0.0076 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 170/300
 - 15s - loss: 0.0076 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 171/300
 - 15s - loss: 0.0076 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 172/300
 - 15s - loss: 0.0076 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 173/300
 - 15s - loss: 0.0076 - val_loss: 0.0079
2019-12-27 07:47:27,654 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-27 07:47:41,540 [INFO] Last epoch loss evaluation: train_loss = 0.007627, val_loss = 0.007802
2019-12-27 07:47:41,541 [INFO] Training complete. time_to_train = 3214.12 sec, 53.57 min
2019-12-27 07:47:41,547 [INFO] Model saved to results_selected_models/selected_ids18_subset_lstm_deep_rep2/best_model.pickle
2019-12-27 07:47:41,550 [INFO] Training history saved to: results_selected_models/selected_ids18_subset_lstm_deep_rep2/training_error_history.csv
/home/hasitha/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py:514: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  max_open_warning, RuntimeWarning)
2019-12-27 07:47:41,693 [INFO] Plot saved to: results_selected_models/selected_ids18_subset_lstm_deep_rep2/training_error_history.png
2019-12-27 07:47:41,815 [INFO] Plot saved to: results_selected_models/selected_ids18_subset_lstm_deep_rep2/training_f1_history.png
2019-12-27 07:47:41,815 [INFO] Making predictions on training, validation, testing data
2019-12-27 07:47:56,885 [INFO] Evaluating predictions (results)
/home/hasitha/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2019-12-27 07:48:08,913 [INFO] Dataset: Testing. Classification report below
2019-12-27 07:48:08,913 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99    535639
                     Bot       1.00      1.00      1.00     11465
        Brute Force -Web       1.00      0.12      0.22        24
        Brute Force -XSS       0.75      0.33      0.46         9
        DDOS attack-HOIC       1.00      1.00      1.00     27446
    DDOS attack-LOIC-UDP       0.69      1.00      0.82        67
  DDoS attacks-LOIC-HTTP       1.00      0.99      0.99     23008
   DoS attacks-GoldenEye       0.99      1.00      1.00      1651
        DoS attacks-Hulk       1.00      1.00      1.00     18477
DoS attacks-SlowHTTPTest       0.74      0.51      0.60      5596
   DoS attacks-Slowloris       0.90      0.95      0.92       440
          FTP-BruteForce       0.71      0.87      0.78      7718
           Infilteration       0.60      0.00      0.01      6403
           SQL Injection       0.00      0.00      0.00         4
          SSH-Bruteforce       1.00      1.00      1.00      7525

               micro avg       0.98      0.98      0.98    645472
               macro avg       0.82      0.72      0.72    645472
            weighted avg       0.98      0.98      0.98    645472

2019-12-27 07:48:08,913 [INFO] Overall accuracy (micro avg): 0.9834400252838233
/home/hasitha/sunanda/ids_experiments/utility.py:328: RuntimeWarning: invalid value encountered in true_divide
  PPV = TP / (TP + FP)    # Precision or positive predictive value
/home/hasitha/sunanda/ids_experiments/utility.py:332: RuntimeWarning: invalid value encountered in true_divide
  FDR = FP / (TP + FP)    # False discovery rate
2019-12-27 07:48:22,543 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9834         0.9834                       0.9834                0.0012                   0.0166  0.9834
1     Macro avg        0.9978         0.8243                       0.7184                0.0045                   0.2816  0.7197
2  Weighted avg        0.9909         0.9797                       0.9834                0.0509                   0.0166  0.9783
2019-12-27 07:48:34,832 [INFO] Dataset: Validation. Classification report below
2019-12-27 07:48:34,832 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99    535636
                     Bot       1.00      1.00      1.00     11464
        Brute Force -Web       1.00      0.20      0.33        25
        Brute Force -XSS       0.86      0.67      0.75         9
        DDOS attack-HOIC       1.00      1.00      1.00     27447
    DDOS attack-LOIC-UDP       0.74      1.00      0.85        68
  DDoS attacks-LOIC-HTTP       1.00      0.99      0.99     23009
   DoS attacks-GoldenEye       1.00      1.00      1.00      1651
        DoS attacks-Hulk       1.00      1.00      1.00     18478
DoS attacks-SlowHTTPTest       0.75      0.50      0.60      5596
   DoS attacks-Slowloris       0.90      0.96      0.93       439
          FTP-BruteForce       0.71      0.88      0.79      7718
           Infilteration       0.50      0.00      0.00      6403
           SQL Injection       0.00      0.00      0.00         4
          SSH-Bruteforce       1.00      1.00      1.00      7525

               micro avg       0.98      0.98      0.98    645472
               macro avg       0.83      0.75      0.75    645472
            weighted avg       0.98      0.98      0.98    645472

2019-12-27 07:48:34,833 [INFO] Overall accuracy (micro avg): 0.9835298820088245
2019-12-27 07:48:48,805 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9835         0.9835                       0.9835                0.0012                   0.0165  0.9835
1     Macro avg        0.9978         0.8288                       0.7463                0.0045                   0.2537  0.7490
2  Weighted avg        0.9909         0.9788                       0.9835                0.0507                   0.0165  0.9784
2019-12-27 07:49:28,671 [INFO] Dataset: Training. Classification report below
2019-12-27 07:49:28,671 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99   1606935
                     Bot       1.00      1.00      1.00     34396
        Brute Force -Web       1.00      0.12      0.22        73
        Brute Force -XSS       1.00      0.50      0.67        26
        DDOS attack-HOIC       1.00      1.00      1.00     82341
    DDOS attack-LOIC-UDP       0.73      0.99      0.84       203
  DDoS attacks-LOIC-HTTP       1.00      0.99      0.99     69029
   DoS attacks-GoldenEye       1.00      1.00      1.00      4954
        DoS attacks-Hulk       1.00      1.00      1.00     55435
DoS attacks-SlowHTTPTest       0.75      0.51      0.61     16787
   DoS attacks-Slowloris       0.92      0.99      0.96      1318
          FTP-BruteForce       0.71      0.88      0.79     23153
           Infilteration       0.74      0.00      0.01     19210
           SQL Injection       0.00      0.00      0.00        12
          SSH-Bruteforce       1.00      1.00      1.00     22576

               micro avg       0.98      0.98      0.98   1936448
               macro avg       0.86      0.73      0.74   1936448
            weighted avg       0.98      0.98      0.98   1936448

2019-12-27 07:49:28,671 [INFO] Overall accuracy (micro avg): 0.9836633878110851
2019-12-27 07:50:13,957 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9837         0.9837                       0.9837                0.0012                   0.0163  0.9837
1     Macro avg        0.9978         0.8551                       0.7318                0.0045                   0.2682  0.7372
2  Weighted avg        0.9910         0.9813                       0.9837                0.0505                   0.0163  0.9785
2019-12-27 07:50:13,996 [INFO] Results saved to: results_selected_models/selected_ids18_subset_lstm_deep_rep2/selected_ids18_subset_lstm_deep_rep2_results.xlsx
2019-12-27 07:50:14,003 [INFO] ================= Finished running experiment no. 2 ================= 

2019-12-27 07:50:14,088 [INFO] Created directory: results_selected_models/selected_ids18_subset_lstm_deep_rep3
2019-12-27 07:50:14,088 [INFO] Initialized logging. log_filename = results_selected_models/selected_ids18_subset_lstm_deep_rep3/run_log.log
2019-12-27 07:50:14,089 [INFO] ================= Running experiment no. 3  ================= 

2019-12-27 07:50:14,089 [INFO] Experiment parameters given below
2019-12-27 07:50:14,089 [INFO] 
{'experiment_num': 3, 'results_dir': 'results_selected_models/selected_ids18_subset_lstm_deep_rep3', 'model': 'lstm', 'model_type': 'classifier', 'normal_label': 'BENIGN', 'lstm_time_steps': 32, 'lstm_layer_units': [64, 32], 'lstm_layer_activations': ['relu', 'relu'], 'lstm_layer_dropout_rates': [0.2, 0.2], 'batch_size': 256, 'epochs': 300, 'early_stop_patience': 50, 'dataset_dir': '../Datasets/small_datasets/ids2018', 'description': 'selected_ids18_subset_lstm_deep_rep3'}
2019-12-27 07:50:14,089 [INFO] Created tensorboard log directory: results_selected_models/selected_ids18_subset_lstm_deep_rep3/tf_logs_run_2019_12_27-07_50_14
2019-12-27 07:50:14,089 [INFO] Loading datsets from: ../Datasets/small_datasets/ids2018
2019-12-27 07:50:14,089 [INFO] Reading X, y files
2019-12-27 07:50:14,089 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_train.h5
2019-12-27 07:50:18,933 [INFO] Reading complete. time_to_read=4.84 seconds
2019-12-27 07:50:18,933 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_val.h5
2019-12-27 07:50:20,497 [INFO] Reading complete. time_to_read=1.56 seconds
2019-12-27 07:50:20,498 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_test.h5
2019-12-27 07:50:22,061 [INFO] Reading complete. time_to_read=1.56 seconds
2019-12-27 07:50:22,062 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_train.h5
2019-12-27 07:50:22,324 [INFO] Reading complete. time_to_read=0.26 seconds
2019-12-27 07:50:22,324 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_val.h5
2019-12-27 07:50:22,411 [INFO] Reading complete. time_to_read=0.09 seconds
2019-12-27 07:50:22,411 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_test.h5
2019-12-27 07:50:22,497 [INFO] Reading complete. time_to_read=0.09 seconds
2019-12-27 07:50:25,309 [INFO] Preparing flow sequences
2019-12-27 07:50:50,177 [INFO] Extracting flows complete. time_taken = 24.86 sec
2019-12-27 07:50:51,531 [INFO] Initializing model
2019-12-27 07:50:52,010 [INFO] _________________________________________________________________
2019-12-27 07:50:52,011 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-27 07:50:52,011 [INFO] =================================================================
2019-12-27 07:50:52,011 [INFO] lstm_25 (LSTM)               (None, 32, 64)            36352     
2019-12-27 07:50:52,011 [INFO] _________________________________________________________________
2019-12-27 07:50:52,011 [INFO] batch_normalization_25 (Batc (None, 32, 64)            256       
2019-12-27 07:50:52,011 [INFO] _________________________________________________________________
2019-12-27 07:50:52,011 [INFO] dropout_25 (Dropout)         (None, 32, 64)            0         
2019-12-27 07:50:52,011 [INFO] _________________________________________________________________
2019-12-27 07:50:52,011 [INFO] lstm_26 (LSTM)               (None, 32, 32)            12416     
2019-12-27 07:50:52,011 [INFO] _________________________________________________________________
2019-12-27 07:50:52,011 [INFO] batch_normalization_26 (Batc (None, 32, 32)            128       
2019-12-27 07:50:52,011 [INFO] _________________________________________________________________
2019-12-27 07:50:52,011 [INFO] dropout_26 (Dropout)         (None, 32, 32)            0         
2019-12-27 07:50:52,011 [INFO] _________________________________________________________________
2019-12-27 07:50:52,011 [INFO] time_distributed_13 (TimeDis (None, 32, 15)            495       
2019-12-27 07:50:52,011 [INFO] =================================================================
2019-12-27 07:50:52,012 [INFO] Total params: 49,647
2019-12-27 07:50:52,012 [INFO] Trainable params: 49,455
2019-12-27 07:50:52,012 [INFO] Non-trainable params: 192
2019-12-27 07:50:52,012 [INFO] _________________________________________________________________
2019-12-27 07:50:52,012 [INFO] Training model
 - val_f1: 0.9780
Epoch 00173: early stopping
Train on 60514 samples, validate on 20171 samples
Epoch 1/300
 - 17s - loss: 0.1657 - val_loss: 0.0886
 - val_f1: 0.7503
Epoch 2/300
 - 15s - loss: 0.0859 - val_loss: 0.0809
 - val_f1: 0.7537
Epoch 3/300
 - 15s - loss: 0.0729 - val_loss: 0.0693
 - val_f1: 0.7597
Epoch 4/300
 - 15s - loss: 0.0617 - val_loss: 0.0588
 - val_f1: 0.7718
Epoch 5/300
 - 15s - loss: 0.0559 - val_loss: 0.0520
 - val_f1: 0.7888
Epoch 6/300
 - 15s - loss: 0.0509 - val_loss: 0.0442
 - val_f1: 0.8063
Epoch 7/300
 - 15s - loss: 0.0467 - val_loss: 0.0384
 - val_f1: 0.8407
Epoch 8/300
 - 15s - loss: 0.0425 - val_loss: 0.0328
 - val_f1: 0.8934
Epoch 9/300
 - 15s - loss: 0.0378 - val_loss: 0.0271
 - val_f1: 0.9197
Epoch 10/300
 - 15s - loss: 0.0335 - val_loss: 0.0230
 - val_f1: 0.9324
Epoch 11/300
 - 15s - loss: 0.0303 - val_loss: 0.0209
 - val_f1: 0.9374
Epoch 12/300
 - 15s - loss: 0.0281 - val_loss: 0.0198
 - val_f1: 0.9413
Epoch 13/300
 - 15s - loss: 0.0263 - val_loss: 0.0186
 - val_f1: 0.9472
Epoch 14/300
 - 15s - loss: 0.0246 - val_loss: 0.0177
 - val_f1: 0.9531
Epoch 15/300
 - 15s - loss: 0.0234 - val_loss: 0.0177
 - val_f1: 0.9530
Epoch 16/300
 - 15s - loss: 0.0223 - val_loss: 0.0167
 - val_f1: 0.9562
Epoch 17/300
 - 15s - loss: 0.0212 - val_loss: 0.0160
 - val_f1: 0.9574
Epoch 18/300
 - 15s - loss: 0.0201 - val_loss: 0.0153
 - val_f1: 0.9572
Epoch 19/300
 - 15s - loss: 0.0186 - val_loss: 0.0138
 - val_f1: 0.9598
Epoch 20/300
 - 15s - loss: 0.0171 - val_loss: 0.0127
 - val_f1: 0.9676
Epoch 21/300
 - 15s - loss: 0.0160 - val_loss: 0.0121
 - val_f1: 0.9681
Epoch 22/300
 - 15s - loss: 0.0151 - val_loss: 0.0111
 - val_f1: 0.9722
Epoch 23/300
 - 15s - loss: 0.0142 - val_loss: 0.0107
 - val_f1: 0.9725
Epoch 24/300
 - 15s - loss: 0.0136 - val_loss: 0.0103
 - val_f1: 0.9737
Epoch 25/300
 - 15s - loss: 0.0130 - val_loss: 0.0101
 - val_f1: 0.9742
Epoch 26/300
 - 15s - loss: 0.0126 - val_loss: 0.0099
 - val_f1: 0.9746
Epoch 27/300
 - 15s - loss: 0.0122 - val_loss: 0.0098
 - val_f1: 0.9741
Epoch 28/300
 - 15s - loss: 0.0119 - val_loss: 0.0095
 - val_f1: 0.9751
Epoch 29/300
 - 15s - loss: 0.0115 - val_loss: 0.0094
 - val_f1: 0.9757
Epoch 30/300
 - 15s - loss: 0.0111 - val_loss: 0.0092
 - val_f1: 0.9757
Epoch 31/300
 - 15s - loss: 0.0108 - val_loss: 0.0091
/home/hasitha/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2019-12-27 08:00:37,343 [INFO] epoch = 30. Intermediate model saved to results_selected_models/selected_ids18_subset_lstm_deep_rep3/_model_epoch_30.pickle
 - val_f1: 0.9762
Epoch 32/300
 - 15s - loss: 0.0105 - val_loss: 0.0089
 - val_f1: 0.9767
Epoch 33/300
 - 15s - loss: 0.0103 - val_loss: 0.0088
 - val_f1: 0.9769
Epoch 34/300
 - 15s - loss: 0.0101 - val_loss: 0.0088
 - val_f1: 0.9768
Epoch 35/300
 - 15s - loss: 0.0099 - val_loss: 0.0086
 - val_f1: 0.9770
Epoch 36/300
 - 15s - loss: 0.0098 - val_loss: 0.0087
 - val_f1: 0.9768
Epoch 37/300
 - 15s - loss: 0.0097 - val_loss: 0.0085
 - val_f1: 0.9772
Epoch 38/300
 - 15s - loss: 0.0095 - val_loss: 0.0085
 - val_f1: 0.9772
Epoch 39/300
 - 15s - loss: 0.0095 - val_loss: 0.0084
 - val_f1: 0.9772
Epoch 40/300
 - 15s - loss: 0.0094 - val_loss: 0.0084
 - val_f1: 0.9773
Epoch 41/300
 - 15s - loss: 0.0093 - val_loss: 0.0084
 - val_f1: 0.9773
Epoch 42/300
 - 15s - loss: 0.0092 - val_loss: 0.0084
 - val_f1: 0.9771
Epoch 43/300
 - 15s - loss: 0.0092 - val_loss: 0.0083
 - val_f1: 0.9773
Epoch 44/300
 - 15s - loss: 0.0091 - val_loss: 0.0083
 - val_f1: 0.9774
Epoch 45/300
 - 15s - loss: 0.0090 - val_loss: 0.0083
 - val_f1: 0.9773
Epoch 46/300
 - 15s - loss: 0.0089 - val_loss: 0.0083
 - val_f1: 0.9775
Epoch 47/300
 - 15s - loss: 0.0089 - val_loss: 0.0083
 - val_f1: 0.9773
Epoch 48/300
 - 15s - loss: 0.0088 - val_loss: 0.0082
 - val_f1: 0.9776
Epoch 49/300
 - 15s - loss: 0.0087 - val_loss: 0.0082
 - val_f1: 0.9775
Epoch 50/300
 - 15s - loss: 0.0087 - val_loss: 0.0082
 - val_f1: 0.9775
Epoch 51/300
 - 15s - loss: 0.0087 - val_loss: 0.0082
 - val_f1: 0.9776
Epoch 52/300
 - 15s - loss: 0.0086 - val_loss: 0.0082
 - val_f1: 0.9774
Epoch 53/300
 - 15s - loss: 0.0086 - val_loss: 0.0083
 - val_f1: 0.9773
Epoch 54/300
 - 15s - loss: 0.0085 - val_loss: 0.0081
 - val_f1: 0.9776
Epoch 55/300
 - 15s - loss: 0.0085 - val_loss: 0.0082
 - val_f1: 0.9774
Epoch 56/300
 - 15s - loss: 0.0085 - val_loss: 0.0081
 - val_f1: 0.9775
Epoch 57/300
 - 15s - loss: 0.0085 - val_loss: 0.0081
 - val_f1: 0.9776
Epoch 58/300
 - 15s - loss: 0.0084 - val_loss: 0.0081
 - val_f1: 0.9776
Epoch 59/300
 - 15s - loss: 0.0084 - val_loss: 0.0081
 - val_f1: 0.9776
Epoch 60/300
 - 15s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9776
Epoch 61/300
 - 15s - loss: 0.0083 - val_loss: 0.0080
2019-12-27 08:09:53,350 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_ids18_subset_lstm_deep_rep3/_model_epoch_60.pickle
 - val_f1: 0.9777
Epoch 62/300
 - 15s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9777
Epoch 63/300
 - 15s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9777
Epoch 64/300
 - 15s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9776
Epoch 65/300
 - 15s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9774
Epoch 66/300
 - 15s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9774
Epoch 67/300
 - 15s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9777
Epoch 68/300
 - 15s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9776
Epoch 69/300
 - 15s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9774
Epoch 70/300
 - 15s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9776
Epoch 71/300
 - 15s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 72/300
 - 15s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9777
Epoch 73/300
 - 15s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9777
Epoch 74/300
 - 15s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9778
Epoch 75/300
 - 15s - loss: 0.0080 - val_loss: 0.0081
 - val_f1: 0.9771
Epoch 76/300
 - 15s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9778
Epoch 77/300
 - 15s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9778
Epoch 78/300
 - 15s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9778
Epoch 79/300
 - 15s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9761
Epoch 80/300
 - 15s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9778
Epoch 81/300
 - 15s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9778
Epoch 82/300
 - 15s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9778
Epoch 83/300
 - 15s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 84/300
 - 15s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9778
Epoch 85/300
 - 15s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9778
Epoch 86/300
 - 15s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 87/300
 - 15s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9779
Epoch 88/300
 - 15s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9778
Epoch 89/300
 - 15s - loss: 0.0080 - val_loss: 0.0084
 - val_f1: 0.9769
Epoch 90/300
 - 15s - loss: 0.0080 - val_loss: 0.0081
 - val_f1: 0.9773
Epoch 91/300
 - 15s - loss: 0.0079 - val_loss: 0.0080
2019-12-27 08:19:09,461 [INFO] epoch = 90. Intermediate model saved to results_selected_models/selected_ids18_subset_lstm_deep_rep3/_model_epoch_90.pickle
 - val_f1: 0.9780
Epoch 92/300
 - 15s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9775
Epoch 93/300
 - 15s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 94/300
 - 15s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 95/300
 - 15s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 96/300
 - 15s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 97/300
 - 15s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 98/300
 - 15s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 99/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 100/300
 - 15s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 101/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 102/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 103/300
 - 15s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 104/300
 - 15s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 105/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 106/300
 - 15s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 107/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 108/300
 - 15s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 109/300
 - 15s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 110/300
 - 15s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 111/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 112/300
 - 15s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 113/300
 - 15s - loss: 0.0078 - val_loss: 0.0080
 - val_f1: 0.9780
Epoch 114/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 115/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 116/300
 - 15s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 117/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 118/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 119/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 120/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9784
Epoch 121/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
2019-12-27 08:28:25,808 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_ids18_subset_lstm_deep_rep3/_model_epoch_120.pickle
 - val_f1: 0.9781
Epoch 122/300
 - 15s - loss: 0.0077 - val_loss: 0.0080
 - val_f1: 0.9777
Epoch 123/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 124/300
 - 15s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 125/300
 - 15s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 126/300
 - 15s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 127/300
 - 15s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 128/300
 - 15s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 129/300
 - 15s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 130/300
 - 15s - loss: 0.0077 - val_loss: 0.0081
 - val_f1: 0.9778
Epoch 131/300
 - 15s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 132/300
 - 15s - loss: 0.0077 - val_loss: 0.0080
 - val_f1: 0.9782
Epoch 133/300
 - 15s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 134/300
 - 15s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 135/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 136/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 137/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 138/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9784
Epoch 139/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 140/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 141/300
 - 15s - loss: 0.0077 - val_loss: 0.0081
 - val_f1: 0.9775
Epoch 142/300
 - 15s - loss: 0.0077 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 143/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
 - val_f1: 0.9781
Epoch 144/300
 - 15s - loss: 0.0076 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 145/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 146/300
 - 15s - loss: 0.0076 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 147/300
 - 15s - loss: 0.0076 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 148/300
 - 15s - loss: 0.0077 - val_loss: 0.0080
 - val_f1: 0.9782
Epoch 149/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
 - val_f1: 0.9780
Epoch 150/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 151/300
 - 15s - loss: 0.0076 - val_loss: 0.0079
2019-12-27 08:37:43,278 [INFO] epoch = 150. Intermediate model saved to results_selected_models/selected_ids18_subset_lstm_deep_rep3/_model_epoch_150.pickle
 - val_f1: 0.9784
Epoch 152/300
 - 15s - loss: 0.0076 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 153/300
 - 15s - loss: 0.0076 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 154/300
 - 15s - loss: 0.0076 - val_loss: 0.0079
 - val_f1: 0.9784
Epoch 155/300
 - 15s - loss: 0.0076 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 156/300
 - 15s - loss: 0.0076 - val_loss: 0.0079
 - val_f1: 0.9784
Epoch 157/300
 - 15s - loss: 0.0076 - val_loss: 0.0079
 - val_f1: 0.9784
Epoch 158/300
 - 15s - loss: 0.0076 - val_loss: 0.0079
 - val_f1: 0.9784
Epoch 159/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
 - val_f1: 0.9781
Epoch 160/300
 - 15s - loss: 0.0077 - val_loss: 0.0080
2019-12-27 08:40:34,312 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-27 08:40:48,389 [INFO] Last epoch loss evaluation: train_loss = 0.007622, val_loss = 0.007794
2019-12-27 08:40:48,390 [INFO] Training complete. time_to_train = 2996.38 sec, 49.94 min
2019-12-27 08:40:48,397 [INFO] Model saved to results_selected_models/selected_ids18_subset_lstm_deep_rep3/best_model.pickle
2019-12-27 08:40:48,438 [INFO] Training history saved to: results_selected_models/selected_ids18_subset_lstm_deep_rep3/training_error_history.csv
/home/hasitha/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py:514: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  max_open_warning, RuntimeWarning)
2019-12-27 08:40:48,665 [INFO] Plot saved to: results_selected_models/selected_ids18_subset_lstm_deep_rep3/training_error_history.png
2019-12-27 08:40:48,793 [INFO] Plot saved to: results_selected_models/selected_ids18_subset_lstm_deep_rep3/training_f1_history.png
2019-12-27 08:40:48,793 [INFO] Making predictions on training, validation, testing data
2019-12-27 08:41:04,516 [INFO] Evaluating predictions (results)
/home/hasitha/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2019-12-27 08:41:16,500 [INFO] Dataset: Testing. Classification report below
2019-12-27 08:41:16,500 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99    535639
                     Bot       1.00      1.00      1.00     11465
        Brute Force -Web       1.00      0.12      0.22        24
        Brute Force -XSS       1.00      0.33      0.50         9
        DDOS attack-HOIC       1.00      1.00      1.00     27446
    DDOS attack-LOIC-UDP       0.68      0.75      0.71        67
  DDoS attacks-LOIC-HTTP       0.99      0.99      0.99     23008
   DoS attacks-GoldenEye       0.99      1.00      0.99      1651
        DoS attacks-Hulk       1.00      1.00      1.00     18477
DoS attacks-SlowHTTPTest       0.72      0.53      0.61      5596
   DoS attacks-Slowloris       0.96      0.88      0.92       440
          FTP-BruteForce       0.71      0.85      0.78      7718
           Infilteration       0.53      0.00      0.00      6403
           SQL Injection       0.00      0.00      0.00         4
          SSH-Bruteforce       1.00      1.00      1.00      7525

               micro avg       0.98      0.98      0.98    645472
               macro avg       0.84      0.70      0.71    645472
            weighted avg       0.98      0.98      0.98    645472

2019-12-27 08:41:16,500 [INFO] Overall accuracy (micro avg): 0.9833517178127014
/home/hasitha/sunanda/ids_experiments/utility.py:328: RuntimeWarning: invalid value encountered in true_divide
  PPV = TP / (TP + FP)    # Precision or positive predictive value
/home/hasitha/sunanda/ids_experiments/utility.py:332: RuntimeWarning: invalid value encountered in true_divide
  FDR = FP / (TP + FP)    # False discovery rate
2019-12-27 08:41:30,152 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9834         0.9834                       0.9834                0.0012                   0.0166  0.9834
1     Macro avg        0.9978         0.8384                       0.6966                0.0045                   0.3034  0.7145
2  Weighted avg        0.9909         0.9789                       0.9834                0.0509                   0.0166  0.9782
2019-12-27 08:41:42,454 [INFO] Dataset: Validation. Classification report below
2019-12-27 08:41:42,454 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99    535636
                     Bot       1.00      1.00      1.00     11464
        Brute Force -Web       0.83      0.20      0.32        25
        Brute Force -XSS       1.00      0.67      0.80         9
        DDOS attack-HOIC       1.00      1.00      1.00     27447
    DDOS attack-LOIC-UDP       0.73      0.68      0.70        68
  DDoS attacks-LOIC-HTTP       0.99      0.99      0.99     23009
   DoS attacks-GoldenEye       1.00      1.00      1.00      1651
        DoS attacks-Hulk       1.00      1.00      1.00     18478
DoS attacks-SlowHTTPTest       0.73      0.53      0.61      5596
   DoS attacks-Slowloris       0.96      0.90      0.93       439
          FTP-BruteForce       0.72      0.86      0.78      7718
           Infilteration       0.43      0.00      0.00      6403
           SQL Injection       0.00      0.00      0.00         4
          SSH-Bruteforce       1.00      1.00      1.00      7525

               micro avg       0.98      0.98      0.98    645472
               macro avg       0.82      0.72      0.74    645472
            weighted avg       0.98      0.98      0.98    645472

2019-12-27 08:41:42,455 [INFO] Overall accuracy (micro avg): 0.9834834043924446
2019-12-27 08:41:56,420 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9835         0.9835                       0.9835                0.0012                   0.0165  0.9835
1     Macro avg        0.9978         0.8247                       0.7213                0.0045                   0.2787  0.7420
2  Weighted avg        0.9909         0.9780                       0.9835                0.0507                   0.0165  0.9784
2019-12-27 08:42:36,156 [INFO] Dataset: Training. Classification report below
2019-12-27 08:42:36,156 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99   1606935
                     Bot       1.00      1.00      1.00     34396
        Brute Force -Web       1.00      0.12      0.22        73
        Brute Force -XSS       1.00      0.46      0.63        26
        DDOS attack-HOIC       1.00      1.00      1.00     82341
    DDOS attack-LOIC-UDP       0.75      0.80      0.78       203
  DDoS attacks-LOIC-HTTP       0.99      0.99      0.99     69029
   DoS attacks-GoldenEye       0.99      1.00      1.00      4954
        DoS attacks-Hulk       1.00      1.00      1.00     55435
DoS attacks-SlowHTTPTest       0.73      0.53      0.61     16787
   DoS attacks-Slowloris       0.99      0.94      0.96      1318
          FTP-BruteForce       0.72      0.86      0.78     23153
           Infilteration       0.71      0.00      0.00     19210
           SQL Injection       1.00      0.08      0.15        12
          SSH-Bruteforce       1.00      1.00      1.00     22576

               micro avg       0.98      0.98      0.98   1936448
               macro avg       0.92      0.72      0.74   1936448
            weighted avg       0.98      0.98      0.98   1936448

2019-12-27 08:42:36,156 [INFO] Overall accuracy (micro avg): 0.9836530596225667
2019-12-27 08:43:21,285 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9837         0.9837                       0.9837                0.0012                   0.0163  0.9837
1     Macro avg        0.9978         0.9244                       0.7190                0.0044                   0.2810  0.7415
2  Weighted avg        0.9910         0.9809                       0.9837                0.0503                   0.0163  0.9785
2019-12-27 08:43:21,325 [INFO] Results saved to: results_selected_models/selected_ids18_subset_lstm_deep_rep3/selected_ids18_subset_lstm_deep_rep3_results.xlsx
2019-12-27 08:43:21,333 [INFO] ================= Finished running experiment no. 3 ================= 

2019-12-27 08:43:21,417 [INFO] Created directory: results_selected_models/selected_ids18_subset_lstm_deep_rep4
2019-12-27 08:43:21,417 [INFO] Initialized logging. log_filename = results_selected_models/selected_ids18_subset_lstm_deep_rep4/run_log.log
2019-12-27 08:43:21,417 [INFO] ================= Running experiment no. 4  ================= 

2019-12-27 08:43:21,417 [INFO] Experiment parameters given below
2019-12-27 08:43:21,417 [INFO] 
{'experiment_num': 4, 'results_dir': 'results_selected_models/selected_ids18_subset_lstm_deep_rep4', 'model': 'lstm', 'model_type': 'classifier', 'normal_label': 'BENIGN', 'lstm_time_steps': 32, 'lstm_layer_units': [64, 32], 'lstm_layer_activations': ['relu', 'relu'], 'lstm_layer_dropout_rates': [0.2, 0.2], 'batch_size': 256, 'epochs': 300, 'early_stop_patience': 50, 'dataset_dir': '../Datasets/small_datasets/ids2018', 'description': 'selected_ids18_subset_lstm_deep_rep4'}
2019-12-27 08:43:21,417 [INFO] Created tensorboard log directory: results_selected_models/selected_ids18_subset_lstm_deep_rep4/tf_logs_run_2019_12_27-08_43_21
2019-12-27 08:43:21,417 [INFO] Loading datsets from: ../Datasets/small_datasets/ids2018
2019-12-27 08:43:21,417 [INFO] Reading X, y files
2019-12-27 08:43:21,417 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_train.h5
2019-12-27 08:43:26,954 [INFO] Reading complete. time_to_read=5.54 seconds
2019-12-27 08:43:26,954 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_val.h5
2019-12-27 08:43:28,652 [INFO] Reading complete. time_to_read=1.70 seconds
2019-12-27 08:43:28,652 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_test.h5
2019-12-27 08:43:30,360 [INFO] Reading complete. time_to_read=1.71 seconds
2019-12-27 08:43:30,361 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_train.h5
2019-12-27 08:43:30,846 [INFO] Reading complete. time_to_read=0.49 seconds
2019-12-27 08:43:30,846 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_val.h5
2019-12-27 08:43:31,037 [INFO] Reading complete. time_to_read=0.19 seconds
2019-12-27 08:43:31,037 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_test.h5
2019-12-27 08:43:31,213 [INFO] Reading complete. time_to_read=0.18 seconds
2019-12-27 08:43:34,021 [INFO] Preparing flow sequences
2019-12-27 08:43:58,888 [INFO] Extracting flows complete. time_taken = 24.87 sec
2019-12-27 08:44:00,277 [INFO] Initializing model
2019-12-27 08:44:00,766 [INFO] _________________________________________________________________
2019-12-27 08:44:00,766 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-27 08:44:00,766 [INFO] =================================================================
2019-12-27 08:44:00,766 [INFO] lstm_27 (LSTM)               (None, 32, 64)            36352     
2019-12-27 08:44:00,766 [INFO] _________________________________________________________________
2019-12-27 08:44:00,766 [INFO] batch_normalization_27 (Batc (None, 32, 64)            256       
2019-12-27 08:44:00,766 [INFO] _________________________________________________________________
2019-12-27 08:44:00,767 [INFO] dropout_27 (Dropout)         (None, 32, 64)            0         
2019-12-27 08:44:00,767 [INFO] _________________________________________________________________
2019-12-27 08:44:00,767 [INFO] lstm_28 (LSTM)               (None, 32, 32)            12416     
2019-12-27 08:44:00,767 [INFO] _________________________________________________________________
2019-12-27 08:44:00,767 [INFO] batch_normalization_28 (Batc (None, 32, 32)            128       
2019-12-27 08:44:00,767 [INFO] _________________________________________________________________
2019-12-27 08:44:00,767 [INFO] dropout_28 (Dropout)         (None, 32, 32)            0         
2019-12-27 08:44:00,767 [INFO] _________________________________________________________________
2019-12-27 08:44:00,767 [INFO] time_distributed_14 (TimeDis (None, 32, 15)            495       
2019-12-27 08:44:00,767 [INFO] =================================================================
2019-12-27 08:44:00,767 [INFO] Total params: 49,647
2019-12-27 08:44:00,767 [INFO] Trainable params: 49,455
2019-12-27 08:44:00,767 [INFO] Non-trainable params: 192
2019-12-27 08:44:00,767 [INFO] _________________________________________________________________
2019-12-27 08:44:00,767 [INFO] Training model
 - val_f1: 0.9781
Epoch 00160: early stopping
Train on 60514 samples, validate on 20171 samples
Epoch 1/300
 - 17s - loss: 0.1691 - val_loss: 0.0877
 - val_f1: 0.7517
Epoch 2/300
 - 15s - loss: 0.0889 - val_loss: 0.0856
 - val_f1: 0.7525
Epoch 3/300
 - 15s - loss: 0.0824 - val_loss: 0.0832
 - val_f1: 0.7524
Epoch 4/300
 - 15s - loss: 0.0751 - val_loss: 0.0758
 - val_f1: 0.7523
Epoch 5/300
 - 15s - loss: 0.0696 - val_loss: 0.0643
 - val_f1: 0.7636
Epoch 6/300
 - 15s - loss: 0.0639 - val_loss: 0.0570
 - val_f1: 0.7821
Epoch 7/300
 - 15s - loss: 0.0600 - val_loss: 0.0527
 - val_f1: 0.7918
Epoch 8/300
 - 15s - loss: 0.0567 - val_loss: 0.0500
 - val_f1: 0.7963
Epoch 9/300
 - 15s - loss: 0.0599 - val_loss: 0.0669
 - val_f1: 0.7688
Epoch 10/300
 - 15s - loss: 0.0540 - val_loss: 0.0653
 - val_f1: 0.7712
Epoch 11/300
 - 15s - loss: 0.0503 - val_loss: 0.0582
 - val_f1: 0.7807
Epoch 12/300
 - 15s - loss: 0.0473 - val_loss: 0.0439
 - val_f1: 0.7980
Epoch 13/300
 - 15s - loss: 0.0443 - val_loss: 0.0374
 - val_f1: 0.8397
Epoch 14/300
 - 15s - loss: 0.0423 - val_loss: 0.0351
 - val_f1: 0.8576
Epoch 15/300
 - 15s - loss: 0.0390 - val_loss: 0.0309
 - val_f1: 0.8907
Epoch 16/300
 - 15s - loss: 0.0364 - val_loss: 0.0280
 - val_f1: 0.9166
Epoch 17/300
 - 15s - loss: 0.0340 - val_loss: 0.0258
 - val_f1: 0.9254
Epoch 18/300
 - 15s - loss: 0.0320 - val_loss: 0.0242
 - val_f1: 0.9292
Epoch 19/300
 - 15s - loss: 0.0304 - val_loss: 0.0230
 - val_f1: 0.9311
Epoch 20/300
 - 15s - loss: 0.0291 - val_loss: 0.0218
 - val_f1: 0.9322
Epoch 21/300
 - 15s - loss: 0.0277 - val_loss: 0.0212
 - val_f1: 0.9354
Epoch 22/300
 - 15s - loss: 0.0262 - val_loss: 0.0199
 - val_f1: 0.9380
Epoch 23/300
 - 15s - loss: 0.0249 - val_loss: 0.0185
 - val_f1: 0.9473
Epoch 24/300
 - 15s - loss: 0.0236 - val_loss: 0.0178
 - val_f1: 0.9503
Epoch 25/300
 - 15s - loss: 0.0224 - val_loss: 0.0177
 - val_f1: 0.9526
Epoch 26/300
 - 15s - loss: 0.0212 - val_loss: 0.0165
 - val_f1: 0.9559
Epoch 27/300
 - 15s - loss: 0.0199 - val_loss: 0.0165
 - val_f1: 0.9555
Epoch 28/300
 - 15s - loss: 0.0183 - val_loss: 0.0161
 - val_f1: 0.9538
Epoch 29/300
 - 15s - loss: 0.0163 - val_loss: 0.0149
 - val_f1: 0.9592
Epoch 30/300
 - 15s - loss: 0.0152 - val_loss: 0.0137
 - val_f1: 0.9610
Epoch 31/300
 - 15s - loss: 0.0145 - val_loss: 0.0126
/home/hasitha/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2019-12-27 08:53:50,084 [INFO] epoch = 30. Intermediate model saved to results_selected_models/selected_ids18_subset_lstm_deep_rep4/_model_epoch_30.pickle
 - val_f1: 0.9630
Epoch 32/300
 - 15s - loss: 0.0138 - val_loss: 0.0130
 - val_f1: 0.9621
Epoch 33/300
 - 15s - loss: 0.0133 - val_loss: 0.0124
 - val_f1: 0.9634
Epoch 34/300
 - 15s - loss: 0.0128 - val_loss: 0.0131
 - val_f1: 0.9614
Epoch 35/300
 - 15s - loss: 0.0125 - val_loss: 0.0123
 - val_f1: 0.9645
Epoch 36/300
 - 15s - loss: 0.0121 - val_loss: 0.0125
 - val_f1: 0.9629
Epoch 37/300
 - 15s - loss: 0.0118 - val_loss: 0.0118
 - val_f1: 0.9672
Epoch 38/300
 - 15s - loss: 0.0114 - val_loss: 0.0114
 - val_f1: 0.9688
Epoch 39/300
 - 15s - loss: 0.0111 - val_loss: 0.0106
 - val_f1: 0.9730
Epoch 40/300
 - 15s - loss: 0.0109 - val_loss: 0.0104
 - val_f1: 0.9729
Epoch 41/300
 - 15s - loss: 0.0106 - val_loss: 0.0099
 - val_f1: 0.9749
Epoch 42/300
 - 15s - loss: 0.0104 - val_loss: 0.0107
 - val_f1: 0.9700
Epoch 43/300
 - 15s - loss: 0.0107 - val_loss: 0.0105
 - val_f1: 0.9734
Epoch 44/300
 - 15s - loss: 0.0104 - val_loss: 0.0100
 - val_f1: 0.9758
Epoch 45/300
 - 15s - loss: 0.0102 - val_loss: 0.0098
 - val_f1: 0.9761
Epoch 46/300
 - 15s - loss: 0.0100 - val_loss: 0.0097
 - val_f1: 0.9751
Epoch 47/300
 - 15s - loss: 0.0099 - val_loss: 0.0095
 - val_f1: 0.9764
Epoch 48/300
 - 15s - loss: 0.0097 - val_loss: 0.0104
 - val_f1: 0.9717
Epoch 49/300
 - 15s - loss: 0.0096 - val_loss: 0.0104
 - val_f1: 0.9703
Epoch 50/300
 - 15s - loss: 0.0095 - val_loss: 0.0092
 - val_f1: 0.9767
Epoch 51/300
 - 15s - loss: 0.0094 - val_loss: 0.0099
 - val_f1: 0.9755
Epoch 52/300
 - 15s - loss: 0.0093 - val_loss: 0.0091
 - val_f1: 0.9765
Epoch 53/300
 - 15s - loss: 0.0092 - val_loss: 0.0091
 - val_f1: 0.9770
Epoch 54/300
 - 15s - loss: 0.0092 - val_loss: 0.0090
 - val_f1: 0.9769
Epoch 55/300
 - 15s - loss: 0.0091 - val_loss: 0.0088
 - val_f1: 0.9769
Epoch 56/300
 - 15s - loss: 0.0091 - val_loss: 0.0091
 - val_f1: 0.9752
Epoch 57/300
 - 15s - loss: 0.0090 - val_loss: 0.0089
 - val_f1: 0.9768
Epoch 58/300
 - 15s - loss: 0.0089 - val_loss: 0.0088
 - val_f1: 0.9766
Epoch 59/300
 - 15s - loss: 0.0089 - val_loss: 0.0089
 - val_f1: 0.9747
Epoch 60/300
 - 15s - loss: 0.0089 - val_loss: 0.0087
 - val_f1: 0.9765
Epoch 61/300
 - 15s - loss: 0.0088 - val_loss: 0.0086
2019-12-27 09:03:08,576 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_ids18_subset_lstm_deep_rep4/_model_epoch_60.pickle
 - val_f1: 0.9772
Epoch 62/300
 - 15s - loss: 0.0093 - val_loss: 0.0104
 - val_f1: 0.9709
Epoch 63/300
 - 15s - loss: 0.0099 - val_loss: 0.0092
 - val_f1: 0.9758
Epoch 64/300
 - 15s - loss: 0.0094 - val_loss: 0.0091
 - val_f1: 0.9760
Epoch 65/300
 - 15s - loss: 0.0092 - val_loss: 0.0089
 - val_f1: 0.9761
Epoch 66/300
 - 15s - loss: 0.0090 - val_loss: 0.0089
 - val_f1: 0.9762
Epoch 67/300
 - 15s - loss: 0.0089 - val_loss: 0.0090
 - val_f1: 0.9758
Epoch 68/300
 - 15s - loss: 0.0088 - val_loss: 0.0090
 - val_f1: 0.9757
Epoch 69/300
 - 15s - loss: 0.0088 - val_loss: 0.0086
 - val_f1: 0.9767
Epoch 70/300
 - 15s - loss: 0.0087 - val_loss: 0.0087
 - val_f1: 0.9767
Epoch 71/300
 - 15s - loss: 0.0087 - val_loss: 0.0087
 - val_f1: 0.9767
Epoch 72/300
 - 15s - loss: 0.0086 - val_loss: 0.0090
 - val_f1: 0.9734
Epoch 73/300
 - 15s - loss: 0.0085 - val_loss: 0.0086
 - val_f1: 0.9763
Epoch 74/300
 - 15s - loss: 0.0084 - val_loss: 0.0085
 - val_f1: 0.9774
Epoch 75/300
 - 15s - loss: 0.0084 - val_loss: 0.0086
 - val_f1: 0.9763
Epoch 76/300
 - 15s - loss: 0.0083 - val_loss: 0.0093
 - val_f1: 0.9722
Epoch 77/300
 - 15s - loss: 0.0083 - val_loss: 0.0084
 - val_f1: 0.9774
Epoch 78/300
 - 15s - loss: 0.0083 - val_loss: 0.0084
 - val_f1: 0.9774
Epoch 79/300
 - 15s - loss: 0.0083 - val_loss: 0.0084
 - val_f1: 0.9770
Epoch 80/300
 - 15s - loss: 0.0082 - val_loss: 0.0084
 - val_f1: 0.9770
Epoch 81/300
 - 15s - loss: 0.0082 - val_loss: 0.0091
 - val_f1: 0.9740
Epoch 82/300
 - 15s - loss: 0.0082 - val_loss: 0.0082
 - val_f1: 0.9778
Epoch 83/300
 - 15s - loss: 0.0082 - val_loss: 0.0083
 - val_f1: 0.9770
Epoch 84/300
 - 15s - loss: 0.0082 - val_loss: 0.0082
 - val_f1: 0.9774
Epoch 85/300
 - 15s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 86/300
 - 15s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9778
Epoch 87/300
 - 15s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 88/300
 - 15s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 89/300
 - 15s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 90/300
 - 15s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 91/300
 - 15s - loss: 0.0081 - val_loss: 0.0081
2019-12-27 09:12:27,720 [INFO] epoch = 90. Intermediate model saved to results_selected_models/selected_ids18_subset_lstm_deep_rep4/_model_epoch_90.pickle
 - val_f1: 0.9772
Epoch 92/300
 - 15s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 93/300
 - 15s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 94/300
 - 15s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 95/300
 - 15s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 96/300
 - 15s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9780
Epoch 97/300
 - 15s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9783
Epoch 98/300
 - 15s - loss: 0.0080 - val_loss: 0.0083
 - val_f1: 0.9768
Epoch 99/300
 - 15s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 100/300
 - 15s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9778
Epoch 101/300
 - 15s - loss: 0.0080 - val_loss: 0.0083
 - val_f1: 0.9770
Epoch 102/300
 - 15s - loss: 0.0079 - val_loss: 0.0081
 - val_f1: 0.9775
Epoch 103/300
 - 15s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9778
Epoch 104/300
 - 15s - loss: 0.0079 - val_loss: 0.0081
 - val_f1: 0.9775
Epoch 105/300
 - 15s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 106/300
 - 15s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9783
Epoch 107/300
 - 15s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9781
Epoch 108/300
 - 15s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 109/300
 - 15s - loss: 0.0079 - val_loss: 0.0084
 - val_f1: 0.9766
Epoch 110/300
 - 15s - loss: 0.0079 - val_loss: 0.0087
 - val_f1: 0.9768
Epoch 111/300
 - 15s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 112/300
 - 15s - loss: 0.0079 - val_loss: 0.0083
 - val_f1: 0.9774
Epoch 113/300
 - 15s - loss: 0.0079 - val_loss: 0.0081
 - val_f1: 0.9773
Epoch 114/300
 - 15s - loss: 0.0079 - val_loss: 0.0082
 - val_f1: 0.9773
Epoch 115/300
 - 15s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 116/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 117/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 118/300
 - 15s - loss: 0.0078 - val_loss: 0.0080
 - val_f1: 0.9781
Epoch 119/300
 - 15s - loss: 0.0078 - val_loss: 0.0082
 - val_f1: 0.9775
Epoch 120/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 121/300
 - 15s - loss: 0.0078 - val_loss: 0.0081
2019-12-27 09:21:46,898 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_ids18_subset_lstm_deep_rep4/_model_epoch_120.pickle
 - val_f1: 0.9775
Epoch 122/300
 - 15s - loss: 0.0078 - val_loss: 0.0080
 - val_f1: 0.9776
Epoch 123/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 124/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 125/300
 - 15s - loss: 0.0078 - val_loss: 0.0080
 - val_f1: 0.9775
Epoch 126/300
 - 15s - loss: 0.0078 - val_loss: 0.0083
 - val_f1: 0.9770
Epoch 127/300
 - 15s - loss: 0.0078 - val_loss: 0.0080
 - val_f1: 0.9784
Epoch 128/300
 - 15s - loss: 0.0078 - val_loss: 0.0080
 - val_f1: 0.9781
Epoch 129/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9777
Epoch 130/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 131/300
 - 15s - loss: 0.0078 - val_loss: 0.0080
 - val_f1: 0.9782
Epoch 132/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 133/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 134/300
 - 15s - loss: 0.0078 - val_loss: 0.0084
 - val_f1: 0.9771
Epoch 135/300
 - 15s - loss: 0.0078 - val_loss: 0.0083
 - val_f1: 0.9772
Epoch 136/300
 - 15s - loss: 0.0078 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 137/300
 - 15s - loss: 0.0078 - val_loss: 0.0082
 - val_f1: 0.9775
Epoch 138/300
 - 15s - loss: 0.0077 - val_loss: 0.0082
 - val_f1: 0.9778
Epoch 139/300
 - 15s - loss: 0.0077 - val_loss: 0.0086
 - val_f1: 0.9769
Epoch 140/300
 - 15s - loss: 0.0077 - val_loss: 0.0081
 - val_f1: 0.9779
Epoch 141/300
 - 15s - loss: 0.0077 - val_loss: 0.0082
 - val_f1: 0.9779
Epoch 142/300
 - 15s - loss: 0.0077 - val_loss: 0.0081
 - val_f1: 0.9779
Epoch 143/300
 - 15s - loss: 0.0077 - val_loss: 0.0081
 - val_f1: 0.9779
Epoch 144/300
 - 15s - loss: 0.0077 - val_loss: 0.0081
 - val_f1: 0.9770
Epoch 145/300
 - 15s - loss: 0.0077 - val_loss: 0.0084
 - val_f1: 0.9769
Epoch 146/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 147/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 148/300
 - 15s - loss: 0.0077 - val_loss: 0.0081
 - val_f1: 0.9779
Epoch 149/300
 - 15s - loss: 0.0077 - val_loss: 0.0081
 - val_f1: 0.9779
Epoch 150/300
 - 15s - loss: 0.0077 - val_loss: 0.0081
 - val_f1: 0.9781
Epoch 151/300
 - 15s - loss: 0.0077 - val_loss: 0.0080
2019-12-27 09:31:06,062 [INFO] epoch = 150. Intermediate model saved to results_selected_models/selected_ids18_subset_lstm_deep_rep4/_model_epoch_150.pickle
 - val_f1: 0.9782
Epoch 152/300
 - 15s - loss: 0.0077 - val_loss: 0.0080
 - val_f1: 0.9781
Epoch 153/300
 - 15s - loss: 0.0077 - val_loss: 0.0080
 - val_f1: 0.9783
Epoch 154/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 155/300
 - 15s - loss: 0.0077 - val_loss: 0.0082
 - val_f1: 0.9772
Epoch 156/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
 - val_f1: 0.9782
Epoch 157/300
 - 15s - loss: 0.0077 - val_loss: 0.0080
 - val_f1: 0.9777
Epoch 158/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 159/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9784
Epoch 160/300
 - 15s - loss: 0.0077 - val_loss: 0.0080
 - val_f1: 0.9782
Epoch 161/300
 - 15s - loss: 0.0077 - val_loss: 0.0080
 - val_f1: 0.9782
Epoch 162/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
 - val_f1: 0.9780
Epoch 163/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
 - val_f1: 0.9782
Epoch 164/300
 - 15s - loss: 0.0076 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 165/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 166/300
 - 15s - loss: 0.0076 - val_loss: 0.0079
 - val_f1: 0.9784
Epoch 167/300
 - 15s - loss: 0.0076 - val_loss: 0.0081
 - val_f1: 0.9780
Epoch 168/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
 - val_f1: 0.9782
Epoch 169/300
 - 15s - loss: 0.0076 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 170/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
2019-12-27 09:37:03,914 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-27 09:37:18,092 [INFO] Last epoch loss evaluation: train_loss = 0.007703, val_loss = 0.007858
2019-12-27 09:37:18,092 [INFO] Training complete. time_to_train = 3197.32 sec, 53.29 min
2019-12-27 09:37:18,098 [INFO] Model saved to results_selected_models/selected_ids18_subset_lstm_deep_rep4/best_model.pickle
2019-12-27 09:37:18,132 [INFO] Training history saved to: results_selected_models/selected_ids18_subset_lstm_deep_rep4/training_error_history.csv
/home/hasitha/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py:514: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  max_open_warning, RuntimeWarning)
2019-12-27 09:37:18,339 [INFO] Plot saved to: results_selected_models/selected_ids18_subset_lstm_deep_rep4/training_error_history.png
2019-12-27 09:37:18,468 [INFO] Plot saved to: results_selected_models/selected_ids18_subset_lstm_deep_rep4/training_f1_history.png
2019-12-27 09:37:18,468 [INFO] Making predictions on training, validation, testing data
2019-12-27 09:37:33,873 [INFO] Evaluating predictions (results)
/home/hasitha/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2019-12-27 09:37:45,978 [INFO] Dataset: Testing. Classification report below
2019-12-27 09:37:45,978 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99    535639
                     Bot       1.00      1.00      1.00     11465
        Brute Force -Web       0.25      0.04      0.07        24
        Brute Force -XSS       0.00      0.00      0.00         9
        DDOS attack-HOIC       1.00      1.00      1.00     27446
    DDOS attack-LOIC-UDP       0.68      0.76      0.72        67
  DDoS attacks-LOIC-HTTP       1.00      0.98      0.99     23008
   DoS attacks-GoldenEye       0.99      1.00      0.99      1651
        DoS attacks-Hulk       1.00      1.00      1.00     18477
DoS attacks-SlowHTTPTest       0.74      0.52      0.61      5596
   DoS attacks-Slowloris       0.95      0.91      0.93       440
          FTP-BruteForce       0.71      0.87      0.78      7718
           Infilteration       0.50      0.00      0.01      6403
           SQL Injection       0.00      0.00      0.00         4
          SSH-Bruteforce       1.00      1.00      1.00      7525

               micro avg       0.98      0.98      0.98    645472
               macro avg       0.72      0.67      0.67    645472
            weighted avg       0.98      0.98      0.98    645472

2019-12-27 09:37:45,978 [INFO] Overall accuracy (micro avg): 0.9833873506519261
/home/hasitha/sunanda/ids_experiments/utility.py:328: RuntimeWarning: invalid value encountered in true_divide
  PPV = TP / (TP + FP)    # Precision or positive predictive value
/home/hasitha/sunanda/ids_experiments/utility.py:332: RuntimeWarning: invalid value encountered in true_divide
  FDR = FP / (TP + FP)    # False discovery rate
2019-12-27 09:37:59,747 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9834         0.9834                       0.9834                0.0012                   0.0166  0.9834
1     Macro avg        0.9978         0.7205                       0.6724                0.0045                   0.3276  0.6732
2  Weighted avg        0.9908         0.9786                       0.9834                0.0515                   0.0166  0.9783
2019-12-27 09:38:12,043 [INFO] Dataset: Validation. Classification report below
2019-12-27 09:38:12,043 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99    535636
                     Bot       1.00      1.00      1.00     11464
        Brute Force -Web       0.25      0.08      0.12        25
        Brute Force -XSS       0.00      0.00      0.00         9
        DDOS attack-HOIC       1.00      1.00      1.00     27447
    DDOS attack-LOIC-UDP       0.74      0.76      0.75        68
  DDoS attacks-LOIC-HTTP       1.00      0.99      0.99     23009
   DoS attacks-GoldenEye       0.99      1.00      1.00      1651
        DoS attacks-Hulk       1.00      1.00      1.00     18478
DoS attacks-SlowHTTPTest       0.75      0.51      0.61      5596
   DoS attacks-Slowloris       0.94      0.91      0.92       439
          FTP-BruteForce       0.71      0.88      0.79      7718
           Infilteration       0.47      0.00      0.01      6403
           SQL Injection       0.00      0.00      0.00         4
          SSH-Bruteforce       1.00      1.00      1.00      7525

               micro avg       0.98      0.98      0.98    645472
               macro avg       0.72      0.68      0.68    645472
            weighted avg       0.98      0.98      0.98    645472

2019-12-27 09:38:12,043 [INFO] Overall accuracy (micro avg): 0.9834694611075306
2019-12-27 09:38:26,009 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9835         0.9835                       0.9835                0.0012                   0.0165  0.9835
1     Macro avg        0.9978         0.7227                       0.6751                0.0045                   0.3249  0.6787
2  Weighted avg        0.9908         0.9784                       0.9835                0.0514                   0.0165  0.9784
2019-12-27 09:39:05,700 [INFO] Dataset: Training. Classification report below
2019-12-27 09:39:05,700 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99   1606935
                     Bot       1.00      1.00      1.00     34396
        Brute Force -Web       0.09      0.01      0.02        73
        Brute Force -XSS       1.00      0.12      0.21        26
        DDOS attack-HOIC       1.00      1.00      1.00     82341
    DDOS attack-LOIC-UDP       0.75      0.89      0.81       203
  DDoS attacks-LOIC-HTTP       1.00      0.99      0.99     69029
   DoS attacks-GoldenEye       0.99      1.00      1.00      4954
        DoS attacks-Hulk       1.00      1.00      1.00     55435
DoS attacks-SlowHTTPTest       0.75      0.52      0.61     16787
   DoS attacks-Slowloris       0.97      0.94      0.95      1318
          FTP-BruteForce       0.71      0.87      0.79     23153
           Infilteration       0.72      0.01      0.01     19210
           SQL Injection       0.50      0.08      0.14        12
          SSH-Bruteforce       1.00      1.00      1.00     22576

               micro avg       0.98      0.98      0.98   1936448
               macro avg       0.83      0.69      0.70   1936448
            weighted avg       0.98      0.98      0.98   1936448

2019-12-27 09:39:05,700 [INFO] Overall accuracy (micro avg): 0.9836313704266781
2019-12-27 09:39:50,771 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9836         0.9836                       0.9836                0.0012                   0.0164  0.9836
1     Macro avg        0.9978         0.8316                       0.6947                0.0045                   0.3053  0.7022
2  Weighted avg        0.9909         0.9811                       0.9836                0.0510                   0.0164  0.9785
2019-12-27 09:39:50,798 [INFO] Results saved to: results_selected_models/selected_ids18_subset_lstm_deep_rep4/selected_ids18_subset_lstm_deep_rep4_results.xlsx
2019-12-27 09:39:50,803 [INFO] ================= Finished running experiment no. 4 ================= 

2019-12-27 09:39:50,891 [INFO] Created directory: results_selected_models/selected_ids18_subset_lstm_deep_rep5
2019-12-27 09:39:50,891 [INFO] Initialized logging. log_filename = results_selected_models/selected_ids18_subset_lstm_deep_rep5/run_log.log
2019-12-27 09:39:50,891 [INFO] ================= Running experiment no. 5  ================= 

2019-12-27 09:39:50,891 [INFO] Experiment parameters given below
2019-12-27 09:39:50,891 [INFO] 
{'experiment_num': 5, 'results_dir': 'results_selected_models/selected_ids18_subset_lstm_deep_rep5', 'model': 'lstm', 'model_type': 'classifier', 'normal_label': 'BENIGN', 'lstm_time_steps': 32, 'lstm_layer_units': [64, 32], 'lstm_layer_activations': ['relu', 'relu'], 'lstm_layer_dropout_rates': [0.2, 0.2], 'batch_size': 256, 'epochs': 300, 'early_stop_patience': 50, 'dataset_dir': '../Datasets/small_datasets/ids2018', 'description': 'selected_ids18_subset_lstm_deep_rep5'}
2019-12-27 09:39:50,891 [INFO] Created tensorboard log directory: results_selected_models/selected_ids18_subset_lstm_deep_rep5/tf_logs_run_2019_12_27-09_39_50
2019-12-27 09:39:50,891 [INFO] Loading datsets from: ../Datasets/small_datasets/ids2018
2019-12-27 09:39:50,892 [INFO] Reading X, y files
2019-12-27 09:39:50,892 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_train.h5
2019-12-27 09:39:56,274 [INFO] Reading complete. time_to_read=5.38 seconds
2019-12-27 09:39:56,275 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_val.h5
2019-12-27 09:39:57,843 [INFO] Reading complete. time_to_read=1.57 seconds
2019-12-27 09:39:57,843 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_test.h5
2019-12-27 09:39:59,503 [INFO] Reading complete. time_to_read=1.66 seconds
2019-12-27 09:39:59,503 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_train.h5
2019-12-27 09:39:59,950 [INFO] Reading complete. time_to_read=0.45 seconds
2019-12-27 09:39:59,950 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_val.h5
2019-12-27 09:40:00,126 [INFO] Reading complete. time_to_read=0.18 seconds
2019-12-27 09:40:00,126 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_test.h5
2019-12-27 09:40:00,311 [INFO] Reading complete. time_to_read=0.19 seconds
2019-12-27 09:40:03,077 [INFO] Preparing flow sequences
2019-12-27 09:40:27,940 [INFO] Extracting flows complete. time_taken = 24.86 sec
2019-12-27 09:40:29,212 [INFO] Initializing model
2019-12-27 09:40:29,665 [INFO] _________________________________________________________________
2019-12-27 09:40:29,665 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-27 09:40:29,665 [INFO] =================================================================
2019-12-27 09:40:29,665 [INFO] lstm_29 (LSTM)               (None, 32, 64)            36352     
2019-12-27 09:40:29,665 [INFO] _________________________________________________________________
2019-12-27 09:40:29,665 [INFO] batch_normalization_29 (Batc (None, 32, 64)            256       
2019-12-27 09:40:29,665 [INFO] _________________________________________________________________
2019-12-27 09:40:29,665 [INFO] dropout_29 (Dropout)         (None, 32, 64)            0         
2019-12-27 09:40:29,665 [INFO] _________________________________________________________________
2019-12-27 09:40:29,665 [INFO] lstm_30 (LSTM)               (None, 32, 32)            12416     
2019-12-27 09:40:29,665 [INFO] _________________________________________________________________
2019-12-27 09:40:29,665 [INFO] batch_normalization_30 (Batc (None, 32, 32)            128       
2019-12-27 09:40:29,665 [INFO] _________________________________________________________________
2019-12-27 09:40:29,665 [INFO] dropout_30 (Dropout)         (None, 32, 32)            0         
2019-12-27 09:40:29,665 [INFO] _________________________________________________________________
2019-12-27 09:40:29,665 [INFO] time_distributed_15 (TimeDis (None, 32, 15)            495       
2019-12-27 09:40:29,666 [INFO] =================================================================
2019-12-27 09:40:29,666 [INFO] Total params: 49,647
2019-12-27 09:40:29,666 [INFO] Trainable params: 49,455
2019-12-27 09:40:29,666 [INFO] Non-trainable params: 192
2019-12-27 09:40:29,666 [INFO] _________________________________________________________________
2019-12-27 09:40:29,666 [INFO] Training model
 - val_f1: 0.9783
Epoch 00170: early stopping
Train on 60514 samples, validate on 20171 samples
Epoch 1/300
 - 17s - loss: 0.1651 - val_loss: 0.0784
 - val_f1: 0.7720
Epoch 2/300
 - 15s - loss: 0.0704 - val_loss: 0.0607
 - val_f1: 0.7836
Epoch 3/300
 - 15s - loss: 0.0526 - val_loss: 0.0461
 - val_f1: 0.7955
Epoch 4/300
 - 15s - loss: 0.0385 - val_loss: 0.0316
 - val_f1: 0.8977
Epoch 5/300
 - 15s - loss: 0.0309 - val_loss: 0.0374
 - val_f1: 0.8375
Epoch 6/300
 - 15s - loss: 0.0255 - val_loss: 0.0246
 - val_f1: 0.9272
Epoch 7/300
 - 15s - loss: 0.0216 - val_loss: 0.0171
 - val_f1: 0.9475
Epoch 8/300
 - 15s - loss: 0.0189 - val_loss: 0.0141
 - val_f1: 0.9538
Epoch 9/300
 - 15s - loss: 0.0165 - val_loss: 0.0123
 - val_f1: 0.9625
Epoch 10/300
 - 15s - loss: 0.0149 - val_loss: 0.0115
 - val_f1: 0.9703
Epoch 11/300
 - 15s - loss: 0.0141 - val_loss: 0.0114
 - val_f1: 0.9702
Epoch 12/300
 - 15s - loss: 0.0135 - val_loss: 0.0107
 - val_f1: 0.9728
Epoch 13/300
 - 15s - loss: 0.0130 - val_loss: 0.0107
 - val_f1: 0.9726
Epoch 14/300
 - 15s - loss: 0.0126 - val_loss: 0.0107
 - val_f1: 0.9729
Epoch 15/300
 - 15s - loss: 0.0122 - val_loss: 0.0104
 - val_f1: 0.9741
Epoch 16/300
 - 15s - loss: 0.0119 - val_loss: 0.0098
 - val_f1: 0.9748
Epoch 17/300
 - 15s - loss: 0.0116 - val_loss: 0.0100
 - val_f1: 0.9740
Epoch 18/300
 - 15s - loss: 0.0112 - val_loss: 0.0095
 - val_f1: 0.9751
Epoch 19/300
 - 15s - loss: 0.0108 - val_loss: 0.0093
 - val_f1: 0.9756
Epoch 20/300
 - 15s - loss: 0.0105 - val_loss: 0.0092
 - val_f1: 0.9759
Epoch 21/300
 - 15s - loss: 0.0103 - val_loss: 0.0089
 - val_f1: 0.9765
Epoch 22/300
 - 15s - loss: 0.0100 - val_loss: 0.0087
 - val_f1: 0.9769
Epoch 23/300
 - 15s - loss: 0.0101 - val_loss: 0.0088
 - val_f1: 0.9768
Epoch 24/300
 - 15s - loss: 0.0098 - val_loss: 0.0088
 - val_f1: 0.9768
Epoch 25/300
 - 15s - loss: 0.0097 - val_loss: 0.0087
 - val_f1: 0.9770
Epoch 26/300
 - 15s - loss: 0.0095 - val_loss: 0.0086
 - val_f1: 0.9771
Epoch 27/300
 - 15s - loss: 0.0094 - val_loss: 0.0085
 - val_f1: 0.9771
Epoch 28/300
 - 15s - loss: 0.0093 - val_loss: 0.0085
 - val_f1: 0.9772
Epoch 29/300
 - 15s - loss: 0.0092 - val_loss: 0.0085
 - val_f1: 0.9771
Epoch 30/300
 - 15s - loss: 0.0091 - val_loss: 0.0086
 - val_f1: 0.9769
Epoch 31/300
 - 15s - loss: 0.0091 - val_loss: 0.0085
/home/hasitha/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2019-12-27 09:50:21,697 [INFO] epoch = 30. Intermediate model saved to results_selected_models/selected_ids18_subset_lstm_deep_rep5/_model_epoch_30.pickle
 - val_f1: 0.9771
Epoch 32/300
 - 15s - loss: 0.0090 - val_loss: 0.0085
 - val_f1: 0.9768
Epoch 33/300
 - 15s - loss: 0.0089 - val_loss: 0.0084
 - val_f1: 0.9773
Epoch 34/300
 - 15s - loss: 0.0089 - val_loss: 0.0084
 - val_f1: 0.9770
Epoch 35/300
 - 15s - loss: 0.0088 - val_loss: 0.0083
 - val_f1: 0.9773
Epoch 36/300
 - 15s - loss: 0.0088 - val_loss: 0.0083
 - val_f1: 0.9772
Epoch 37/300
 - 15s - loss: 0.0087 - val_loss: 0.0083
 - val_f1: 0.9773
Epoch 38/300
 - 15s - loss: 0.0087 - val_loss: 0.0083
 - val_f1: 0.9773
Epoch 39/300
 - 15s - loss: 0.0086 - val_loss: 0.0083
 - val_f1: 0.9773
Epoch 40/300
 - 15s - loss: 0.0086 - val_loss: 0.0085
 - val_f1: 0.9773
Epoch 41/300
 - 15s - loss: 0.0086 - val_loss: 0.0082
 - val_f1: 0.9774
Epoch 42/300
 - 15s - loss: 0.0085 - val_loss: 0.0082
 - val_f1: 0.9774
Epoch 43/300
 - 15s - loss: 0.0085 - val_loss: 0.0083
 - val_f1: 0.9772
Epoch 44/300
 - 15s - loss: 0.0085 - val_loss: 0.0083
 - val_f1: 0.9773
Epoch 45/300
 - 15s - loss: 0.0085 - val_loss: 0.0082
 - val_f1: 0.9773
Epoch 46/300
 - 15s - loss: 0.0084 - val_loss: 0.0082
 - val_f1: 0.9775
Epoch 47/300
 - 15s - loss: 0.0084 - val_loss: 0.0081
 - val_f1: 0.9775
Epoch 48/300
 - 15s - loss: 0.0084 - val_loss: 0.0082
 - val_f1: 0.9769
Epoch 49/300
 - 15s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9775
Epoch 50/300
 - 15s - loss: 0.0083 - val_loss: 0.0082
 - val_f1: 0.9774
Epoch 51/300
 - 15s - loss: 0.0083 - val_loss: 0.0082
 - val_f1: 0.9771
Epoch 52/300
 - 15s - loss: 0.0083 - val_loss: 0.0083
 - val_f1: 0.9763
Epoch 53/300
 - 15s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9777
Epoch 54/300
 - 15s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9775
Epoch 55/300
 - 15s - loss: 0.0082 - val_loss: 0.0082
 - val_f1: 0.9743
Epoch 56/300
 - 15s - loss: 0.0082 - val_loss: 0.0082
 - val_f1: 0.9774
Epoch 57/300
 - 15s - loss: 0.0082 - val_loss: 0.0084
 - val_f1: 0.9765
Epoch 58/300
 - 15s - loss: 0.0082 - val_loss: 0.0082
 - val_f1: 0.9751
Epoch 59/300
 - 15s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9771
Epoch 60/300
 - 15s - loss: 0.0081 - val_loss: 0.0082
 - val_f1: 0.9768
Epoch 61/300
 - 15s - loss: 0.0081 - val_loss: 0.0082
2019-12-27 09:59:42,331 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_ids18_subset_lstm_deep_rep5/_model_epoch_60.pickle
 - val_f1: 0.9757
Epoch 62/300
 - 15s - loss: 0.0081 - val_loss: 0.0082
 - val_f1: 0.9769
Epoch 63/300
 - 15s - loss: 0.0081 - val_loss: 0.0083
 - val_f1: 0.9769
Epoch 64/300
 - 15s - loss: 0.0080 - val_loss: 0.0081
 - val_f1: 0.9776
Epoch 65/300
 - 15s - loss: 0.0080 - val_loss: 0.0081
 - val_f1: 0.9773
Epoch 66/300
 - 15s - loss: 0.0080 - val_loss: 0.0081
 - val_f1: 0.9776
Epoch 67/300
 - 15s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9776
Epoch 68/300
 - 15s - loss: 0.0080 - val_loss: 0.0081
 - val_f1: 0.9778
Epoch 69/300
 - 15s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9777
Epoch 70/300
 - 15s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9778
Epoch 71/300
 - 15s - loss: 0.0080 - val_loss: 0.0081
 - val_f1: 0.9776
Epoch 72/300
 - 15s - loss: 0.0080 - val_loss: 0.0081
 - val_f1: 0.9775
Epoch 73/300
 - 15s - loss: 0.0080 - val_loss: 0.0081
 - val_f1: 0.9776
Epoch 74/300
 - 15s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9778
Epoch 75/300
 - 15s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9778
Epoch 76/300
 - 15s - loss: 0.0079 - val_loss: 0.0081
 - val_f1: 0.9775
Epoch 77/300
 - 15s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9777
Epoch 78/300
 - 15s - loss: 0.0079 - val_loss: 0.0081
 - val_f1: 0.9776
Epoch 79/300
 - 15s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9765
Epoch 80/300
 - 15s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9778
Epoch 81/300
 - 15s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 82/300
 - 15s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9777
Epoch 83/300
 - 15s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 84/300
 - 15s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 85/300
 - 15s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 86/300
 - 15s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9781
Epoch 87/300
 - 15s - loss: 0.0079 - val_loss: 0.0081
 - val_f1: 0.9777
Epoch 88/300
 - 15s - loss: 0.0079 - val_loss: 0.0081
 - val_f1: 0.9777
Epoch 89/300
 - 15s - loss: 0.0079 - val_loss: 0.0081
 - val_f1: 0.9775
Epoch 90/300
 - 15s - loss: 0.0078 - val_loss: 0.0080
 - val_f1: 0.9780
Epoch 91/300
 - 15s - loss: 0.0079 - val_loss: 0.0082
2019-12-27 10:09:03,200 [INFO] epoch = 90. Intermediate model saved to results_selected_models/selected_ids18_subset_lstm_deep_rep5/_model_epoch_90.pickle
 - val_f1: 0.9779
Epoch 92/300
 - 15s - loss: 0.0078 - val_loss: 0.0081
 - val_f1: 0.9777
Epoch 93/300
 - 15s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 94/300
 - 15s - loss: 0.0078 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 95/300
 - 15s - loss: 0.0078 - val_loss: 0.0083
 - val_f1: 0.9775
Epoch 96/300
 - 15s - loss: 0.0078 - val_loss: 0.0084
 - val_f1: 0.9772
Epoch 97/300
 - 15s - loss: 0.0078 - val_loss: 0.0081
 - val_f1: 0.9780
Epoch 98/300
 - 15s - loss: 0.0078 - val_loss: 0.0084
 - val_f1: 0.9774
Epoch 99/300
 - 15s - loss: 0.0078 - val_loss: 0.0083
 - val_f1: 0.9773
Epoch 100/300
 - 15s - loss: 0.0078 - val_loss: 0.0082
 - val_f1: 0.9780
Epoch 101/300
 - 15s - loss: 0.0078 - val_loss: 0.0081
 - val_f1: 0.9777
Epoch 102/300
 - 15s - loss: 0.0078 - val_loss: 0.0082
 - val_f1: 0.9774
Epoch 103/300
 - 15s - loss: 0.0078 - val_loss: 0.0088
 - val_f1: 0.9761
Epoch 104/300
 - 15s - loss: 0.0078 - val_loss: 0.0083
 - val_f1: 0.9772
Epoch 105/300
 - 15s - loss: 0.0078 - val_loss: 0.0081
 - val_f1: 0.9776
Epoch 106/300
 - 15s - loss: 0.0078 - val_loss: 0.0081
 - val_f1: 0.9779
Epoch 107/300
 - 15s - loss: 0.0078 - val_loss: 0.0082
 - val_f1: 0.9773
Epoch 108/300
 - 15s - loss: 0.0078 - val_loss: 0.0082
 - val_f1: 0.9774
Epoch 109/300
 - 15s - loss: 0.0078 - val_loss: 0.0080
 - val_f1: 0.9781
Epoch 110/300
 - 15s - loss: 0.0077 - val_loss: 0.0081
 - val_f1: 0.9779
Epoch 111/300
 - 15s - loss: 0.0077 - val_loss: 0.0083
 - val_f1: 0.9770
Epoch 112/300
 - 15s - loss: 0.0077 - val_loss: 0.0080
 - val_f1: 0.9781
Epoch 113/300
 - 15s - loss: 0.0077 - val_loss: 0.0080
 - val_f1: 0.9777
Epoch 114/300
 - 15s - loss: 0.0077 - val_loss: 0.0080
 - val_f1: 0.9783
Epoch 115/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 116/300
 - 15s - loss: 0.0077 - val_loss: 0.0081
 - val_f1: 0.9775
Epoch 117/300
 - 15s - loss: 0.0077 - val_loss: 0.0082
 - val_f1: 0.9771
Epoch 118/300
 - 15s - loss: 0.0077 - val_loss: 0.0082
 - val_f1: 0.9776
Epoch 119/300
 - 15s - loss: 0.0077 - val_loss: 0.0082
 - val_f1: 0.9774
Epoch 120/300
 - 15s - loss: 0.0077 - val_loss: 0.0080
 - val_f1: 0.9777
Epoch 121/300
 - 15s - loss: 0.0077 - val_loss: 0.0081
2019-12-27 10:18:23,875 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_ids18_subset_lstm_deep_rep5/_model_epoch_120.pickle
 - val_f1: 0.9779
Epoch 122/300
 - 15s - loss: 0.0077 - val_loss: 0.0087
 - val_f1: 0.9763
Epoch 123/300
 - 15s - loss: 0.0077 - val_loss: 0.0087
 - val_f1: 0.9764
Epoch 124/300
 - 15s - loss: 0.0077 - val_loss: 0.0129
 - val_f1: 0.9628
Epoch 125/300
 - 15s - loss: 0.0077 - val_loss: 0.0086
 - val_f1: 0.9769
Epoch 126/300
 - 15s - loss: 0.0077 - val_loss: 0.0085
 - val_f1: 0.9767
Epoch 127/300
 - 15s - loss: 0.0077 - val_loss: 0.0087
 - val_f1: 0.9772
Epoch 128/300
 - 15s - loss: 0.0077 - val_loss: 0.0085
 - val_f1: 0.9771
Epoch 129/300
 - 15s - loss: 0.0077 - val_loss: 0.0090
 - val_f1: 0.9758
Epoch 130/300
 - 15s - loss: 0.0077 - val_loss: 0.0084
 - val_f1: 0.9773
Epoch 131/300
 - 15s - loss: 0.0077 - val_loss: 0.0085
 - val_f1: 0.9768
Epoch 132/300
 - 15s - loss: 0.0077 - val_loss: 0.0080
 - val_f1: 0.9781
Epoch 133/300
 - 15s - loss: 0.0077 - val_loss: 0.0082
 - val_f1: 0.9776
Epoch 134/300
 - 15s - loss: 0.0077 - val_loss: 0.0081
 - val_f1: 0.9779
Epoch 135/300
 - 15s - loss: 0.0077 - val_loss: 0.0083
 - val_f1: 0.9772
Epoch 136/300
 - 15s - loss: 0.0077 - val_loss: 0.0080
 - val_f1: 0.9776
Epoch 137/300
 - 15s - loss: 0.0077 - val_loss: 0.0081
 - val_f1: 0.9775
Epoch 138/300
 - 15s - loss: 0.0077 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 139/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 140/300
 - 15s - loss: 0.0077 - val_loss: 0.0082
 - val_f1: 0.9774
Epoch 141/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 142/300
 - 15s - loss: 0.0076 - val_loss: 0.0082
 - val_f1: 0.9777
Epoch 143/300
 - 15s - loss: 0.0077 - val_loss: 0.0082
 - val_f1: 0.9776
Epoch 144/300
 - 15s - loss: 0.0076 - val_loss: 0.0081
 - val_f1: 0.9779
Epoch 145/300
 - 15s - loss: 0.0076 - val_loss: 0.0081
 - val_f1: 0.9779
Epoch 146/300
 - 15s - loss: 0.0077 - val_loss: 0.0082
 - val_f1: 0.9777
Epoch 147/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
 - val_f1: 0.9782
Epoch 148/300
 - 15s - loss: 0.0076 - val_loss: 0.0079
 - val_f1: 0.9784
Epoch 149/300
 - 15s - loss: 0.0076 - val_loss: 0.0081
 - val_f1: 0.9781
Epoch 150/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
 - val_f1: 0.9781
Epoch 151/300
 - 15s - loss: 0.0076 - val_loss: 0.0079
2019-12-27 10:27:44,773 [INFO] epoch = 150. Intermediate model saved to results_selected_models/selected_ids18_subset_lstm_deep_rep5/_model_epoch_150.pickle
 - val_f1: 0.9781
Epoch 152/300
 - 15s - loss: 0.0077 - val_loss: 0.0080
 - val_f1: 0.9783
Epoch 153/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
 - val_f1: 0.9782
Epoch 154/300
 - 15s - loss: 0.0076 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 155/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
 - val_f1: 0.9781
Epoch 156/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
 - val_f1: 0.9782
Epoch 157/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
 - val_f1: 0.9782
Epoch 158/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
 - val_f1: 0.9784
Epoch 159/300
 - 15s - loss: 0.0076 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 160/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
 - val_f1: 0.9781
Epoch 161/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
 - val_f1: 0.9781
Epoch 162/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
 - val_f1: 0.9782
Epoch 163/300
 - 15s - loss: 0.0076 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 164/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
 - val_f1: 0.9778
Epoch 165/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
 - val_f1: 0.9783
Epoch 166/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
 - val_f1: 0.9783
Epoch 167/300
 - 15s - loss: 0.0076 - val_loss: 0.0080
 - val_f1: 0.9782
Epoch 168/300
 - 15s - loss: 0.0076 - val_loss: 0.0081
 - val_f1: 0.9780
Epoch 169/300
 - 15s - loss: 0.0076 - val_loss: 0.0081
 - val_f1: 0.9779
Epoch 170/300
 - 15s - loss: 0.0075 - val_loss: 0.0080
 - val_f1: 0.9780
Epoch 171/300
 - 15s - loss: 0.0076 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 172/300
 - 15s - loss: 0.0075 - val_loss: 0.0081
 - val_f1: 0.9779
Epoch 173/300
 - 15s - loss: 0.0076 - val_loss: 0.0081
 - val_f1: 0.9780
Epoch 174/300
 - 15s - loss: 0.0075 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 175/300
 - 15s - loss: 0.0075 - val_loss: 0.0081
 - val_f1: 0.9781
Epoch 176/300
 - 15s - loss: 0.0075 - val_loss: 0.0080
 - val_f1: 0.9781
Epoch 177/300
 - 15s - loss: 0.0075 - val_loss: 0.0080
 - val_f1: 0.9782
Epoch 178/300
 - 15s - loss: 0.0075 - val_loss: 0.0081
 - val_f1: 0.9780
Epoch 179/300
 - 15s - loss: 0.0075 - val_loss: 0.0080
 - val_f1: 0.9783
Epoch 180/300
 - 15s - loss: 0.0075 - val_loss: 0.0080
 - val_f1: 0.9782
Epoch 181/300
 - 15s - loss: 0.0075 - val_loss: 0.0081
2019-12-27 10:37:05,941 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_ids18_subset_lstm_deep_rep5/_model_epoch_180.pickle
 - val_f1: 0.9781
Epoch 182/300
 - 15s - loss: 0.0075 - val_loss: 0.0080
 - val_f1: 0.9780
Epoch 183/300
 - 15s - loss: 0.0075 - val_loss: 0.0080
 - val_f1: 0.9781
Epoch 184/300
 - 15s - loss: 0.0075 - val_loss: 0.0081
 - val_f1: 0.9782
Epoch 185/300
 - 15s - loss: 0.0075 - val_loss: 0.0081
 - val_f1: 0.9781
Epoch 186/300
 - 15s - loss: 0.0075 - val_loss: 0.0081
 - val_f1: 0.9782
Epoch 187/300
 - 15s - loss: 0.0075 - val_loss: 0.0080
 - val_f1: 0.9783
Epoch 188/300
 - 15s - loss: 0.0075 - val_loss: 0.0080
 - val_f1: 0.9782
Epoch 189/300
 - 15s - loss: 0.0075 - val_loss: 0.0080
 - val_f1: 0.9782
Epoch 190/300
 - 15s - loss: 0.0075 - val_loss: 0.0081
 - val_f1: 0.9782
Epoch 191/300
 - 15s - loss: 0.0075 - val_loss: 0.0080
 - val_f1: 0.9782
Epoch 192/300
 - 15s - loss: 0.0075 - val_loss: 0.0081
 - val_f1: 0.9781
Epoch 193/300
 - 15s - loss: 0.0075 - val_loss: 0.0082
 - val_f1: 0.9781
Epoch 194/300
 - 15s - loss: 0.0075 - val_loss: 0.0081
 - val_f1: 0.9780
Epoch 195/300
 - 15s - loss: 0.0075 - val_loss: 0.0082
 - val_f1: 0.9778
Epoch 196/300
 - 15s - loss: 0.0075 - val_loss: 0.0082
 - val_f1: 0.9780
Epoch 197/300
 - 15s - loss: 0.0075 - val_loss: 0.0080
 - val_f1: 0.9782
Epoch 198/300
 - 15s - loss: 0.0075 - val_loss: 0.0080
2019-12-27 10:42:27,560 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-27 10:42:42,141 [INFO] Last epoch loss evaluation: train_loss = 0.007596, val_loss = 0.007895
2019-12-27 10:42:42,141 [INFO] Training complete. time_to_train = 3732.47 sec, 62.21 min
2019-12-27 10:42:42,148 [INFO] Model saved to results_selected_models/selected_ids18_subset_lstm_deep_rep5/best_model.pickle
2019-12-27 10:42:42,151 [INFO] Training history saved to: results_selected_models/selected_ids18_subset_lstm_deep_rep5/training_error_history.csv
/home/hasitha/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py:514: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  max_open_warning, RuntimeWarning)
2019-12-27 10:42:42,299 [INFO] Plot saved to: results_selected_models/selected_ids18_subset_lstm_deep_rep5/training_error_history.png
2019-12-27 10:42:42,420 [INFO] Plot saved to: results_selected_models/selected_ids18_subset_lstm_deep_rep5/training_f1_history.png
2019-12-27 10:42:42,420 [INFO] Making predictions on training, validation, testing data
2019-12-27 10:42:58,273 [INFO] Evaluating predictions (results)
2019-12-27 10:43:10,332 [INFO] Dataset: Testing. Classification report below
2019-12-27 10:43:10,332 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99    535639
                     Bot       1.00      1.00      1.00     11465
        Brute Force -Web       0.00      0.00      0.00        24
        Brute Force -XSS       1.00      0.22      0.36         9
        DDOS attack-HOIC       1.00      1.00      1.00     27446
    DDOS attack-LOIC-UDP       0.68      0.88      0.77        67
  DDoS attacks-LOIC-HTTP       0.99      0.99      0.99     23008
   DoS attacks-GoldenEye       0.99      1.00      1.00      1651
        DoS attacks-Hulk       1.00      1.00      1.00     18477
DoS attacks-SlowHTTPTest       0.72      0.54      0.61      5596
   DoS attacks-Slowloris       0.95      0.95      0.95       440
          FTP-BruteForce       0.72      0.85      0.78      7718
           Infilteration       0.40      0.00      0.00      6403
           SQL Injection       0.00      0.00      0.00         4
          SSH-Bruteforce       1.00      1.00      1.00      7525

               micro avg       0.98      0.98      0.98    645472
               macro avg       0.76      0.70      0.70    645472
            weighted avg       0.98      0.98      0.98    645472

2019-12-27 10:43:10,332 [INFO] Overall accuracy (micro avg): 0.9834849536463239
/home/hasitha/sunanda/ids_experiments/utility.py:334: RuntimeWarning: invalid value encountered in true_divide
  F1 = (2 * PPV * TPR) / (PPV + TPR)  # F1 score
2019-12-27 10:43:24,015 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9835         0.9835                       0.9835                0.0012                   0.0165  0.9835
1     Macro avg        0.9978         0.7623                       0.6950                0.0044                   0.3050  0.6968
2  Weighted avg        0.9910         0.9776                       0.9835                0.0499                   0.0165  0.9784
/home/hasitha/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2019-12-27 10:43:36,313 [INFO] Dataset: Validation. Classification report below
2019-12-27 10:43:36,313 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99    535636
                     Bot       1.00      1.00      1.00     11464
        Brute Force -Web       0.00      0.00      0.00        25
        Brute Force -XSS       1.00      0.56      0.71         9
        DDOS attack-HOIC       1.00      1.00      1.00     27447
    DDOS attack-LOIC-UDP       0.73      0.88      0.80        68
  DDoS attacks-LOIC-HTTP       0.99      0.99      0.99     23009
   DoS attacks-GoldenEye       1.00      1.00      1.00      1651
        DoS attacks-Hulk       1.00      1.00      1.00     18478
DoS attacks-SlowHTTPTest       0.72      0.53      0.61      5596
   DoS attacks-Slowloris       0.94      0.95      0.94       439
          FTP-BruteForce       0.71      0.85      0.78      7718
           Infilteration       0.42      0.00      0.00      6403
           SQL Injection       0.00      0.00      0.00         4
          SSH-Bruteforce       1.00      1.00      1.00      7525

               micro avg       0.98      0.98      0.98    645472
               macro avg       0.77      0.72      0.72    645472
            weighted avg       0.98      0.98      0.98    645472

2019-12-27 10:43:36,313 [INFO] Overall accuracy (micro avg): 0.9834865029002032
/home/hasitha/sunanda/ids_experiments/utility.py:328: RuntimeWarning: invalid value encountered in true_divide
  PPV = TP / (TP + FP)    # Precision or positive predictive value
/home/hasitha/sunanda/ids_experiments/utility.py:332: RuntimeWarning: invalid value encountered in true_divide
  FDR = FP / (TP + FP)    # False discovery rate
2019-12-27 10:43:50,293 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9835         0.9835                       0.9835                0.0012                   0.0165  0.9835
1     Macro avg        0.9978         0.7663                       0.7172                0.0044                   0.2828  0.7218
2  Weighted avg        0.9910         0.9778                       0.9835                0.0499                   0.0165  0.9784
2019-12-27 10:44:30,150 [INFO] Dataset: Training. Classification report below
2019-12-27 10:44:30,150 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99   1606935
                     Bot       1.00      1.00      1.00     34396
        Brute Force -Web       0.00      0.00      0.00        73
        Brute Force -XSS       1.00      0.35      0.51        26
        DDOS attack-HOIC       1.00      1.00      1.00     82341
    DDOS attack-LOIC-UDP       0.71      0.90      0.80       203
  DDoS attacks-LOIC-HTTP       0.99      0.99      0.99     69029
   DoS attacks-GoldenEye       1.00      1.00      1.00      4954
        DoS attacks-Hulk       1.00      1.00      1.00     55435
DoS attacks-SlowHTTPTest       0.74      0.54      0.62     16787
   DoS attacks-Slowloris       0.96      0.99      0.98      1318
          FTP-BruteForce       0.72      0.86      0.78     23153
           Infilteration       0.67      0.00      0.00     19210
           SQL Injection       1.00      0.17      0.29        12
          SSH-Bruteforce       1.00      1.00      1.00     22576

               micro avg       0.98      0.98      0.98   1936448
               macro avg       0.85      0.72      0.73   1936448
            weighted avg       0.98      0.98      0.98   1936448

2019-12-27 10:44:30,150 [INFO] Overall accuracy (micro avg): 0.9838580741646561
2019-12-27 10:45:15,395 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9839         0.9839                       0.9839                0.0012                   0.0161  0.9839
1     Macro avg        0.9978         0.8521                       0.7198                0.0044                   0.2802  0.7313
2  Weighted avg        0.9911         0.9807                       0.9839                0.0496                   0.0161  0.9787
2019-12-27 10:45:15,433 [INFO] Results saved to: results_selected_models/selected_ids18_subset_lstm_deep_rep5/selected_ids18_subset_lstm_deep_rep5_results.xlsx
2019-12-27 10:45:15,438 [INFO] ================= Finished running experiment no. 5 ================= 

2019-12-27 10:45:15,523 [INFO] ================= Finished running 15 experiments ================= 

 - val_f1: 0.9782
Epoch 00198: early stopping
2019-12-28 01:09:26,244 [INFO] Created directory: results_selected_models/selected_kdd99_lstm_deep_rep1
2019-12-28 01:09:26,245 [INFO] Initialized logging. log_filename = results_selected_models/selected_kdd99_lstm_deep_rep1/run_log.log
2019-12-28 01:09:26,245 [INFO] ================= Running experiment no. 1  ================= 

2019-12-28 01:09:26,245 [INFO] Experiment parameters given below
2019-12-28 01:09:26,245 [INFO] 
{'experiment_num': 1, 'results_dir': 'results_selected_models/selected_kdd99_lstm_deep_rep1', 'model': 'lstm', 'model_type': 'classifier', 'normal_label': 'normal', 'lstm_time_steps': 32, 'lstm_layer_units': [32, 16], 'lstm_layer_activations': ['relu', 'relu'], 'lstm_layer_dropout_rates': [0.2, 0.2], 'batch_size': 256, 'epochs': 300, 'early_stop_patience': 50, 'dataset_dir': '../Datasets/full_datasets/kdd99_five_classes', 'description': 'selected_kdd99_lstm_deep_rep1'}
2019-12-28 01:09:26,245 [INFO] Created tensorboard log directory: results_selected_models/selected_kdd99_lstm_deep_rep1/tf_logs_run_2019_12_28-01_09_26
2019-12-28 01:09:26,245 [INFO] Loading datsets from: ../Datasets/full_datasets/kdd99_five_classes
2019-12-28 01:09:26,246 [INFO] Reading X, y files
2019-12-28 01:09:26,246 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/X_train.h5
2019-12-28 01:09:32,639 [INFO] Reading complete. time_to_read=6.39 seconds
2019-12-28 01:09:32,639 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/X_val.h5
2019-12-28 01:09:34,331 [INFO] Reading complete. time_to_read=1.69 seconds
2019-12-28 01:09:34,332 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/X_test.h5
2019-12-28 01:09:34,805 [INFO] Reading complete. time_to_read=0.47 seconds
2019-12-28 01:09:34,806 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/y_train.h5
2019-12-28 01:09:35,003 [INFO] Reading complete. time_to_read=0.20 seconds
2019-12-28 01:09:35,003 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/y_val.h5
2019-12-28 01:09:35,056 [INFO] Reading complete. time_to_read=0.05 seconds
2019-12-28 01:09:35,056 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/y_test.h5
2019-12-28 01:09:35,075 [INFO] Reading complete. time_to_read=0.02 seconds
2019-12-28 01:09:40,778 [INFO] Preparing flow sequences
2019-12-28 01:10:28,147 [INFO] Extracting flows complete. time_taken = 47.37 sec
2019-12-28 01:10:29,773 [INFO] Initializing model
2019-12-28 01:10:30,405 [INFO] _________________________________________________________________
2019-12-28 01:10:30,405 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-28 01:10:30,405 [INFO] =================================================================
2019-12-28 01:10:30,405 [INFO] lstm_6 (LSTM)                (None, 32, 32)            19968     
2019-12-28 01:10:30,405 [INFO] _________________________________________________________________
2019-12-28 01:10:30,406 [INFO] batch_normalization_6 (Batch (None, 32, 32)            128       
2019-12-28 01:10:30,406 [INFO] _________________________________________________________________
2019-12-28 01:10:30,406 [INFO] dropout_6 (Dropout)          (None, 32, 32)            0         
2019-12-28 01:10:30,406 [INFO] _________________________________________________________________
2019-12-28 01:10:30,406 [INFO] lstm_7 (LSTM)                (None, 32, 16)            3136      
2019-12-28 01:10:30,406 [INFO] _________________________________________________________________
2019-12-28 01:10:30,406 [INFO] batch_normalization_7 (Batch (None, 32, 16)            64        
2019-12-28 01:10:30,406 [INFO] _________________________________________________________________
2019-12-28 01:10:30,407 [INFO] dropout_7 (Dropout)          (None, 32, 16)            0         
2019-12-28 01:10:30,407 [INFO] _________________________________________________________________
2019-12-28 01:10:30,407 [INFO] time_distributed_6 (TimeDist (None, 32, 5)             85        
2019-12-28 01:10:30,407 [INFO] =================================================================
2019-12-28 01:10:30,407 [INFO] Total params: 23,381
2019-12-28 01:10:30,407 [INFO] Trainable params: 23,285
2019-12-28 01:10:30,407 [INFO] Non-trainable params: 96
2019-12-28 01:10:30,407 [INFO] _________________________________________________________________
2019-12-28 01:10:30,408 [INFO] Training model
 - val_f1: 0.9998
Epoch 00084: early stopping
Train on 122460 samples, validate on 30615 samples
Epoch 1/300
 - 50s - loss: 0.1076 - val_loss: 0.0052
 - val_f1: 0.9974
Epoch 2/300
 - 47s - loss: 0.0058 - val_loss: 0.0022
 - val_f1: 0.9988
Epoch 3/300
 - 47s - loss: 0.0031 - val_loss: 0.0014
 - val_f1: 0.9991
Epoch 4/300
 - 47s - loss: 0.0021 - val_loss: 0.0011
 - val_f1: 0.9992
Epoch 5/300
 - 47s - loss: 0.0016 - val_loss: 9.2625e-04
 - val_f1: 0.9993
Epoch 6/300
 - 47s - loss: 0.0013 - val_loss: 7.8060e-04
 - val_f1: 0.9994
Epoch 7/300
 - 47s - loss: 0.0011 - val_loss: 6.7014e-04
 - val_f1: 0.9994
Epoch 8/300
 - 47s - loss: 9.7168e-04 - val_loss: 5.6049e-04
 - val_f1: 0.9996
Epoch 9/300
 - 47s - loss: 8.4408e-04 - val_loss: 5.5147e-04
 - val_f1: 0.9997
Epoch 10/300
 - 47s - loss: 7.7246e-04 - val_loss: 4.8717e-04
 - val_f1: 0.9997
Epoch 11/300
 - 47s - loss: 7.1068e-04 - val_loss: 4.8352e-04
 - val_f1: 0.9997
Epoch 12/300
 - 47s - loss: 6.4753e-04 - val_loss: 4.7976e-04
 - val_f1: 0.9997
Epoch 13/300
 - 47s - loss: 6.2567e-04 - val_loss: 4.4481e-04
 - val_f1: 0.9997
Epoch 14/300
 - 47s - loss: 5.9670e-04 - val_loss: 4.4077e-04
 - val_f1: 0.9997
Epoch 15/300
 - 47s - loss: 5.6271e-04 - val_loss: 4.7324e-04
 - val_f1: 0.9997
Epoch 16/300
 - 47s - loss: 5.6300e-04 - val_loss: 4.1756e-04
 - val_f1: 0.9997
Epoch 17/300
 - 47s - loss: 5.4097e-04 - val_loss: 4.4918e-04
 - val_f1: 0.9997
Epoch 18/300
 - 47s - loss: 5.0955e-04 - val_loss: 4.2054e-04
 - val_f1: 0.9997
Epoch 19/300
 - 47s - loss: 4.7371e-04 - val_loss: 4.2104e-04
 - val_f1: 0.9997
Epoch 20/300
 - 47s - loss: 4.7026e-04 - val_loss: 4.0448e-04
 - val_f1: 0.9997
Epoch 21/300
 - 47s - loss: 4.5064e-04 - val_loss: 3.8898e-04
 - val_f1: 0.9998
Epoch 22/300
 - 47s - loss: 4.4484e-04 - val_loss: 4.0329e-04
 - val_f1: 0.9998
Epoch 23/300
 - 47s - loss: 4.0946e-04 - val_loss: 4.0987e-04
 - val_f1: 0.9998
Epoch 24/300
 - 47s - loss: 4.0641e-04 - val_loss: 3.8042e-04
 - val_f1: 0.9998
Epoch 25/300
 - 47s - loss: 4.0100e-04 - val_loss: 3.8271e-04
 - val_f1: 0.9998
Epoch 26/300
 - 47s - loss: 3.8123e-04 - val_loss: 3.7375e-04
 - val_f1: 0.9998
Epoch 27/300
 - 47s - loss: 3.7477e-04 - val_loss: 3.9036e-04
 - val_f1: 0.9998
Epoch 28/300
 - 47s - loss: 3.6764e-04 - val_loss: 3.6241e-04
 - val_f1: 0.9998
Epoch 29/300
 - 47s - loss: 3.6518e-04 - val_loss: 3.4120e-04
 - val_f1: 0.9998
Epoch 30/300
 - 47s - loss: 3.7300e-04 - val_loss: 3.5230e-04
 - val_f1: 0.9998
Epoch 31/300
 - 47s - loss: 3.4415e-04 - val_loss: 3.5136e-04
2019-12-28 01:40:28,436 [INFO] epoch = 30. Intermediate model saved to results_selected_models/selected_kdd99_lstm_deep_rep1/_model_epoch_30.pickle
 - val_f1: 0.9998
Epoch 32/300
 - 47s - loss: 3.5316e-04 - val_loss: 3.5927e-04
 - val_f1: 0.9998
Epoch 33/300
 - 47s - loss: 3.3443e-04 - val_loss: 3.5608e-04
 - val_f1: 0.9998
Epoch 34/300
 - 47s - loss: 3.2036e-04 - val_loss: 3.5092e-04
 - val_f1: 0.9998
Epoch 35/300
 - 47s - loss: 3.1840e-04 - val_loss: 3.3052e-04
 - val_f1: 0.9998
Epoch 36/300
 - 47s - loss: 3.1651e-04 - val_loss: 3.2456e-04
 - val_f1: 0.9998
Epoch 37/300
 - 47s - loss: 3.2145e-04 - val_loss: 3.4184e-04
 - val_f1: 0.9998
Epoch 38/300
 - 47s - loss: 3.0438e-04 - val_loss: 3.2476e-04
 - val_f1: 0.9998
Epoch 39/300
 - 47s - loss: 2.9614e-04 - val_loss: 3.3230e-04
 - val_f1: 0.9998
Epoch 40/300
 - 47s - loss: 3.0375e-04 - val_loss: 3.3752e-04
 - val_f1: 0.9998
Epoch 41/300
 - 47s - loss: 2.9225e-04 - val_loss: 3.3365e-04
 - val_f1: 0.9998
Epoch 42/300
 - 47s - loss: 2.9557e-04 - val_loss: 3.2052e-04
 - val_f1: 0.9998
Epoch 43/300
 - 47s - loss: 2.8118e-04 - val_loss: 3.4282e-04
 - val_f1: 0.9998
Epoch 44/300
 - 47s - loss: 2.7803e-04 - val_loss: 3.2610e-04
 - val_f1: 0.9998
Epoch 45/300
 - 47s - loss: 2.7791e-04 - val_loss: 3.6156e-04
 - val_f1: 0.9998
Epoch 46/300
 - 47s - loss: 2.7395e-04 - val_loss: 3.2162e-04
 - val_f1: 0.9998
Epoch 47/300
 - 47s - loss: 2.6186e-04 - val_loss: 3.4911e-04
 - val_f1: 0.9998
Epoch 48/300
 - 47s - loss: 2.7197e-04 - val_loss: 3.4341e-04
 - val_f1: 0.9998
Epoch 49/300
 - 47s - loss: 2.6384e-04 - val_loss: 3.3903e-04
 - val_f1: 0.9998
Epoch 50/300
 - 47s - loss: 2.6131e-04 - val_loss: 3.4597e-04
 - val_f1: 0.9998
Epoch 51/300
 - 47s - loss: 2.6075e-04 - val_loss: 3.1209e-04
 - val_f1: 0.9998
Epoch 52/300
 - 47s - loss: 2.5599e-04 - val_loss: 3.3916e-04
 - val_f1: 0.9998
Epoch 53/300
 - 47s - loss: 2.4954e-04 - val_loss: 3.1829e-04
 - val_f1: 0.9998
Epoch 54/300
 - 47s - loss: 2.4663e-04 - val_loss: 3.3779e-04
 - val_f1: 0.9998
Epoch 55/300
 - 47s - loss: 2.4841e-04 - val_loss: 3.1939e-04
 - val_f1: 0.9998
Epoch 56/300
 - 47s - loss: 2.4364e-04 - val_loss: 3.4860e-04
 - val_f1: 0.9998
Epoch 57/300
 - 47s - loss: 2.4548e-04 - val_loss: 3.3487e-04
 - val_f1: 0.9998
Epoch 58/300
 - 47s - loss: 2.4111e-04 - val_loss: 3.4412e-04
 - val_f1: 0.9998
Epoch 59/300
 - 47s - loss: 2.3521e-04 - val_loss: 3.2563e-04
 - val_f1: 0.9998
Epoch 60/300
 - 47s - loss: 2.4911e-04 - val_loss: 3.2220e-04
 - val_f1: 0.9998
Epoch 61/300
 - 47s - loss: 2.3696e-04 - val_loss: 3.3867e-04
2019-12-28 02:09:31,319 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_kdd99_lstm_deep_rep1/_model_epoch_60.pickle
 - val_f1: 0.9998
Epoch 62/300
 - 47s - loss: 2.2966e-04 - val_loss: 3.5444e-04
 - val_f1: 0.9998
Epoch 63/300
 - 47s - loss: 2.2048e-04 - val_loss: 3.5396e-04
 - val_f1: 0.9998
Epoch 64/300
 - 47s - loss: 2.2071e-04 - val_loss: 3.4062e-04
 - val_f1: 0.9998
Epoch 65/300
 - 47s - loss: 2.2757e-04 - val_loss: 3.3987e-04
 - val_f1: 0.9998
Epoch 66/300
 - 47s - loss: 2.3759e-04 - val_loss: 3.4767e-04
 - val_f1: 0.9998
Epoch 67/300
 - 47s - loss: 2.2601e-04 - val_loss: 3.4512e-04
 - val_f1: 0.9998
Epoch 68/300
 - 47s - loss: 2.1746e-04 - val_loss: 3.6009e-04
 - val_f1: 0.9998
Epoch 69/300
 - 47s - loss: 2.1979e-04 - val_loss: 3.7438e-04
 - val_f1: 0.9998
Epoch 70/300
 - 47s - loss: 2.2847e-04 - val_loss: 3.5050e-04
 - val_f1: 0.9998
Epoch 71/300
 - 47s - loss: 2.2446e-04 - val_loss: 3.7542e-04
 - val_f1: 0.9998
Epoch 72/300
 - 47s - loss: 2.2712e-04 - val_loss: 3.5336e-04
 - val_f1: 0.9998
Epoch 73/300
 - 47s - loss: 2.1349e-04 - val_loss: 3.7793e-04
 - val_f1: 0.9998
Epoch 74/300
 - 47s - loss: 2.1384e-04 - val_loss: 3.3934e-04
 - val_f1: 0.9998
Epoch 75/300
 - 47s - loss: 2.2568e-04 - val_loss: 3.5817e-04
 - val_f1: 0.9998
Epoch 76/300
 - 47s - loss: 2.0810e-04 - val_loss: 3.8420e-04
 - val_f1: 0.9998
Epoch 77/300
 - 47s - loss: 2.1743e-04 - val_loss: 3.6345e-04
 - val_f1: 0.9998
Epoch 78/300
 - 47s - loss: 2.0904e-04 - val_loss: 3.6969e-04
 - val_f1: 0.9998
Epoch 79/300
 - 47s - loss: 2.1859e-04 - val_loss: 3.8258e-04
 - val_f1: 0.9998
Epoch 80/300
 - 47s - loss: 2.1140e-04 - val_loss: 3.5496e-04
 - val_f1: 0.9998
Epoch 81/300
 - 47s - loss: 2.1049e-04 - val_loss: 3.6987e-04
 - val_f1: 0.9998
Epoch 82/300
 - 47s - loss: 2.1989e-04 - val_loss: 3.5913e-04
 - val_f1: 0.9998
Epoch 83/300
 - 47s - loss: 2.2089e-04 - val_loss: 3.8535e-04
 - val_f1: 0.9998
Epoch 84/300
 - 47s - loss: 2.2466e-04 - val_loss: 3.7948e-04
 - val_f1: 0.9998
Epoch 85/300
 - 47s - loss: 2.1618e-04 - val_loss: 3.4011e-04
 - val_f1: 0.9998
Epoch 86/300
 - 47s - loss: 1.9948e-04 - val_loss: 3.5282e-04
 - val_f1: 0.9998
Epoch 87/300
 - 47s - loss: 2.0174e-04 - val_loss: 3.5574e-04
 - val_f1: 0.9998
Epoch 88/300
 - 47s - loss: 2.0052e-04 - val_loss: 3.4658e-04
 - val_f1: 0.9998
Epoch 89/300
 - 47s - loss: 2.1713e-04 - val_loss: 3.7355e-04
 - val_f1: 0.9998
Epoch 90/300
 - 47s - loss: 1.9572e-04 - val_loss: 3.7163e-04
 - val_f1: 0.9998
Epoch 91/300
 - 47s - loss: 2.0240e-04 - val_loss: 3.9052e-04
2019-12-28 02:38:34,505 [INFO] epoch = 90. Intermediate model saved to results_selected_models/selected_kdd99_lstm_deep_rep1/_model_epoch_90.pickle
 - val_f1: 0.9998
Epoch 92/300
 - 47s - loss: 1.9466e-04 - val_loss: 3.7317e-04
 - val_f1: 0.9998
Epoch 93/300
 - 47s - loss: 1.9379e-04 - val_loss: 3.7897e-04
 - val_f1: 0.9998
Epoch 94/300
 - 47s - loss: 1.9833e-04 - val_loss: 4.0422e-04
 - val_f1: 0.9998
Epoch 95/300
 - 47s - loss: 1.8908e-04 - val_loss: 3.7061e-04
 - val_f1: 0.9998
Epoch 96/300
 - 47s - loss: 1.9106e-04 - val_loss: 3.9777e-04
 - val_f1: 0.9998
Epoch 97/300
 - 47s - loss: 2.0072e-04 - val_loss: 3.5345e-04
 - val_f1: 0.9998
Epoch 98/300
 - 47s - loss: 1.9124e-04 - val_loss: 3.5955e-04
 - val_f1: 0.9998
Epoch 99/300
 - 47s - loss: 1.9741e-04 - val_loss: 3.8407e-04
 - val_f1: 0.9998
Epoch 100/300
 - 47s - loss: 1.9488e-04 - val_loss: 3.7628e-04
 - val_f1: 0.9998
Epoch 101/300
 - 47s - loss: 1.9549e-04 - val_loss: 3.7967e-04
2019-12-28 02:48:26,851 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-28 02:49:21,223 [INFO] Last epoch loss evaluation: train_loss = 0.000148, val_loss = 0.000312
2019-12-28 02:49:21,223 [INFO] Training complete. time_to_train = 5930.82 sec, 98.85 min
2019-12-28 02:49:21,229 [INFO] Model saved to results_selected_models/selected_kdd99_lstm_deep_rep1/best_model.pickle
2019-12-28 02:49:21,231 [INFO] Training history saved to: results_selected_models/selected_kdd99_lstm_deep_rep1/training_error_history.csv
2019-12-28 02:49:21,405 [INFO] Plot saved to: results_selected_models/selected_kdd99_lstm_deep_rep1/training_error_history.png
2019-12-28 02:49:21,567 [INFO] Plot saved to: results_selected_models/selected_kdd99_lstm_deep_rep1/training_f1_history.png
2019-12-28 02:49:21,567 [INFO] Making predictions on training, validation, testing data
2019-12-28 02:50:17,785 [INFO] Evaluating predictions (results)
/home/sunanda/anaconda3/envs/ml_env_cpu/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2019-12-28 02:50:26,507 [INFO] Dataset: Testing. Classification report below
2019-12-28 02:50:26,507 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      0.97      0.99    229853
     normal.       0.72      0.98      0.83     60580
       probe       0.71      0.65      0.68      4166
         r2l       0.76      0.02      0.04     13773
         u2r       0.00      0.00      0.00      2636

    accuracy                           0.92    311008
   macro avg       0.64      0.53      0.51    311008
weighted avg       0.92      0.92      0.90    311008

2019-12-28 02:50:26,507 [INFO] Overall accuracy (micro avg): 0.9214007356723943
/home/sunanda/research/ids_experiments/utility.py:328: RuntimeWarning: invalid value encountered in true_divide
  PPV = TP / (TP + FP)    # Precision or positive predictive value
/home/sunanda/research/ids_experiments/utility.py:332: RuntimeWarning: invalid value encountered in true_divide
  FDR = FP / (TP + FP)    # False discovery rate
2019-12-28 02:50:35,862 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9214         0.9214                       0.9214                0.0196                   0.0786  0.9214
1     Macro avg        0.9686         0.6381                       0.5253                0.0202                   0.4747  0.5064
2  Weighted avg        0.9680         0.9217                       0.9214                0.0223                   0.0786  0.9018
2019-12-28 02:51:06,472 [INFO] Dataset: Validation. Classification report below
2019-12-28 02:51:06,472 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00    776671
     normal.       1.00      1.00      1.00    194553
       probe       1.00      0.99      1.00      8221
         r2l       0.93      0.81      0.87       225
         u2r       0.00      0.00      0.00        10

    accuracy                           1.00    979680
   macro avg       0.79      0.76      0.77    979680
weighted avg       1.00      1.00      1.00    979680

2019-12-28 02:51:06,472 [INFO] Overall accuracy (micro avg): 0.9998295361750775
2019-12-28 02:51:39,504 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9998         0.9998                       0.9998                0.0000                   0.0002  0.9998
1     Macro avg        0.9999         0.7859                       0.7605                0.0001                   0.2395  0.7723
2  Weighted avg        0.9999         0.9998                       0.9998                0.0001                   0.0002  0.9998
2019-12-28 02:53:52,655 [INFO] Dataset: Training. Classification report below
2019-12-28 02:53:52,655 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00   3106675
     normal.       1.00      1.00      1.00    778221
       probe       1.00      1.00      1.00     32881
         r2l       0.94      0.88      0.91       901
         u2r       1.00      0.02      0.05        42

    accuracy                           1.00   3918720
   macro avg       0.99      0.78      0.79   3918720
weighted avg       1.00      1.00      1.00   3918720

2019-12-28 02:53:52,655 [INFO] Overall accuracy (micro avg): 0.9998841458435408
2019-12-28 02:56:16,322 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9999         0.9999                       0.9999                0.0000                   0.0001  0.9999
1     Macro avg        1.0000         0.9875                       0.7794                0.0000                   0.2206  0.7902
2  Weighted avg        1.0000         0.9999                       0.9999                0.0001                   0.0001  0.9999
2019-12-28 02:56:16,369 [INFO] Results saved to: results_selected_models/selected_kdd99_lstm_deep_rep1/selected_kdd99_lstm_deep_rep1_results.xlsx
2019-12-28 02:56:16,376 [INFO] ================= Finished running experiment no. 1 ================= 

2019-12-28 02:56:16,490 [INFO] Created directory: results_selected_models/selected_kdd99_lstm_deep_rep2
2019-12-28 02:56:16,490 [INFO] Initialized logging. log_filename = results_selected_models/selected_kdd99_lstm_deep_rep2/run_log.log
2019-12-28 02:56:16,490 [INFO] ================= Running experiment no. 2  ================= 

2019-12-28 02:56:16,491 [INFO] Experiment parameters given below
2019-12-28 02:56:16,491 [INFO] 
{'experiment_num': 2, 'results_dir': 'results_selected_models/selected_kdd99_lstm_deep_rep2', 'model': 'lstm', 'model_type': 'classifier', 'normal_label': 'normal', 'lstm_time_steps': 32, 'lstm_layer_units': [32, 16], 'lstm_layer_activations': ['relu', 'relu'], 'lstm_layer_dropout_rates': [0.2, 0.2], 'batch_size': 256, 'epochs': 300, 'early_stop_patience': 50, 'dataset_dir': '../Datasets/full_datasets/kdd99_five_classes', 'description': 'selected_kdd99_lstm_deep_rep2'}
2019-12-28 02:56:16,491 [INFO] Created tensorboard log directory: results_selected_models/selected_kdd99_lstm_deep_rep2/tf_logs_run_2019_12_28-02_56_16
2019-12-28 02:56:16,491 [INFO] Loading datsets from: ../Datasets/full_datasets/kdd99_five_classes
2019-12-28 02:56:16,491 [INFO] Reading X, y files
2019-12-28 02:56:16,491 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/X_train.h5
2019-12-28 02:56:23,162 [INFO] Reading complete. time_to_read=6.67 seconds
2019-12-28 02:56:23,163 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/X_val.h5
2019-12-28 02:56:24,833 [INFO] Reading complete. time_to_read=1.67 seconds
2019-12-28 02:56:24,833 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/X_test.h5
2019-12-28 02:56:25,310 [INFO] Reading complete. time_to_read=0.48 seconds
2019-12-28 02:56:25,310 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/y_train.h5
2019-12-28 02:56:25,517 [INFO] Reading complete. time_to_read=0.21 seconds
2019-12-28 02:56:25,517 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/y_val.h5
2019-12-28 02:56:25,571 [INFO] Reading complete. time_to_read=0.05 seconds
2019-12-28 02:56:25,571 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/y_test.h5
2019-12-28 02:56:25,591 [INFO] Reading complete. time_to_read=0.02 seconds
2019-12-28 02:56:31,285 [INFO] Preparing flow sequences
2019-12-28 02:57:18,681 [INFO] Extracting flows complete. time_taken = 47.40 sec
2019-12-28 02:57:20,317 [INFO] Initializing model
2019-12-28 02:57:20,951 [INFO] _________________________________________________________________
2019-12-28 02:57:20,951 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-28 02:57:20,951 [INFO] =================================================================
2019-12-28 02:57:20,951 [INFO] lstm_8 (LSTM)                (None, 32, 32)            19968     
2019-12-28 02:57:20,951 [INFO] _________________________________________________________________
2019-12-28 02:57:20,952 [INFO] batch_normalization_8 (Batch (None, 32, 32)            128       
2019-12-28 02:57:20,952 [INFO] _________________________________________________________________
2019-12-28 02:57:20,952 [INFO] dropout_8 (Dropout)          (None, 32, 32)            0         
2019-12-28 02:57:20,952 [INFO] _________________________________________________________________
2019-12-28 02:57:20,952 [INFO] lstm_9 (LSTM)                (None, 32, 16)            3136      
2019-12-28 02:57:20,952 [INFO] _________________________________________________________________
2019-12-28 02:57:20,952 [INFO] batch_normalization_9 (Batch (None, 32, 16)            64        
2019-12-28 02:57:20,952 [INFO] _________________________________________________________________
2019-12-28 02:57:20,952 [INFO] dropout_9 (Dropout)          (None, 32, 16)            0         
2019-12-28 02:57:20,952 [INFO] _________________________________________________________________
2019-12-28 02:57:20,952 [INFO] time_distributed_7 (TimeDist (None, 32, 5)             85        
2019-12-28 02:57:20,953 [INFO] =================================================================
2019-12-28 02:57:20,953 [INFO] Total params: 23,381
2019-12-28 02:57:20,953 [INFO] Trainable params: 23,285
2019-12-28 02:57:20,953 [INFO] Non-trainable params: 96
2019-12-28 02:57:20,953 [INFO] _________________________________________________________________
2019-12-28 02:57:20,953 [INFO] Training model
 - val_f1: 0.9998
Epoch 00101: early stopping
Train on 122460 samples, validate on 30615 samples
Epoch 1/300
 - 49s - loss: 0.0802 - val_loss: 0.0041
 - val_f1: 0.9981
Epoch 2/300
 - 46s - loss: 0.0046 - val_loss: 0.0021
 - val_f1: 0.9989
Epoch 3/300
 - 47s - loss: 0.0028 - val_loss: 0.0014
 - val_f1: 0.9991
Epoch 4/300
 - 47s - loss: 0.0020 - val_loss: 0.0011
 - val_f1: 0.9992
Epoch 5/300
 - 47s - loss: 0.0015 - val_loss: 8.2617e-04
 - val_f1: 0.9994
Epoch 6/300
 - 47s - loss: 0.0012 - val_loss: 7.5111e-04
 - val_f1: 0.9995
Epoch 7/300
 - 47s - loss: 0.0010 - val_loss: 6.4654e-04
 - val_f1: 0.9996
Epoch 8/300
 - 47s - loss: 8.8806e-04 - val_loss: 6.2393e-04
 - val_f1: 0.9996
Epoch 9/300
 - 47s - loss: 8.0894e-04 - val_loss: 5.4205e-04
 - val_f1: 0.9996
Epoch 10/300
 - 47s - loss: 7.3509e-04 - val_loss: 5.1483e-04
 - val_f1: 0.9997
Epoch 11/300
 - 46s - loss: 6.7914e-04 - val_loss: 5.2986e-04
 - val_f1: 0.9997
Epoch 12/300
 - 46s - loss: 6.4316e-04 - val_loss: 4.5632e-04
 - val_f1: 0.9997
Epoch 13/300
 - 46s - loss: 5.9175e-04 - val_loss: 4.5667e-04
 - val_f1: 0.9997
Epoch 14/300
 - 46s - loss: 5.6287e-04 - val_loss: 4.3302e-04
 - val_f1: 0.9997
Epoch 15/300
 - 46s - loss: 5.4629e-04 - val_loss: 3.9631e-04
 - val_f1: 0.9998
Epoch 16/300
 - 46s - loss: 5.0532e-04 - val_loss: 3.9502e-04
 - val_f1: 0.9997
Epoch 17/300
 - 46s - loss: 4.9634e-04 - val_loss: 3.9960e-04
 - val_f1: 0.9998
Epoch 18/300
 - 46s - loss: 4.6989e-04 - val_loss: 3.7991e-04
 - val_f1: 0.9998
Epoch 19/300
 - 46s - loss: 4.5334e-04 - val_loss: 3.7550e-04
 - val_f1: 0.9998
Epoch 20/300
 - 46s - loss: 4.4245e-04 - val_loss: 3.8611e-04
 - val_f1: 0.9998
Epoch 21/300
 - 46s - loss: 4.1326e-04 - val_loss: 3.4475e-04
 - val_f1: 0.9998
Epoch 22/300
 - 46s - loss: 3.8772e-04 - val_loss: 3.4811e-04
 - val_f1: 0.9998
Epoch 23/300
 - 46s - loss: 3.8143e-04 - val_loss: 3.5336e-04
 - val_f1: 0.9998
Epoch 24/300
 - 46s - loss: 3.7648e-04 - val_loss: 3.2219e-04
 - val_f1: 0.9998
Epoch 25/300
 - 46s - loss: 3.6396e-04 - val_loss: 3.4447e-04
 - val_f1: 0.9998
Epoch 26/300
 - 46s - loss: 3.5874e-04 - val_loss: 3.5403e-04
 - val_f1: 0.9998
Epoch 27/300
 - 46s - loss: 3.4910e-04 - val_loss: 3.6383e-04
 - val_f1: 0.9998
Epoch 28/300
 - 46s - loss: 3.4289e-04 - val_loss: 3.4389e-04
 - val_f1: 0.9998
Epoch 29/300
 - 46s - loss: 3.3708e-04 - val_loss: 3.5536e-04
 - val_f1: 0.9998
Epoch 30/300
 - 46s - loss: 3.3285e-04 - val_loss: 3.2128e-04
 - val_f1: 0.9998
Epoch 31/300
 - 46s - loss: 3.2078e-04 - val_loss: 3.1620e-04
2019-12-28 03:27:07,349 [INFO] epoch = 30. Intermediate model saved to results_selected_models/selected_kdd99_lstm_deep_rep2/_model_epoch_30.pickle
 - val_f1: 0.9998
Epoch 32/300
 - 46s - loss: 3.0851e-04 - val_loss: 3.2714e-04
 - val_f1: 0.9998
Epoch 33/300
 - 46s - loss: 3.0249e-04 - val_loss: 3.1400e-04
 - val_f1: 0.9998
Epoch 34/300
 - 46s - loss: 3.1758e-04 - val_loss: 3.4629e-04
 - val_f1: 0.9998
Epoch 35/300
 - 46s - loss: 3.1228e-04 - val_loss: 3.4104e-04
 - val_f1: 0.9998
Epoch 36/300
 - 46s - loss: 2.8597e-04 - val_loss: 3.3118e-04
 - val_f1: 0.9998
Epoch 37/300
 - 46s - loss: 2.9002e-04 - val_loss: 3.2100e-04
 - val_f1: 0.9998
Epoch 38/300
 - 46s - loss: 2.7657e-04 - val_loss: 3.1781e-04
 - val_f1: 0.9998
Epoch 39/300
 - 46s - loss: 2.8259e-04 - val_loss: 3.1206e-04
 - val_f1: 0.9998
Epoch 40/300
 - 46s - loss: 2.7067e-04 - val_loss: 3.3473e-04
 - val_f1: 0.9998
Epoch 41/300
 - 46s - loss: 2.8011e-04 - val_loss: 3.3043e-04
 - val_f1: 0.9998
Epoch 42/300
 - 46s - loss: 2.7519e-04 - val_loss: 3.1768e-04
 - val_f1: 0.9998
Epoch 43/300
 - 46s - loss: 2.7211e-04 - val_loss: 3.2921e-04
 - val_f1: 0.9998
Epoch 44/300
 - 46s - loss: 2.8865e-04 - val_loss: 3.2913e-04
 - val_f1: 0.9998
Epoch 45/300
 - 46s - loss: 2.7124e-04 - val_loss: 3.3344e-04
 - val_f1: 0.9998
Epoch 46/300
 - 46s - loss: 2.7367e-04 - val_loss: 3.4454e-04
 - val_f1: 0.9998
Epoch 47/300
 - 46s - loss: 2.7530e-04 - val_loss: 3.3939e-04
 - val_f1: 0.9998
Epoch 48/300
 - 46s - loss: 2.6757e-04 - val_loss: 3.0988e-04
 - val_f1: 0.9998
Epoch 49/300
 - 46s - loss: 2.4476e-04 - val_loss: 3.0662e-04
 - val_f1: 0.9998
Epoch 50/300
 - 46s - loss: 2.5468e-04 - val_loss: 3.0753e-04
 - val_f1: 0.9998
Epoch 51/300
 - 46s - loss: 2.5628e-04 - val_loss: 3.1655e-04
 - val_f1: 0.9998
Epoch 52/300
 - 46s - loss: 2.4122e-04 - val_loss: 3.2480e-04
 - val_f1: 0.9998
Epoch 53/300
 - 46s - loss: 2.4391e-04 - val_loss: 3.3096e-04
 - val_f1: 0.9998
Epoch 54/300
 - 46s - loss: 2.4761e-04 - val_loss: 3.3309e-04
 - val_f1: 0.9998
Epoch 55/300
 - 46s - loss: 2.6052e-04 - val_loss: 3.1387e-04
 - val_f1: 0.9998
Epoch 56/300
 - 46s - loss: 2.3918e-04 - val_loss: 3.2826e-04
 - val_f1: 0.9998
Epoch 57/300
 - 46s - loss: 2.2379e-04 - val_loss: 3.0992e-04
 - val_f1: 0.9998
Epoch 58/300
 - 46s - loss: 2.3038e-04 - val_loss: 3.0876e-04
 - val_f1: 0.9998
Epoch 59/300
 - 46s - loss: 2.4119e-04 - val_loss: 3.1019e-04
 - val_f1: 0.9999
Epoch 60/300
 - 46s - loss: 2.2137e-04 - val_loss: 3.2112e-04
 - val_f1: 0.9998
Epoch 61/300
 - 46s - loss: 2.3243e-04 - val_loss: 3.3936e-04
2019-12-28 03:55:52,339 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_kdd99_lstm_deep_rep2/_model_epoch_60.pickle
 - val_f1: 0.9998
Epoch 62/300
 - 46s - loss: 2.2047e-04 - val_loss: 3.1581e-04
 - val_f1: 0.9998
Epoch 63/300
 - 46s - loss: 2.3375e-04 - val_loss: 3.0978e-04
 - val_f1: 0.9998
Epoch 64/300
 - 46s - loss: 2.3832e-04 - val_loss: 3.1779e-04
 - val_f1: 0.9998
Epoch 65/300
 - 46s - loss: 2.3446e-04 - val_loss: 3.1589e-04
 - val_f1: 0.9998
Epoch 66/300
 - 46s - loss: 2.3398e-04 - val_loss: 3.2465e-04
 - val_f1: 0.9998
Epoch 67/300
 - 46s - loss: 2.3391e-04 - val_loss: 3.2987e-04
 - val_f1: 0.9998
Epoch 68/300
 - 46s - loss: 2.2276e-04 - val_loss: 3.3871e-04
 - val_f1: 0.9998
Epoch 69/300
 - 46s - loss: 2.1369e-04 - val_loss: 3.5675e-04
 - val_f1: 0.9998
Epoch 70/300
 - 46s - loss: 2.0413e-04 - val_loss: 3.1424e-04
 - val_f1: 0.9998
Epoch 71/300
 - 46s - loss: 2.1344e-04 - val_loss: 3.1511e-04
 - val_f1: 0.9998
Epoch 72/300
 - 46s - loss: 2.0853e-04 - val_loss: 3.3425e-04
 - val_f1: 0.9998
Epoch 73/300
 - 46s - loss: 2.2214e-04 - val_loss: 3.3177e-04
 - val_f1: 0.9998
Epoch 74/300
 - 46s - loss: 2.0423e-04 - val_loss: 3.3401e-04
 - val_f1: 0.9998
Epoch 75/300
 - 46s - loss: 2.0105e-04 - val_loss: 3.1903e-04
 - val_f1: 0.9998
Epoch 76/300
 - 46s - loss: 1.9775e-04 - val_loss: 3.3102e-04
 - val_f1: 0.9998
Epoch 77/300
 - 46s - loss: 1.9216e-04 - val_loss: 3.5916e-04
 - val_f1: 0.9998
Epoch 78/300
 - 46s - loss: 1.9748e-04 - val_loss: 3.3223e-04
 - val_f1: 0.9998
Epoch 79/300
 - 46s - loss: 1.9673e-04 - val_loss: 3.4027e-04
 - val_f1: 0.9998
Epoch 80/300
 - 46s - loss: 1.9759e-04 - val_loss: 3.0997e-04
 - val_f1: 0.9998
Epoch 81/300
 - 46s - loss: 1.8969e-04 - val_loss: 4.7367e-04
 - val_f1: 0.9998
Epoch 82/300
 - 46s - loss: 2.0314e-04 - val_loss: 3.3721e-04
 - val_f1: 0.9998
Epoch 83/300
 - 46s - loss: 1.9994e-04 - val_loss: 3.3428e-04
 - val_f1: 0.9998
Epoch 84/300
 - 46s - loss: 2.0446e-04 - val_loss: 3.4657e-04
 - val_f1: 0.9998
Epoch 85/300
 - 46s - loss: 2.0439e-04 - val_loss: 3.4028e-04
 - val_f1: 0.9998
Epoch 86/300
 - 46s - loss: 1.9294e-04 - val_loss: 3.4291e-04
 - val_f1: 0.9998
Epoch 87/300
 - 46s - loss: 1.9037e-04 - val_loss: 3.7412e-04
 - val_f1: 0.9998
Epoch 88/300
 - 46s - loss: 1.9349e-04 - val_loss: 3.4525e-04
 - val_f1: 0.9998
Epoch 89/300
 - 46s - loss: 1.7979e-04 - val_loss: 3.6404e-04
 - val_f1: 0.9998
Epoch 90/300
 - 46s - loss: 1.8896e-04 - val_loss: 3.5612e-04
 - val_f1: 0.9998
Epoch 91/300
 - 46s - loss: 1.7628e-04 - val_loss: 3.2898e-04
2019-12-28 04:24:35,515 [INFO] epoch = 90. Intermediate model saved to results_selected_models/selected_kdd99_lstm_deep_rep2/_model_epoch_90.pickle
 - val_f1: 0.9998
Epoch 92/300
 - 46s - loss: 1.8037e-04 - val_loss: 3.4243e-04
 - val_f1: 0.9998
Epoch 93/300
 - 46s - loss: 1.7333e-04 - val_loss: 3.6752e-04
 - val_f1: 0.9998
Epoch 94/300
 - 46s - loss: 1.8302e-04 - val_loss: 3.7134e-04
 - val_f1: 0.9998
Epoch 95/300
 - 46s - loss: 1.7971e-04 - val_loss: 3.4680e-04
 - val_f1: 0.9998
Epoch 96/300
 - 46s - loss: 1.7888e-04 - val_loss: 3.5298e-04
 - val_f1: 0.9998
Epoch 97/300
 - 46s - loss: 1.7478e-04 - val_loss: 3.6961e-04
 - val_f1: 0.9998
Epoch 98/300
 - 46s - loss: 1.7695e-04 - val_loss: 3.6409e-04
 - val_f1: 0.9998
Epoch 99/300
 - 46s - loss: 1.7168e-04 - val_loss: 3.5725e-04
2019-12-28 04:32:26,376 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-28 04:33:20,744 [INFO] Last epoch loss evaluation: train_loss = 0.000153, val_loss = 0.000307
2019-12-28 04:33:20,744 [INFO] Training complete. time_to_train = 5759.79 sec, 96.00 min
2019-12-28 04:33:20,750 [INFO] Model saved to results_selected_models/selected_kdd99_lstm_deep_rep2/best_model.pickle
2019-12-28 04:33:20,752 [INFO] Training history saved to: results_selected_models/selected_kdd99_lstm_deep_rep2/training_error_history.csv
2019-12-28 04:33:20,938 [INFO] Plot saved to: results_selected_models/selected_kdd99_lstm_deep_rep2/training_error_history.png
2019-12-28 04:33:21,108 [INFO] Plot saved to: results_selected_models/selected_kdd99_lstm_deep_rep2/training_f1_history.png
2019-12-28 04:33:21,108 [INFO] Making predictions on training, validation, testing data
2019-12-28 04:34:17,076 [INFO] Evaluating predictions (results)
2019-12-28 04:34:25,767 [INFO] Dataset: Testing. Classification report below
2019-12-28 04:34:25,767 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      0.97      0.99    229853
     normal.       0.73      0.98      0.84     60580
       probe       0.86      0.76      0.81      4166
         r2l       0.64      0.05      0.10     13773
         u2r       0.50      0.00      0.00      2636

    accuracy                           0.92    311008
   macro avg       0.75      0.55      0.55    311008
weighted avg       0.92      0.92      0.91    311008

2019-12-28 04:34:25,767 [INFO] Overall accuracy (micro avg): 0.9245807181808828
2019-12-28 04:34:35,102 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9246         0.9246                       0.9246                0.0189                   0.0754  0.9246
1     Macro avg        0.9698         0.7461                       0.5547                0.0195                   0.4453  0.5471
2  Weighted avg        0.9684         0.9236                       0.9246                0.0223                   0.0754  0.9073
2019-12-28 04:35:05,656 [INFO] Dataset: Validation. Classification report below
2019-12-28 04:35:05,656 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00    776671
     normal.       1.00      1.00      1.00    194553
       probe       1.00      0.99      1.00      8221
         r2l       0.89      0.88      0.89       225
         u2r       0.67      0.20      0.31        10

    accuracy                           1.00    979680
   macro avg       0.91      0.82      0.84    979680
weighted avg       1.00      1.00      1.00    979680

2019-12-28 04:35:05,656 [INFO] Overall accuracy (micro avg): 0.9998356606238772
2019-12-28 04:35:38,617 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9998         0.9998                       0.9998                0.0000                   0.0002  0.9998
1     Macro avg        0.9999         0.9111                       0.8156                0.0001                   0.1844  0.8382
2  Weighted avg        0.9999         0.9998                       0.9998                0.0001                   0.0002  0.9998
2019-12-28 04:37:51,587 [INFO] Dataset: Training. Classification report below
2019-12-28 04:37:51,587 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00   3106675
     normal.       1.00      1.00      1.00    778221
       probe       1.00      1.00      1.00     32881
         r2l       0.91      0.94      0.92       901
         u2r       0.89      0.40      0.56        42

    accuracy                           1.00   3918720
   macro avg       0.96      0.87      0.90   3918720
weighted avg       1.00      1.00      1.00   3918720

2019-12-28 04:37:51,587 [INFO] Overall accuracy (micro avg): 0.9998946084435734
2019-12-28 04:40:15,105 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9999         0.9999                       0.9999                0.0000                   0.0001  0.9999
1     Macro avg        1.0000         0.9605                       0.8676                0.0000                   0.1324  0.8955
2  Weighted avg        1.0000         0.9999                       0.9999                0.0001                   0.0001  0.9999
2019-12-28 04:40:15,153 [INFO] Results saved to: results_selected_models/selected_kdd99_lstm_deep_rep2/selected_kdd99_lstm_deep_rep2_results.xlsx
2019-12-28 04:40:15,160 [INFO] ================= Finished running experiment no. 2 ================= 

2019-12-28 04:40:15,275 [INFO] Created directory: results_selected_models/selected_kdd99_lstm_deep_rep3
2019-12-28 04:40:15,275 [INFO] Initialized logging. log_filename = results_selected_models/selected_kdd99_lstm_deep_rep3/run_log.log
2019-12-28 04:40:15,276 [INFO] ================= Running experiment no. 3  ================= 

2019-12-28 04:40:15,276 [INFO] Experiment parameters given below
2019-12-28 04:40:15,276 [INFO] 
{'experiment_num': 3, 'results_dir': 'results_selected_models/selected_kdd99_lstm_deep_rep3', 'model': 'lstm', 'model_type': 'classifier', 'normal_label': 'normal', 'lstm_time_steps': 32, 'lstm_layer_units': [32, 16], 'lstm_layer_activations': ['relu', 'relu'], 'lstm_layer_dropout_rates': [0.2, 0.2], 'batch_size': 256, 'epochs': 300, 'early_stop_patience': 50, 'dataset_dir': '../Datasets/full_datasets/kdd99_five_classes', 'description': 'selected_kdd99_lstm_deep_rep3'}
2019-12-28 04:40:15,276 [INFO] Created tensorboard log directory: results_selected_models/selected_kdd99_lstm_deep_rep3/tf_logs_run_2019_12_28-04_40_15
2019-12-28 04:40:15,276 [INFO] Loading datsets from: ../Datasets/full_datasets/kdd99_five_classes
2019-12-28 04:40:15,276 [INFO] Reading X, y files
2019-12-28 04:40:15,276 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/X_train.h5
2019-12-28 04:40:21,911 [INFO] Reading complete. time_to_read=6.63 seconds
2019-12-28 04:40:21,911 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/X_val.h5
2019-12-28 04:40:23,598 [INFO] Reading complete. time_to_read=1.69 seconds
2019-12-28 04:40:23,598 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/X_test.h5
2019-12-28 04:40:24,077 [INFO] Reading complete. time_to_read=0.48 seconds
2019-12-28 04:40:24,077 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/y_train.h5
2019-12-28 04:40:24,294 [INFO] Reading complete. time_to_read=0.22 seconds
2019-12-28 04:40:24,294 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/y_val.h5
2019-12-28 04:40:24,348 [INFO] Reading complete. time_to_read=0.05 seconds
2019-12-28 04:40:24,348 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/y_test.h5
2019-12-28 04:40:24,367 [INFO] Reading complete. time_to_read=0.02 seconds
2019-12-28 04:40:30,078 [INFO] Preparing flow sequences
2019-12-28 04:41:17,492 [INFO] Extracting flows complete. time_taken = 47.41 sec
2019-12-28 04:41:19,108 [INFO] Initializing model
2019-12-28 04:41:19,558 [INFO] _________________________________________________________________
2019-12-28 04:41:19,558 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-28 04:41:19,558 [INFO] =================================================================
2019-12-28 04:41:19,558 [INFO] lstm_10 (LSTM)               (None, 32, 32)            19968     
2019-12-28 04:41:19,558 [INFO] _________________________________________________________________
2019-12-28 04:41:19,558 [INFO] batch_normalization_10 (Batc (None, 32, 32)            128       
2019-12-28 04:41:19,558 [INFO] _________________________________________________________________
2019-12-28 04:41:19,558 [INFO] dropout_10 (Dropout)         (None, 32, 32)            0         
2019-12-28 04:41:19,558 [INFO] _________________________________________________________________
2019-12-28 04:41:19,559 [INFO] lstm_11 (LSTM)               (None, 32, 16)            3136      
2019-12-28 04:41:19,559 [INFO] _________________________________________________________________
2019-12-28 04:41:19,559 [INFO] batch_normalization_11 (Batc (None, 32, 16)            64        
2019-12-28 04:41:19,559 [INFO] _________________________________________________________________
2019-12-28 04:41:19,559 [INFO] dropout_11 (Dropout)         (None, 32, 16)            0         
2019-12-28 04:41:19,559 [INFO] _________________________________________________________________
2019-12-28 04:41:19,559 [INFO] time_distributed_8 (TimeDist (None, 32, 5)             85        
2019-12-28 04:41:19,559 [INFO] =================================================================
2019-12-28 04:41:19,559 [INFO] Total params: 23,381
2019-12-28 04:41:19,559 [INFO] Trainable params: 23,285
2019-12-28 04:41:19,559 [INFO] Non-trainable params: 96
2019-12-28 04:41:19,560 [INFO] _________________________________________________________________
2019-12-28 04:41:19,560 [INFO] Training model
 - val_f1: 0.9998
Epoch 00099: early stopping
Train on 122460 samples, validate on 30615 samples
Epoch 1/300
 - 49s - loss: 0.1090 - val_loss: 0.0059
 - val_f1: 0.9971
Epoch 2/300
 - 46s - loss: 0.0062 - val_loss: 0.0024
 - val_f1: 0.9987
Epoch 3/300
 - 46s - loss: 0.0036 - val_loss: 0.0018
 - val_f1: 0.9989
Epoch 4/300
 - 46s - loss: 0.0025 - val_loss: 0.0013
 - val_f1: 0.9991
Epoch 5/300
 - 46s - loss: 0.0020 - val_loss: 0.0011
 - val_f1: 0.9992
Epoch 6/300
 - 46s - loss: 0.0016 - val_loss: 8.7996e-04
 - val_f1: 0.9993
Epoch 7/300
 - 46s - loss: 0.0014 - val_loss: 7.6356e-04
 - val_f1: 0.9994
Epoch 8/300
 - 46s - loss: 0.0012 - val_loss: 6.5818e-04
 - val_f1: 0.9996
Epoch 9/300
 - 46s - loss: 0.0011 - val_loss: 6.0626e-04
 - val_f1: 0.9996
Epoch 10/300
 - 46s - loss: 9.5233e-04 - val_loss: 5.5930e-04
 - val_f1: 0.9997
Epoch 11/300
 - 46s - loss: 8.7281e-04 - val_loss: 5.3144e-04
 - val_f1: 0.9997
Epoch 12/300
 - 46s - loss: 8.2532e-04 - val_loss: 5.1235e-04
 - val_f1: 0.9997
Epoch 13/300
 - 46s - loss: 7.4911e-04 - val_loss: 4.9907e-04
 - val_f1: 0.9997
Epoch 14/300
 - 46s - loss: 7.1156e-04 - val_loss: 4.7481e-04
 - val_f1: 0.9997
Epoch 15/300
 - 46s - loss: 6.6385e-04 - val_loss: 4.7920e-04
 - val_f1: 0.9997
Epoch 16/300
 - 46s - loss: 6.0688e-04 - val_loss: 4.4400e-04
 - val_f1: 0.9997
Epoch 17/300
 - 46s - loss: 5.9852e-04 - val_loss: 4.1634e-04
 - val_f1: 0.9997
Epoch 18/300
 - 46s - loss: 5.6576e-04 - val_loss: 3.9873e-04
 - val_f1: 0.9998
Epoch 19/300
 - 46s - loss: 5.3286e-04 - val_loss: 4.1899e-04
 - val_f1: 0.9997
Epoch 20/300
 - 46s - loss: 5.1624e-04 - val_loss: 3.8357e-04
 - val_f1: 0.9998
Epoch 21/300
 - 46s - loss: 4.9439e-04 - val_loss: 3.8661e-04
 - val_f1: 0.9997
Epoch 22/300
 - 46s - loss: 4.8759e-04 - val_loss: 3.9247e-04
 - val_f1: 0.9997
Epoch 23/300
 - 46s - loss: 4.3957e-04 - val_loss: 3.6026e-04
 - val_f1: 0.9998
Epoch 24/300
 - 46s - loss: 4.5501e-04 - val_loss: 3.9033e-04
 - val_f1: 0.9998
Epoch 25/300
 - 46s - loss: 4.2157e-04 - val_loss: 3.6348e-04
 - val_f1: 0.9998
Epoch 26/300
 - 46s - loss: 4.1290e-04 - val_loss: 4.3603e-04
 - val_f1: 0.9998
Epoch 27/300
 - 46s - loss: 4.0528e-04 - val_loss: 3.5380e-04
 - val_f1: 0.9998
Epoch 28/300
 - 46s - loss: 3.9159e-04 - val_loss: 3.7531e-04
 - val_f1: 0.9998
Epoch 29/300
 - 46s - loss: 3.8879e-04 - val_loss: 3.5897e-04
 - val_f1: 0.9998
Epoch 30/300
 - 46s - loss: 3.8365e-04 - val_loss: 3.3993e-04
 - val_f1: 0.9998
Epoch 31/300
 - 46s - loss: 3.6398e-04 - val_loss: 3.3996e-04
2019-12-28 05:11:05,752 [INFO] epoch = 30. Intermediate model saved to results_selected_models/selected_kdd99_lstm_deep_rep3/_model_epoch_30.pickle
 - val_f1: 0.9998
Epoch 32/300
 - 46s - loss: 3.5411e-04 - val_loss: 3.5640e-04
 - val_f1: 0.9998
Epoch 33/300
 - 46s - loss: 3.6577e-04 - val_loss: 3.2473e-04
 - val_f1: 0.9998
Epoch 34/300
 - 46s - loss: 3.3702e-04 - val_loss: 3.4108e-04
 - val_f1: 0.9998
Epoch 35/300
 - 46s - loss: 3.4701e-04 - val_loss: 3.3436e-04
 - val_f1: 0.9998
Epoch 36/300
 - 46s - loss: 3.4068e-04 - val_loss: 3.3601e-04
 - val_f1: 0.9998
Epoch 37/300
 - 46s - loss: 3.3434e-04 - val_loss: 3.2303e-04
 - val_f1: 0.9998
Epoch 38/300
 - 46s - loss: 3.1924e-04 - val_loss: 3.2348e-04
 - val_f1: 0.9998
Epoch 39/300
 - 46s - loss: 3.1916e-04 - val_loss: 3.4664e-04
 - val_f1: 0.9998
Epoch 40/300
 - 46s - loss: 3.0498e-04 - val_loss: 3.5022e-04
 - val_f1: 0.9998
Epoch 41/300
 - 46s - loss: 3.1232e-04 - val_loss: 3.2243e-04
 - val_f1: 0.9998
Epoch 42/300
 - 46s - loss: 3.0466e-04 - val_loss: 3.4925e-04
 - val_f1: 0.9998
Epoch 43/300
 - 46s - loss: 2.9831e-04 - val_loss: 3.4162e-04
 - val_f1: 0.9998
Epoch 44/300
 - 46s - loss: 2.9138e-04 - val_loss: 3.4494e-04
 - val_f1: 0.9998
Epoch 45/300
 - 46s - loss: 3.0759e-04 - val_loss: 3.1922e-04
 - val_f1: 0.9998
Epoch 46/300
 - 46s - loss: 2.8421e-04 - val_loss: 3.2141e-04
 - val_f1: 0.9998
Epoch 47/300
 - 46s - loss: 2.9097e-04 - val_loss: 3.3551e-04
 - val_f1: 0.9998
Epoch 48/300
 - 46s - loss: 2.7935e-04 - val_loss: 3.1908e-04
 - val_f1: 0.9998
Epoch 49/300
 - 46s - loss: 2.9313e-04 - val_loss: 3.2173e-04
 - val_f1: 0.9998
Epoch 50/300
 - 46s - loss: 2.9092e-04 - val_loss: 3.2326e-04
 - val_f1: 0.9998
Epoch 51/300
 - 46s - loss: 2.6713e-04 - val_loss: 3.2681e-04
 - val_f1: 0.9998
Epoch 52/300
 - 46s - loss: 2.8446e-04 - val_loss: 3.2891e-04
 - val_f1: 0.9998
Epoch 53/300
 - 46s - loss: 2.8255e-04 - val_loss: 3.3130e-04
 - val_f1: 0.9998
Epoch 54/300
 - 46s - loss: 2.5784e-04 - val_loss: 3.2787e-04
 - val_f1: 0.9998
Epoch 55/300
 - 46s - loss: 2.6339e-04 - val_loss: 3.1240e-04
 - val_f1: 0.9998
Epoch 56/300
 - 46s - loss: 2.5899e-04 - val_loss: 3.4465e-04
 - val_f1: 0.9998
Epoch 57/300
 - 46s - loss: 2.5892e-04 - val_loss: 3.1003e-04
 - val_f1: 0.9998
Epoch 58/300
 - 46s - loss: 2.5522e-04 - val_loss: 3.1212e-04
 - val_f1: 0.9998
Epoch 59/300
 - 46s - loss: 2.6579e-04 - val_loss: 3.2774e-04
 - val_f1: 0.9998
Epoch 60/300
 - 46s - loss: 2.4861e-04 - val_loss: 3.3414e-04
 - val_f1: 0.9998
Epoch 61/300
 - 46s - loss: 2.5289e-04 - val_loss: 3.2620e-04
2019-12-28 05:39:56,699 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_kdd99_lstm_deep_rep3/_model_epoch_60.pickle
 - val_f1: 0.9998
Epoch 62/300
 - 46s - loss: 2.5167e-04 - val_loss: 3.1798e-04
 - val_f1: 0.9998
Epoch 63/300
 - 46s - loss: 2.4485e-04 - val_loss: 3.2563e-04
 - val_f1: 0.9998
Epoch 64/300
 - 46s - loss: 2.4168e-04 - val_loss: 3.4607e-04
 - val_f1: 0.9998
Epoch 65/300
 - 46s - loss: 2.4355e-04 - val_loss: 3.4210e-04
 - val_f1: 0.9998
Epoch 66/300
 - 46s - loss: 2.3823e-04 - val_loss: 3.3522e-04
 - val_f1: 0.9998
Epoch 67/300
 - 46s - loss: 2.3816e-04 - val_loss: 3.4111e-04
 - val_f1: 0.9998
Epoch 68/300
 - 46s - loss: 2.3665e-04 - val_loss: 3.3242e-04
 - val_f1: 0.9998
Epoch 69/300
 - 46s - loss: 2.2848e-04 - val_loss: 3.2693e-04
 - val_f1: 0.9998
Epoch 70/300
 - 46s - loss: 2.2540e-04 - val_loss: 3.2836e-04
 - val_f1: 0.9998
Epoch 71/300
 - 46s - loss: 2.3636e-04 - val_loss: 3.3520e-04
 - val_f1: 0.9998
Epoch 72/300
 - 46s - loss: 2.2606e-04 - val_loss: 3.3662e-04
 - val_f1: 0.9998
Epoch 73/300
 - 46s - loss: 2.3060e-04 - val_loss: 3.3502e-04
 - val_f1: 0.9998
Epoch 74/300
 - 46s - loss: 2.2149e-04 - val_loss: 3.3984e-04
 - val_f1: 0.9998
Epoch 75/300
 - 46s - loss: 2.2885e-04 - val_loss: 3.4005e-04
 - val_f1: 0.9998
Epoch 76/300
 - 46s - loss: 2.2876e-04 - val_loss: 3.0542e-04
 - val_f1: 0.9998
Epoch 77/300
 - 46s - loss: 2.2047e-04 - val_loss: 3.3076e-04
 - val_f1: 0.9998
Epoch 78/300
 - 46s - loss: 2.1509e-04 - val_loss: 3.2798e-04
 - val_f1: 0.9998
Epoch 79/300
 - 46s - loss: 2.0904e-04 - val_loss: 3.2457e-04
 - val_f1: 0.9998
Epoch 80/300
 - 46s - loss: 2.2718e-04 - val_loss: 3.3500e-04
 - val_f1: 0.9998
Epoch 81/300
 - 46s - loss: 2.1551e-04 - val_loss: 3.4303e-04
 - val_f1: 0.9998
Epoch 82/300
 - 46s - loss: 2.0964e-04 - val_loss: 3.3537e-04
 - val_f1: 0.9998
Epoch 83/300
 - 46s - loss: 2.1847e-04 - val_loss: 3.2496e-04
 - val_f1: 0.9998
Epoch 84/300
 - 46s - loss: 2.1770e-04 - val_loss: 3.3169e-04
 - val_f1: 0.9998
Epoch 85/300
 - 46s - loss: 2.0742e-04 - val_loss: 3.4101e-04
 - val_f1: 0.9998
Epoch 86/300
 - 46s - loss: 2.2380e-04 - val_loss: 3.3261e-04
 - val_f1: 0.9998
Epoch 87/300
 - 46s - loss: 2.1326e-04 - val_loss: 3.7254e-04
 - val_f1: 0.9998
Epoch 88/300
 - 46s - loss: 2.1416e-04 - val_loss: 3.4776e-04
 - val_f1: 0.9998
Epoch 89/300
 - 46s - loss: 2.0863e-04 - val_loss: 3.6446e-04
 - val_f1: 0.9998
Epoch 90/300
 - 47s - loss: 2.1485e-04 - val_loss: 3.5094e-04
 - val_f1: 0.9998
Epoch 91/300
 - 47s - loss: 2.1042e-04 - val_loss: 3.5060e-04
2019-12-28 06:08:48,550 [INFO] epoch = 90. Intermediate model saved to results_selected_models/selected_kdd99_lstm_deep_rep3/_model_epoch_90.pickle
 - val_f1: 0.9998
Epoch 92/300
 - 47s - loss: 1.9829e-04 - val_loss: 3.3823e-04
 - val_f1: 0.9998
Epoch 93/300
 - 51s - loss: 2.0377e-04 - val_loss: 3.3785e-04
 - val_f1: 0.9998
Epoch 94/300
 - 47s - loss: 1.9667e-04 - val_loss: 3.3867e-04
 - val_f1: 0.9998
Epoch 95/300
 - 47s - loss: 1.9838e-04 - val_loss: 3.3805e-04
 - val_f1: 0.9998
Epoch 96/300
 - 47s - loss: 2.0295e-04 - val_loss: 3.7154e-04
 - val_f1: 0.9998
Epoch 97/300
 - 47s - loss: 1.8387e-04 - val_loss: 3.7415e-04
 - val_f1: 0.9998
Epoch 98/300
 - 47s - loss: 1.9356e-04 - val_loss: 3.9735e-04
 - val_f1: 0.9998
Epoch 99/300
 - 47s - loss: 2.1438e-04 - val_loss: 3.9692e-04
 - val_f1: 0.9998
Epoch 100/300
 - 47s - loss: 1.9563e-04 - val_loss: 3.6179e-04
 - val_f1: 0.9998
Epoch 101/300
 - 47s - loss: 1.9626e-04 - val_loss: 3.9133e-04
 - val_f1: 0.9998
Epoch 102/300
 - 47s - loss: 1.8608e-04 - val_loss: 3.8243e-04
 - val_f1: 0.9998
Epoch 103/300
 - 47s - loss: 1.9273e-04 - val_loss: 3.4825e-04
 - val_f1: 0.9998
Epoch 104/300
 - 47s - loss: 1.9306e-04 - val_loss: 3.6499e-04
 - val_f1: 0.9998
Epoch 105/300
 - 47s - loss: 1.8299e-04 - val_loss: 4.0219e-04
 - val_f1: 0.9998
Epoch 106/300
 - 46s - loss: 1.9188e-04 - val_loss: 3.8778e-04
 - val_f1: 0.9998
Epoch 107/300
 - 47s - loss: 1.7698e-04 - val_loss: 3.8512e-04
 - val_f1: 0.9998
Epoch 108/300
 - 47s - loss: 1.7831e-04 - val_loss: 3.8338e-04
 - val_f1: 0.9998
Epoch 109/300
 - 47s - loss: 1.8792e-04 - val_loss: 3.7858e-04
 - val_f1: 0.9998
Epoch 110/300
 - 47s - loss: 1.8940e-04 - val_loss: 3.8516e-04
 - val_f1: 0.9998
Epoch 111/300
 - 47s - loss: 1.6965e-04 - val_loss: 3.6533e-04
 - val_f1: 0.9998
Epoch 112/300
 - 47s - loss: 1.7092e-04 - val_loss: 3.6937e-04
 - val_f1: 0.9998
Epoch 113/300
 - 47s - loss: 1.7808e-04 - val_loss: 3.7476e-04
 - val_f1: 0.9998
Epoch 114/300
 - 47s - loss: 1.7506e-04 - val_loss: 3.6491e-04
 - val_f1: 0.9998
Epoch 115/300
 - 47s - loss: 1.6534e-04 - val_loss: 3.4399e-04
 - val_f1: 0.9998
Epoch 116/300
 - 47s - loss: 1.6867e-04 - val_loss: 3.9054e-04
 - val_f1: 0.9998
Epoch 117/300
 - 47s - loss: 1.7664e-04 - val_loss: 3.9404e-04
 - val_f1: 0.9998
Epoch 118/300
 - 47s - loss: 1.6223e-04 - val_loss: 3.8814e-04
 - val_f1: 0.9998
Epoch 119/300
 - 47s - loss: 1.7964e-04 - val_loss: 3.7551e-04
 - val_f1: 0.9998
Epoch 120/300
 - 47s - loss: 1.7243e-04 - val_loss: 3.8126e-04
 - val_f1: 0.9998
Epoch 121/300
 - 47s - loss: 1.6486e-04 - val_loss: 3.7569e-04
2019-12-28 06:37:50,060 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_kdd99_lstm_deep_rep3/_model_epoch_120.pickle
 - val_f1: 0.9998
Epoch 122/300
 - 46s - loss: 1.6919e-04 - val_loss: 3.7204e-04
 - val_f1: 0.9998
Epoch 123/300
 - 47s - loss: 1.8123e-04 - val_loss: 3.7448e-04
 - val_f1: 0.9998
Epoch 124/300
 - 47s - loss: 1.6914e-04 - val_loss: 3.7204e-04
 - val_f1: 0.9998
Epoch 125/300
 - 47s - loss: 1.5662e-04 - val_loss: 3.8240e-04
 - val_f1: 0.9998
Epoch 126/300
 - 47s - loss: 1.7003e-04 - val_loss: 3.7976e-04
2019-12-28 06:42:50,802 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-28 06:43:45,580 [INFO] Last epoch loss evaluation: train_loss = 0.000135, val_loss = 0.000305
2019-12-28 06:43:45,581 [INFO] Training complete. time_to_train = 7346.02 sec, 122.43 min
2019-12-28 06:43:45,586 [INFO] Model saved to results_selected_models/selected_kdd99_lstm_deep_rep3/best_model.pickle
2019-12-28 06:43:45,589 [INFO] Training history saved to: results_selected_models/selected_kdd99_lstm_deep_rep3/training_error_history.csv
2019-12-28 06:43:45,761 [INFO] Plot saved to: results_selected_models/selected_kdd99_lstm_deep_rep3/training_error_history.png
2019-12-28 06:43:45,923 [INFO] Plot saved to: results_selected_models/selected_kdd99_lstm_deep_rep3/training_f1_history.png
2019-12-28 06:43:45,923 [INFO] Making predictions on training, validation, testing data
2019-12-28 06:44:42,053 [INFO] Evaluating predictions (results)
2019-12-28 06:44:50,683 [INFO] Dataset: Testing. Classification report below
2019-12-28 06:44:50,683 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      0.97      0.99    229853
     normal.       0.73      0.98      0.84     60580
       probe       0.75      0.76      0.75      4166
         r2l       0.90      0.03      0.06     13773
         u2r       0.90      0.00      0.01      2636

    accuracy                           0.92    311008
   macro avg       0.86      0.55      0.53    311008
weighted avg       0.94      0.92      0.90    311008

2019-12-28 06:44:50,683 [INFO] Overall accuracy (micro avg): 0.9231434561168844
2019-12-28 06:45:00,019 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9231         0.9231                       0.9231                0.0192                   0.0769  0.9231
1     Macro avg        0.9693         0.8560                       0.5496                0.0195                   0.4504  0.5278
2  Weighted avg        0.9683         0.9371                       0.9231                0.0208                   0.0769  0.9043
2019-12-28 06:45:30,626 [INFO] Dataset: Validation. Classification report below
2019-12-28 06:45:30,627 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00    776671
     normal.       1.00      1.00      1.00    194553
       probe       1.00      0.99      1.00      8221
         r2l       0.88      0.85      0.86       225
         u2r       0.67      0.20      0.31        10

    accuracy                           1.00    979680
   macro avg       0.91      0.81      0.83    979680
weighted avg       1.00      1.00      1.00    979680

2019-12-28 06:45:30,627 [INFO] Overall accuracy (micro avg): 0.9998325983994774
2019-12-28 06:46:03,645 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9998         0.9998                       0.9998                0.0000                   0.0002  0.9998
1     Macro avg        0.9999         0.9083                       0.8092                0.0001                   0.1908  0.8336
2  Weighted avg        0.9999         0.9998                       0.9998                0.0001                   0.0002  0.9998
2019-12-28 06:48:17,126 [INFO] Dataset: Training. Classification report below
2019-12-28 06:48:17,126 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00   3106675
     normal.       1.00      1.00      1.00    778221
       probe       1.00      1.00      1.00     32881
         r2l       0.89      0.91      0.90       901
         u2r       0.91      0.50      0.65        42

    accuracy                           1.00   3918720
   macro avg       0.96      0.88      0.91   3918720
weighted avg       1.00      1.00      1.00   3918720

2019-12-28 06:48:17,126 [INFO] Overall accuracy (micro avg): 0.9998986914094398
2019-12-28 06:50:41,173 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9999         0.9999                       0.9999                0.0000                   0.0001  0.9999
1     Macro avg        1.0000         0.9614                       0.8817                0.0000                   0.1183  0.9095
2  Weighted avg        1.0000         0.9999                       0.9999                0.0001                   0.0001  0.9999
2019-12-28 06:50:41,220 [INFO] Results saved to: results_selected_models/selected_kdd99_lstm_deep_rep3/selected_kdd99_lstm_deep_rep3_results.xlsx
2019-12-28 06:50:41,227 [INFO] ================= Finished running experiment no. 3 ================= 

2019-12-28 06:50:41,337 [INFO] Created directory: results_selected_models/selected_kdd99_lstm_deep_rep4
2019-12-28 06:50:41,337 [INFO] Initialized logging. log_filename = results_selected_models/selected_kdd99_lstm_deep_rep4/run_log.log
2019-12-28 06:50:41,337 [INFO] ================= Running experiment no. 4  ================= 

2019-12-28 06:50:41,337 [INFO] Experiment parameters given below
2019-12-28 06:50:41,337 [INFO] 
{'experiment_num': 4, 'results_dir': 'results_selected_models/selected_kdd99_lstm_deep_rep4', 'model': 'lstm', 'model_type': 'classifier', 'normal_label': 'normal', 'lstm_time_steps': 32, 'lstm_layer_units': [32, 16], 'lstm_layer_activations': ['relu', 'relu'], 'lstm_layer_dropout_rates': [0.2, 0.2], 'batch_size': 256, 'epochs': 300, 'early_stop_patience': 50, 'dataset_dir': '../Datasets/full_datasets/kdd99_five_classes', 'description': 'selected_kdd99_lstm_deep_rep4'}
2019-12-28 06:50:41,337 [INFO] Created tensorboard log directory: results_selected_models/selected_kdd99_lstm_deep_rep4/tf_logs_run_2019_12_28-06_50_41
2019-12-28 06:50:41,337 [INFO] Loading datsets from: ../Datasets/full_datasets/kdd99_five_classes
2019-12-28 06:50:41,338 [INFO] Reading X, y files
2019-12-28 06:50:41,338 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/X_train.h5
2019-12-28 06:50:48,019 [INFO] Reading complete. time_to_read=6.68 seconds
2019-12-28 06:50:48,019 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/X_val.h5
2019-12-28 06:50:49,719 [INFO] Reading complete. time_to_read=1.70 seconds
2019-12-28 06:50:49,719 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/X_test.h5
2019-12-28 06:50:50,200 [INFO] Reading complete. time_to_read=0.48 seconds
2019-12-28 06:50:50,200 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/y_train.h5
2019-12-28 06:50:50,402 [INFO] Reading complete. time_to_read=0.20 seconds
2019-12-28 06:50:50,402 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/y_val.h5
2019-12-28 06:50:50,457 [INFO] Reading complete. time_to_read=0.05 seconds
2019-12-28 06:50:50,457 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/y_test.h5
2019-12-28 06:50:50,477 [INFO] Reading complete. time_to_read=0.02 seconds
2019-12-28 06:50:56,164 [INFO] Preparing flow sequences
2019-12-28 06:51:43,570 [INFO] Extracting flows complete. time_taken = 47.41 sec
2019-12-28 06:51:45,206 [INFO] Initializing model
2019-12-28 06:51:45,658 [INFO] _________________________________________________________________
2019-12-28 06:51:45,658 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-28 06:51:45,658 [INFO] =================================================================
2019-12-28 06:51:45,658 [INFO] lstm_12 (LSTM)               (None, 32, 32)            19968     
2019-12-28 06:51:45,658 [INFO] _________________________________________________________________
2019-12-28 06:51:45,659 [INFO] batch_normalization_12 (Batc (None, 32, 32)            128       
2019-12-28 06:51:45,659 [INFO] _________________________________________________________________
2019-12-28 06:51:45,659 [INFO] dropout_12 (Dropout)         (None, 32, 32)            0         
2019-12-28 06:51:45,659 [INFO] _________________________________________________________________
2019-12-28 06:51:45,659 [INFO] lstm_13 (LSTM)               (None, 32, 16)            3136      
2019-12-28 06:51:45,659 [INFO] _________________________________________________________________
2019-12-28 06:51:45,659 [INFO] batch_normalization_13 (Batc (None, 32, 16)            64        
2019-12-28 06:51:45,659 [INFO] _________________________________________________________________
2019-12-28 06:51:45,659 [INFO] dropout_13 (Dropout)         (None, 32, 16)            0         
2019-12-28 06:51:45,659 [INFO] _________________________________________________________________
2019-12-28 06:51:45,659 [INFO] time_distributed_9 (TimeDist (None, 32, 5)             85        
2019-12-28 06:51:45,660 [INFO] =================================================================
2019-12-28 06:51:45,660 [INFO] Total params: 23,381
2019-12-28 06:51:45,660 [INFO] Trainable params: 23,285
2019-12-28 06:51:45,660 [INFO] Non-trainable params: 96
2019-12-28 06:51:45,660 [INFO] _________________________________________________________________
2019-12-28 06:51:45,660 [INFO] Training model
 - val_f1: 0.9998
Epoch 00126: early stopping
Train on 122460 samples, validate on 30615 samples
Epoch 1/300
 - 49s - loss: 0.2725 - val_loss: 0.1151
 - val_f1: 0.8971
Epoch 2/300
 - 46s - loss: 0.1176 - val_loss: 0.0746
 - val_f1: 0.9415
Epoch 3/300
 - 46s - loss: 0.0950 - val_loss: 0.0572
 - val_f1: 0.9523
Epoch 4/300
 - 46s - loss: 0.0809 - val_loss: 0.0494
 - val_f1: 0.9604
Epoch 5/300
 - 46s - loss: 0.0716 - val_loss: 0.0400
 - val_f1: 0.9665
Epoch 6/300
 - 46s - loss: 0.0658 - val_loss: 0.0412
 - val_f1: 0.9644
Epoch 7/300
 - 46s - loss: 0.0578 - val_loss: 0.0321
 - val_f1: 0.9731
Epoch 8/300
 - 46s - loss: 0.0507 - val_loss: 0.0249
 - val_f1: 0.9779
Epoch 9/300
 - 46s - loss: 0.0448 - val_loss: 0.0217
 - val_f1: 0.9799
Epoch 10/300
 - 46s - loss: 0.0471 - val_loss: 0.0562
 - val_f1: 0.9608
Epoch 11/300
 - 46s - loss: 0.0718 - val_loss: 0.0429
 - val_f1: 0.9614
Epoch 12/300
 - 46s - loss: 0.0468 - val_loss: 0.0221
 - val_f1: 0.9797
Epoch 13/300
 - 46s - loss: 0.0360 - val_loss: 0.0184
 - val_f1: 0.9819
Epoch 14/300
 - 46s - loss: 0.0310 - val_loss: 0.0151
 - val_f1: 0.9839
Epoch 15/300
 - 46s - loss: 0.0266 - val_loss: 0.0131
 - val_f1: 0.9848
Epoch 16/300
 - 46s - loss: 0.0233 - val_loss: 0.0111
 - val_f1: 0.9883
Epoch 17/300
 - 46s - loss: 0.0198 - val_loss: 0.0092
 - val_f1: 0.9925
Epoch 18/300
 - 46s - loss: 0.0170 - val_loss: 0.0073
 - val_f1: 0.9951
Epoch 19/300
 - 46s - loss: 0.0145 - val_loss: 0.0061
 - val_f1: 0.9960
Epoch 20/300
 - 46s - loss: 0.0126 - val_loss: 0.0055
 - val_f1: 0.9964
Epoch 21/300
 - 46s - loss: 0.0109 - val_loss: 0.0047
 - val_f1: 0.9971
Epoch 22/300
 - 46s - loss: 0.0094 - val_loss: 0.0043
 - val_f1: 0.9975
Epoch 23/300
 - 46s - loss: 0.0082 - val_loss: 0.0037
 - val_f1: 0.9978
Epoch 24/300
 - 46s - loss: 0.0073 - val_loss: 0.0033
 - val_f1: 0.9980
Epoch 25/300
 - 46s - loss: 0.0064 - val_loss: 0.0030
 - val_f1: 0.9982
Epoch 26/300
 - 46s - loss: 0.0057 - val_loss: 0.0027
 - val_f1: 0.9985
Epoch 27/300
 - 46s - loss: 0.0049 - val_loss: 0.0024
 - val_f1: 0.9986
Epoch 28/300
 - 46s - loss: 0.0045 - val_loss: 0.0022
 - val_f1: 0.9987
Epoch 29/300
 - 46s - loss: 0.0041 - val_loss: 0.0020
 - val_f1: 0.9988
Epoch 30/300
 - 46s - loss: 0.0036 - val_loss: 0.0019
 - val_f1: 0.9989
Epoch 31/300
 - 46s - loss: 0.0033 - val_loss: 0.0017
2019-12-28 07:21:28,883 [INFO] epoch = 30. Intermediate model saved to results_selected_models/selected_kdd99_lstm_deep_rep4/_model_epoch_30.pickle
 - val_f1: 0.9989
Epoch 32/300
 - 46s - loss: 0.0029 - val_loss: 0.0015
 - val_f1: 0.9990
Epoch 33/300
 - 46s - loss: 0.0025 - val_loss: 0.0014
 - val_f1: 0.9991
Epoch 34/300
 - 46s - loss: 0.0023 - val_loss: 0.0013
 - val_f1: 0.9991
Epoch 35/300
 - 46s - loss: 0.0020 - val_loss: 0.0011
 - val_f1: 0.9992
Epoch 36/300
 - 46s - loss: 0.0018 - val_loss: 0.0011
 - val_f1: 0.9993
Epoch 37/300
 - 46s - loss: 0.0016 - val_loss: 9.6412e-04
 - val_f1: 0.9993
Epoch 38/300
 - 46s - loss: 0.0015 - val_loss: 9.8941e-04
 - val_f1: 0.9994
Epoch 39/300
 - 46s - loss: 0.0013 - val_loss: 8.5137e-04
 - val_f1: 0.9994
Epoch 40/300
 - 46s - loss: 0.0012 - val_loss: 8.4260e-04
 - val_f1: 0.9995
Epoch 41/300
 - 46s - loss: 0.0011 - val_loss: 7.8278e-04
 - val_f1: 0.9995
Epoch 42/300
 - 46s - loss: 0.0010 - val_loss: 7.4035e-04
 - val_f1: 0.9995
Epoch 43/300
 - 46s - loss: 9.2516e-04 - val_loss: 7.6707e-04
 - val_f1: 0.9995
Epoch 44/300
 - 46s - loss: 8.5380e-04 - val_loss: 6.0735e-04
 - val_f1: 0.9996
Epoch 45/300
 - 46s - loss: 8.1723e-04 - val_loss: 7.4296e-04
 - val_f1: 0.9996
Epoch 46/300
 - 46s - loss: 7.6422e-04 - val_loss: 6.5915e-04
 - val_f1: 0.9996
Epoch 47/300
 - 46s - loss: 7.1541e-04 - val_loss: 5.9144e-04
 - val_f1: 0.9996
Epoch 48/300
 - 46s - loss: 6.8291e-04 - val_loss: 5.2505e-04
 - val_f1: 0.9997
Epoch 49/300
 - 46s - loss: 6.4011e-04 - val_loss: 5.0576e-04
 - val_f1: 0.9997
Epoch 50/300
 - 46s - loss: 6.1543e-04 - val_loss: 4.8635e-04
 - val_f1: 0.9997
Epoch 51/300
 - 46s - loss: 5.7975e-04 - val_loss: 4.8493e-04
 - val_f1: 0.9997
Epoch 52/300
 - 46s - loss: 5.4845e-04 - val_loss: 5.1346e-04
 - val_f1: 0.9997
Epoch 53/300
 - 46s - loss: 5.3869e-04 - val_loss: 4.9936e-04
 - val_f1: 0.9997
Epoch 54/300
 - 46s - loss: 4.9827e-04 - val_loss: 4.1175e-04
 - val_f1: 0.9998
Epoch 55/300
 - 46s - loss: 4.7900e-04 - val_loss: 4.2674e-04
 - val_f1: 0.9997
Epoch 56/300
 - 46s - loss: 4.7921e-04 - val_loss: 3.9791e-04
 - val_f1: 0.9998
Epoch 57/300
 - 46s - loss: 4.4543e-04 - val_loss: 4.6533e-04
 - val_f1: 0.9997
Epoch 58/300
 - 46s - loss: 4.2790e-04 - val_loss: 4.5803e-04
 - val_f1: 0.9997
Epoch 59/300
 - 46s - loss: 4.2137e-04 - val_loss: 3.9424e-04
 - val_f1: 0.9998
Epoch 60/300
 - 46s - loss: 4.1743e-04 - val_loss: 3.7669e-04
 - val_f1: 0.9998
Epoch 61/300
 - 46s - loss: 4.0136e-04 - val_loss: 3.7201e-04
2019-12-28 07:50:13,879 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_kdd99_lstm_deep_rep4/_model_epoch_60.pickle
 - val_f1: 0.9998
Epoch 62/300
 - 46s - loss: 3.6544e-04 - val_loss: 4.1873e-04
 - val_f1: 0.9998
Epoch 63/300
 - 46s - loss: 3.6815e-04 - val_loss: 3.6322e-04
 - val_f1: 0.9998
Epoch 64/300
 - 46s - loss: 3.6031e-04 - val_loss: 3.5775e-04
 - val_f1: 0.9998
Epoch 65/300
 - 46s - loss: 3.5968e-04 - val_loss: 3.8606e-04
 - val_f1: 0.9998
Epoch 66/300
 - 46s - loss: 3.5637e-04 - val_loss: 3.6843e-04
 - val_f1: 0.9998
Epoch 67/300
 - 46s - loss: 3.3553e-04 - val_loss: 3.6203e-04
 - val_f1: 0.9998
Epoch 68/300
 - 46s - loss: 3.4543e-04 - val_loss: 3.5922e-04
 - val_f1: 0.9998
Epoch 69/300
 - 46s - loss: 3.3096e-04 - val_loss: 3.7956e-04
 - val_f1: 0.9998
Epoch 70/300
 - 46s - loss: 3.2379e-04 - val_loss: 3.9306e-04
 - val_f1: 0.9998
Epoch 71/300
 - 46s - loss: 3.1710e-04 - val_loss: 3.7901e-04
 - val_f1: 0.9998
Epoch 72/300
 - 46s - loss: 3.1802e-04 - val_loss: 3.7252e-04
 - val_f1: 0.9998
Epoch 73/300
 - 46s - loss: 3.1020e-04 - val_loss: 3.3131e-04
 - val_f1: 0.9998
Epoch 74/300
 - 46s - loss: 3.0779e-04 - val_loss: 3.2971e-04
 - val_f1: 0.9998
Epoch 75/300
 - 46s - loss: 3.0601e-04 - val_loss: 3.5784e-04
 - val_f1: 0.9998
Epoch 76/300
 - 46s - loss: 3.0071e-04 - val_loss: 3.4579e-04
 - val_f1: 0.9998
Epoch 77/300
 - 46s - loss: 2.8919e-04 - val_loss: 3.3847e-04
 - val_f1: 0.9998
Epoch 78/300
 - 46s - loss: 2.8463e-04 - val_loss: 3.7044e-04
 - val_f1: 0.9998
Epoch 79/300
 - 46s - loss: 2.8542e-04 - val_loss: 3.2591e-04
 - val_f1: 0.9998
Epoch 80/300
 - 46s - loss: 2.8055e-04 - val_loss: 3.5403e-04
 - val_f1: 0.9998
Epoch 81/300
 - 46s - loss: 2.8345e-04 - val_loss: 3.5607e-04
 - val_f1: 0.9998
Epoch 82/300
 - 46s - loss: 2.8318e-04 - val_loss: 3.2790e-04
 - val_f1: 0.9998
Epoch 83/300
 - 46s - loss: 2.6759e-04 - val_loss: 3.4600e-04
 - val_f1: 0.9998
Epoch 84/300
 - 46s - loss: 2.5792e-04 - val_loss: 3.5244e-04
 - val_f1: 0.9998
Epoch 85/300
 - 46s - loss: 2.5118e-04 - val_loss: 3.5660e-04
 - val_f1: 0.9998
Epoch 86/300
 - 46s - loss: 2.5273e-04 - val_loss: 3.3646e-04
 - val_f1: 0.9998
Epoch 87/300
 - 46s - loss: 2.5239e-04 - val_loss: 3.3605e-04
 - val_f1: 0.9998
Epoch 88/300
 - 46s - loss: 2.3858e-04 - val_loss: 3.6332e-04
 - val_f1: 0.9998
Epoch 89/300
 - 46s - loss: 2.4347e-04 - val_loss: 3.2637e-04
 - val_f1: 0.9998
Epoch 90/300
 - 46s - loss: 2.4787e-04 - val_loss: 3.4376e-04
 - val_f1: 0.9998
Epoch 91/300
 - 46s - loss: 2.4130e-04 - val_loss: 3.5017e-04
2019-12-28 08:18:59,051 [INFO] epoch = 90. Intermediate model saved to results_selected_models/selected_kdd99_lstm_deep_rep4/_model_epoch_90.pickle
 - val_f1: 0.9998
Epoch 92/300
 - 46s - loss: 2.3362e-04 - val_loss: 3.8269e-04
 - val_f1: 0.9998
Epoch 93/300
 - 46s - loss: 2.2560e-04 - val_loss: 3.7008e-04
 - val_f1: 0.9998
Epoch 94/300
 - 46s - loss: 2.2876e-04 - val_loss: 3.8744e-04
 - val_f1: 0.9998
Epoch 95/300
 - 46s - loss: 2.4447e-04 - val_loss: 3.5274e-04
 - val_f1: 0.9998
Epoch 96/300
 - 46s - loss: 2.3327e-04 - val_loss: 3.4127e-04
 - val_f1: 0.9998
Epoch 97/300
 - 46s - loss: 2.1786e-04 - val_loss: 3.6954e-04
 - val_f1: 0.9998
Epoch 98/300
 - 46s - loss: 2.2145e-04 - val_loss: 3.6061e-04
 - val_f1: 0.9998
Epoch 99/300
 - 46s - loss: 2.1983e-04 - val_loss: 3.4893e-04
 - val_f1: 0.9998
Epoch 100/300
 - 46s - loss: 2.0804e-04 - val_loss: 3.5017e-04
 - val_f1: 0.9998
Epoch 101/300
 - 46s - loss: 2.1907e-04 - val_loss: 3.2968e-04
 - val_f1: 0.9998
Epoch 102/300
 - 46s - loss: 2.0751e-04 - val_loss: 3.6326e-04
 - val_f1: 0.9998
Epoch 103/300
 - 46s - loss: 2.1971e-04 - val_loss: 3.4000e-04
 - val_f1: 0.9998
Epoch 104/300
 - 46s - loss: 2.1633e-04 - val_loss: 3.4132e-04
 - val_f1: 0.9998
Epoch 105/300
 - 46s - loss: 1.9676e-04 - val_loss: 3.5971e-04
 - val_f1: 0.9998
Epoch 106/300
 - 46s - loss: 2.0762e-04 - val_loss: 3.4974e-04
 - val_f1: 0.9998
Epoch 107/300
 - 46s - loss: 2.0995e-04 - val_loss: 3.6269e-04
 - val_f1: 0.9998
Epoch 108/300
 - 46s - loss: 2.0347e-04 - val_loss: 3.3593e-04
 - val_f1: 0.9998
Epoch 109/300
 - 46s - loss: 2.1163e-04 - val_loss: 3.3503e-04
 - val_f1: 0.9998
Epoch 110/300
 - 46s - loss: 2.1114e-04 - val_loss: 3.3042e-04
 - val_f1: 0.9998
Epoch 111/300
 - 46s - loss: 2.0384e-04 - val_loss: 3.5319e-04
 - val_f1: 0.9998
Epoch 112/300
 - 46s - loss: 1.9596e-04 - val_loss: 3.5508e-04
 - val_f1: 0.9998
Epoch 113/300
 - 46s - loss: 1.8623e-04 - val_loss: 3.6521e-04
 - val_f1: 0.9998
Epoch 114/300
 - 46s - loss: 1.9624e-04 - val_loss: 3.6826e-04
 - val_f1: 0.9998
Epoch 115/300
 - 46s - loss: 1.8971e-04 - val_loss: 3.5305e-04
 - val_f1: 0.9998
Epoch 116/300
 - 46s - loss: 1.8141e-04 - val_loss: 3.7095e-04
 - val_f1: 0.9998
Epoch 117/300
 - 46s - loss: 2.0299e-04 - val_loss: 3.5198e-04
 - val_f1: 0.9998
Epoch 118/300
 - 46s - loss: 1.8684e-04 - val_loss: 3.9655e-04
 - val_f1: 0.9998
Epoch 119/300
 - 46s - loss: 1.8405e-04 - val_loss: 3.8170e-04
 - val_f1: 0.9998
Epoch 120/300
 - 46s - loss: 1.9359e-04 - val_loss: 3.4359e-04
 - val_f1: 0.9998
Epoch 121/300
 - 46s - loss: 1.9504e-04 - val_loss: 3.7820e-04
2019-12-28 08:47:44,263 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_kdd99_lstm_deep_rep4/_model_epoch_120.pickle
 - val_f1: 0.9998
Epoch 122/300
 - 46s - loss: 1.9010e-04 - val_loss: 3.8970e-04
 - val_f1: 0.9998
Epoch 123/300
 - 46s - loss: 1.8548e-04 - val_loss: 3.7260e-04
 - val_f1: 0.9998
Epoch 124/300
 - 46s - loss: 1.8186e-04 - val_loss: 3.6180e-04
 - val_f1: 0.9998
Epoch 125/300
 - 46s - loss: 1.8295e-04 - val_loss: 3.5150e-04
 - val_f1: 0.9998
Epoch 126/300
 - 57s - loss: 1.8837e-04 - val_loss: 4.0411e-04
 - val_f1: 0.9998
Epoch 127/300
 - 46s - loss: 1.6629e-04 - val_loss: 3.7051e-04
 - val_f1: 0.9998
Epoch 128/300
 - 46s - loss: 1.7586e-04 - val_loss: 3.6135e-04
 - val_f1: 0.9998
Epoch 129/300
 - 46s - loss: 1.7949e-04 - val_loss: 3.5853e-04
2019-12-28 08:55:46,346 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-28 08:56:41,597 [INFO] Last epoch loss evaluation: train_loss = 0.000186, val_loss = 0.000326
2019-12-28 08:56:41,597 [INFO] Training complete. time_to_train = 7495.94 sec, 124.93 min
2019-12-28 08:56:41,603 [INFO] Model saved to results_selected_models/selected_kdd99_lstm_deep_rep4/best_model.pickle
2019-12-28 08:56:41,606 [INFO] Training history saved to: results_selected_models/selected_kdd99_lstm_deep_rep4/training_error_history.csv
2019-12-28 08:56:41,783 [INFO] Plot saved to: results_selected_models/selected_kdd99_lstm_deep_rep4/training_error_history.png
2019-12-28 08:56:41,951 [INFO] Plot saved to: results_selected_models/selected_kdd99_lstm_deep_rep4/training_f1_history.png
2019-12-28 08:56:41,951 [INFO] Making predictions on training, validation, testing data
2019-12-28 08:57:38,184 [INFO] Evaluating predictions (results)
2019-12-28 08:57:46,866 [INFO] Dataset: Testing. Classification report below
2019-12-28 08:57:46,866 [INFO] 
              precision    recall  f1-score   support

         dos       0.99      0.97      0.98    229853
     normal.       0.73      0.98      0.84     60580
       probe       0.80      0.78      0.79      4166
         r2l       0.88      0.01      0.03     13773
         u2r       0.43      0.00      0.01      2636

    accuracy                           0.92    311008
   macro avg       0.77      0.55      0.53    311008
weighted avg       0.93      0.92      0.90    311008

2019-12-28 08:57:46,866 [INFO] Overall accuracy (micro avg): 0.921378228212779
2019-12-28 08:57:56,189 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9214         0.9214                       0.9214                0.0197                   0.0786  0.9214
1     Macro avg        0.9686         0.7689                       0.5507                0.0223                   0.4493  0.5293
2  Weighted avg        0.9649         0.9293                       0.9214                0.0327                   0.0786  0.9012
2019-12-28 08:58:26,792 [INFO] Dataset: Validation. Classification report below
2019-12-28 08:58:26,792 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00    776671
     normal.       1.00      1.00      1.00    194553
       probe       1.00      0.99      1.00      8221
         r2l       0.86      0.82      0.84       225
         u2r       0.50      0.10      0.17        10

    accuracy                           1.00    979680
   macro avg       0.87      0.78      0.80    979680
weighted avg       1.00      1.00      1.00    979680

2019-12-28 08:58:26,792 [INFO] Overall accuracy (micro avg): 0.9998101420872122
2019-12-28 08:58:59,811 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9998         0.9998                       0.9998                0.0000                   0.0002  0.9998
1     Macro avg        0.9999         0.8719                       0.7823                0.0001                   0.2177  0.8003
2  Weighted avg        0.9999         0.9998                       0.9998                0.0001                   0.0002  0.9998
2019-12-28 09:01:12,996 [INFO] Dataset: Training. Classification report below
2019-12-28 09:01:12,996 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00   3106675
     normal.       1.00      1.00      1.00    778221
       probe       1.00      1.00      1.00     32881
         r2l       0.87      0.89      0.88       901
         u2r       0.81      0.40      0.54        42

    accuracy                           1.00   3918720
   macro avg       0.93      0.86      0.88   3918720
weighted avg       1.00      1.00      1.00   3918720

2019-12-28 09:01:12,996 [INFO] Overall accuracy (micro avg): 0.9998586273068757
2019-12-28 09:03:36,725 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9999         0.9999                       0.9999                0.0000                   0.0001  0.9999
1     Macro avg        0.9999         0.9348                       0.8574                0.0000                   0.1426  0.8826
2  Weighted avg        1.0000         0.9999                       0.9999                0.0001                   0.0001  0.9999
2019-12-28 09:03:36,772 [INFO] Results saved to: results_selected_models/selected_kdd99_lstm_deep_rep4/selected_kdd99_lstm_deep_rep4_results.xlsx
2019-12-28 09:03:36,778 [INFO] ================= Finished running experiment no. 4 ================= 

2019-12-28 09:03:36,889 [INFO] Created directory: results_selected_models/selected_kdd99_lstm_deep_rep5
2019-12-28 09:03:36,890 [INFO] Initialized logging. log_filename = results_selected_models/selected_kdd99_lstm_deep_rep5/run_log.log
2019-12-28 09:03:36,890 [INFO] ================= Running experiment no. 5  ================= 

2019-12-28 09:03:36,890 [INFO] Experiment parameters given below
2019-12-28 09:03:36,890 [INFO] 
{'experiment_num': 5, 'results_dir': 'results_selected_models/selected_kdd99_lstm_deep_rep5', 'model': 'lstm', 'model_type': 'classifier', 'normal_label': 'normal', 'lstm_time_steps': 32, 'lstm_layer_units': [32, 16], 'lstm_layer_activations': ['relu', 'relu'], 'lstm_layer_dropout_rates': [0.2, 0.2], 'batch_size': 256, 'epochs': 300, 'early_stop_patience': 50, 'dataset_dir': '../Datasets/full_datasets/kdd99_five_classes', 'description': 'selected_kdd99_lstm_deep_rep5'}
2019-12-28 09:03:36,890 [INFO] Created tensorboard log directory: results_selected_models/selected_kdd99_lstm_deep_rep5/tf_logs_run_2019_12_28-09_03_36
2019-12-28 09:03:36,890 [INFO] Loading datsets from: ../Datasets/full_datasets/kdd99_five_classes
2019-12-28 09:03:36,890 [INFO] Reading X, y files
2019-12-28 09:03:36,891 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/X_train.h5
2019-12-28 09:03:43,616 [INFO] Reading complete. time_to_read=6.73 seconds
2019-12-28 09:03:43,616 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/X_val.h5
2019-12-28 09:03:45,345 [INFO] Reading complete. time_to_read=1.73 seconds
2019-12-28 09:03:45,345 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/X_test.h5
2019-12-28 09:03:45,831 [INFO] Reading complete. time_to_read=0.49 seconds
2019-12-28 09:03:45,831 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/y_train.h5
2019-12-28 09:03:46,039 [INFO] Reading complete. time_to_read=0.21 seconds
2019-12-28 09:03:46,039 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/y_val.h5
2019-12-28 09:03:46,095 [INFO] Reading complete. time_to_read=0.06 seconds
2019-12-28 09:03:46,095 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/y_test.h5
2019-12-28 09:03:46,116 [INFO] Reading complete. time_to_read=0.02 seconds
2019-12-28 09:03:51,806 [INFO] Preparing flow sequences
2019-12-28 09:04:38,931 [INFO] Extracting flows complete. time_taken = 47.13 sec
2019-12-28 09:04:40,560 [INFO] Initializing model
2019-12-28 09:04:41,015 [INFO] _________________________________________________________________
2019-12-28 09:04:41,015 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-28 09:04:41,015 [INFO] =================================================================
2019-12-28 09:04:41,015 [INFO] lstm_14 (LSTM)               (None, 32, 32)            19968     
2019-12-28 09:04:41,015 [INFO] _________________________________________________________________
2019-12-28 09:04:41,015 [INFO] batch_normalization_14 (Batc (None, 32, 32)            128       
2019-12-28 09:04:41,015 [INFO] _________________________________________________________________
2019-12-28 09:04:41,015 [INFO] dropout_14 (Dropout)         (None, 32, 32)            0         
2019-12-28 09:04:41,015 [INFO] _________________________________________________________________
2019-12-28 09:04:41,015 [INFO] lstm_15 (LSTM)               (None, 32, 16)            3136      
2019-12-28 09:04:41,015 [INFO] _________________________________________________________________
2019-12-28 09:04:41,016 [INFO] batch_normalization_15 (Batc (None, 32, 16)            64        
2019-12-28 09:04:41,016 [INFO] _________________________________________________________________
2019-12-28 09:04:41,016 [INFO] dropout_15 (Dropout)         (None, 32, 16)            0         
2019-12-28 09:04:41,016 [INFO] _________________________________________________________________
2019-12-28 09:04:41,016 [INFO] time_distributed_10 (TimeDis (None, 32, 5)             85        
2019-12-28 09:04:41,016 [INFO] =================================================================
2019-12-28 09:04:41,016 [INFO] Total params: 23,381
2019-12-28 09:04:41,016 [INFO] Trainable params: 23,285
2019-12-28 09:04:41,016 [INFO] Non-trainable params: 96
2019-12-28 09:04:41,016 [INFO] _________________________________________________________________
2019-12-28 09:04:41,017 [INFO] Training model
 - val_f1: 0.9998
Epoch 00129: early stopping
Train on 122460 samples, validate on 30615 samples
Epoch 1/300
 - 50s - loss: 0.1611 - val_loss: 0.0116
 - val_f1: 0.9951
Epoch 2/300
 - 47s - loss: 0.0100 - val_loss: 0.0033
 - val_f1: 0.9982
Epoch 3/300
 - 47s - loss: 0.0051 - val_loss: 0.0023
 - val_f1: 0.9986
Epoch 4/300
 - 46s - loss: 0.0036 - val_loss: 0.0019
 - val_f1: 0.9988
Epoch 5/300
 - 46s - loss: 0.0029 - val_loss: 0.0016
 - val_f1: 0.9990
Epoch 6/300
 - 46s - loss: 0.0023 - val_loss: 0.0014
 - val_f1: 0.9991
Epoch 7/300
 - 46s - loss: 0.0019 - val_loss: 0.0012
 - val_f1: 0.9992
Epoch 8/300
 - 46s - loss: 0.0017 - val_loss: 9.7815e-04
 - val_f1: 0.9993
Epoch 9/300
 - 46s - loss: 0.0014 - val_loss: 8.7075e-04
 - val_f1: 0.9993
Epoch 10/300
 - 46s - loss: 0.0013 - val_loss: 7.1311e-04
 - val_f1: 0.9995
Epoch 11/300
 - 46s - loss: 0.0011 - val_loss: 6.4937e-04
 - val_f1: 0.9996
Epoch 12/300
 - 46s - loss: 9.6132e-04 - val_loss: 6.0040e-04
 - val_f1: 0.9996
Epoch 13/300
 - 47s - loss: 8.8269e-04 - val_loss: 5.6258e-04
 - val_f1: 0.9997
Epoch 14/300
 - 46s - loss: 7.7563e-04 - val_loss: 5.0055e-04
 - val_f1: 0.9997
Epoch 15/300
 - 46s - loss: 7.1656e-04 - val_loss: 4.7528e-04
 - val_f1: 0.9997
Epoch 16/300
 - 46s - loss: 6.5883e-04 - val_loss: 5.3051e-04
 - val_f1: 0.9997
Epoch 17/300
 - 46s - loss: 6.2833e-04 - val_loss: 4.4873e-04
 - val_f1: 0.9997
Epoch 18/300
 - 46s - loss: 5.9855e-04 - val_loss: 4.4320e-04
 - val_f1: 0.9997
Epoch 19/300
 - 46s - loss: 5.5348e-04 - val_loss: 4.3398e-04
 - val_f1: 0.9997
Epoch 20/300
 - 46s - loss: 5.2952e-04 - val_loss: 4.0155e-04
 - val_f1: 0.9998
Epoch 21/300
 - 46s - loss: 5.2175e-04 - val_loss: 4.2030e-04
 - val_f1: 0.9997
Epoch 22/300
 - 46s - loss: 4.7846e-04 - val_loss: 3.9079e-04
 - val_f1: 0.9998
Epoch 23/300
 - 46s - loss: 4.7500e-04 - val_loss: 3.7771e-04
 - val_f1: 0.9998
Epoch 24/300
 - 46s - loss: 4.5911e-04 - val_loss: 3.6754e-04
 - val_f1: 0.9997
Epoch 25/300
 - 46s - loss: 4.3497e-04 - val_loss: 3.7700e-04
 - val_f1: 0.9998
Epoch 26/300
 - 46s - loss: 4.1745e-04 - val_loss: 3.5199e-04
 - val_f1: 0.9998
Epoch 27/300
 - 46s - loss: 4.1330e-04 - val_loss: 3.6890e-04
 - val_f1: 0.9998
Epoch 28/300
 - 46s - loss: 3.9238e-04 - val_loss: 3.5375e-04
 - val_f1: 0.9998
Epoch 29/300
 - 46s - loss: 4.1051e-04 - val_loss: 3.7284e-04
 - val_f1: 0.9998
Epoch 30/300
 - 46s - loss: 3.7259e-04 - val_loss: 3.6124e-04
 - val_f1: 0.9998
Epoch 31/300
 - 46s - loss: 3.8344e-04 - val_loss: 3.4839e-04
2019-12-28 09:34:36,765 [INFO] epoch = 30. Intermediate model saved to results_selected_models/selected_kdd99_lstm_deep_rep5/_model_epoch_30.pickle
 - val_f1: 0.9998
Epoch 32/300
 - 46s - loss: 3.6663e-04 - val_loss: 3.3283e-04
 - val_f1: 0.9998
Epoch 33/300
 - 46s - loss: 3.5194e-04 - val_loss: 3.3778e-04
 - val_f1: 0.9998
Epoch 34/300
 - 47s - loss: 3.3816e-04 - val_loss: 3.3914e-04
 - val_f1: 0.9998
Epoch 35/300
 - 46s - loss: 3.3919e-04 - val_loss: 3.3236e-04
 - val_f1: 0.9998
Epoch 36/300
 - 46s - loss: 3.4902e-04 - val_loss: 3.0414e-04
 - val_f1: 0.9998
Epoch 37/300
 - 46s - loss: 3.2873e-04 - val_loss: 3.0257e-04
 - val_f1: 0.9998
Epoch 38/300
 - 46s - loss: 3.2617e-04 - val_loss: 3.2437e-04
 - val_f1: 0.9998
Epoch 39/300
 - 46s - loss: 3.2268e-04 - val_loss: 3.0319e-04
 - val_f1: 0.9998
Epoch 40/300
 - 46s - loss: 3.0977e-04 - val_loss: 3.0471e-04
 - val_f1: 0.9998
Epoch 41/300
 - 46s - loss: 2.9929e-04 - val_loss: 3.2995e-04
 - val_f1: 0.9998
Epoch 42/300
 - 46s - loss: 3.0198e-04 - val_loss: 3.5069e-04
 - val_f1: 0.9998
Epoch 43/300
 - 46s - loss: 3.0060e-04 - val_loss: 3.1426e-04
 - val_f1: 0.9998
Epoch 44/300
 - 47s - loss: 2.8456e-04 - val_loss: 3.0836e-04
 - val_f1: 0.9998
Epoch 45/300
 - 46s - loss: 2.9465e-04 - val_loss: 3.0825e-04
 - val_f1: 0.9998
Epoch 46/300
 - 46s - loss: 2.7894e-04 - val_loss: 3.0162e-04
 - val_f1: 0.9998
Epoch 47/300
 - 46s - loss: 2.7615e-04 - val_loss: 2.9964e-04
 - val_f1: 0.9998
Epoch 48/300
 - 46s - loss: 2.7073e-04 - val_loss: 3.0763e-04
 - val_f1: 0.9998
Epoch 49/300
 - 46s - loss: 2.7227e-04 - val_loss: 3.2202e-04
 - val_f1: 0.9998
Epoch 50/300
 - 46s - loss: 2.7331e-04 - val_loss: 3.1905e-04
 - val_f1: 0.9998
Epoch 51/300
 - 46s - loss: 2.7215e-04 - val_loss: 3.2789e-04
 - val_f1: 0.9998
Epoch 52/300
 - 46s - loss: 2.6952e-04 - val_loss: 3.0520e-04
 - val_f1: 0.9998
Epoch 53/300
 - 46s - loss: 2.6857e-04 - val_loss: 2.9372e-04
 - val_f1: 0.9998
Epoch 54/300
 - 46s - loss: 2.4984e-04 - val_loss: 3.0103e-04
 - val_f1: 0.9998
Epoch 55/300
 - 46s - loss: 2.5120e-04 - val_loss: 3.0083e-04
 - val_f1: 0.9998
Epoch 56/300
 - 46s - loss: 2.5360e-04 - val_loss: 2.9918e-04
 - val_f1: 0.9998
Epoch 57/300
 - 46s - loss: 2.4785e-04 - val_loss: 3.0542e-04
 - val_f1: 0.9998
Epoch 58/300
 - 46s - loss: 2.4827e-04 - val_loss: 3.1473e-04
 - val_f1: 0.9998
Epoch 59/300
 - 46s - loss: 2.3942e-04 - val_loss: 3.3844e-04
 - val_f1: 0.9998
Epoch 60/300
 - 46s - loss: 2.3782e-04 - val_loss: 2.8458e-04
 - val_f1: 0.9998
Epoch 61/300
 - 46s - loss: 2.2619e-04 - val_loss: 3.1674e-04
2019-12-28 10:03:32,776 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_kdd99_lstm_deep_rep5/_model_epoch_60.pickle
 - val_f1: 0.9998
Epoch 62/300
 - 46s - loss: 2.3670e-04 - val_loss: 3.2105e-04
 - val_f1: 0.9998
Epoch 63/300
 - 46s - loss: 2.5040e-04 - val_loss: 3.1631e-04
 - val_f1: 0.9998
Epoch 64/300
 - 46s - loss: 2.3003e-04 - val_loss: 3.0092e-04
 - val_f1: 0.9998
Epoch 65/300
 - 47s - loss: 2.2830e-04 - val_loss: 2.8559e-04
 - val_f1: 0.9998
Epoch 66/300
 - 46s - loss: 2.2805e-04 - val_loss: 2.7725e-04
 - val_f1: 0.9998
Epoch 67/300
 - 46s - loss: 2.1708e-04 - val_loss: 3.0983e-04
 - val_f1: 0.9998
Epoch 68/300
 - 46s - loss: 2.3348e-04 - val_loss: 3.0898e-04
 - val_f1: 0.9998
Epoch 69/300
 - 46s - loss: 2.3428e-04 - val_loss: 2.9926e-04
 - val_f1: 0.9998
Epoch 70/300
 - 46s - loss: 2.2032e-04 - val_loss: 3.2887e-04
 - val_f1: 0.9998
Epoch 71/300
 - 48s - loss: 2.2418e-04 - val_loss: 3.1637e-04
 - val_f1: 0.9998
Epoch 72/300
 - 47s - loss: 2.2272e-04 - val_loss: 3.0216e-04
 - val_f1: 0.9998
Epoch 73/300
 - 47s - loss: 2.2102e-04 - val_loss: 2.9952e-04
 - val_f1: 0.9998
Epoch 74/300
 - 47s - loss: 2.0640e-04 - val_loss: 3.2386e-04
 - val_f1: 0.9998
Epoch 75/300
 - 47s - loss: 2.2469e-04 - val_loss: 3.1970e-04
 - val_f1: 0.9998
Epoch 76/300
 - 47s - loss: 2.0482e-04 - val_loss: 3.1470e-04
 - val_f1: 0.9998
Epoch 77/300
 - 47s - loss: 2.0351e-04 - val_loss: 3.1685e-04
 - val_f1: 0.9998
Epoch 78/300
 - 47s - loss: 2.1193e-04 - val_loss: 3.5600e-04
 - val_f1: 0.9998
Epoch 79/300
 - 47s - loss: 2.0827e-04 - val_loss: 3.2181e-04
 - val_f1: 0.9998
Epoch 80/300
 - 47s - loss: 2.0967e-04 - val_loss: 3.0660e-04
 - val_f1: 0.9998
Epoch 81/300
 - 47s - loss: 2.0493e-04 - val_loss: 3.1484e-04
 - val_f1: 0.9998
Epoch 82/300
 - 47s - loss: 1.9640e-04 - val_loss: 3.2759e-04
 - val_f1: 0.9998
Epoch 83/300
 - 47s - loss: 1.9427e-04 - val_loss: 3.2782e-04
 - val_f1: 0.9998
Epoch 84/300
 - 47s - loss: 2.0101e-04 - val_loss: 3.1017e-04
 - val_f1: 0.9998
Epoch 85/300
 - 47s - loss: 1.8632e-04 - val_loss: 3.3123e-04
 - val_f1: 0.9998
Epoch 86/300
 - 47s - loss: 1.9077e-04 - val_loss: 3.1896e-04
 - val_f1: 0.9998
Epoch 87/300
 - 47s - loss: 1.9598e-04 - val_loss: 3.1573e-04
 - val_f1: 0.9998
Epoch 88/300
 - 47s - loss: 1.9884e-04 - val_loss: 3.4465e-04
 - val_f1: 0.9998
Epoch 89/300
 - 47s - loss: 2.0574e-04 - val_loss: 3.3715e-04
 - val_f1: 0.9998
Epoch 90/300
 - 47s - loss: 2.0115e-04 - val_loss: 3.3441e-04
 - val_f1: 0.9998
Epoch 91/300
 - 47s - loss: 1.9247e-04 - val_loss: 3.2502e-04
2019-12-28 10:32:37,918 [INFO] epoch = 90. Intermediate model saved to results_selected_models/selected_kdd99_lstm_deep_rep5/_model_epoch_90.pickle
 - val_f1: 0.9998
Epoch 92/300
 - 47s - loss: 2.0065e-04 - val_loss: 3.3025e-04
 - val_f1: 0.9998
Epoch 93/300
 - 47s - loss: 1.8771e-04 - val_loss: 3.1678e-04
 - val_f1: 0.9998
Epoch 94/300
 - 47s - loss: 1.8956e-04 - val_loss: 3.3370e-04
 - val_f1: 0.9998
Epoch 95/300
 - 47s - loss: 1.9533e-04 - val_loss: 3.1173e-04
 - val_f1: 0.9998
Epoch 96/300
 - 47s - loss: 1.7893e-04 - val_loss: 3.4406e-04
 - val_f1: 0.9998
Epoch 97/300
 - 47s - loss: 1.8751e-04 - val_loss: 3.3795e-04
 - val_f1: 0.9998
Epoch 98/300
 - 47s - loss: 1.9090e-04 - val_loss: 3.5799e-04
 - val_f1: 0.9998
Epoch 99/300
 - 47s - loss: 1.8588e-04 - val_loss: 3.5140e-04
 - val_f1: 0.9998
Epoch 100/300
 - 47s - loss: 1.8366e-04 - val_loss: 3.3799e-04
 - val_f1: 0.9998
Epoch 101/300
 - 47s - loss: 1.7599e-04 - val_loss: 3.3613e-04
 - val_f1: 0.9998
Epoch 102/300
 - 47s - loss: 1.9025e-04 - val_loss: 3.3526e-04
 - val_f1: 0.9998
Epoch 103/300
 - 47s - loss: 1.8128e-04 - val_loss: 3.1848e-04
 - val_f1: 0.9998
Epoch 104/300
 - 47s - loss: 1.7716e-04 - val_loss: 3.3547e-04
 - val_f1: 0.9998
Epoch 105/300
 - 47s - loss: 1.7735e-04 - val_loss: 3.3695e-04
 - val_f1: 0.9998
Epoch 106/300
 - 47s - loss: 1.7680e-04 - val_loss: 3.8442e-04
 - val_f1: 0.9998
Epoch 107/300
 - 47s - loss: 1.8813e-04 - val_loss: 3.5925e-04
 - val_f1: 0.9998
Epoch 108/300
 - 47s - loss: 1.7724e-04 - val_loss: 3.4590e-04
 - val_f1: 0.9998
Epoch 109/300
 - 47s - loss: 1.7126e-04 - val_loss: 3.7995e-04
 - val_f1: 0.9998
Epoch 110/300
 - 47s - loss: 1.7535e-04 - val_loss: 3.3085e-04
 - val_f1: 0.9998
Epoch 111/300
 - 47s - loss: 1.7211e-04 - val_loss: 3.3833e-04
 - val_f1: 0.9998
Epoch 112/300
 - 47s - loss: 1.8528e-04 - val_loss: 3.2391e-04
 - val_f1: 0.9998
Epoch 113/300
 - 47s - loss: 1.7063e-04 - val_loss: 3.6379e-04
 - val_f1: 0.9998
Epoch 114/300
 - 47s - loss: 1.7075e-04 - val_loss: 3.2181e-04
 - val_f1: 0.9998
Epoch 115/300
 - 47s - loss: 1.6447e-04 - val_loss: 3.2701e-04
 - val_f1: 0.9998
Epoch 116/300
 - 47s - loss: 1.7897e-04 - val_loss: 3.4083e-04
2019-12-28 10:57:04,929 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-28 10:58:00,569 [INFO] Last epoch loss evaluation: train_loss = 0.000136, val_loss = 0.000277
2019-12-28 10:58:00,569 [INFO] Training complete. time_to_train = 6799.55 sec, 113.33 min
2019-12-28 10:58:00,576 [INFO] Model saved to results_selected_models/selected_kdd99_lstm_deep_rep5/best_model.pickle
2019-12-28 10:58:00,578 [INFO] Training history saved to: results_selected_models/selected_kdd99_lstm_deep_rep5/training_error_history.csv
2019-12-28 10:58:00,769 [INFO] Plot saved to: results_selected_models/selected_kdd99_lstm_deep_rep5/training_error_history.png
2019-12-28 10:58:00,938 [INFO] Plot saved to: results_selected_models/selected_kdd99_lstm_deep_rep5/training_f1_history.png
2019-12-28 10:58:00,938 [INFO] Making predictions on training, validation, testing data
2019-12-28 10:58:57,704 [INFO] Evaluating predictions (results)
2019-12-28 10:59:06,402 [INFO] Dataset: Testing. Classification report below
2019-12-28 10:59:06,402 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      0.97      0.99    229853
     normal.       0.73      0.99      0.84     60580
       probe       0.83      0.76      0.80      4166
         r2l       0.87      0.05      0.09     13773
         u2r       0.06      0.00      0.00      2636

    accuracy                           0.92    311008
   macro avg       0.70      0.55      0.54    311008
weighted avg       0.93      0.92      0.91    311008

2019-12-28 10:59:06,402 [INFO] Overall accuracy (micro avg): 0.923866910175944
2019-12-28 10:59:15,742 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9239         0.9239                       0.9239                0.0190                   0.0761  0.9239
1     Macro avg        0.9695         0.6987                       0.5537                0.0199                   0.4463  0.5420
2  Weighted avg        0.9675         0.9290                       0.9239                0.0234                   0.0761  0.9060
2019-12-28 10:59:46,320 [INFO] Dataset: Validation. Classification report below
2019-12-28 10:59:46,320 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00    776671
     normal.       1.00      1.00      1.00    194553
       probe       1.00      0.99      1.00      8221
         r2l       0.91      0.90      0.90       225
         u2r       0.50      0.10      0.17        10

    accuracy                           1.00    979680
   macro avg       0.88      0.80      0.81    979680
weighted avg       1.00      1.00      1.00    979680

2019-12-28 10:59:46,320 [INFO] Overall accuracy (micro avg): 0.9998499510044097
2019-12-28 11:00:19,318 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9998         0.9998                       0.9998                0.0000                   0.0002  0.9998
1     Macro avg        0.9999         0.8813                       0.7983                0.0001                   0.2017  0.8131
2  Weighted avg        0.9999         0.9998                       0.9998                0.0001                   0.0002  0.9998
2019-12-28 11:02:32,433 [INFO] Dataset: Training. Classification report below
2019-12-28 11:02:32,433 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00   3106675
     normal.       1.00      1.00      1.00    778221
       probe       1.00      1.00      1.00     32881
         r2l       0.91      0.94      0.93       901
         u2r       0.95      0.48      0.63        42

    accuracy                           1.00   3918720
   macro avg       0.97      0.88      0.91   3918720
weighted avg       1.00      1.00      1.00   3918720

2019-12-28 11:02:32,433 [INFO] Overall accuracy (micro avg): 0.9998997121509065
2019-12-28 11:04:56,163 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9999         0.9999                       0.9999                0.0000                   0.0001  0.9999
1     Macro avg        1.0000         0.9727                       0.8832                0.0000                   0.1168  0.9120
2  Weighted avg        1.0000         0.9999                       0.9999                0.0001                   0.0001  0.9999
2019-12-28 11:04:56,210 [INFO] Results saved to: results_selected_models/selected_kdd99_lstm_deep_rep5/selected_kdd99_lstm_deep_rep5_results.xlsx
2019-12-28 11:04:56,217 [INFO] ================= Finished running experiment no. 5 ================= 

2019-12-28 11:04:56,327 [INFO] ================= Finished running 10 experiments ================= 

 - val_f1: 0.9998
Epoch 00116: early stopping
