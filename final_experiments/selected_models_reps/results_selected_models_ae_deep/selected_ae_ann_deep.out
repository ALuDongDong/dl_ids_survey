2019-12-24 01:12:03,362 [INFO] Created directory: results_selected_models/selected_nsl_ae_ann_deep_rep1
2019-12-24 01:12:03,362 [INFO] Initialized logging. log_filename = results_selected_models/selected_nsl_ae_ann_deep_rep1/run_log.log
2019-12-24 01:12:03,362 [INFO] ================= Running experiment no. 1  ================= 

2019-12-24 01:12:03,362 [INFO] Experiment parameters given below
2019-12-24 01:12:03,362 [INFO] 
{'experiment_num': 1, 'results_dir': 'results_selected_models/selected_nsl_ae_ann_deep_rep1', 'model_type': 'classifier', 'model': 'ae_ann', 'normal_label': 'normal', 'scaling_type': 'NA', 'unsupervised_ratio': 0.25, 'ae_encoder_units': [128, 64, 32], 'ae_encoder_activations': ['relu', 'relu', 'relu'], 'ae_encoder_dropout_rates': [0.2, 0.2, 0.2], 'ae_encoder_l1_param': -1, 'ae_decoder_units': [64, 128], 'ae_decoder_activations': ['relu', 'relu'], 'ae_decoder_dropout_rates': [0.2, 0.2], 'output_activation': 'sigmoid', 'loss_function': 'binary_crossentropy', 'ae_epochs': 200, 'ann_layer_units': [64], 'ann_layer_activations': ['relu'], 'ann_layer_dropout_rates': [0.2], 'ann_epochs': 200, 'early_stop_patience': 50, 'batch_size': 256, 'dataset_dir': '../Datasets/small_datasets/nsl_kdd_five_classes', 'description': 'selected_nsl_ae_ann_deep_rep1'}
2019-12-24 01:12:03,362 [INFO] Created tensorboard log directory: results_selected_models/selected_nsl_ae_ann_deep_rep1/tf_logs_run_2019_12_24-01_12_03
2019-12-24 01:12:03,362 [INFO] Loading datsets from: ../Datasets/small_datasets/nsl_kdd_five_classes
2019-12-24 01:12:03,363 [INFO] Reading X, y files
2019-12-24 01:12:03,363 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_train.h5
2019-12-24 01:12:03,594 [INFO] Reading complete. time_to_read=0.23 seconds
2019-12-24 01:12:03,594 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_val.h5
2019-12-24 01:12:03,657 [INFO] Reading complete. time_to_read=0.06 seconds
2019-12-24 01:12:03,657 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_test.h5
2019-12-24 01:12:03,714 [INFO] Reading complete. time_to_read=0.06 seconds
2019-12-24 01:12:03,714 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_train.h5
2019-12-24 01:12:03,722 [INFO] Reading complete. time_to_read=0.01 seconds
2019-12-24 01:12:03,722 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_val.h5
2019-12-24 01:12:03,727 [INFO] Reading complete. time_to_read=0.00 seconds
2019-12-24 01:12:03,727 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_test.h5
2019-12-24 01:12:03,731 [INFO] Reading complete. time_to_read=0.00 seconds
2019-12-24 01:12:03,847 [INFO] Initializing model
2019-12-24 01:12:04,324 [INFO] _________________________________________________________________
2019-12-24 01:12:04,324 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-24 01:12:04,324 [INFO] =================================================================
2019-12-24 01:12:04,324 [INFO] dense_21 (Dense)             (None, 128)               15744     
2019-12-24 01:12:04,324 [INFO] _________________________________________________________________
2019-12-24 01:12:04,324 [INFO] batch_normalization_11 (Batc (None, 128)               512       
2019-12-24 01:12:04,324 [INFO] _________________________________________________________________
2019-12-24 01:12:04,324 [INFO] dropout_11 (Dropout)         (None, 128)               0         
2019-12-24 01:12:04,324 [INFO] _________________________________________________________________
2019-12-24 01:12:04,325 [INFO] dense_22 (Dense)             (None, 64)                8256      
2019-12-24 01:12:04,325 [INFO] _________________________________________________________________
2019-12-24 01:12:04,325 [INFO] batch_normalization_12 (Batc (None, 64)                256       
2019-12-24 01:12:04,325 [INFO] _________________________________________________________________
2019-12-24 01:12:04,325 [INFO] dropout_12 (Dropout)         (None, 64)                0         
2019-12-24 01:12:04,325 [INFO] _________________________________________________________________
2019-12-24 01:12:04,325 [INFO] dense_23 (Dense)             (None, 32)                2080      
2019-12-24 01:12:04,325 [INFO] _________________________________________________________________
2019-12-24 01:12:04,325 [INFO] batch_normalization_13 (Batc (None, 32)                128       
2019-12-24 01:12:04,325 [INFO] _________________________________________________________________
2019-12-24 01:12:04,325 [INFO] dropout_13 (Dropout)         (None, 32)                0         
2019-12-24 01:12:04,325 [INFO] _________________________________________________________________
2019-12-24 01:12:04,326 [INFO] dense_24 (Dense)             (None, 64)                2112      
2019-12-24 01:12:04,326 [INFO] _________________________________________________________________
2019-12-24 01:12:04,326 [INFO] batch_normalization_14 (Batc (None, 64)                256       
2019-12-24 01:12:04,326 [INFO] _________________________________________________________________
2019-12-24 01:12:04,326 [INFO] dropout_14 (Dropout)         (None, 64)                0         
2019-12-24 01:12:04,326 [INFO] _________________________________________________________________
2019-12-24 01:12:04,326 [INFO] dense_25 (Dense)             (None, 128)               8320      
2019-12-24 01:12:04,326 [INFO] _________________________________________________________________
2019-12-24 01:12:04,326 [INFO] batch_normalization_15 (Batc (None, 128)               512       
2019-12-24 01:12:04,326 [INFO] _________________________________________________________________
2019-12-24 01:12:04,326 [INFO] dropout_15 (Dropout)         (None, 128)               0         
2019-12-24 01:12:04,326 [INFO] _________________________________________________________________
2019-12-24 01:12:04,326 [INFO] dense_26 (Dense)             (None, 122)               15738     
2019-12-24 01:12:04,327 [INFO] =================================================================
2019-12-24 01:12:04,327 [INFO] Total params: 53,914
2019-12-24 01:12:04,327 [INFO] Trainable params: 53,082
2019-12-24 01:12:04,327 [INFO] Non-trainable params: 832
2019-12-24 01:12:04,327 [INFO] _________________________________________________________________
2019-12-24 01:12:04,451 [INFO] _________________________________________________________________
2019-12-24 01:12:04,451 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-24 01:12:04,451 [INFO] =================================================================
2019-12-24 01:12:04,451 [INFO] dense_27 (Dense)             (None, 64)                2112      
2019-12-24 01:12:04,451 [INFO] _________________________________________________________________
2019-12-24 01:12:04,451 [INFO] batch_normalization_16 (Batc (None, 64)                256       
2019-12-24 01:12:04,451 [INFO] _________________________________________________________________
2019-12-24 01:12:04,451 [INFO] dropout_16 (Dropout)         (None, 64)                0         
2019-12-24 01:12:04,451 [INFO] _________________________________________________________________
2019-12-24 01:12:04,451 [INFO] dense_28 (Dense)             (None, 5)                 325       
2019-12-24 01:12:04,451 [INFO] =================================================================
2019-12-24 01:12:04,452 [INFO] Total params: 2,693
2019-12-24 01:12:04,452 [INFO] Trainable params: 2,565
2019-12-24 01:12:04,452 [INFO] Non-trainable params: 128
2019-12-24 01:12:04,452 [INFO] _________________________________________________________________
2019-12-24 01:12:04,452 [INFO] Training model
2019-12-24 01:12:04,452 [INFO] Splitting train set into 2 sets (unsupervised, supervised), random_seed = None
2019-12-24 01:12:05,585 [INFO] Split sizes (instances). total = 100778, unsupervised = 25194, supervised = 75584, unsupervised dataset hash = 055e6b264225ed52ab045ea7015f86dd1b969af8
2019-12-24 01:12:05,585 [INFO] Training autoencoder
 - val_f1: 0.9996
Epoch 00146: early stopping
Train on 25194 samples, validate on 25195 samples
Epoch 1/200
 - 4s - loss: 0.3962 - val_loss: -4.7792e-01
Epoch 2/200
 - 1s - loss: -6.5080e-01 - val_loss: -1.3481e+00
Epoch 3/200
 - 1s - loss: -1.4027e+00 - val_loss: -1.8301e+00
Epoch 4/200
 - 1s - loss: -1.8579e+00 - val_loss: -2.2011e+00
Epoch 5/200
 - 1s - loss: -2.1535e+00 - val_loss: -2.4610e+00
Epoch 6/200
 - 1s - loss: -2.3637e+00 - val_loss: -2.6344e+00
Epoch 7/200
 - 1s - loss: -2.5035e+00 - val_loss: -2.7710e+00
Epoch 8/200
 - 1s - loss: -2.6071e+00 - val_loss: -2.8691e+00
Epoch 9/200
 - 1s - loss: -2.6934e+00 - val_loss: -2.9321e+00
Epoch 10/200
 - 1s - loss: -2.7545e+00 - val_loss: -2.9691e+00
Epoch 11/200
 - 1s - loss: -2.7945e+00 - val_loss: -2.9909e+00
Epoch 12/200
 - 1s - loss: -2.8304e+00 - val_loss: -3.0113e+00
Epoch 13/200
 - 1s - loss: -2.8583e+00 - val_loss: -3.0313e+00
Epoch 14/200
 - 1s - loss: -2.8864e+00 - val_loss: -3.0394e+00
Epoch 15/200
 - 1s - loss: -2.9003e+00 - val_loss: -3.0517e+00
Epoch 16/200
 - 1s - loss: -2.9197e+00 - val_loss: -3.0587e+00
Epoch 17/200
 - 1s - loss: -2.9270e+00 - val_loss: -3.0689e+00
Epoch 18/200
 - 1s - loss: -2.9467e+00 - val_loss: -3.0758e+00
Epoch 19/200
 - 1s - loss: -2.9508e+00 - val_loss: -3.0799e+00
Epoch 20/200
 - 1s - loss: -2.9639e+00 - val_loss: -3.0868e+00
Epoch 21/200
 - 1s - loss: -2.9684e+00 - val_loss: -3.0933e+00
2019-12-24 01:12:36,353 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep1/ae_model_epoch_20.pickle
Epoch 22/200
 - 1s - loss: -2.9787e+00 - val_loss: -3.0979e+00
Epoch 23/200
 - 1s - loss: -2.9898e+00 - val_loss: -3.1009e+00
Epoch 24/200
 - 1s - loss: -2.9941e+00 - val_loss: -3.1072e+00
Epoch 25/200
 - 1s - loss: -2.9997e+00 - val_loss: -3.1080e+00
Epoch 26/200
 - 1s - loss: -3.0035e+00 - val_loss: -3.1143e+00
Epoch 27/200
 - 1s - loss: -3.0103e+00 - val_loss: -3.1178e+00
Epoch 28/200
 - 1s - loss: -3.0145e+00 - val_loss: -3.1243e+00
Epoch 29/200
 - 1s - loss: -3.0229e+00 - val_loss: -3.1260e+00
Epoch 30/200
 - 1s - loss: -3.0261e+00 - val_loss: -3.1298e+00
Epoch 31/200
 - 1s - loss: -3.0274e+00 - val_loss: -3.1307e+00
Epoch 32/200
 - 1s - loss: -3.0358e+00 - val_loss: -3.1341e+00
Epoch 33/200
 - 1s - loss: -3.0401e+00 - val_loss: -3.1374e+00
Epoch 34/200
 - 1s - loss: -3.0451e+00 - val_loss: -3.1393e+00
Epoch 35/200
 - 1s - loss: -3.0494e+00 - val_loss: -3.1447e+00
Epoch 36/200
 - 1s - loss: -3.0486e+00 - val_loss: -3.1452e+00
Epoch 37/200
 - 1s - loss: -3.0514e+00 - val_loss: -3.1445e+00
Epoch 38/200
 - 1s - loss: -3.0533e+00 - val_loss: -3.1482e+00
Epoch 39/200
 - 1s - loss: -3.0536e+00 - val_loss: -3.1492e+00
Epoch 40/200
 - 1s - loss: -3.0598e+00 - val_loss: -3.1491e+00
Epoch 41/200
 - 1s - loss: -3.0624e+00 - val_loss: -3.1515e+00
2019-12-24 01:12:58,683 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep1/ae_model_epoch_40.pickle
Epoch 42/200
 - 1s - loss: -3.0670e+00 - val_loss: -3.1528e+00
Epoch 43/200
 - 1s - loss: -3.0724e+00 - val_loss: -3.1539e+00
Epoch 44/200
 - 1s - loss: -3.0678e+00 - val_loss: -3.1551e+00
Epoch 45/200
 - 1s - loss: -3.0712e+00 - val_loss: -3.1553e+00
Epoch 46/200
 - 1s - loss: -3.0749e+00 - val_loss: -3.1557e+00
Epoch 47/200
 - 1s - loss: -3.0767e+00 - val_loss: -3.1573e+00
Epoch 48/200
 - 1s - loss: -3.0833e+00 - val_loss: -3.1597e+00
Epoch 49/200
 - 1s - loss: -3.0804e+00 - val_loss: -3.1599e+00
Epoch 50/200
 - 1s - loss: -3.0786e+00 - val_loss: -3.1610e+00
Epoch 51/200
 - 1s - loss: -3.0829e+00 - val_loss: -3.1618e+00
Epoch 52/200
 - 1s - loss: -3.0864e+00 - val_loss: -3.1617e+00
Epoch 53/200
 - 1s - loss: -3.0874e+00 - val_loss: -3.1644e+00
Epoch 54/200
 - 1s - loss: -3.0876e+00 - val_loss: -3.1639e+00
Epoch 55/200
 - 1s - loss: -3.0899e+00 - val_loss: -3.1660e+00
Epoch 56/200
 - 1s - loss: -3.0911e+00 - val_loss: -3.1658e+00
Epoch 57/200
 - 1s - loss: -3.0936e+00 - val_loss: -3.1666e+00
Epoch 58/200
 - 1s - loss: -3.0929e+00 - val_loss: -3.1677e+00
Epoch 59/200
 - 1s - loss: -3.0963e+00 - val_loss: -3.1669e+00
Epoch 60/200
 - 1s - loss: -3.0984e+00 - val_loss: -3.1683e+00
Epoch 61/200
 - 1s - loss: -3.0999e+00 - val_loss: -3.1696e+00
2019-12-24 01:13:21,031 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep1/ae_model_epoch_60.pickle
Epoch 62/200
 - 1s - loss: -3.1017e+00 - val_loss: -3.1704e+00
Epoch 63/200
 - 1s - loss: -3.0993e+00 - val_loss: -3.1708e+00
Epoch 64/200
 - 1s - loss: -3.1049e+00 - val_loss: -3.1710e+00
Epoch 65/200
 - 1s - loss: -3.1048e+00 - val_loss: -3.1723e+00
Epoch 66/200
 - 1s - loss: -3.1054e+00 - val_loss: -3.1723e+00
Epoch 67/200
 - 1s - loss: -3.1045e+00 - val_loss: -3.1728e+00
Epoch 68/200
 - 1s - loss: -3.1056e+00 - val_loss: -3.1727e+00
Epoch 69/200
 - 1s - loss: -3.1092e+00 - val_loss: -3.1738e+00
Epoch 70/200
 - 1s - loss: -3.1090e+00 - val_loss: -3.1738e+00
Epoch 71/200
 - 1s - loss: -3.1096e+00 - val_loss: -3.1740e+00
Epoch 72/200
 - 1s - loss: -3.1129e+00 - val_loss: -3.1760e+00
Epoch 73/200
 - 1s - loss: -3.1110e+00 - val_loss: -3.1757e+00
Epoch 74/200
 - 1s - loss: -3.1137e+00 - val_loss: -3.1754e+00
Epoch 75/200
 - 1s - loss: -3.1114e+00 - val_loss: -3.1757e+00
Epoch 76/200
 - 1s - loss: -3.1141e+00 - val_loss: -3.1783e+00
Epoch 77/200
 - 1s - loss: -3.1148e+00 - val_loss: -3.1787e+00
Epoch 78/200
 - 1s - loss: -3.1165e+00 - val_loss: -3.1787e+00
Epoch 79/200
 - 1s - loss: -3.1172e+00 - val_loss: -3.1796e+00
Epoch 80/200
 - 1s - loss: -3.1189e+00 - val_loss: -3.1806e+00
Epoch 81/200
 - 1s - loss: -3.1190e+00 - val_loss: -3.1808e+00
2019-12-24 01:13:43,364 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep1/ae_model_epoch_80.pickle
Epoch 82/200
 - 1s - loss: -3.1217e+00 - val_loss: -3.1807e+00
Epoch 83/200
 - 1s - loss: -3.1177e+00 - val_loss: -3.1799e+00
Epoch 84/200
 - 1s - loss: -3.1231e+00 - val_loss: -3.1812e+00
Epoch 85/200
 - 1s - loss: -3.1228e+00 - val_loss: -3.1825e+00
Epoch 86/200
 - 1s - loss: -3.1224e+00 - val_loss: -3.1816e+00
Epoch 87/200
 - 1s - loss: -3.1213e+00 - val_loss: -3.1820e+00
Epoch 88/200
 - 1s - loss: -3.1232e+00 - val_loss: -3.1836e+00
Epoch 89/200
 - 1s - loss: -3.1235e+00 - val_loss: -3.1844e+00
Epoch 90/200
 - 1s - loss: -3.1225e+00 - val_loss: -3.1846e+00
Epoch 91/200
 - 1s - loss: -3.1247e+00 - val_loss: -3.1838e+00
Epoch 92/200
 - 1s - loss: -3.1241e+00 - val_loss: -3.1831e+00
Epoch 93/200
 - 1s - loss: -3.1229e+00 - val_loss: -3.1846e+00
Epoch 94/200
 - 1s - loss: -3.1271e+00 - val_loss: -3.1848e+00
Epoch 95/200
 - 1s - loss: -3.1282e+00 - val_loss: -3.1851e+00
Epoch 96/200
 - 1s - loss: -3.1258e+00 - val_loss: -3.1858e+00
Epoch 97/200
 - 1s - loss: -3.1294e+00 - val_loss: -3.1856e+00
Epoch 98/200
 - 1s - loss: -3.1309e+00 - val_loss: -3.1859e+00
Epoch 99/200
 - 1s - loss: -3.1313e+00 - val_loss: -3.1872e+00
Epoch 100/200
 - 1s - loss: -3.1280e+00 - val_loss: -3.1865e+00
Epoch 101/200
 - 1s - loss: -3.1295e+00 - val_loss: -3.1877e+00
2019-12-24 01:14:05,707 [INFO] epoch = 100. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep1/ae_model_epoch_100.pickle
Epoch 102/200
 - 1s - loss: -3.1321e+00 - val_loss: -3.1867e+00
Epoch 103/200
 - 1s - loss: -3.1338e+00 - val_loss: -3.1876e+00
Epoch 104/200
 - 1s - loss: -3.1326e+00 - val_loss: -3.1877e+00
Epoch 105/200
 - 1s - loss: -3.1366e+00 - val_loss: -3.1891e+00
Epoch 106/200
 - 1s - loss: -3.1348e+00 - val_loss: -3.1881e+00
Epoch 107/200
 - 1s - loss: -3.1348e+00 - val_loss: -3.1900e+00
Epoch 108/200
 - 1s - loss: -3.1329e+00 - val_loss: -3.1891e+00
Epoch 109/200
 - 1s - loss: -3.1391e+00 - val_loss: -3.1892e+00
Epoch 110/200
 - 1s - loss: -3.1376e+00 - val_loss: -3.1898e+00
Epoch 111/200
 - 1s - loss: -3.1369e+00 - val_loss: -3.1895e+00
Epoch 112/200
 - 1s - loss: -3.1348e+00 - val_loss: -3.1901e+00
Epoch 113/200
 - 1s - loss: -3.1363e+00 - val_loss: -3.1903e+00
Epoch 114/200
 - 1s - loss: -3.1379e+00 - val_loss: -3.1909e+00
Epoch 115/200
 - 1s - loss: -3.1370e+00 - val_loss: -3.1910e+00
Epoch 116/200
 - 1s - loss: -3.1399e+00 - val_loss: -3.1913e+00
Epoch 117/200
 - 1s - loss: -3.1411e+00 - val_loss: -3.1921e+00
Epoch 118/200
 - 1s - loss: -3.1387e+00 - val_loss: -3.1918e+00
Epoch 119/200
 - 1s - loss: -3.1416e+00 - val_loss: -3.1921e+00
Epoch 120/200
 - 1s - loss: -3.1381e+00 - val_loss: -3.1928e+00
Epoch 121/200
 - 1s - loss: -3.1404e+00 - val_loss: -3.1928e+00
2019-12-24 01:14:28,018 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep1/ae_model_epoch_120.pickle
Epoch 122/200
 - 1s - loss: -3.1414e+00 - val_loss: -3.1933e+00
Epoch 123/200
 - 1s - loss: -3.1412e+00 - val_loss: -3.1938e+00
Epoch 124/200
 - 1s - loss: -3.1393e+00 - val_loss: -3.1937e+00
Epoch 125/200
 - 1s - loss: -3.1429e+00 - val_loss: -3.1928e+00
Epoch 126/200
 - 1s - loss: -3.1436e+00 - val_loss: -3.1941e+00
Epoch 127/200
 - 1s - loss: -3.1424e+00 - val_loss: -3.1948e+00
Epoch 128/200
 - 1s - loss: -3.1453e+00 - val_loss: -3.1944e+00
Epoch 129/200
 - 1s - loss: -3.1414e+00 - val_loss: -3.1933e+00
Epoch 130/200
 - 1s - loss: -3.1436e+00 - val_loss: -3.1946e+00
Epoch 131/200
 - 1s - loss: -3.1441e+00 - val_loss: -3.1955e+00
Epoch 132/200
 - 1s - loss: -3.1448e+00 - val_loss: -3.1952e+00
Epoch 133/200
 - 1s - loss: -3.1455e+00 - val_loss: -3.1958e+00
Epoch 134/200
 - 1s - loss: -3.1476e+00 - val_loss: -3.1958e+00
Epoch 135/200
 - 1s - loss: -3.1475e+00 - val_loss: -3.1961e+00
Epoch 136/200
 - 1s - loss: -3.1458e+00 - val_loss: -3.1958e+00
Epoch 137/200
 - 1s - loss: -3.1463e+00 - val_loss: -3.1962e+00
Epoch 138/200
 - 1s - loss: -3.1468e+00 - val_loss: -3.1968e+00
Epoch 139/200
 - 1s - loss: -3.1466e+00 - val_loss: -3.1975e+00
Epoch 140/200
 - 1s - loss: -3.1489e+00 - val_loss: -3.1983e+00
Epoch 141/200
 - 1s - loss: -3.1481e+00 - val_loss: -3.1974e+00
2019-12-24 01:14:50,350 [INFO] epoch = 140. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep1/ae_model_epoch_140.pickle
Epoch 142/200
 - 1s - loss: -3.1450e+00 - val_loss: -3.1979e+00
Epoch 143/200
 - 1s - loss: -3.1486e+00 - val_loss: -3.1987e+00
Epoch 144/200
 - 1s - loss: -3.1469e+00 - val_loss: -3.1983e+00
Epoch 145/200
 - 1s - loss: -3.1470e+00 - val_loss: -3.1979e+00
Epoch 146/200
 - 1s - loss: -3.1500e+00 - val_loss: -3.1977e+00
Epoch 147/200
 - 1s - loss: -3.1496e+00 - val_loss: -3.1974e+00
Epoch 148/200
 - 1s - loss: -3.1498e+00 - val_loss: -3.1983e+00
Epoch 149/200
 - 1s - loss: -3.1507e+00 - val_loss: -3.1992e+00
Epoch 150/200
 - 1s - loss: -3.1518e+00 - val_loss: -3.1988e+00
Epoch 151/200
 - 1s - loss: -3.1520e+00 - val_loss: -3.1992e+00
Epoch 152/200
 - 1s - loss: -3.1526e+00 - val_loss: -3.2000e+00
Epoch 153/200
 - 1s - loss: -3.1511e+00 - val_loss: -3.1993e+00
Epoch 154/200
 - 1s - loss: -3.1538e+00 - val_loss: -3.1997e+00
Epoch 155/200
 - 1s - loss: -3.1515e+00 - val_loss: -3.1995e+00
Epoch 156/200
 - 1s - loss: -3.1517e+00 - val_loss: -3.1994e+00
Epoch 157/200
 - 1s - loss: -3.1485e+00 - val_loss: -3.1995e+00
Epoch 158/200
 - 1s - loss: -3.1533e+00 - val_loss: -3.2005e+00
Epoch 159/200
 - 1s - loss: -3.1497e+00 - val_loss: -3.1997e+00
Epoch 160/200
 - 1s - loss: -3.1521e+00 - val_loss: -3.2005e+00
Epoch 161/200
 - 1s - loss: -3.1540e+00 - val_loss: -3.2014e+00
2019-12-24 01:15:12,674 [INFO] epoch = 160. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep1/ae_model_epoch_160.pickle
Epoch 162/200
 - 1s - loss: -3.1564e+00 - val_loss: -3.2006e+00
Epoch 163/200
 - 1s - loss: -3.1547e+00 - val_loss: -3.2003e+00
Epoch 164/200
 - 1s - loss: -3.1546e+00 - val_loss: -3.2013e+00
Epoch 165/200
 - 1s - loss: -3.1567e+00 - val_loss: -3.2012e+00
Epoch 166/200
 - 1s - loss: -3.1544e+00 - val_loss: -3.2014e+00
Epoch 167/200
 - 1s - loss: -3.1544e+00 - val_loss: -3.2016e+00
Epoch 168/200
 - 1s - loss: -3.1548e+00 - val_loss: -3.2008e+00
Epoch 169/200
 - 1s - loss: -3.1561e+00 - val_loss: -3.2015e+00
Epoch 170/200
 - 1s - loss: -3.1555e+00 - val_loss: -3.2012e+00
Epoch 171/200
 - 1s - loss: -3.1564e+00 - val_loss: -3.2010e+00
Epoch 172/200
 - 1s - loss: -3.1571e+00 - val_loss: -3.2021e+00
Epoch 173/200
 - 1s - loss: -3.1570e+00 - val_loss: -3.2023e+00
Epoch 174/200
 - 1s - loss: -3.1532e+00 - val_loss: -3.2016e+00
Epoch 175/200
 - 1s - loss: -3.1557e+00 - val_loss: -3.2022e+00
Epoch 176/200
 - 1s - loss: -3.1573e+00 - val_loss: -3.2023e+00
Epoch 177/200
 - 1s - loss: -3.1549e+00 - val_loss: -3.2020e+00
Epoch 178/200
 - 1s - loss: -3.1570e+00 - val_loss: -3.2021e+00
Epoch 179/200
 - 1s - loss: -3.1565e+00 - val_loss: -3.2026e+00
Epoch 180/200
 - 1s - loss: -3.1570e+00 - val_loss: -3.2034e+00
Epoch 181/200
 - 1s - loss: -3.1587e+00 - val_loss: -3.2034e+00
2019-12-24 01:15:34,991 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep1/ae_model_epoch_180.pickle
Epoch 182/200
 - 1s - loss: -3.1580e+00 - val_loss: -3.2029e+00
Epoch 183/200
 - 1s - loss: -3.1615e+00 - val_loss: -3.2031e+00
Epoch 184/200
 - 1s - loss: -3.1594e+00 - val_loss: -3.2033e+00
Epoch 185/200
 - 1s - loss: -3.1580e+00 - val_loss: -3.2034e+00
Epoch 186/200
 - 1s - loss: -3.1582e+00 - val_loss: -3.2024e+00
Epoch 187/200
 - 1s - loss: -3.1600e+00 - val_loss: -3.2031e+00
Epoch 188/200
 - 1s - loss: -3.1599e+00 - val_loss: -3.2030e+00
Epoch 189/200
 - 1s - loss: -3.1585e+00 - val_loss: -3.2033e+00
Epoch 190/200
 - 1s - loss: -3.1597e+00 - val_loss: -3.2031e+00
Epoch 191/200
 - 1s - loss: -3.1540e+00 - val_loss: -3.2024e+00
Epoch 192/200
 - 1s - loss: -3.1614e+00 - val_loss: -3.2035e+00
Epoch 193/200
 - 1s - loss: -3.1608e+00 - val_loss: -3.2040e+00
Epoch 194/200
 - 1s - loss: -3.1588e+00 - val_loss: -3.2039e+00
Epoch 195/200
 - 1s - loss: -3.1633e+00 - val_loss: -3.2046e+00
Epoch 196/200
 - 1s - loss: -3.1585e+00 - val_loss: -3.2038e+00
Epoch 197/200
 - 1s - loss: -3.1617e+00 - val_loss: -3.2046e+00
Epoch 198/200
 - 1s - loss: -3.1599e+00 - val_loss: -3.2049e+00
Epoch 199/200
 - 1s - loss: -3.1608e+00 - val_loss: -3.2052e+00
Epoch 200/200
 - 1s - loss: -3.1607e+00 - val_loss: -3.2042e+00
2019-12-24 01:15:56,168 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-24 01:15:58,630 [INFO] Last epoch loss evaluation: train_loss = -3.229300, val_loss = -3.205168
2019-12-24 01:15:58,630 [INFO] Training autoencoder complete
2019-12-24 01:15:58,630 [INFO] Encoding data for supervised training
2019-12-24 01:16:02,337 [INFO] Encoding complete
2019-12-24 01:16:02,337 [INFO] Training neural network layers (after autoencoder)
Train on 75584 samples, validate on 25195 samples
Epoch 1/200
 - 2s - loss: 0.1237 - val_loss: 0.0400
 - val_f1: 0.9711
Epoch 2/200
 - 1s - loss: 0.0408 - val_loss: 0.0289
 - val_f1: 0.9742
Epoch 3/200
 - 1s - loss: 0.0319 - val_loss: 0.0240
 - val_f1: 0.9761
Epoch 4/200
 - 1s - loss: 0.0266 - val_loss: 0.0196
 - val_f1: 0.9817
Epoch 5/200
 - 1s - loss: 0.0237 - val_loss: 0.0177
 - val_f1: 0.9854
Epoch 6/200
 - 1s - loss: 0.0211 - val_loss: 0.0155
 - val_f1: 0.9866
Epoch 7/200
 - 1s - loss: 0.0199 - val_loss: 0.0154
 - val_f1: 0.9885
Epoch 8/200
 - 1s - loss: 0.0185 - val_loss: 0.0135
 - val_f1: 0.9895
Epoch 9/200
 - 1s - loss: 0.0181 - val_loss: 0.0138
 - val_f1: 0.9882
Epoch 10/200
 - 1s - loss: 0.0174 - val_loss: 0.0127
 - val_f1: 0.9907
Epoch 11/200
 - 1s - loss: 0.0169 - val_loss: 0.0119
 - val_f1: 0.9912
Epoch 12/200
 - 1s - loss: 0.0163 - val_loss: 0.0127
 - val_f1: 0.9890
Epoch 13/200
 - 1s - loss: 0.0160 - val_loss: 0.0116
 - val_f1: 0.9908
Epoch 14/200
 - 1s - loss: 0.0155 - val_loss: 0.0118
 - val_f1: 0.9903
Epoch 15/200
 - 1s - loss: 0.0152 - val_loss: 0.0123
 - val_f1: 0.9901
Epoch 16/200
 - 1s - loss: 0.0147 - val_loss: 0.0113
 - val_f1: 0.9911
Epoch 17/200
 - 1s - loss: 0.0145 - val_loss: 0.0111
 - val_f1: 0.9907
Epoch 18/200
 - 1s - loss: 0.0143 - val_loss: 0.0107
 - val_f1: 0.9913
Epoch 19/200
 - 1s - loss: 0.0135 - val_loss: 0.0107
 - val_f1: 0.9913
Epoch 20/200
 - 1s - loss: 0.0137 - val_loss: 0.0111
 - val_f1: 0.9907
Epoch 21/200
 - 1s - loss: 0.0137 - val_loss: 0.0105
2019-12-24 01:16:35,873 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep1/ann_model_epoch_20.pickle
 - val_f1: 0.9918
Epoch 22/200
 - 1s - loss: 0.0133 - val_loss: 0.0099
 - val_f1: 0.9923
Epoch 23/200
 - 1s - loss: 0.0130 - val_loss: 0.0098
 - val_f1: 0.9921
Epoch 24/200
 - 1s - loss: 0.0130 - val_loss: 0.0112
 - val_f1: 0.9906
Epoch 25/200
 - 1s - loss: 0.0127 - val_loss: 0.0103
 - val_f1: 0.9916
Epoch 26/200
 - 1s - loss: 0.0124 - val_loss: 0.0099
 - val_f1: 0.9917
Epoch 27/200
 - 1s - loss: 0.0122 - val_loss: 0.0102
 - val_f1: 0.9914
Epoch 28/200
 - 1s - loss: 0.0127 - val_loss: 0.0099
 - val_f1: 0.9913
Epoch 29/200
 - 1s - loss: 0.0128 - val_loss: 0.0106
 - val_f1: 0.9913
Epoch 30/200
 - 1s - loss: 0.0124 - val_loss: 0.0094
 - val_f1: 0.9923
Epoch 31/200
 - 1s - loss: 0.0122 - val_loss: 0.0094
 - val_f1: 0.9927
Epoch 32/200
 - 1s - loss: 0.0122 - val_loss: 0.0095
 - val_f1: 0.9920
Epoch 33/200
 - 1s - loss: 0.0120 - val_loss: 0.0099
 - val_f1: 0.9916
Epoch 34/200
 - 1s - loss: 0.0118 - val_loss: 0.0096
 - val_f1: 0.9914
Epoch 35/200
 - 1s - loss: 0.0115 - val_loss: 0.0094
 - val_f1: 0.9925
Epoch 36/200
 - 1s - loss: 0.0116 - val_loss: 0.0093
 - val_f1: 0.9922
Epoch 37/200
 - 1s - loss: 0.0116 - val_loss: 0.0102
 - val_f1: 0.9914
Epoch 38/200
 - 1s - loss: 0.0117 - val_loss: 0.0096
 - val_f1: 0.9921
Epoch 39/200
 - 1s - loss: 0.0119 - val_loss: 0.0094
 - val_f1: 0.9926
Epoch 40/200
 - 1s - loss: 0.0114 - val_loss: 0.0094
 - val_f1: 0.9931
Epoch 41/200
 - 1s - loss: 0.0115 - val_loss: 0.0097
2019-12-24 01:17:04,375 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep1/ann_model_epoch_40.pickle
 - val_f1: 0.9924
Epoch 42/200
 - 1s - loss: 0.0113 - val_loss: 0.0099
 - val_f1: 0.9917
Epoch 43/200
 - 1s - loss: 0.0117 - val_loss: 0.0096
 - val_f1: 0.9928
Epoch 44/200
 - 1s - loss: 0.0113 - val_loss: 0.0099
 - val_f1: 0.9915
Epoch 45/200
 - 1s - loss: 0.0110 - val_loss: 0.0094
 - val_f1: 0.9921
Epoch 46/200
 - 1s - loss: 0.0115 - val_loss: 0.0099
 - val_f1: 0.9926
Epoch 47/200
 - 1s - loss: 0.0112 - val_loss: 0.0098
 - val_f1: 0.9916
Epoch 48/200
 - 1s - loss: 0.0111 - val_loss: 0.0096
 - val_f1: 0.9919
Epoch 49/200
 - 1s - loss: 0.0112 - val_loss: 0.0102
 - val_f1: 0.9907
Epoch 50/200
 - 1s - loss: 0.0109 - val_loss: 0.0097
 - val_f1: 0.9913
Epoch 51/200
 - 1s - loss: 0.0112 - val_loss: 0.0094
 - val_f1: 0.9917
Epoch 52/200
 - 1s - loss: 0.0109 - val_loss: 0.0097
 - val_f1: 0.9909
Epoch 53/200
 - 1s - loss: 0.0108 - val_loss: 0.0092
 - val_f1: 0.9927
Epoch 54/200
 - 1s - loss: 0.0108 - val_loss: 0.0103
 - val_f1: 0.9910
Epoch 55/200
 - 1s - loss: 0.0107 - val_loss: 0.0094
 - val_f1: 0.9924
Epoch 56/200
 - 1s - loss: 0.0108 - val_loss: 0.0094
 - val_f1: 0.9925
Epoch 57/200
 - 1s - loss: 0.0111 - val_loss: 0.0089
 - val_f1: 0.9925
Epoch 58/200
 - 1s - loss: 0.0106 - val_loss: 0.0100
 - val_f1: 0.9927
Epoch 59/200
 - 1s - loss: 0.0106 - val_loss: 0.0091
 - val_f1: 0.9929
Epoch 60/200
 - 1s - loss: 0.0107 - val_loss: 0.0096
 - val_f1: 0.9914
Epoch 61/200
 - 1s - loss: 0.0105 - val_loss: 0.0096
2019-12-24 01:17:32,719 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep1/ann_model_epoch_60.pickle
 - val_f1: 0.9918
Epoch 62/200
 - 1s - loss: 0.0107 - val_loss: 0.0087
 - val_f1: 0.9921
Epoch 63/200
 - 1s - loss: 0.0105 - val_loss: 0.0109
 - val_f1: 0.9913
Epoch 64/200
 - 1s - loss: 0.0105 - val_loss: 0.0089
 - val_f1: 0.9920
Epoch 65/200
 - 1s - loss: 0.0101 - val_loss: 0.0089
 - val_f1: 0.9934
Epoch 66/200
 - 1s - loss: 0.0102 - val_loss: 0.0089
 - val_f1: 0.9932
Epoch 67/200
 - 1s - loss: 0.0103 - val_loss: 0.0094
 - val_f1: 0.9923
Epoch 68/200
 - 1s - loss: 0.0103 - val_loss: 0.0088
 - val_f1: 0.9935
Epoch 69/200
 - 1s - loss: 0.0105 - val_loss: 0.0101
 - val_f1: 0.9918
Epoch 70/200
 - 1s - loss: 0.0104 - val_loss: 0.0090
 - val_f1: 0.9931
Epoch 71/200
 - 1s - loss: 0.0102 - val_loss: 0.0089
 - val_f1: 0.9920
Epoch 72/200
 - 1s - loss: 0.0104 - val_loss: 0.0087
 - val_f1: 0.9932
Epoch 73/200
 - 1s - loss: 0.0103 - val_loss: 0.0089
 - val_f1: 0.9930
Epoch 74/200
 - 1s - loss: 0.0102 - val_loss: 0.0089
 - val_f1: 0.9927
Epoch 75/200
 - 1s - loss: 0.0104 - val_loss: 0.0098
 - val_f1: 0.9929
Epoch 76/200
 - 1s - loss: 0.0103 - val_loss: 0.0088
 - val_f1: 0.9934
Epoch 77/200
 - 1s - loss: 0.0101 - val_loss: 0.0086
 - val_f1: 0.9921
Epoch 78/200
 - 1s - loss: 0.0097 - val_loss: 0.0088
 - val_f1: 0.9937
Epoch 79/200
 - 1s - loss: 0.0099 - val_loss: 0.0083
 - val_f1: 0.9932
Epoch 80/200
 - 1s - loss: 0.0099 - val_loss: 0.0088
 - val_f1: 0.9927
Epoch 81/200
 - 1s - loss: 0.0099 - val_loss: 0.0089
2019-12-24 01:18:01,053 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep1/ann_model_epoch_80.pickle
 - val_f1: 0.9920
Epoch 82/200
 - 1s - loss: 0.0097 - val_loss: 0.0088
 - val_f1: 0.9934
Epoch 83/200
 - 1s - loss: 0.0096 - val_loss: 0.0088
 - val_f1: 0.9927
Epoch 84/200
 - 1s - loss: 0.0097 - val_loss: 0.0089
 - val_f1: 0.9921
Epoch 85/200
 - 1s - loss: 0.0099 - val_loss: 0.0091
 - val_f1: 0.9922
Epoch 86/200
 - 1s - loss: 0.0095 - val_loss: 0.0086
 - val_f1: 0.9925
Epoch 87/200
 - 1s - loss: 0.0101 - val_loss: 0.0091
 - val_f1: 0.9932
Epoch 88/200
 - 1s - loss: 0.0092 - val_loss: 0.0090
 - val_f1: 0.9935
Epoch 89/200
 - 1s - loss: 0.0097 - val_loss: 0.0094
 - val_f1: 0.9930
Epoch 90/200
 - 1s - loss: 0.0098 - val_loss: 0.0088
 - val_f1: 0.9932
Epoch 91/200
 - 1s - loss: 0.0098 - val_loss: 0.0086
 - val_f1: 0.9934
Epoch 92/200
 - 1s - loss: 0.0098 - val_loss: 0.0086
 - val_f1: 0.9935
Epoch 93/200
 - 1s - loss: 0.0094 - val_loss: 0.0087
 - val_f1: 0.9935
Epoch 94/200
 - 1s - loss: 0.0096 - val_loss: 0.0092
 - val_f1: 0.9930
Epoch 95/200
 - 1s - loss: 0.0096 - val_loss: 0.0090
 - val_f1: 0.9922
Epoch 96/200
 - 1s - loss: 0.0095 - val_loss: 0.0091
 - val_f1: 0.9924
Epoch 97/200
 - 1s - loss: 0.0090 - val_loss: 0.0088
 - val_f1: 0.9931
Epoch 98/200
 - 1s - loss: 0.0097 - val_loss: 0.0090
 - val_f1: 0.9931
Epoch 99/200
 - 1s - loss: 0.0097 - val_loss: 0.0094
 - val_f1: 0.9929
Epoch 100/200
 - 1s - loss: 0.0091 - val_loss: 0.0083
 - val_f1: 0.9938
Epoch 101/200
 - 1s - loss: 0.0095 - val_loss: 0.0089
2019-12-24 01:18:29,290 [INFO] epoch = 100. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep1/ann_model_epoch_100.pickle
 - val_f1: 0.9930
Epoch 102/200
 - 1s - loss: 0.0094 - val_loss: 0.0085
 - val_f1: 0.9931
Epoch 103/200
 - 1s - loss: 0.0089 - val_loss: 0.0085
 - val_f1: 0.9939
Epoch 104/200
 - 1s - loss: 0.0091 - val_loss: 0.0094
 - val_f1: 0.9926
Epoch 105/200
 - 1s - loss: 0.0091 - val_loss: 0.0086
 - val_f1: 0.9935
Epoch 106/200
 - 1s - loss: 0.0094 - val_loss: 0.0091
 - val_f1: 0.9923
Epoch 107/200
 - 1s - loss: 0.0093 - val_loss: 0.0087
 - val_f1: 0.9927
Epoch 108/200
 - 1s - loss: 0.0094 - val_loss: 0.0086
 - val_f1: 0.9927
Epoch 109/200
 - 1s - loss: 0.0094 - val_loss: 0.0100
 - val_f1: 0.9922
Epoch 110/200
 - 1s - loss: 0.0093 - val_loss: 0.0090
 - val_f1: 0.9929
Epoch 111/200
 - 1s - loss: 0.0089 - val_loss: 0.0087
 - val_f1: 0.9926
Epoch 112/200
 - 1s - loss: 0.0093 - val_loss: 0.0088
 - val_f1: 0.9926
Epoch 113/200
 - 1s - loss: 0.0092 - val_loss: 0.0085
 - val_f1: 0.9938
Epoch 114/200
 - 1s - loss: 0.0094 - val_loss: 0.0089
 - val_f1: 0.9937
Epoch 115/200
 - 1s - loss: 0.0097 - val_loss: 0.0087
 - val_f1: 0.9932
Epoch 116/200
 - 1s - loss: 0.0094 - val_loss: 0.0087
 - val_f1: 0.9930
Epoch 117/200
 - 1s - loss: 0.0088 - val_loss: 0.0087
 - val_f1: 0.9939
Epoch 118/200
 - 1s - loss: 0.0094 - val_loss: 0.0085
 - val_f1: 0.9935
Epoch 119/200
 - 1s - loss: 0.0094 - val_loss: 0.0089
 - val_f1: 0.9932
Epoch 120/200
 - 1s - loss: 0.0093 - val_loss: 0.0084
 - val_f1: 0.9939
Epoch 121/200
 - 1s - loss: 0.0090 - val_loss: 0.0081
2019-12-24 01:18:57,577 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep1/ann_model_epoch_120.pickle
 - val_f1: 0.9938
Epoch 122/200
 - 1s - loss: 0.0087 - val_loss: 0.0087
 - val_f1: 0.9931
Epoch 123/200
 - 1s - loss: 0.0088 - val_loss: 0.0085
 - val_f1: 0.9932
Epoch 124/200
 - 1s - loss: 0.0091 - val_loss: 0.0083
 - val_f1: 0.9938
Epoch 125/200
 - 1s - loss: 0.0092 - val_loss: 0.0087
 - val_f1: 0.9927
Epoch 126/200
 - 1s - loss: 0.0094 - val_loss: 0.0085
 - val_f1: 0.9938
Epoch 127/200
 - 1s - loss: 0.0084 - val_loss: 0.0082
 - val_f1: 0.9937
Epoch 128/200
 - 1s - loss: 0.0092 - val_loss: 0.0086
 - val_f1: 0.9935
Epoch 129/200
 - 1s - loss: 0.0089 - val_loss: 0.0090
 - val_f1: 0.9930
Epoch 130/200
 - 1s - loss: 0.0089 - val_loss: 0.0088
 - val_f1: 0.9925
Epoch 131/200
 - 1s - loss: 0.0089 - val_loss: 0.0088
 - val_f1: 0.9926
Epoch 132/200
 - 1s - loss: 0.0091 - val_loss: 0.0081
 - val_f1: 0.9941
Epoch 133/200
 - 1s - loss: 0.0089 - val_loss: 0.0079
 - val_f1: 0.9938
Epoch 134/200
 - 1s - loss: 0.0088 - val_loss: 0.0086
 - val_f1: 0.9934
Epoch 135/200
 - 1s - loss: 0.0091 - val_loss: 0.0080
 - val_f1: 0.9939
Epoch 136/200
 - 1s - loss: 0.0087 - val_loss: 0.0086
 - val_f1: 0.9936
Epoch 137/200
 - 1s - loss: 0.0089 - val_loss: 0.0088
 - val_f1: 0.9921
Epoch 138/200
 - 1s - loss: 0.0092 - val_loss: 0.0079
 - val_f1: 0.9938
Epoch 139/200
 - 1s - loss: 0.0088 - val_loss: 0.0079
 - val_f1: 0.9937
Epoch 140/200
 - 1s - loss: 0.0089 - val_loss: 0.0092
 - val_f1: 0.9916
Epoch 141/200
 - 1s - loss: 0.0087 - val_loss: 0.0084
2019-12-24 01:19:25,898 [INFO] epoch = 140. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep1/ann_model_epoch_140.pickle
 - val_f1: 0.9940
Epoch 142/200
 - 1s - loss: 0.0085 - val_loss: 0.0083
 - val_f1: 0.9940
Epoch 143/200
 - 1s - loss: 0.0090 - val_loss: 0.0080
 - val_f1: 0.9940
Epoch 144/200
 - 1s - loss: 0.0085 - val_loss: 0.0083
 - val_f1: 0.9941
Epoch 145/200
 - 1s - loss: 0.0087 - val_loss: 0.0079
 - val_f1: 0.9941
Epoch 146/200
 - 1s - loss: 0.0085 - val_loss: 0.0083
 - val_f1: 0.9939
Epoch 147/200
 - 1s - loss: 0.0087 - val_loss: 0.0084
 - val_f1: 0.9939
Epoch 148/200
 - 1s - loss: 0.0086 - val_loss: 0.0083
 - val_f1: 0.9941
Epoch 149/200
 - 1s - loss: 0.0088 - val_loss: 0.0086
 - val_f1: 0.9936
Epoch 150/200
 - 1s - loss: 0.0087 - val_loss: 0.0087
 - val_f1: 0.9924
Epoch 151/200
 - 1s - loss: 0.0084 - val_loss: 0.0083
 - val_f1: 0.9933
Epoch 152/200
 - 1s - loss: 0.0085 - val_loss: 0.0083
 - val_f1: 0.9933
Epoch 153/200
 - 1s - loss: 0.0085 - val_loss: 0.0095
 - val_f1: 0.9921
Epoch 154/200
 - 1s - loss: 0.0086 - val_loss: 0.0081
 - val_f1: 0.9939
Epoch 155/200
 - 1s - loss: 0.0085 - val_loss: 0.0086
 - val_f1: 0.9927
Epoch 156/200
 - 1s - loss: 0.0084 - val_loss: 0.0083
 - val_f1: 0.9937
Epoch 157/200
 - 1s - loss: 0.0085 - val_loss: 0.0079
 - val_f1: 0.9943
Epoch 158/200
 - 1s - loss: 0.0087 - val_loss: 0.0080
 - val_f1: 0.9936
Epoch 159/200
 - 1s - loss: 0.0087 - val_loss: 0.0083
 - val_f1: 0.9937
Epoch 160/200
 - 1s - loss: 0.0089 - val_loss: 0.0080
 - val_f1: 0.9942
Epoch 161/200
 - 1s - loss: 0.0084 - val_loss: 0.0082
2019-12-24 01:19:54,199 [INFO] epoch = 160. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep1/ann_model_epoch_160.pickle
 - val_f1: 0.9941
Epoch 162/200
 - 1s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9945
Epoch 163/200
 - 1s - loss: 0.0087 - val_loss: 0.0081
 - val_f1: 0.9942
Epoch 164/200
 - 1s - loss: 0.0085 - val_loss: 0.0084
 - val_f1: 0.9935
Epoch 165/200
 - 1s - loss: 0.0086 - val_loss: 0.0089
 - val_f1: 0.9928
Epoch 166/200
 - 1s - loss: 0.0087 - val_loss: 0.0084
 - val_f1: 0.9940
Epoch 167/200
 - 1s - loss: 0.0085 - val_loss: 0.0091
 - val_f1: 0.9927
Epoch 168/200
 - 1s - loss: 0.0082 - val_loss: 0.0092
 - val_f1: 0.9932
Epoch 169/200
 - 1s - loss: 0.0083 - val_loss: 0.0087
 - val_f1: 0.9927
Epoch 170/200
 - 1s - loss: 0.0086 - val_loss: 0.0083
 - val_f1: 0.9936
Epoch 171/200
 - 1s - loss: 0.0084 - val_loss: 0.0085
 - val_f1: 0.9936
Epoch 172/200
 - 1s - loss: 0.0084 - val_loss: 0.0090
 - val_f1: 0.9926
Epoch 173/200
 - 1s - loss: 0.0085 - val_loss: 0.0086
 - val_f1: 0.9930
Epoch 174/200
 - 1s - loss: 0.0084 - val_loss: 0.0081
 - val_f1: 0.9944
Epoch 175/200
 - 1s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9941
Epoch 176/200
 - 1s - loss: 0.0085 - val_loss: 0.0084
 - val_f1: 0.9934
Epoch 177/200
 - 1s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9943
Epoch 178/200
 - 1s - loss: 0.0083 - val_loss: 0.0084
 - val_f1: 0.9928
Epoch 179/200
 - 1s - loss: 0.0083 - val_loss: 0.0083
 - val_f1: 0.9935
Epoch 180/200
 - 1s - loss: 0.0084 - val_loss: 0.0083
 - val_f1: 0.9937
Epoch 181/200
 - 1s - loss: 0.0085 - val_loss: 0.0081
2019-12-24 01:20:22,506 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep1/ann_model_epoch_180.pickle
 - val_f1: 0.9945
Epoch 182/200
 - 1s - loss: 0.0080 - val_loss: 0.0083
 - val_f1: 0.9931
Epoch 183/200
 - 1s - loss: 0.0082 - val_loss: 0.0084
 - val_f1: 0.9930
Epoch 184/200
 - 1s - loss: 0.0082 - val_loss: 0.0086
 - val_f1: 0.9929
Epoch 185/200
 - 1s - loss: 0.0082 - val_loss: 0.0083
 - val_f1: 0.9932
Epoch 186/200
 - 1s - loss: 0.0084 - val_loss: 0.0087
 - val_f1: 0.9925
Epoch 187/200
 - 1s - loss: 0.0083 - val_loss: 0.0079
 - val_f1: 0.9943
Epoch 188/200
 - 1s - loss: 0.0084 - val_loss: 0.0091
 - val_f1: 0.9927
Epoch 189/200
 - 1s - loss: 0.0088 - val_loss: 0.0094
 - val_f1: 0.9928
Epoch 190/200
 - 1s - loss: 0.0085 - val_loss: 0.0080
 - val_f1: 0.9939
Epoch 191/200
 - 1s - loss: 0.0088 - val_loss: 0.0080
 - val_f1: 0.9941
Epoch 192/200
 - 1s - loss: 0.0088 - val_loss: 0.0082
 - val_f1: 0.9941
Epoch 193/200
 - 1s - loss: 0.0086 - val_loss: 0.0084
 - val_f1: 0.9926
Epoch 194/200
 - 1s - loss: 0.0086 - val_loss: 0.0087
 - val_f1: 0.9927
Epoch 195/200
 - 1s - loss: 0.0089 - val_loss: 0.0082
 - val_f1: 0.9931
Epoch 196/200
 - 1s - loss: 0.0084 - val_loss: 0.0088
 - val_f1: 0.9928
Epoch 197/200
 - 1s - loss: 0.0087 - val_loss: 0.0080
 - val_f1: 0.9933
Epoch 198/200
 - 1s - loss: 0.0085 - val_loss: 0.0083
 - val_f1: 0.9935
Epoch 199/200
 - 1s - loss: 0.0084 - val_loss: 0.0079
 - val_f1: 0.9935
Epoch 200/200
 - 1s - loss: 0.0080 - val_loss: 0.0083
2019-12-24 01:20:50,027 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-24 01:20:53,123 [INFO] Last epoch loss evaluation: train_loss = 0.006475, val_loss = 0.007854
2019-12-24 01:20:53,124 [INFO] Training complete. time_to_train = 528.67 sec, 8.81 min
2019-12-24 01:20:53,144 [INFO] Model saved to results_selected_models/selected_nsl_ae_ann_deep_rep1/best_model.pickle
2019-12-24 01:20:53,330 [INFO] Plot saved to: results_selected_models/selected_nsl_ae_ann_deep_rep1/training_error_history.png
2019-12-24 01:20:53,508 [INFO] Plot saved to: results_selected_models/selected_nsl_ae_ann_deep_rep1/training_f1_history.png
2019-12-24 01:20:53,509 [INFO] Making predictions on training, validation, testing data
2019-12-24 01:21:01,825 [INFO] Evaluating predictions (results)
2019-12-24 01:21:02,378 [INFO] Dataset: Testing. Classification report below
2019-12-24 01:21:02,378 [INFO] 
              precision    recall  f1-score   support

         dos       0.92      0.82      0.87      7458
      normal       0.67      0.97      0.80      9711
       probe       0.81      0.59      0.68      2421
         r2l       0.91      0.03      0.06      2421
         u2r       0.50      0.02      0.03       533

    accuracy                           0.76     22544
   macro avg       0.76      0.49      0.49     22544
weighted avg       0.79      0.76      0.71     22544

2019-12-24 01:21:02,378 [INFO] Overall accuracy (micro avg): 0.7568310858765082
2019-12-24 01:21:02,976 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.7568         0.7568                       0.7568                0.0608                   0.2432  0.7568
1     Macro avg        0.9027         0.7628                       0.4854                0.0822                   0.5146  0.4864
2  Weighted avg        0.8613         0.7902                       0.7568                0.1677                   0.2432  0.7094
2019-12-24 01:21:03,651 [INFO] Dataset: Validation. Classification report below
2019-12-24 01:21:03,651 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00      9186
      normal       0.99      1.00      0.99     13469
       probe       0.99      0.97      0.98      2331
         r2l       0.95      0.73      0.83       199
         u2r       0.80      0.40      0.53        10

    accuracy                           0.99     25195
   macro avg       0.95      0.82      0.87     25195
weighted avg       0.99      0.99      0.99     25195

2019-12-24 01:21:03,652 [INFO] Overall accuracy (micro avg): 0.993530462393332
2019-12-24 01:21:04,322 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9935         0.9935                       0.9935                0.0016                   0.0065  0.9935
1     Macro avg        0.9974         0.9456                       0.8209                0.0024                   0.1791  0.8672
2  Weighted avg        0.9961         0.9934                       0.9935                0.0057                   0.0065  0.9933
2019-12-24 01:21:07,166 [INFO] Dataset: Training. Classification report below
2019-12-24 01:21:07,169 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00     36741
      normal       0.99      1.00      0.99     53874
       probe       1.00      0.97      0.98      9325
         r2l       0.94      0.73      0.82       796
         u2r       0.71      0.40      0.52        42

    accuracy                           0.99    100778
   macro avg       0.93      0.82      0.86    100778
weighted avg       0.99      0.99      0.99    100778

2019-12-24 01:21:07,169 [INFO] Overall accuracy (micro avg): 0.9935700252039136
2019-12-24 01:21:10,218 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9936         0.9936                       0.9936                0.0016                   0.0064  0.9936
1     Macro avg        0.9974         0.9259                       0.8204                0.0025                   0.1796  0.8623
2  Weighted avg        0.9961         0.9934                       0.9936                0.0063                   0.0064  0.9934
2019-12-24 01:21:10,265 [INFO] Results saved to: results_selected_models/selected_nsl_ae_ann_deep_rep1/selected_nsl_ae_ann_deep_rep1_results.xlsx
2019-12-24 01:21:10,266 [INFO] ================= Finished running experiment no. 1 ================= 

2019-12-24 01:21:10,267 [INFO] Created directory: results_selected_models/selected_nsl_ae_ann_deep_rep2
2019-12-24 01:21:10,267 [INFO] Initialized logging. log_filename = results_selected_models/selected_nsl_ae_ann_deep_rep2/run_log.log
2019-12-24 01:21:10,267 [INFO] ================= Running experiment no. 2  ================= 

2019-12-24 01:21:10,267 [INFO] Experiment parameters given below
2019-12-24 01:21:10,267 [INFO] 
{'experiment_num': 2, 'results_dir': 'results_selected_models/selected_nsl_ae_ann_deep_rep2', 'model_type': 'classifier', 'model': 'ae_ann', 'normal_label': 'normal', 'scaling_type': 'NA', 'unsupervised_ratio': 0.25, 'ae_encoder_units': [128, 64, 32], 'ae_encoder_activations': ['relu', 'relu', 'relu'], 'ae_encoder_dropout_rates': [0.2, 0.2, 0.2], 'ae_encoder_l1_param': -1, 'ae_decoder_units': [64, 128], 'ae_decoder_activations': ['relu', 'relu'], 'ae_decoder_dropout_rates': [0.2, 0.2], 'output_activation': 'sigmoid', 'loss_function': 'binary_crossentropy', 'ae_epochs': 200, 'ann_layer_units': [64], 'ann_layer_activations': ['relu'], 'ann_layer_dropout_rates': [0.2], 'ann_epochs': 200, 'early_stop_patience': 50, 'batch_size': 256, 'dataset_dir': '../Datasets/small_datasets/nsl_kdd_five_classes', 'description': 'selected_nsl_ae_ann_deep_rep2'}
2019-12-24 01:21:10,267 [INFO] Created tensorboard log directory: results_selected_models/selected_nsl_ae_ann_deep_rep2/tf_logs_run_2019_12_24-01_21_10
2019-12-24 01:21:10,267 [INFO] Loading datsets from: ../Datasets/small_datasets/nsl_kdd_five_classes
2019-12-24 01:21:10,267 [INFO] Reading X, y files
2019-12-24 01:21:10,267 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_train.h5
2019-12-24 01:21:10,501 [INFO] Reading complete. time_to_read=0.23 seconds
2019-12-24 01:21:10,501 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_val.h5
2019-12-24 01:21:10,564 [INFO] Reading complete. time_to_read=0.06 seconds
2019-12-24 01:21:10,564 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_test.h5
2019-12-24 01:21:10,620 [INFO] Reading complete. time_to_read=0.06 seconds
2019-12-24 01:21:10,621 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_train.h5
2019-12-24 01:21:10,629 [INFO] Reading complete. time_to_read=0.01 seconds
2019-12-24 01:21:10,629 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_val.h5
2019-12-24 01:21:10,634 [INFO] Reading complete. time_to_read=0.00 seconds
2019-12-24 01:21:10,634 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_test.h5
2019-12-24 01:21:10,638 [INFO] Reading complete. time_to_read=0.00 seconds
2019-12-24 01:21:10,749 [INFO] Initializing model
2019-12-24 01:21:11,241 [INFO] _________________________________________________________________
2019-12-24 01:21:11,242 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-24 01:21:11,242 [INFO] =================================================================
2019-12-24 01:21:11,242 [INFO] dense_29 (Dense)             (None, 128)               15744     
2019-12-24 01:21:11,242 [INFO] _________________________________________________________________
2019-12-24 01:21:11,242 [INFO] batch_normalization_17 (Batc (None, 128)               512       
2019-12-24 01:21:11,242 [INFO] _________________________________________________________________
2019-12-24 01:21:11,242 [INFO] dropout_17 (Dropout)         (None, 128)               0         
2019-12-24 01:21:11,242 [INFO] _________________________________________________________________
2019-12-24 01:21:11,242 [INFO] dense_30 (Dense)             (None, 64)                8256      
2019-12-24 01:21:11,242 [INFO] _________________________________________________________________
2019-12-24 01:21:11,242 [INFO] batch_normalization_18 (Batc (None, 64)                256       
2019-12-24 01:21:11,243 [INFO] _________________________________________________________________
2019-12-24 01:21:11,243 [INFO] dropout_18 (Dropout)         (None, 64)                0         
2019-12-24 01:21:11,243 [INFO] _________________________________________________________________
2019-12-24 01:21:11,243 [INFO] dense_31 (Dense)             (None, 32)                2080      
2019-12-24 01:21:11,243 [INFO] _________________________________________________________________
2019-12-24 01:21:11,243 [INFO] batch_normalization_19 (Batc (None, 32)                128       
2019-12-24 01:21:11,243 [INFO] _________________________________________________________________
2019-12-24 01:21:11,243 [INFO] dropout_19 (Dropout)         (None, 32)                0         
2019-12-24 01:21:11,243 [INFO] _________________________________________________________________
2019-12-24 01:21:11,243 [INFO] dense_32 (Dense)             (None, 64)                2112      
2019-12-24 01:21:11,243 [INFO] _________________________________________________________________
2019-12-24 01:21:11,243 [INFO] batch_normalization_20 (Batc (None, 64)                256       
2019-12-24 01:21:11,244 [INFO] _________________________________________________________________
2019-12-24 01:21:11,244 [INFO] dropout_20 (Dropout)         (None, 64)                0         
2019-12-24 01:21:11,244 [INFO] _________________________________________________________________
2019-12-24 01:21:11,244 [INFO] dense_33 (Dense)             (None, 128)               8320      
2019-12-24 01:21:11,244 [INFO] _________________________________________________________________
2019-12-24 01:21:11,244 [INFO] batch_normalization_21 (Batc (None, 128)               512       
2019-12-24 01:21:11,244 [INFO] _________________________________________________________________
2019-12-24 01:21:11,244 [INFO] dropout_21 (Dropout)         (None, 128)               0         
2019-12-24 01:21:11,244 [INFO] _________________________________________________________________
2019-12-24 01:21:11,244 [INFO] dense_34 (Dense)             (None, 122)               15738     
2019-12-24 01:21:11,244 [INFO] =================================================================
2019-12-24 01:21:11,245 [INFO] Total params: 53,914
2019-12-24 01:21:11,245 [INFO] Trainable params: 53,082
2019-12-24 01:21:11,245 [INFO] Non-trainable params: 832
2019-12-24 01:21:11,245 [INFO] _________________________________________________________________
2019-12-24 01:21:11,371 [INFO] _________________________________________________________________
2019-12-24 01:21:11,371 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-24 01:21:11,371 [INFO] =================================================================
2019-12-24 01:21:11,371 [INFO] dense_35 (Dense)             (None, 64)                2112      
2019-12-24 01:21:11,371 [INFO] _________________________________________________________________
2019-12-24 01:21:11,371 [INFO] batch_normalization_22 (Batc (None, 64)                256       
2019-12-24 01:21:11,371 [INFO] _________________________________________________________________
2019-12-24 01:21:11,371 [INFO] dropout_22 (Dropout)         (None, 64)                0         
2019-12-24 01:21:11,371 [INFO] _________________________________________________________________
2019-12-24 01:21:11,372 [INFO] dense_36 (Dense)             (None, 5)                 325       
2019-12-24 01:21:11,372 [INFO] =================================================================
2019-12-24 01:21:11,372 [INFO] Total params: 2,693
2019-12-24 01:21:11,372 [INFO] Trainable params: 2,565
2019-12-24 01:21:11,372 [INFO] Non-trainable params: 128
2019-12-24 01:21:11,372 [INFO] _________________________________________________________________
2019-12-24 01:21:11,372 [INFO] Training model
2019-12-24 01:21:11,372 [INFO] Splitting train set into 2 sets (unsupervised, supervised), random_seed = None
2019-12-24 01:21:12,447 [INFO] Split sizes (instances). total = 100778, unsupervised = 25194, supervised = 75584, unsupervised dataset hash = 8685dd7ee75fb2153d4d748fa62d2ecff1648211
2019-12-24 01:21:12,450 [INFO] Training autoencoder
 - val_f1: 0.9943
Train on 25194 samples, validate on 25195 samples
Epoch 1/200
 - 4s - loss: 0.4217 - val_loss: -4.1975e-01
Epoch 2/200
 - 1s - loss: -5.9475e-01 - val_loss: -1.2880e+00
Epoch 3/200
 - 1s - loss: -1.3499e+00 - val_loss: -1.8082e+00
Epoch 4/200
 - 1s - loss: -1.8257e+00 - val_loss: -2.1944e+00
Epoch 5/200
 - 1s - loss: -2.1474e+00 - val_loss: -2.4662e+00
Epoch 6/200
 - 1s - loss: -2.3646e+00 - val_loss: -2.6673e+00
Epoch 7/200
 - 1s - loss: -2.5199e+00 - val_loss: -2.7981e+00
Epoch 8/200
 - 1s - loss: -2.6270e+00 - val_loss: -2.8838e+00
Epoch 9/200
 - 1s - loss: -2.7023e+00 - val_loss: -2.9305e+00
Epoch 10/200
 - 1s - loss: -2.7568e+00 - val_loss: -2.9702e+00
Epoch 11/200
 - 1s - loss: -2.8028e+00 - val_loss: -2.9990e+00
Epoch 12/200
 - 1s - loss: -2.8395e+00 - val_loss: -3.0187e+00
Epoch 13/200
 - 1s - loss: -2.8577e+00 - val_loss: -3.0296e+00
Epoch 14/200
 - 1s - loss: -2.8878e+00 - val_loss: -3.0393e+00
Epoch 15/200
 - 1s - loss: -2.9018e+00 - val_loss: -3.0527e+00
Epoch 16/200
 - 1s - loss: -2.9201e+00 - val_loss: -3.0619e+00
Epoch 17/200
 - 1s - loss: -2.9314e+00 - val_loss: -3.0695e+00
Epoch 18/200
 - 1s - loss: -2.9449e+00 - val_loss: -3.0759e+00
Epoch 19/200
 - 1s - loss: -2.9526e+00 - val_loss: -3.0837e+00
Epoch 20/200
 - 1s - loss: -2.9650e+00 - val_loss: -3.0864e+00
Epoch 21/200
 - 1s - loss: -2.9676e+00 - val_loss: -3.0905e+00
2019-12-24 01:21:44,898 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep2/ae_model_epoch_20.pickle
Epoch 22/200
 - 1s - loss: -2.9817e+00 - val_loss: -3.0963e+00
Epoch 23/200
 - 1s - loss: -2.9839e+00 - val_loss: -3.1015e+00
Epoch 24/200
 - 1s - loss: -2.9938e+00 - val_loss: -3.1060e+00
Epoch 25/200
 - 1s - loss: -3.0008e+00 - val_loss: -3.1089e+00
Epoch 26/200
 - 1s - loss: -3.0075e+00 - val_loss: -3.1112e+00
Epoch 27/200
 - 1s - loss: -3.0103e+00 - val_loss: -3.1155e+00
Epoch 28/200
 - 1s - loss: -3.0124e+00 - val_loss: -3.1194e+00
Epoch 29/200
 - 1s - loss: -3.0193e+00 - val_loss: -3.1215e+00
Epoch 30/200
 - 1s - loss: -3.0276e+00 - val_loss: -3.1238e+00
Epoch 31/200
 - 1s - loss: -3.0309e+00 - val_loss: -3.1271e+00
Epoch 32/200
 - 1s - loss: -3.0323e+00 - val_loss: -3.1302e+00
Epoch 33/200
 - 1s - loss: -3.0349e+00 - val_loss: -3.1332e+00
Epoch 34/200
 - 1s - loss: -3.0403e+00 - val_loss: -3.1369e+00
Epoch 35/200
 - 1s - loss: -3.0467e+00 - val_loss: -3.1399e+00
Epoch 36/200
 - 1s - loss: -3.0521e+00 - val_loss: -3.1427e+00
Epoch 37/200
 - 1s - loss: -3.0529e+00 - val_loss: -3.1428e+00
Epoch 38/200
 - 1s - loss: -3.0532e+00 - val_loss: -3.1468e+00
Epoch 39/200
 - 1s - loss: -3.0602e+00 - val_loss: -3.1504e+00
Epoch 40/200
 - 1s - loss: -3.0635e+00 - val_loss: -3.1494e+00
Epoch 41/200
 - 1s - loss: -3.0655e+00 - val_loss: -3.1522e+00
2019-12-24 01:22:07,713 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep2/ae_model_epoch_40.pickle
Epoch 42/200
 - 1s - loss: -3.0671e+00 - val_loss: -3.1525e+00
Epoch 43/200
 - 1s - loss: -3.0692e+00 - val_loss: -3.1526e+00
Epoch 44/200
 - 1s - loss: -3.0713e+00 - val_loss: -3.1544e+00
Epoch 45/200
 - 1s - loss: -3.0731e+00 - val_loss: -3.1572e+00
Epoch 46/200
 - 1s - loss: -3.0742e+00 - val_loss: -3.1569e+00
Epoch 47/200
 - 1s - loss: -3.0733e+00 - val_loss: -3.1587e+00
Epoch 48/200
 - 1s - loss: -3.0803e+00 - val_loss: -3.1591e+00
Epoch 49/200
 - 1s - loss: -3.0834e+00 - val_loss: -3.1588e+00
Epoch 50/200
 - 1s - loss: -3.0866e+00 - val_loss: -3.1609e+00
Epoch 51/200
 - 1s - loss: -3.0845e+00 - val_loss: -3.1621e+00
Epoch 52/200
 - 1s - loss: -3.0851e+00 - val_loss: -3.1631e+00
Epoch 53/200
 - 1s - loss: -3.0847e+00 - val_loss: -3.1636e+00
Epoch 54/200
 - 1s - loss: -3.0876e+00 - val_loss: -3.1645e+00
Epoch 55/200
 - 1s - loss: -3.0916e+00 - val_loss: -3.1651e+00
Epoch 56/200
 - 1s - loss: -3.0921e+00 - val_loss: -3.1662e+00
Epoch 57/200
 - 1s - loss: -3.0921e+00 - val_loss: -3.1657e+00
Epoch 58/200
 - 1s - loss: -3.0930e+00 - val_loss: -3.1670e+00
Epoch 59/200
 - 1s - loss: -3.0957e+00 - val_loss: -3.1682e+00
Epoch 60/200
 - 1s - loss: -3.0944e+00 - val_loss: -3.1685e+00
Epoch 61/200
 - 1s - loss: -3.0989e+00 - val_loss: -3.1699e+00
2019-12-24 01:22:30,538 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep2/ae_model_epoch_60.pickle
Epoch 62/200
 - 1s - loss: -3.1009e+00 - val_loss: -3.1701e+00
Epoch 63/200
 - 1s - loss: -3.0992e+00 - val_loss: -3.1707e+00
Epoch 64/200
 - 1s - loss: -3.1030e+00 - val_loss: -3.1708e+00
Epoch 65/200
 - 1s - loss: -3.1049e+00 - val_loss: -3.1721e+00
Epoch 66/200
 - 1s - loss: -3.1063e+00 - val_loss: -3.1727e+00
Epoch 67/200
 - 1s - loss: -3.1073e+00 - val_loss: -3.1727e+00
Epoch 68/200
 - 1s - loss: -3.1078e+00 - val_loss: -3.1729e+00
Epoch 69/200
 - 1s - loss: -3.1071e+00 - val_loss: -3.1718e+00
Epoch 70/200
 - 1s - loss: -3.1059e+00 - val_loss: -3.1742e+00
Epoch 71/200
 - 1s - loss: -3.1101e+00 - val_loss: -3.1739e+00
Epoch 72/200
 - 1s - loss: -3.1137e+00 - val_loss: -3.1754e+00
Epoch 73/200
 - 1s - loss: -3.1132e+00 - val_loss: -3.1753e+00
Epoch 74/200
 - 1s - loss: -3.1157e+00 - val_loss: -3.1769e+00
Epoch 75/200
 - 1s - loss: -3.1158e+00 - val_loss: -3.1764e+00
Epoch 76/200
 - 1s - loss: -3.1133e+00 - val_loss: -3.1756e+00
Epoch 77/200
 - 1s - loss: -3.1171e+00 - val_loss: -3.1771e+00
Epoch 78/200
 - 1s - loss: -3.1164e+00 - val_loss: -3.1770e+00
Epoch 79/200
 - 1s - loss: -3.1185e+00 - val_loss: -3.1776e+00
Epoch 80/200
 - 1s - loss: -3.1143e+00 - val_loss: -3.1773e+00
Epoch 81/200
 - 1s - loss: -3.1183e+00 - val_loss: -3.1785e+00
2019-12-24 01:22:53,348 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep2/ae_model_epoch_80.pickle
Epoch 82/200
 - 1s - loss: -3.1198e+00 - val_loss: -3.1794e+00
Epoch 83/200
 - 1s - loss: -3.1190e+00 - val_loss: -3.1802e+00
Epoch 84/200
 - 1s - loss: -3.1209e+00 - val_loss: -3.1794e+00
Epoch 85/200
 - 1s - loss: -3.1238e+00 - val_loss: -3.1808e+00
Epoch 86/200
 - 1s - loss: -3.1209e+00 - val_loss: -3.1805e+00
Epoch 87/200
 - 1s - loss: -3.1240e+00 - val_loss: -3.1814e+00
Epoch 88/200
 - 1s - loss: -3.1239e+00 - val_loss: -3.1821e+00
Epoch 89/200
 - 1s - loss: -3.1277e+00 - val_loss: -3.1830e+00
Epoch 90/200
 - 1s - loss: -3.1260e+00 - val_loss: -3.1830e+00
Epoch 91/200
 - 1s - loss: -3.1254e+00 - val_loss: -3.1817e+00
Epoch 92/200
 - 1s - loss: -3.1261e+00 - val_loss: -3.1825e+00
Epoch 93/200
 - 1s - loss: -3.1299e+00 - val_loss: -3.1837e+00
Epoch 94/200
 - 1s - loss: -3.1274e+00 - val_loss: -3.1834e+00
Epoch 95/200
 - 1s - loss: -3.1312e+00 - val_loss: -3.1841e+00
Epoch 96/200
 - 1s - loss: -3.1313e+00 - val_loss: -3.1842e+00
Epoch 97/200
 - 1s - loss: -3.1334e+00 - val_loss: -3.1860e+00
Epoch 98/200
 - 1s - loss: -3.1316e+00 - val_loss: -3.1871e+00
Epoch 99/200
 - 1s - loss: -3.1323e+00 - val_loss: -3.1875e+00
Epoch 100/200
 - 1s - loss: -3.1312e+00 - val_loss: -3.1870e+00
Epoch 101/200
 - 1s - loss: -3.1343e+00 - val_loss: -3.1886e+00
2019-12-24 01:23:16,187 [INFO] epoch = 100. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep2/ae_model_epoch_100.pickle
Epoch 102/200
 - 1s - loss: -3.1349e+00 - val_loss: -3.1876e+00
Epoch 103/200
 - 1s - loss: -3.1343e+00 - val_loss: -3.1876e+00
Epoch 104/200
 - 1s - loss: -3.1367e+00 - val_loss: -3.1891e+00
Epoch 105/200
 - 1s - loss: -3.1275e+00 - val_loss: -3.1896e+00
Epoch 106/200
 - 1s - loss: -3.1393e+00 - val_loss: -3.1899e+00
Epoch 107/200
 - 1s - loss: -3.1370e+00 - val_loss: -3.1899e+00
Epoch 108/200
 - 1s - loss: -3.1383e+00 - val_loss: -3.1903e+00
Epoch 109/200
 - 1s - loss: -3.1384e+00 - val_loss: -3.1905e+00
Epoch 110/200
 - 1s - loss: -3.1381e+00 - val_loss: -3.1902e+00
Epoch 111/200
 - 1s - loss: -3.1417e+00 - val_loss: -3.1902e+00
Epoch 112/200
 - 1s - loss: -3.1364e+00 - val_loss: -3.1913e+00
Epoch 113/200
 - 1s - loss: -3.1390e+00 - val_loss: -3.1916e+00
Epoch 114/200
 - 1s - loss: -3.1396e+00 - val_loss: -3.1912e+00
Epoch 115/200
 - 1s - loss: -3.1405e+00 - val_loss: -3.1919e+00
Epoch 116/200
 - 1s - loss: -3.1444e+00 - val_loss: -3.1925e+00
Epoch 117/200
 - 1s - loss: -3.1403e+00 - val_loss: -3.1922e+00
Epoch 118/200
 - 1s - loss: -3.1415e+00 - val_loss: -3.1921e+00
Epoch 119/200
 - 1s - loss: -3.1416e+00 - val_loss: -3.1929e+00
Epoch 120/200
 - 1s - loss: -3.1419e+00 - val_loss: -3.1932e+00
Epoch 121/200
 - 1s - loss: -3.1457e+00 - val_loss: -3.1941e+00
2019-12-24 01:23:38,996 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep2/ae_model_epoch_120.pickle
Epoch 122/200
 - 1s - loss: -3.1416e+00 - val_loss: -3.1938e+00
Epoch 123/200
 - 1s - loss: -3.1433e+00 - val_loss: -3.1947e+00
Epoch 124/200
 - 1s - loss: -3.1455e+00 - val_loss: -3.1949e+00
Epoch 125/200
 - 1s - loss: -3.1449e+00 - val_loss: -3.1942e+00
Epoch 126/200
 - 1s - loss: -3.1448e+00 - val_loss: -3.1948e+00
Epoch 127/200
 - 1s - loss: -3.1435e+00 - val_loss: -3.1943e+00
Epoch 128/200
 - 1s - loss: -3.1439e+00 - val_loss: -3.1943e+00
Epoch 129/200
 - 1s - loss: -3.1454e+00 - val_loss: -3.1948e+00
Epoch 130/200
 - 1s - loss: -3.1460e+00 - val_loss: -3.1956e+00
Epoch 131/200
 - 1s - loss: -3.1455e+00 - val_loss: -3.1952e+00
Epoch 132/200
 - 1s - loss: -3.1504e+00 - val_loss: -3.1957e+00
Epoch 133/200
 - 1s - loss: -3.1459e+00 - val_loss: -3.1948e+00
Epoch 134/200
 - 1s - loss: -3.1458e+00 - val_loss: -3.1958e+00
Epoch 135/200
 - 1s - loss: -3.1437e+00 - val_loss: -3.1967e+00
Epoch 136/200
 - 1s - loss: -3.1470e+00 - val_loss: -3.1965e+00
Epoch 137/200
 - 1s - loss: -3.1495e+00 - val_loss: -3.1957e+00
Epoch 138/200
 - 1s - loss: -3.1505e+00 - val_loss: -3.1962e+00
Epoch 139/200
 - 1s - loss: -3.1473e+00 - val_loss: -3.1973e+00
Epoch 140/200
 - 1s - loss: -3.1481e+00 - val_loss: -3.1965e+00
Epoch 141/200
 - 1s - loss: -3.1490e+00 - val_loss: -3.1977e+00
2019-12-24 01:24:01,821 [INFO] epoch = 140. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep2/ae_model_epoch_140.pickle
Epoch 142/200
 - 1s - loss: -3.1509e+00 - val_loss: -3.1976e+00
Epoch 143/200
 - 1s - loss: -3.1505e+00 - val_loss: -3.1983e+00
Epoch 144/200
 - 1s - loss: -3.1510e+00 - val_loss: -3.1978e+00
Epoch 145/200
 - 1s - loss: -3.1497e+00 - val_loss: -3.1977e+00
Epoch 146/200
 - 1s - loss: -3.1494e+00 - val_loss: -3.1973e+00
Epoch 147/200
 - 1s - loss: -3.1505e+00 - val_loss: -3.1978e+00
Epoch 148/200
 - 1s - loss: -3.1501e+00 - val_loss: -3.1984e+00
Epoch 149/200
 - 1s - loss: -3.1480e+00 - val_loss: -3.1982e+00
Epoch 150/200
 - 1s - loss: -3.1514e+00 - val_loss: -3.1978e+00
Epoch 151/200
 - 1s - loss: -3.1520e+00 - val_loss: -3.1978e+00
Epoch 152/200
 - 1s - loss: -3.1513e+00 - val_loss: -3.1974e+00
Epoch 153/200
 - 1s - loss: -3.1530e+00 - val_loss: -3.1978e+00
Epoch 154/200
 - 1s - loss: -3.1502e+00 - val_loss: -3.1971e+00
Epoch 155/200
 - 1s - loss: -3.1558e+00 - val_loss: -3.1979e+00
Epoch 156/200
 - 1s - loss: -3.1542e+00 - val_loss: -3.1980e+00
Epoch 157/200
 - 1s - loss: -3.1545e+00 - val_loss: -3.1997e+00
Epoch 158/200
 - 1s - loss: -3.1528e+00 - val_loss: -3.1993e+00
Epoch 159/200
 - 1s - loss: -3.1535e+00 - val_loss: -3.1986e+00
Epoch 160/200
 - 1s - loss: -3.1542e+00 - val_loss: -3.1987e+00
Epoch 161/200
 - 1s - loss: -3.1573e+00 - val_loss: -3.1996e+00
2019-12-24 01:24:24,628 [INFO] epoch = 160. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep2/ae_model_epoch_160.pickle
Epoch 162/200
 - 1s - loss: -3.1551e+00 - val_loss: -3.1994e+00
Epoch 163/200
 - 1s - loss: -3.1550e+00 - val_loss: -3.1996e+00
Epoch 164/200
 - 1s - loss: -3.1553e+00 - val_loss: -3.1991e+00
Epoch 165/200
 - 1s - loss: -3.1540e+00 - val_loss: -3.1992e+00
Epoch 166/200
 - 1s - loss: -3.1545e+00 - val_loss: -3.1993e+00
Epoch 167/200
 - 1s - loss: -3.1557e+00 - val_loss: -3.1994e+00
Epoch 168/200
 - 1s - loss: -3.1558e+00 - val_loss: -3.1995e+00
Epoch 169/200
 - 1s - loss: -3.1553e+00 - val_loss: -3.1998e+00
Epoch 170/200
 - 1s - loss: -3.1582e+00 - val_loss: -3.1998e+00
Epoch 171/200
 - 1s - loss: -3.1538e+00 - val_loss: -3.2001e+00
Epoch 172/200
 - 1s - loss: -3.1575e+00 - val_loss: -3.2001e+00
Epoch 173/200
 - 1s - loss: -3.1570e+00 - val_loss: -3.2002e+00
Epoch 174/200
 - 1s - loss: -3.1574e+00 - val_loss: -3.2009e+00
Epoch 175/200
 - 1s - loss: -3.1546e+00 - val_loss: -3.2008e+00
Epoch 176/200
 - 1s - loss: -3.1558e+00 - val_loss: -3.2004e+00
Epoch 177/200
 - 1s - loss: -3.1582e+00 - val_loss: -3.2016e+00
Epoch 178/200
 - 1s - loss: -3.1613e+00 - val_loss: -3.2018e+00
Epoch 179/200
 - 1s - loss: -3.1577e+00 - val_loss: -3.2009e+00
Epoch 180/200
 - 1s - loss: -3.1586e+00 - val_loss: -3.2016e+00
Epoch 181/200
 - 1s - loss: -3.1578e+00 - val_loss: -3.2014e+00
2019-12-24 01:24:47,448 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep2/ae_model_epoch_180.pickle
Epoch 182/200
 - 1s - loss: -3.1608e+00 - val_loss: -3.2017e+00
Epoch 183/200
 - 1s - loss: -3.1587e+00 - val_loss: -3.2023e+00
Epoch 184/200
 - 1s - loss: -3.1573e+00 - val_loss: -3.2013e+00
Epoch 185/200
 - 1s - loss: -3.1574e+00 - val_loss: -3.2016e+00
Epoch 186/200
 - 1s - loss: -3.1582e+00 - val_loss: -3.2008e+00
Epoch 187/200
 - 1s - loss: -3.1586e+00 - val_loss: -3.2020e+00
Epoch 188/200
 - 1s - loss: -3.1604e+00 - val_loss: -3.2018e+00
Epoch 189/200
 - 1s - loss: -3.1602e+00 - val_loss: -3.2014e+00
Epoch 190/200
 - 1s - loss: -3.1626e+00 - val_loss: -3.2027e+00
Epoch 191/200
 - 1s - loss: -3.1627e+00 - val_loss: -3.2021e+00
Epoch 192/200
 - 1s - loss: -3.1599e+00 - val_loss: -3.2022e+00
Epoch 193/200
 - 1s - loss: -3.1623e+00 - val_loss: -3.2023e+00
Epoch 194/200
 - 1s - loss: -3.1621e+00 - val_loss: -3.2023e+00
Epoch 195/200
 - 1s - loss: -3.1626e+00 - val_loss: -3.2028e+00
Epoch 196/200
 - 1s - loss: -3.1604e+00 - val_loss: -3.2025e+00
Epoch 197/200
 - 1s - loss: -3.1610e+00 - val_loss: -3.2038e+00
Epoch 198/200
 - 1s - loss: -3.1653e+00 - val_loss: -3.2032e+00
Epoch 199/200
 - 1s - loss: -3.1606e+00 - val_loss: -3.2022e+00
Epoch 200/200
 - 1s - loss: -3.1595e+00 - val_loss: -3.2024e+00
2019-12-24 01:25:09,141 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-24 01:25:11,954 [INFO] Last epoch loss evaluation: train_loss = -3.231802, val_loss = -3.203753
2019-12-24 01:25:11,954 [INFO] Training autoencoder complete
2019-12-24 01:25:11,954 [INFO] Encoding data for supervised training
2019-12-24 01:25:15,977 [INFO] Encoding complete
2019-12-24 01:25:15,977 [INFO] Training neural network layers (after autoencoder)
Train on 75584 samples, validate on 25195 samples
Epoch 1/200
 - 2s - loss: 0.1420 - val_loss: 0.0447
 - val_f1: 0.9678
Epoch 2/200
 - 1s - loss: 0.0454 - val_loss: 0.0311
 - val_f1: 0.9714
Epoch 3/200
 - 1s - loss: 0.0349 - val_loss: 0.0254
 - val_f1: 0.9746
Epoch 4/200
 - 1s - loss: 0.0295 - val_loss: 0.0229
 - val_f1: 0.9790
Epoch 5/200
 - 1s - loss: 0.0264 - val_loss: 0.0183
 - val_f1: 0.9859
Epoch 6/200
 - 1s - loss: 0.0241 - val_loss: 0.0173
 - val_f1: 0.9858
Epoch 7/200
 - 1s - loss: 0.0223 - val_loss: 0.0158
 - val_f1: 0.9858
Epoch 8/200
 - 1s - loss: 0.0211 - val_loss: 0.0157
 - val_f1: 0.9877
Epoch 9/200
 - 1s - loss: 0.0203 - val_loss: 0.0147
 - val_f1: 0.9878
Epoch 10/200
 - 1s - loss: 0.0193 - val_loss: 0.0136
 - val_f1: 0.9895
Epoch 11/200
 - 1s - loss: 0.0185 - val_loss: 0.0132
 - val_f1: 0.9887
Epoch 12/200
 - 1s - loss: 0.0188 - val_loss: 0.0162
 - val_f1: 0.9853
Epoch 13/200
 - 1s - loss: 0.0174 - val_loss: 0.0122
 - val_f1: 0.9904
Epoch 14/200
 - 1s - loss: 0.0175 - val_loss: 0.0122
 - val_f1: 0.9900
Epoch 15/200
 - 1s - loss: 0.0167 - val_loss: 0.0122
 - val_f1: 0.9903
Epoch 16/200
 - 1s - loss: 0.0165 - val_loss: 0.0118
 - val_f1: 0.9905
Epoch 17/200
 - 1s - loss: 0.0159 - val_loss: 0.0114
 - val_f1: 0.9907
Epoch 18/200
 - 1s - loss: 0.0163 - val_loss: 0.0117
 - val_f1: 0.9906
Epoch 19/200
 - 1s - loss: 0.0157 - val_loss: 0.0114
 - val_f1: 0.9909
Epoch 20/200
 - 1s - loss: 0.0153 - val_loss: 0.0111
 - val_f1: 0.9915
Epoch 21/200
 - 1s - loss: 0.0155 - val_loss: 0.0121
2019-12-24 01:25:52,180 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep2/ann_model_epoch_20.pickle
 - val_f1: 0.9900
Epoch 22/200
 - 1s - loss: 0.0153 - val_loss: 0.0113
 - val_f1: 0.9910
Epoch 23/200
 - 1s - loss: 0.0148 - val_loss: 0.0107
 - val_f1: 0.9923
Epoch 24/200
 - 1s - loss: 0.0143 - val_loss: 0.0110
 - val_f1: 0.9908
Epoch 25/200
 - 1s - loss: 0.0138 - val_loss: 0.0104
 - val_f1: 0.9918
Epoch 26/200
 - 1s - loss: 0.0142 - val_loss: 0.0111
 - val_f1: 0.9894
Epoch 27/200
 - 1s - loss: 0.0141 - val_loss: 0.0107
 - val_f1: 0.9909
Epoch 28/200
 - 1s - loss: 0.0141 - val_loss: 0.0107
 - val_f1: 0.9917
Epoch 29/200
 - 1s - loss: 0.0135 - val_loss: 0.0106
 - val_f1: 0.9924
Epoch 30/200
 - 1s - loss: 0.0139 - val_loss: 0.0103
 - val_f1: 0.9921
Epoch 31/200
 - 1s - loss: 0.0136 - val_loss: 0.0112
 - val_f1: 0.9914
Epoch 32/200
 - 1s - loss: 0.0130 - val_loss: 0.0100
 - val_f1: 0.9927
Epoch 33/200
 - 1s - loss: 0.0133 - val_loss: 0.0104
 - val_f1: 0.9914
Epoch 34/200
 - 1s - loss: 0.0131 - val_loss: 0.0109
 - val_f1: 0.9901
Epoch 35/200
 - 1s - loss: 0.0130 - val_loss: 0.0100
 - val_f1: 0.9915
Epoch 36/200
 - 1s - loss: 0.0126 - val_loss: 0.0102
 - val_f1: 0.9920
Epoch 37/200
 - 1s - loss: 0.0124 - val_loss: 0.0095
 - val_f1: 0.9928
Epoch 38/200
 - 1s - loss: 0.0130 - val_loss: 0.0096
 - val_f1: 0.9925
Epoch 39/200
 - 1s - loss: 0.0126 - val_loss: 0.0101
 - val_f1: 0.9916
Epoch 40/200
 - 1s - loss: 0.0125 - val_loss: 0.0104
 - val_f1: 0.9916
Epoch 41/200
 - 1s - loss: 0.0121 - val_loss: 0.0105
2019-12-24 01:26:22,264 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep2/ann_model_epoch_40.pickle
 - val_f1: 0.9917
Epoch 42/200
 - 1s - loss: 0.0119 - val_loss: 0.0096
 - val_f1: 0.9927
Epoch 43/200
 - 1s - loss: 0.0122 - val_loss: 0.0094
 - val_f1: 0.9923
Epoch 44/200
 - 1s - loss: 0.0123 - val_loss: 0.0096
 - val_f1: 0.9929
Epoch 45/200
 - 1s - loss: 0.0122 - val_loss: 0.0095
 - val_f1: 0.9929
Epoch 46/200
 - 1s - loss: 0.0120 - val_loss: 0.0096
 - val_f1: 0.9923
Epoch 47/200
 - 1s - loss: 0.0124 - val_loss: 0.0094
 - val_f1: 0.9926
Epoch 48/200
 - 1s - loss: 0.0115 - val_loss: 0.0094
 - val_f1: 0.9930
Epoch 49/200
 - 1s - loss: 0.0115 - val_loss: 0.0093
 - val_f1: 0.9923
Epoch 50/200
 - 1s - loss: 0.0116 - val_loss: 0.0093
 - val_f1: 0.9925
Epoch 51/200
 - 1s - loss: 0.0119 - val_loss: 0.0114
 - val_f1: 0.9890
Epoch 52/200
 - 1s - loss: 0.0119 - val_loss: 0.0093
 - val_f1: 0.9926
Epoch 53/200
 - 1s - loss: 0.0117 - val_loss: 0.0091
 - val_f1: 0.9927
Epoch 54/200
 - 1s - loss: 0.0120 - val_loss: 0.0101
 - val_f1: 0.9914
Epoch 55/200
 - 1s - loss: 0.0125 - val_loss: 0.0095
 - val_f1: 0.9927
Epoch 56/200
 - 1s - loss: 0.0123 - val_loss: 0.0094
 - val_f1: 0.9920
Epoch 57/200
 - 1s - loss: 0.0119 - val_loss: 0.0097
 - val_f1: 0.9927
Epoch 58/200
 - 1s - loss: 0.0114 - val_loss: 0.0089
 - val_f1: 0.9929
Epoch 59/200
 - 1s - loss: 0.0115 - val_loss: 0.0090
 - val_f1: 0.9923
Epoch 60/200
 - 1s - loss: 0.0113 - val_loss: 0.0089
 - val_f1: 0.9924
Epoch 61/200
 - 1s - loss: 0.0112 - val_loss: 0.0089
2019-12-24 01:26:52,197 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep2/ann_model_epoch_60.pickle
 - val_f1: 0.9930
Epoch 62/200
 - 1s - loss: 0.0113 - val_loss: 0.0089
 - val_f1: 0.9931
Epoch 63/200
 - 1s - loss: 0.0116 - val_loss: 0.0091
 - val_f1: 0.9927
Epoch 64/200
 - 1s - loss: 0.0108 - val_loss: 0.0089
 - val_f1: 0.9931
Epoch 65/200
 - 1s - loss: 0.0113 - val_loss: 0.0087
 - val_f1: 0.9929
Epoch 66/200
 - 1s - loss: 0.0108 - val_loss: 0.0091
 - val_f1: 0.9929
Epoch 67/200
 - 1s - loss: 0.0109 - val_loss: 0.0087
 - val_f1: 0.9934
Epoch 68/200
 - 1s - loss: 0.0109 - val_loss: 0.0096
 - val_f1: 0.9917
Epoch 69/200
 - 1s - loss: 0.0109 - val_loss: 0.0089
 - val_f1: 0.9936
Epoch 70/200
 - 1s - loss: 0.0109 - val_loss: 0.0093
 - val_f1: 0.9927
Epoch 71/200
 - 1s - loss: 0.0110 - val_loss: 0.0087
 - val_f1: 0.9933
Epoch 72/200
 - 1s - loss: 0.0108 - val_loss: 0.0089
 - val_f1: 0.9924
Epoch 73/200
 - 1s - loss: 0.0112 - val_loss: 0.0095
 - val_f1: 0.9921
Epoch 74/200
 - 1s - loss: 0.0109 - val_loss: 0.0090
 - val_f1: 0.9927
Epoch 75/200
 - 1s - loss: 0.0108 - val_loss: 0.0087
 - val_f1: 0.9934
Epoch 76/200
 - 1s - loss: 0.0105 - val_loss: 0.0089
 - val_f1: 0.9928
Epoch 77/200
 - 1s - loss: 0.0109 - val_loss: 0.0086
 - val_f1: 0.9933
Epoch 78/200
 - 1s - loss: 0.0107 - val_loss: 0.0089
 - val_f1: 0.9932
Epoch 79/200
 - 1s - loss: 0.0105 - val_loss: 0.0089
 - val_f1: 0.9922
Epoch 80/200
 - 1s - loss: 0.0108 - val_loss: 0.0092
 - val_f1: 0.9916
Epoch 81/200
 - 1s - loss: 0.0111 - val_loss: 0.0088
2019-12-24 01:27:22,282 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep2/ann_model_epoch_80.pickle
 - val_f1: 0.9928
Epoch 82/200
 - 1s - loss: 0.0106 - val_loss: 0.0087
 - val_f1: 0.9931
Epoch 83/200
 - 1s - loss: 0.0106 - val_loss: 0.0084
 - val_f1: 0.9932
Epoch 84/200
 - 1s - loss: 0.0105 - val_loss: 0.0093
 - val_f1: 0.9921
Epoch 85/200
 - 1s - loss: 0.0106 - val_loss: 0.0084
 - val_f1: 0.9931
Epoch 86/200
 - 1s - loss: 0.0105 - val_loss: 0.0084
 - val_f1: 0.9932
Epoch 87/200
 - 1s - loss: 0.0103 - val_loss: 0.0085
 - val_f1: 0.9931
Epoch 88/200
 - 1s - loss: 0.0104 - val_loss: 0.0086
 - val_f1: 0.9936
Epoch 89/200
 - 1s - loss: 0.0106 - val_loss: 0.0084
 - val_f1: 0.9931
Epoch 90/200
 - 1s - loss: 0.0107 - val_loss: 0.0094
 - val_f1: 0.9921
Epoch 91/200
 - 1s - loss: 0.0107 - val_loss: 0.0087
 - val_f1: 0.9930
Epoch 92/200
 - 1s - loss: 0.0103 - val_loss: 0.0092
 - val_f1: 0.9930
Epoch 93/200
 - 1s - loss: 0.0102 - val_loss: 0.0082
 - val_f1: 0.9935
Epoch 94/200
 - 1s - loss: 0.0099 - val_loss: 0.0085
 - val_f1: 0.9932
Epoch 95/200
 - 1s - loss: 0.0105 - val_loss: 0.0090
 - val_f1: 0.9924
Epoch 96/200
 - 1s - loss: 0.0101 - val_loss: 0.0090
 - val_f1: 0.9925
Epoch 97/200
 - 1s - loss: 0.0097 - val_loss: 0.0085
 - val_f1: 0.9935
Epoch 98/200
 - 1s - loss: 0.0100 - val_loss: 0.0085
 - val_f1: 0.9931
Epoch 99/200
 - 1s - loss: 0.0101 - val_loss: 0.0094
 - val_f1: 0.9922
Epoch 100/200
 - 1s - loss: 0.0103 - val_loss: 0.0091
 - val_f1: 0.9923
Epoch 101/200
 - 1s - loss: 0.0104 - val_loss: 0.0087
2019-12-24 01:27:52,244 [INFO] epoch = 100. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep2/ann_model_epoch_100.pickle
 - val_f1: 0.9930
Epoch 102/200
 - 1s - loss: 0.0101 - val_loss: 0.0085
 - val_f1: 0.9938
Epoch 103/200
 - 1s - loss: 0.0105 - val_loss: 0.0085
 - val_f1: 0.9935
Epoch 104/200
 - 1s - loss: 0.0097 - val_loss: 0.0081
 - val_f1: 0.9934
Epoch 105/200
 - 1s - loss: 0.0107 - val_loss: 0.0084
 - val_f1: 0.9936
Epoch 106/200
 - 1s - loss: 0.0100 - val_loss: 0.0087
 - val_f1: 0.9931
Epoch 107/200
 - 1s - loss: 0.0100 - val_loss: 0.0087
 - val_f1: 0.9932
Epoch 108/200
 - 1s - loss: 0.0100 - val_loss: 0.0088
 - val_f1: 0.9932
Epoch 109/200
 - 1s - loss: 0.0101 - val_loss: 0.0106
 - val_f1: 0.9900
Epoch 110/200
 - 1s - loss: 0.0103 - val_loss: 0.0084
 - val_f1: 0.9938
Epoch 111/200
 - 1s - loss: 0.0100 - val_loss: 0.0084
 - val_f1: 0.9930
Epoch 112/200
 - 1s - loss: 0.0099 - val_loss: 0.0085
 - val_f1: 0.9934
Epoch 113/200
 - 1s - loss: 0.0100 - val_loss: 0.0085
 - val_f1: 0.9931
Epoch 114/200
 - 1s - loss: 0.0100 - val_loss: 0.0083
 - val_f1: 0.9933
Epoch 115/200
 - 1s - loss: 0.0098 - val_loss: 0.0090
 - val_f1: 0.9929
Epoch 116/200
 - 1s - loss: 0.0097 - val_loss: 0.0081
 - val_f1: 0.9941
Epoch 117/200
 - 1s - loss: 0.0098 - val_loss: 0.0084
 - val_f1: 0.9938
Epoch 118/200
 - 1s - loss: 0.0097 - val_loss: 0.0085
 - val_f1: 0.9934
Epoch 119/200
 - 1s - loss: 0.0096 - val_loss: 0.0089
 - val_f1: 0.9934
Epoch 120/200
 - 1s - loss: 0.0099 - val_loss: 0.0100
 - val_f1: 0.9920
Epoch 121/200
 - 1s - loss: 0.0103 - val_loss: 0.0083
2019-12-24 01:28:22,313 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep2/ann_model_epoch_120.pickle
 - val_f1: 0.9938
Epoch 122/200
 - 1s - loss: 0.0096 - val_loss: 0.0085
 - val_f1: 0.9935
Epoch 123/200
 - 1s - loss: 0.0092 - val_loss: 0.0082
 - val_f1: 0.9937
Epoch 124/200
 - 1s - loss: 0.0093 - val_loss: 0.0085
 - val_f1: 0.9929
Epoch 125/200
 - 1s - loss: 0.0097 - val_loss: 0.0089
 - val_f1: 0.9930
Epoch 126/200
 - 1s - loss: 0.0098 - val_loss: 0.0082
 - val_f1: 0.9938
Epoch 127/200
 - 1s - loss: 0.0099 - val_loss: 0.0082
 - val_f1: 0.9929
Epoch 128/200
 - 1s - loss: 0.0100 - val_loss: 0.0085
 - val_f1: 0.9934
Epoch 129/200
 - 1s - loss: 0.0096 - val_loss: 0.0083
 - val_f1: 0.9938
Epoch 130/200
 - 1s - loss: 0.0098 - val_loss: 0.0086
 - val_f1: 0.9924
Epoch 131/200
 - 1s - loss: 0.0101 - val_loss: 0.0086
 - val_f1: 0.9934
Epoch 132/200
 - 1s - loss: 0.0094 - val_loss: 0.0085
 - val_f1: 0.9932
Epoch 133/200
 - 1s - loss: 0.0097 - val_loss: 0.0080
 - val_f1: 0.9935
Epoch 134/200
 - 1s - loss: 0.0095 - val_loss: 0.0084
 - val_f1: 0.9936
Epoch 135/200
 - 1s - loss: 0.0096 - val_loss: 0.0081
 - val_f1: 0.9930
Epoch 136/200
 - 1s - loss: 0.0095 - val_loss: 0.0084
 - val_f1: 0.9933
Epoch 137/200
 - 1s - loss: 0.0097 - val_loss: 0.0085
 - val_f1: 0.9932
Epoch 138/200
 - 1s - loss: 0.0094 - val_loss: 0.0079
 - val_f1: 0.9936
Epoch 139/200
 - 1s - loss: 0.0094 - val_loss: 0.0082
 - val_f1: 0.9934
Epoch 140/200
 - 1s - loss: 0.0091 - val_loss: 0.0083
 - val_f1: 0.9937
Epoch 141/200
 - 1s - loss: 0.0098 - val_loss: 0.0080
2019-12-24 01:28:52,207 [INFO] epoch = 140. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep2/ann_model_epoch_140.pickle
 - val_f1: 0.9938
Epoch 142/200
 - 1s - loss: 0.0094 - val_loss: 0.0079
 - val_f1: 0.9936
Epoch 143/200
 - 1s - loss: 0.0094 - val_loss: 0.0082
 - val_f1: 0.9934
Epoch 144/200
 - 1s - loss: 0.0095 - val_loss: 0.0084
 - val_f1: 0.9934
Epoch 145/200
 - 1s - loss: 0.0094 - val_loss: 0.0081
 - val_f1: 0.9936
Epoch 146/200
 - 1s - loss: 0.0093 - val_loss: 0.0084
 - val_f1: 0.9934
Epoch 147/200
 - 1s - loss: 0.0096 - val_loss: 0.0083
 - val_f1: 0.9938
Epoch 148/200
 - 1s - loss: 0.0091 - val_loss: 0.0077
 - val_f1: 0.9938
Epoch 149/200
 - 1s - loss: 0.0092 - val_loss: 0.0080
 - val_f1: 0.9937
Epoch 150/200
 - 1s - loss: 0.0092 - val_loss: 0.0082
 - val_f1: 0.9939
Epoch 151/200
 - 1s - loss: 0.0091 - val_loss: 0.0078
 - val_f1: 0.9938
Epoch 152/200
 - 1s - loss: 0.0095 - val_loss: 0.0085
 - val_f1: 0.9931
Epoch 153/200
 - 1s - loss: 0.0098 - val_loss: 0.0081
 - val_f1: 0.9940
Epoch 154/200
 - 1s - loss: 0.0090 - val_loss: 0.0084
 - val_f1: 0.9933
Epoch 155/200
 - 1s - loss: 0.0091 - val_loss: 0.0077
 - val_f1: 0.9940
Epoch 156/200
 - 1s - loss: 0.0089 - val_loss: 0.0078
 - val_f1: 0.9935
Epoch 157/200
 - 1s - loss: 0.0090 - val_loss: 0.0083
 - val_f1: 0.9940
Epoch 158/200
 - 1s - loss: 0.0093 - val_loss: 0.0079
 - val_f1: 0.9941
Epoch 159/200
 - 1s - loss: 0.0091 - val_loss: 0.0081
 - val_f1: 0.9940
Epoch 160/200
 - 1s - loss: 0.0092 - val_loss: 0.0078
 - val_f1: 0.9937
Epoch 161/200
 - 1s - loss: 0.0090 - val_loss: 0.0079
2019-12-24 01:29:22,196 [INFO] epoch = 160. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep2/ann_model_epoch_160.pickle
 - val_f1: 0.9938
Epoch 162/200
 - 1s - loss: 0.0092 - val_loss: 0.0083
 - val_f1: 0.9931
Epoch 163/200
 - 1s - loss: 0.0094 - val_loss: 0.0081
 - val_f1: 0.9933
Epoch 164/200
 - 1s - loss: 0.0090 - val_loss: 0.0080
 - val_f1: 0.9941
Epoch 165/200
 - 1s - loss: 0.0091 - val_loss: 0.0082
 - val_f1: 0.9937
Epoch 166/200
 - 1s - loss: 0.0092 - val_loss: 0.0083
 - val_f1: 0.9938
Epoch 167/200
 - 1s - loss: 0.0092 - val_loss: 0.0083
 - val_f1: 0.9935
Epoch 168/200
 - 1s - loss: 0.0090 - val_loss: 0.0082
 - val_f1: 0.9939
Epoch 169/200
 - 1s - loss: 0.0090 - val_loss: 0.0082
 - val_f1: 0.9930
Epoch 170/200
 - 1s - loss: 0.0088 - val_loss: 0.0080
 - val_f1: 0.9937
Epoch 171/200
 - 1s - loss: 0.0090 - val_loss: 0.0077
 - val_f1: 0.9944
Epoch 172/200
 - 1s - loss: 0.0092 - val_loss: 0.0083
 - val_f1: 0.9934
Epoch 173/200
 - 1s - loss: 0.0088 - val_loss: 0.0080
 - val_f1: 0.9933
Epoch 174/200
 - 1s - loss: 0.0090 - val_loss: 0.0078
 - val_f1: 0.9941
Epoch 175/200
 - 1s - loss: 0.0088 - val_loss: 0.0080
 - val_f1: 0.9939
Epoch 176/200
 - 1s - loss: 0.0092 - val_loss: 0.0080
 - val_f1: 0.9939
Epoch 177/200
 - 1s - loss: 0.0090 - val_loss: 0.0081
 - val_f1: 0.9934
Epoch 178/200
 - 1s - loss: 0.0089 - val_loss: 0.0082
 - val_f1: 0.9943
Epoch 179/200
 - 1s - loss: 0.0089 - val_loss: 0.0082
 - val_f1: 0.9934
Epoch 180/200
 - 1s - loss: 0.0089 - val_loss: 0.0081
 - val_f1: 0.9938
Epoch 181/200
 - 1s - loss: 0.0090 - val_loss: 0.0082
2019-12-24 01:29:52,138 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep2/ann_model_epoch_180.pickle
 - val_f1: 0.9938
Epoch 182/200
 - 1s - loss: 0.0087 - val_loss: 0.0084
 - val_f1: 0.9933
Epoch 183/200
 - 1s - loss: 0.0092 - val_loss: 0.0078
 - val_f1: 0.9935
Epoch 184/200
 - 1s - loss: 0.0089 - val_loss: 0.0086
 - val_f1: 0.9936
Epoch 185/200
 - 1s - loss: 0.0089 - val_loss: 0.0077
 - val_f1: 0.9942
Epoch 186/200
 - 1s - loss: 0.0089 - val_loss: 0.0078
 - val_f1: 0.9941
Epoch 187/200
 - 1s - loss: 0.0087 - val_loss: 0.0079
 - val_f1: 0.9941
Epoch 188/200
 - 1s - loss: 0.0090 - val_loss: 0.0078
 - val_f1: 0.9940
Epoch 189/200
 - 1s - loss: 0.0088 - val_loss: 0.0080
 - val_f1: 0.9938
Epoch 190/200
 - 1s - loss: 0.0089 - val_loss: 0.0081
 - val_f1: 0.9934
Epoch 191/200
 - 1s - loss: 0.0090 - val_loss: 0.0089
 - val_f1: 0.9924
Epoch 192/200
 - 1s - loss: 0.0091 - val_loss: 0.0082
 - val_f1: 0.9934
Epoch 193/200
 - 1s - loss: 0.0090 - val_loss: 0.0080
 - val_f1: 0.9933
Epoch 194/200
 - 1s - loss: 0.0090 - val_loss: 0.0082
 - val_f1: 0.9936
Epoch 195/200
 - 1s - loss: 0.0088 - val_loss: 0.0077
 - val_f1: 0.9935
Epoch 196/200
 - 1s - loss: 0.0088 - val_loss: 0.0078
 - val_f1: 0.9940
Epoch 197/200
 - 1s - loss: 0.0085 - val_loss: 0.0077
 - val_f1: 0.9942
Epoch 198/200
 - 1s - loss: 0.0091 - val_loss: 0.0082
 - val_f1: 0.9934
Epoch 199/200
 - 1s - loss: 0.0088 - val_loss: 0.0079
 - val_f1: 0.9938
Epoch 200/200
 - 1s - loss: 0.0086 - val_loss: 0.0081
2019-12-24 01:30:21,337 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-24 01:30:24,662 [INFO] Last epoch loss evaluation: train_loss = 0.006138, val_loss = 0.007668
2019-12-24 01:30:24,662 [INFO] Training complete. time_to_train = 553.29 sec, 9.22 min
2019-12-24 01:30:24,684 [INFO] Model saved to results_selected_models/selected_nsl_ae_ann_deep_rep2/best_model.pickle
2019-12-24 01:30:24,874 [INFO] Plot saved to: results_selected_models/selected_nsl_ae_ann_deep_rep2/training_error_history.png
2019-12-24 01:30:25,052 [INFO] Plot saved to: results_selected_models/selected_nsl_ae_ann_deep_rep2/training_f1_history.png
2019-12-24 01:30:25,052 [INFO] Making predictions on training, validation, testing data
2019-12-24 01:30:33,887 [INFO] Evaluating predictions (results)
2019-12-24 01:30:34,440 [INFO] Dataset: Testing. Classification report below
2019-12-24 01:30:34,440 [INFO] 
              precision    recall  f1-score   support

         dos       0.94      0.81      0.87      7458
      normal       0.68      0.97      0.80      9711
       probe       0.88      0.74      0.81      2421
         r2l       0.89      0.03      0.05      2421
         u2r       0.56      0.02      0.04       533

    accuracy                           0.77     22544
   macro avg       0.79      0.51      0.51     22544
weighted avg       0.81      0.77      0.72     22544

2019-12-24 01:30:34,440 [INFO] Overall accuracy (micro avg): 0.7705819730305181
2019-12-24 01:30:35,039 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.7706         0.7706                       0.7706                0.0574                   0.2294  0.7706
1     Macro avg        0.9082         0.7886                       0.5140                0.0782                   0.4860  0.5123
2  Weighted avg        0.8664         0.8052                       0.7706                0.1617                   0.2294  0.7246
2019-12-24 01:30:35,716 [INFO] Dataset: Validation. Classification report below
2019-12-24 01:30:35,717 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00      9186
      normal       0.99      1.00      0.99     13469
       probe       0.98      0.98      0.98      2331
         r2l       0.91      0.86      0.89       199
         u2r       0.60      0.30      0.40        10

    accuracy                           0.99     25195
   macro avg       0.90      0.83      0.85     25195
weighted avg       0.99      0.99      0.99     25195

2019-12-24 01:30:35,717 [INFO] Overall accuracy (micro avg): 0.9939273665409804
2019-12-24 01:30:36,391 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9939         0.9939                       0.9939                0.0015                   0.0061  0.9939
1     Macro avg        0.9976         0.8978                       0.8274                0.0020                   0.1726  0.8524
2  Weighted avg        0.9962         0.9938                       0.9939                0.0040                   0.0061  0.9938
2019-12-24 01:30:39,237 [INFO] Dataset: Training. Classification report below
2019-12-24 01:30:39,238 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00     36741
      normal       0.99      1.00      1.00     53874
       probe       0.99      0.99      0.99      9325
         r2l       0.89      0.86      0.87       796
         u2r       0.71      0.48      0.57        42

    accuracy                           0.99    100778
   macro avg       0.92      0.86      0.88    100778
weighted avg       0.99      0.99      0.99    100778

2019-12-24 01:30:39,238 [INFO] Overall accuracy (micro avg): 0.9945325368632043
2019-12-24 01:30:42,284 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9945         0.9945                       0.9945                0.0014                   0.0055  0.9945
1     Macro avg        0.9978         0.9162                       0.8628                0.0018                   0.1372  0.8847
2  Weighted avg        0.9966         0.9945                       0.9945                0.0038                   0.0055  0.9945
2019-12-24 01:30:42,330 [INFO] Results saved to: results_selected_models/selected_nsl_ae_ann_deep_rep2/selected_nsl_ae_ann_deep_rep2_results.xlsx
2019-12-24 01:30:42,331 [INFO] ================= Finished running experiment no. 2 ================= 

2019-12-24 01:30:42,332 [INFO] Created directory: results_selected_models/selected_nsl_ae_ann_deep_rep3
2019-12-24 01:30:42,332 [INFO] Initialized logging. log_filename = results_selected_models/selected_nsl_ae_ann_deep_rep3/run_log.log
2019-12-24 01:30:42,332 [INFO] ================= Running experiment no. 3  ================= 

2019-12-24 01:30:42,332 [INFO] Experiment parameters given below
2019-12-24 01:30:42,332 [INFO] 
{'experiment_num': 3, 'results_dir': 'results_selected_models/selected_nsl_ae_ann_deep_rep3', 'model_type': 'classifier', 'model': 'ae_ann', 'normal_label': 'normal', 'scaling_type': 'NA', 'unsupervised_ratio': 0.25, 'ae_encoder_units': [128, 64, 32], 'ae_encoder_activations': ['relu', 'relu', 'relu'], 'ae_encoder_dropout_rates': [0.2, 0.2, 0.2], 'ae_encoder_l1_param': -1, 'ae_decoder_units': [64, 128], 'ae_decoder_activations': ['relu', 'relu'], 'ae_decoder_dropout_rates': [0.2, 0.2], 'output_activation': 'sigmoid', 'loss_function': 'binary_crossentropy', 'ae_epochs': 200, 'ann_layer_units': [64], 'ann_layer_activations': ['relu'], 'ann_layer_dropout_rates': [0.2], 'ann_epochs': 200, 'early_stop_patience': 50, 'batch_size': 256, 'dataset_dir': '../Datasets/small_datasets/nsl_kdd_five_classes', 'description': 'selected_nsl_ae_ann_deep_rep3'}
2019-12-24 01:30:42,332 [INFO] Created tensorboard log directory: results_selected_models/selected_nsl_ae_ann_deep_rep3/tf_logs_run_2019_12_24-01_30_42
2019-12-24 01:30:42,332 [INFO] Loading datsets from: ../Datasets/small_datasets/nsl_kdd_five_classes
2019-12-24 01:30:42,332 [INFO] Reading X, y files
2019-12-24 01:30:42,332 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_train.h5
2019-12-24 01:30:42,565 [INFO] Reading complete. time_to_read=0.23 seconds
2019-12-24 01:30:42,565 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_val.h5
2019-12-24 01:30:42,628 [INFO] Reading complete. time_to_read=0.06 seconds
2019-12-24 01:30:42,628 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_test.h5
2019-12-24 01:30:42,685 [INFO] Reading complete. time_to_read=0.06 seconds
2019-12-24 01:30:42,685 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_train.h5
2019-12-24 01:30:42,694 [INFO] Reading complete. time_to_read=0.01 seconds
2019-12-24 01:30:42,694 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_val.h5
2019-12-24 01:30:42,699 [INFO] Reading complete. time_to_read=0.00 seconds
2019-12-24 01:30:42,699 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_test.h5
2019-12-24 01:30:42,703 [INFO] Reading complete. time_to_read=0.00 seconds
2019-12-24 01:30:42,821 [INFO] Initializing model
2019-12-24 01:30:43,323 [INFO] _________________________________________________________________
2019-12-24 01:30:43,323 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-24 01:30:43,323 [INFO] =================================================================
2019-12-24 01:30:43,323 [INFO] dense_37 (Dense)             (None, 128)               15744     
2019-12-24 01:30:43,323 [INFO] _________________________________________________________________
2019-12-24 01:30:43,323 [INFO] batch_normalization_23 (Batc (None, 128)               512       
2019-12-24 01:30:43,324 [INFO] _________________________________________________________________
2019-12-24 01:30:43,324 [INFO] dropout_23 (Dropout)         (None, 128)               0         
2019-12-24 01:30:43,324 [INFO] _________________________________________________________________
2019-12-24 01:30:43,324 [INFO] dense_38 (Dense)             (None, 64)                8256      
2019-12-24 01:30:43,324 [INFO] _________________________________________________________________
2019-12-24 01:30:43,324 [INFO] batch_normalization_24 (Batc (None, 64)                256       
2019-12-24 01:30:43,324 [INFO] _________________________________________________________________
2019-12-24 01:30:43,324 [INFO] dropout_24 (Dropout)         (None, 64)                0         
2019-12-24 01:30:43,324 [INFO] _________________________________________________________________
2019-12-24 01:30:43,324 [INFO] dense_39 (Dense)             (None, 32)                2080      
2019-12-24 01:30:43,324 [INFO] _________________________________________________________________
2019-12-24 01:30:43,325 [INFO] batch_normalization_25 (Batc (None, 32)                128       
2019-12-24 01:30:43,325 [INFO] _________________________________________________________________
2019-12-24 01:30:43,325 [INFO] dropout_25 (Dropout)         (None, 32)                0         
2019-12-24 01:30:43,325 [INFO] _________________________________________________________________
2019-12-24 01:30:43,325 [INFO] dense_40 (Dense)             (None, 64)                2112      
2019-12-24 01:30:43,325 [INFO] _________________________________________________________________
2019-12-24 01:30:43,325 [INFO] batch_normalization_26 (Batc (None, 64)                256       
2019-12-24 01:30:43,325 [INFO] _________________________________________________________________
2019-12-24 01:30:43,325 [INFO] dropout_26 (Dropout)         (None, 64)                0         
2019-12-24 01:30:43,325 [INFO] _________________________________________________________________
2019-12-24 01:30:43,325 [INFO] dense_41 (Dense)             (None, 128)               8320      
2019-12-24 01:30:43,325 [INFO] _________________________________________________________________
2019-12-24 01:30:43,325 [INFO] batch_normalization_27 (Batc (None, 128)               512       
2019-12-24 01:30:43,326 [INFO] _________________________________________________________________
2019-12-24 01:30:43,326 [INFO] dropout_27 (Dropout)         (None, 128)               0         
2019-12-24 01:30:43,326 [INFO] _________________________________________________________________
2019-12-24 01:30:43,326 [INFO] dense_42 (Dense)             (None, 122)               15738     
2019-12-24 01:30:43,326 [INFO] =================================================================
2019-12-24 01:30:43,326 [INFO] Total params: 53,914
2019-12-24 01:30:43,326 [INFO] Trainable params: 53,082
2019-12-24 01:30:43,326 [INFO] Non-trainable params: 832
2019-12-24 01:30:43,326 [INFO] _________________________________________________________________
2019-12-24 01:30:43,454 [INFO] _________________________________________________________________
2019-12-24 01:30:43,455 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-24 01:30:43,455 [INFO] =================================================================
2019-12-24 01:30:43,455 [INFO] dense_43 (Dense)             (None, 64)                2112      
2019-12-24 01:30:43,455 [INFO] _________________________________________________________________
2019-12-24 01:30:43,455 [INFO] batch_normalization_28 (Batc (None, 64)                256       
2019-12-24 01:30:43,455 [INFO] _________________________________________________________________
2019-12-24 01:30:43,455 [INFO] dropout_28 (Dropout)         (None, 64)                0         
2019-12-24 01:30:43,455 [INFO] _________________________________________________________________
2019-12-24 01:30:43,455 [INFO] dense_44 (Dense)             (None, 5)                 325       
2019-12-24 01:30:43,455 [INFO] =================================================================
2019-12-24 01:30:43,456 [INFO] Total params: 2,693
2019-12-24 01:30:43,456 [INFO] Trainable params: 2,565
2019-12-24 01:30:43,456 [INFO] Non-trainable params: 128
2019-12-24 01:30:43,456 [INFO] _________________________________________________________________
2019-12-24 01:30:43,456 [INFO] Training model
2019-12-24 01:30:43,456 [INFO] Splitting train set into 2 sets (unsupervised, supervised), random_seed = None
2019-12-24 01:30:44,616 [INFO] Split sizes (instances). total = 100778, unsupervised = 25194, supervised = 75584, unsupervised dataset hash = 439af618b655a4ef64e00cc4634502f0bb4daa79
2019-12-24 01:30:44,617 [INFO] Training autoencoder
 - val_f1: 0.9933
Train on 25194 samples, validate on 25195 samples
Epoch 1/200
 - 4s - loss: 0.4076 - val_loss: -3.9175e-01
Epoch 2/200
 - 1s - loss: -6.0352e-01 - val_loss: -1.2697e+00
Epoch 3/200
 - 1s - loss: -1.3517e+00 - val_loss: -1.8114e+00
Epoch 4/200
 - 1s - loss: -1.8293e+00 - val_loss: -2.1848e+00
Epoch 5/200
 - 1s - loss: -2.1436e+00 - val_loss: -2.4508e+00
Epoch 6/200
 - 1s - loss: -2.3524e+00 - val_loss: -2.6445e+00
Epoch 7/200
 - 1s - loss: -2.5068e+00 - val_loss: -2.7834e+00
Epoch 8/200
 - 1s - loss: -2.6158e+00 - val_loss: -2.8744e+00
Epoch 9/200
 - 1s - loss: -2.6938e+00 - val_loss: -2.9402e+00
Epoch 10/200
 - 1s - loss: -2.7507e+00 - val_loss: -2.9757e+00
Epoch 11/200
 - 1s - loss: -2.7950e+00 - val_loss: -3.0010e+00
Epoch 12/200
 - 1s - loss: -2.8306e+00 - val_loss: -3.0187e+00
Epoch 13/200
 - 1s - loss: -2.8523e+00 - val_loss: -3.0319e+00
Epoch 14/200
 - 1s - loss: -2.8794e+00 - val_loss: -3.0455e+00
Epoch 15/200
 - 1s - loss: -2.9018e+00 - val_loss: -3.0566e+00
Epoch 16/200
 - 1s - loss: -2.9163e+00 - val_loss: -3.0664e+00
Epoch 17/200
 - 1s - loss: -2.9229e+00 - val_loss: -3.0703e+00
Epoch 18/200
 - 1s - loss: -2.9416e+00 - val_loss: -3.0770e+00
Epoch 19/200
 - 1s - loss: -2.9483e+00 - val_loss: -3.0825e+00
Epoch 20/200
 - 1s - loss: -2.9575e+00 - val_loss: -3.0875e+00
Epoch 21/200
 - 1s - loss: -2.9665e+00 - val_loss: -3.0941e+00
2019-12-24 01:31:18,544 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep3/ae_model_epoch_20.pickle
Epoch 22/200
 - 1s - loss: -2.9730e+00 - val_loss: -3.0978e+00
Epoch 23/200
 - 1s - loss: -2.9809e+00 - val_loss: -3.1007e+00
Epoch 24/200
 - 1s - loss: -2.9917e+00 - val_loss: -3.1054e+00
Epoch 25/200
 - 1s - loss: -2.9932e+00 - val_loss: -3.1093e+00
Epoch 26/200
 - 1s - loss: -2.9999e+00 - val_loss: -3.1121e+00
Epoch 27/200
 - 1s - loss: -3.0032e+00 - val_loss: -3.1138e+00
Epoch 28/200
 - 1s - loss: -3.0082e+00 - val_loss: -3.1184e+00
Epoch 29/200
 - 1s - loss: -3.0166e+00 - val_loss: -3.1223e+00
Epoch 30/200
 - 1s - loss: -3.0232e+00 - val_loss: -3.1257e+00
Epoch 31/200
 - 1s - loss: -3.0265e+00 - val_loss: -3.1301e+00
Epoch 32/200
 - 1s - loss: -3.0294e+00 - val_loss: -3.1326e+00
Epoch 33/200
 - 1s - loss: -3.0306e+00 - val_loss: -3.1353e+00
Epoch 34/200
 - 1s - loss: -3.0380e+00 - val_loss: -3.1379e+00
Epoch 35/200
 - 1s - loss: -3.0445e+00 - val_loss: -3.1419e+00
Epoch 36/200
 - 1s - loss: -3.0413e+00 - val_loss: -3.1441e+00
Epoch 37/200
 - 1s - loss: -3.0417e+00 - val_loss: -3.1429e+00
Epoch 38/200
 - 1s - loss: -3.0502e+00 - val_loss: -3.1466e+00
Epoch 39/200
 - 1s - loss: -3.0535e+00 - val_loss: -3.1486e+00
Epoch 40/200
 - 1s - loss: -3.0538e+00 - val_loss: -3.1497e+00
Epoch 41/200
 - 1s - loss: -3.0598e+00 - val_loss: -3.1505e+00
2019-12-24 01:31:41,724 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep3/ae_model_epoch_40.pickle
Epoch 42/200
 - 1s - loss: -3.0570e+00 - val_loss: -3.1541e+00
Epoch 43/200
 - 1s - loss: -3.0618e+00 - val_loss: -3.1548e+00
Epoch 44/200
 - 1s - loss: -3.0649e+00 - val_loss: -3.1556e+00
Epoch 45/200
 - 1s - loss: -3.0697e+00 - val_loss: -3.1574e+00
Epoch 46/200
 - 1s - loss: -3.0691e+00 - val_loss: -3.1577e+00
Epoch 47/200
 - 1s - loss: -3.0735e+00 - val_loss: -3.1593e+00
Epoch 48/200
 - 1s - loss: -3.0736e+00 - val_loss: -3.1608e+00
Epoch 49/200
 - 1s - loss: -3.0748e+00 - val_loss: -3.1606e+00
Epoch 50/200
 - 1s - loss: -3.0772e+00 - val_loss: -3.1622e+00
Epoch 51/200
 - 1s - loss: -3.0777e+00 - val_loss: -3.1639e+00
Epoch 52/200
 - 1s - loss: -3.0789e+00 - val_loss: -3.1651e+00
Epoch 53/200
 - 1s - loss: -3.0861e+00 - val_loss: -3.1660e+00
Epoch 54/200
 - 1s - loss: -3.0854e+00 - val_loss: -3.1668e+00
Epoch 55/200
 - 1s - loss: -3.0858e+00 - val_loss: -3.1691e+00
Epoch 56/200
 - 1s - loss: -3.0891e+00 - val_loss: -3.1696e+00
Epoch 57/200
 - 1s - loss: -3.0889e+00 - val_loss: -3.1710e+00
Epoch 58/200
 - 1s - loss: -3.0920e+00 - val_loss: -3.1710e+00
Epoch 59/200
 - 1s - loss: -3.0964e+00 - val_loss: -3.1708e+00
Epoch 60/200
 - 1s - loss: -3.0930e+00 - val_loss: -3.1723e+00
Epoch 61/200
 - 1s - loss: -3.0981e+00 - val_loss: -3.1743e+00
2019-12-24 01:32:04,967 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep3/ae_model_epoch_60.pickle
Epoch 62/200
 - 1s - loss: -3.1006e+00 - val_loss: -3.1743e+00
Epoch 63/200
 - 1s - loss: -3.0996e+00 - val_loss: -3.1751e+00
Epoch 64/200
 - 1s - loss: -3.0936e+00 - val_loss: -3.1749e+00
Epoch 65/200
 - 1s - loss: -3.1006e+00 - val_loss: -3.1763e+00
Epoch 66/200
 - 1s - loss: -3.1073e+00 - val_loss: -3.1769e+00
Epoch 67/200
 - 1s - loss: -3.1024e+00 - val_loss: -3.1777e+00
Epoch 68/200
 - 1s - loss: -3.1049e+00 - val_loss: -3.1785e+00
Epoch 69/200
 - 1s - loss: -3.1067e+00 - val_loss: -3.1776e+00
Epoch 70/200
 - 1s - loss: -3.1062e+00 - val_loss: -3.1782e+00
Epoch 71/200
 - 1s - loss: -3.1087e+00 - val_loss: -3.1799e+00
Epoch 72/200
 - 1s - loss: -3.1077e+00 - val_loss: -3.1800e+00
Epoch 73/200
 - 1s - loss: -3.1068e+00 - val_loss: -3.1801e+00
Epoch 74/200
 - 1s - loss: -3.1128e+00 - val_loss: -3.1806e+00
Epoch 75/200
 - 1s - loss: -3.1139e+00 - val_loss: -3.1822e+00
Epoch 76/200
 - 1s - loss: -3.1123e+00 - val_loss: -3.1824e+00
Epoch 77/200
 - 1s - loss: -3.1148e+00 - val_loss: -3.1834e+00
Epoch 78/200
 - 1s - loss: -3.1133e+00 - val_loss: -3.1838e+00
Epoch 79/200
 - 1s - loss: -3.1158e+00 - val_loss: -3.1835e+00
Epoch 80/200
 - 1s - loss: -3.1144e+00 - val_loss: -3.1841e+00
Epoch 81/200
 - 1s - loss: -3.1161e+00 - val_loss: -3.1849e+00
2019-12-24 01:32:28,158 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep3/ae_model_epoch_80.pickle
Epoch 82/200
 - 1s - loss: -3.1178e+00 - val_loss: -3.1851e+00
Epoch 83/200
 - 1s - loss: -3.1191e+00 - val_loss: -3.1852e+00
Epoch 84/200
 - 1s - loss: -3.1172e+00 - val_loss: -3.1850e+00
Epoch 85/200
 - 1s - loss: -3.1196e+00 - val_loss: -3.1856e+00
Epoch 86/200
 - 1s - loss: -3.1203e+00 - val_loss: -3.1863e+00
Epoch 87/200
 - 1s - loss: -3.1218e+00 - val_loss: -3.1862e+00
Epoch 88/200
 - 1s - loss: -3.1213e+00 - val_loss: -3.1867e+00
Epoch 89/200
 - 1s - loss: -3.1213e+00 - val_loss: -3.1859e+00
Epoch 90/200
 - 1s - loss: -3.1238e+00 - val_loss: -3.1876e+00
Epoch 91/200
 - 1s - loss: -3.1260e+00 - val_loss: -3.1888e+00
Epoch 92/200
 - 1s - loss: -3.1252e+00 - val_loss: -3.1882e+00
Epoch 93/200
 - 1s - loss: -3.1240e+00 - val_loss: -3.1885e+00
Epoch 94/200
 - 1s - loss: -3.1270e+00 - val_loss: -3.1899e+00
Epoch 95/200
 - 1s - loss: -3.1276e+00 - val_loss: -3.1898e+00
Epoch 96/200
 - 1s - loss: -3.1274e+00 - val_loss: -3.1891e+00
Epoch 97/200
 - 1s - loss: -3.1272e+00 - val_loss: -3.1899e+00
Epoch 98/200
 - 1s - loss: -3.1275e+00 - val_loss: -3.1909e+00
Epoch 99/200
 - 1s - loss: -3.1327e+00 - val_loss: -3.1913e+00
Epoch 100/200
 - 1s - loss: -3.1327e+00 - val_loss: -3.1910e+00
Epoch 101/200
 - 1s - loss: -3.1310e+00 - val_loss: -3.1921e+00
2019-12-24 01:32:51,360 [INFO] epoch = 100. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep3/ae_model_epoch_100.pickle
Epoch 102/200
 - 1s - loss: -3.1305e+00 - val_loss: -3.1920e+00
Epoch 103/200
 - 1s - loss: -3.1322e+00 - val_loss: -3.1920e+00
Epoch 104/200
 - 1s - loss: -3.1325e+00 - val_loss: -3.1912e+00
Epoch 105/200
 - 1s - loss: -3.1334e+00 - val_loss: -3.1905e+00
Epoch 106/200
 - 1s - loss: -3.1333e+00 - val_loss: -3.1929e+00
Epoch 107/200
 - 1s - loss: -3.1329e+00 - val_loss: -3.1940e+00
Epoch 108/200
 - 1s - loss: -3.1337e+00 - val_loss: -3.1931e+00
Epoch 109/200
 - 1s - loss: -3.1369e+00 - val_loss: -3.1937e+00
Epoch 110/200
 - 1s - loss: -3.1380e+00 - val_loss: -3.1935e+00
Epoch 111/200
 - 1s - loss: -3.1345e+00 - val_loss: -3.1932e+00
Epoch 112/200
 - 1s - loss: -3.1332e+00 - val_loss: -3.1936e+00
Epoch 113/200
 - 1s - loss: -3.1395e+00 - val_loss: -3.1949e+00
Epoch 114/200
 - 1s - loss: -3.1380e+00 - val_loss: -3.1947e+00
Epoch 115/200
 - 1s - loss: -3.1417e+00 - val_loss: -3.1937e+00
Epoch 116/200
 - 1s - loss: -3.1374e+00 - val_loss: -3.1947e+00
Epoch 117/200
 - 1s - loss: -3.1357e+00 - val_loss: -3.1951e+00
Epoch 118/200
 - 1s - loss: -3.1377e+00 - val_loss: -3.1951e+00
Epoch 119/200
 - 1s - loss: -3.1401e+00 - val_loss: -3.1945e+00
Epoch 120/200
 - 1s - loss: -3.1396e+00 - val_loss: -3.1968e+00
Epoch 121/200
 - 1s - loss: -3.1401e+00 - val_loss: -3.1963e+00
2019-12-24 01:33:14,558 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep3/ae_model_epoch_120.pickle
Epoch 122/200
 - 1s - loss: -3.1411e+00 - val_loss: -3.1952e+00
Epoch 123/200
 - 1s - loss: -3.1402e+00 - val_loss: -3.1961e+00
Epoch 124/200
 - 1s - loss: -3.1414e+00 - val_loss: -3.1969e+00
Epoch 125/200
 - 1s - loss: -3.1462e+00 - val_loss: -3.1983e+00
Epoch 126/200
 - 1s - loss: -3.1416e+00 - val_loss: -3.1973e+00
Epoch 127/200
 - 1s - loss: -3.1484e+00 - val_loss: -3.1983e+00
Epoch 128/200
 - 1s - loss: -3.1435e+00 - val_loss: -3.1986e+00
Epoch 129/200
 - 1s - loss: -3.1430e+00 - val_loss: -3.1989e+00
Epoch 130/200
 - 1s - loss: -3.1437e+00 - val_loss: -3.1984e+00
Epoch 131/200
 - 1s - loss: -3.1446e+00 - val_loss: -3.1983e+00
Epoch 132/200
 - 1s - loss: -3.1457e+00 - val_loss: -3.1989e+00
Epoch 133/200
 - 1s - loss: -3.1460e+00 - val_loss: -3.1995e+00
Epoch 134/200
 - 1s - loss: -3.1438e+00 - val_loss: -3.1997e+00
Epoch 135/200
 - 1s - loss: -3.1436e+00 - val_loss: -3.2003e+00
Epoch 136/200
 - 1s - loss: -3.1469e+00 - val_loss: -3.2006e+00
Epoch 137/200
 - 1s - loss: -3.1466e+00 - val_loss: -3.2003e+00
Epoch 138/200
 - 1s - loss: -3.1447e+00 - val_loss: -3.2003e+00
Epoch 139/200
 - 1s - loss: -3.1459e+00 - val_loss: -3.1988e+00
Epoch 140/200
 - 1s - loss: -3.1469e+00 - val_loss: -3.1985e+00
Epoch 141/200
 - 1s - loss: -3.1457e+00 - val_loss: -3.1994e+00
2019-12-24 01:33:37,736 [INFO] epoch = 140. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep3/ae_model_epoch_140.pickle
Epoch 142/200
 - 1s - loss: -3.1462e+00 - val_loss: -3.2001e+00
Epoch 143/200
 - 1s - loss: -3.1483e+00 - val_loss: -3.1998e+00
Epoch 144/200
 - 1s - loss: -3.1500e+00 - val_loss: -3.2006e+00
Epoch 145/200
 - 1s - loss: -3.1523e+00 - val_loss: -3.2019e+00
Epoch 146/200
 - 1s - loss: -3.1489e+00 - val_loss: -3.2011e+00
Epoch 147/200
 - 1s - loss: -3.1488e+00 - val_loss: -3.2015e+00
Epoch 148/200
 - 1s - loss: -3.1494e+00 - val_loss: -3.2018e+00
Epoch 149/200
 - 1s - loss: -3.1504e+00 - val_loss: -3.2025e+00
Epoch 150/200
 - 1s - loss: -3.1458e+00 - val_loss: -3.2009e+00
Epoch 151/200
 - 1s - loss: -3.1465e+00 - val_loss: -3.2016e+00
Epoch 152/200
 - 1s - loss: -3.1497e+00 - val_loss: -3.2015e+00
Epoch 153/200
 - 1s - loss: -3.1512e+00 - val_loss: -3.2010e+00
Epoch 154/200
 - 1s - loss: -3.1521e+00 - val_loss: -3.2017e+00
Epoch 155/200
 - 1s - loss: -3.1490e+00 - val_loss: -3.2018e+00
Epoch 156/200
 - 1s - loss: -3.1525e+00 - val_loss: -3.2021e+00
Epoch 157/200
 - 1s - loss: -3.1534e+00 - val_loss: -3.2029e+00
Epoch 158/200
 - 1s - loss: -3.1495e+00 - val_loss: -3.2022e+00
Epoch 159/200
 - 1s - loss: -3.1542e+00 - val_loss: -3.2032e+00
Epoch 160/200
 - 1s - loss: -3.1517e+00 - val_loss: -3.2026e+00
Epoch 161/200
 - 1s - loss: -3.1533e+00 - val_loss: -3.2031e+00
2019-12-24 01:34:00,932 [INFO] epoch = 160. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep3/ae_model_epoch_160.pickle
Epoch 162/200
 - 1s - loss: -3.1538e+00 - val_loss: -3.2025e+00
Epoch 163/200
 - 1s - loss: -3.1532e+00 - val_loss: -3.2028e+00
Epoch 164/200
 - 1s - loss: -3.1525e+00 - val_loss: -3.2029e+00
Epoch 165/200
 - 1s - loss: -3.1533e+00 - val_loss: -3.2043e+00
Epoch 166/200
 - 1s - loss: -3.1518e+00 - val_loss: -3.2031e+00
Epoch 167/200
 - 1s - loss: -3.1532e+00 - val_loss: -3.2036e+00
Epoch 168/200
 - 1s - loss: -3.1532e+00 - val_loss: -3.2033e+00
Epoch 169/200
 - 1s - loss: -3.1530e+00 - val_loss: -3.2037e+00
Epoch 170/200
 - 1s - loss: -3.1559e+00 - val_loss: -3.2047e+00
Epoch 171/200
 - 1s - loss: -3.1537e+00 - val_loss: -3.2035e+00
Epoch 172/200
 - 1s - loss: -3.1563e+00 - val_loss: -3.2044e+00
Epoch 173/200
 - 1s - loss: -3.1559e+00 - val_loss: -3.2025e+00
Epoch 174/200
 - 1s - loss: -3.1567e+00 - val_loss: -3.2048e+00
Epoch 175/200
 - 1s - loss: -3.1513e+00 - val_loss: -3.2042e+00
Epoch 176/200
 - 1s - loss: -3.1558e+00 - val_loss: -3.2044e+00
Epoch 177/200
 - 1s - loss: -3.1550e+00 - val_loss: -3.2053e+00
Epoch 178/200
 - 1s - loss: -3.1555e+00 - val_loss: -3.2047e+00
Epoch 179/200
 - 1s - loss: -3.1558e+00 - val_loss: -3.2036e+00
Epoch 180/200
 - 1s - loss: -3.1553e+00 - val_loss: -3.2040e+00
Epoch 181/200
 - 1s - loss: -3.1574e+00 - val_loss: -3.2049e+00
2019-12-24 01:34:24,162 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep3/ae_model_epoch_180.pickle
Epoch 182/200
 - 1s - loss: -3.1570e+00 - val_loss: -3.2055e+00
Epoch 183/200
 - 1s - loss: -3.1602e+00 - val_loss: -3.2052e+00
Epoch 184/200
 - 1s - loss: -3.1520e+00 - val_loss: -3.2043e+00
Epoch 185/200
 - 1s - loss: -3.1562e+00 - val_loss: -3.2060e+00
Epoch 186/200
 - 1s - loss: -3.1576e+00 - val_loss: -3.2051e+00
Epoch 187/200
 - 1s - loss: -3.1589e+00 - val_loss: -3.2068e+00
Epoch 188/200
 - 1s - loss: -3.1564e+00 - val_loss: -3.2061e+00
Epoch 189/200
 - 1s - loss: -3.1575e+00 - val_loss: -3.2056e+00
Epoch 190/200
 - 1s - loss: -3.1571e+00 - val_loss: -3.2054e+00
Epoch 191/200
 - 1s - loss: -3.1564e+00 - val_loss: -3.2053e+00
Epoch 192/200
 - 1s - loss: -3.1588e+00 - val_loss: -3.2042e+00
Epoch 193/200
 - 1s - loss: -3.1597e+00 - val_loss: -3.2057e+00
Epoch 194/200
 - 1s - loss: -3.1618e+00 - val_loss: -3.2063e+00
Epoch 195/200
 - 1s - loss: -3.1589e+00 - val_loss: -3.2056e+00
Epoch 196/200
 - 1s - loss: -3.1630e+00 - val_loss: -3.2069e+00
Epoch 197/200
 - 1s - loss: -3.1596e+00 - val_loss: -3.2063e+00
Epoch 198/200
 - 1s - loss: -3.1576e+00 - val_loss: -3.2059e+00
Epoch 199/200
 - 1s - loss: -3.1584e+00 - val_loss: -3.2064e+00
Epoch 200/200
 - 1s - loss: -3.1600e+00 - val_loss: -3.2070e+00
2019-12-24 01:34:46,149 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-24 01:34:49,298 [INFO] Last epoch loss evaluation: train_loss = -3.232054, val_loss = -3.207036
2019-12-24 01:34:49,299 [INFO] Training autoencoder complete
2019-12-24 01:34:49,299 [INFO] Encoding data for supervised training
2019-12-24 01:34:53,564 [INFO] Encoding complete
2019-12-24 01:34:53,564 [INFO] Training neural network layers (after autoencoder)
Train on 75584 samples, validate on 25195 samples
Epoch 1/200
 - 2s - loss: 0.1490 - val_loss: 0.0481
 - val_f1: 0.9684
Epoch 2/200
 - 1s - loss: 0.0483 - val_loss: 0.0344
 - val_f1: 0.9732
Epoch 3/200
 - 1s - loss: 0.0382 - val_loss: 0.0284
 - val_f1: 0.9767
Epoch 4/200
 - 1s - loss: 0.0334 - val_loss: 0.0261
 - val_f1: 0.9802
Epoch 5/200
 - 1s - loss: 0.0296 - val_loss: 0.0227
 - val_f1: 0.9816
Epoch 6/200
 - 1s - loss: 0.0274 - val_loss: 0.0206
 - val_f1: 0.9844
Epoch 7/200
 - 1s - loss: 0.0255 - val_loss: 0.0200
 - val_f1: 0.9834
Epoch 8/200
 - 1s - loss: 0.0240 - val_loss: 0.0185
 - val_f1: 0.9843
Epoch 9/200
 - 1s - loss: 0.0224 - val_loss: 0.0171
 - val_f1: 0.9852
Epoch 10/200
 - 1s - loss: 0.0215 - val_loss: 0.0155
 - val_f1: 0.9876
Epoch 11/200
 - 1s - loss: 0.0204 - val_loss: 0.0147
 - val_f1: 0.9897
Epoch 12/200
 - 1s - loss: 0.0197 - val_loss: 0.0143
 - val_f1: 0.9895
Epoch 13/200
 - 1s - loss: 0.0191 - val_loss: 0.0137
 - val_f1: 0.9876
Epoch 14/200
 - 1s - loss: 0.0189 - val_loss: 0.0131
 - val_f1: 0.9890
Epoch 15/200
 - 1s - loss: 0.0179 - val_loss: 0.0130
 - val_f1: 0.9897
Epoch 16/200
 - 1s - loss: 0.0178 - val_loss: 0.0129
 - val_f1: 0.9896
Epoch 17/200
 - 1s - loss: 0.0172 - val_loss: 0.0120
 - val_f1: 0.9908
Epoch 18/200
 - 1s - loss: 0.0166 - val_loss: 0.0123
 - val_f1: 0.9891
Epoch 19/200
 - 1s - loss: 0.0167 - val_loss: 0.0121
 - val_f1: 0.9906
Epoch 20/200
 - 1s - loss: 0.0159 - val_loss: 0.0115
 - val_f1: 0.9908
Epoch 21/200
 - 1s - loss: 0.0159 - val_loss: 0.0116
2019-12-24 01:35:32,661 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep3/ann_model_epoch_20.pickle
 - val_f1: 0.9910
Epoch 22/200
 - 1s - loss: 0.0154 - val_loss: 0.0133
 - val_f1: 0.9887
Epoch 23/200
 - 1s - loss: 0.0156 - val_loss: 0.0113
 - val_f1: 0.9915
Epoch 24/200
 - 1s - loss: 0.0151 - val_loss: 0.0108
 - val_f1: 0.9919
Epoch 25/200
 - 1s - loss: 0.0145 - val_loss: 0.0110
 - val_f1: 0.9916
Epoch 26/200
 - 1s - loss: 0.0144 - val_loss: 0.0114
 - val_f1: 0.9911
Epoch 27/200
 - 1s - loss: 0.0143 - val_loss: 0.0110
 - val_f1: 0.9904
Epoch 28/200
 - 1s - loss: 0.0139 - val_loss: 0.0110
 - val_f1: 0.9913
Epoch 29/200
 - 1s - loss: 0.0138 - val_loss: 0.0105
 - val_f1: 0.9914
Epoch 30/200
 - 1s - loss: 0.0137 - val_loss: 0.0124
 - val_f1: 0.9889
Epoch 31/200
 - 1s - loss: 0.0140 - val_loss: 0.0111
 - val_f1: 0.9902
Epoch 32/200
 - 1s - loss: 0.0138 - val_loss: 0.0110
 - val_f1: 0.9898
Epoch 33/200
 - 1s - loss: 0.0133 - val_loss: 0.0104
 - val_f1: 0.9920
Epoch 34/200
 - 1s - loss: 0.0128 - val_loss: 0.0103
 - val_f1: 0.9919
Epoch 35/200
 - 1s - loss: 0.0131 - val_loss: 0.0103
 - val_f1: 0.9911
Epoch 36/200
 - 1s - loss: 0.0132 - val_loss: 0.0106
 - val_f1: 0.9914
Epoch 37/200
 - 1s - loss: 0.0130 - val_loss: 0.0106
 - val_f1: 0.9913
Epoch 38/200
 - 1s - loss: 0.0128 - val_loss: 0.0101
 - val_f1: 0.9922
Epoch 39/200
 - 1s - loss: 0.0125 - val_loss: 0.0106
 - val_f1: 0.9917
Epoch 40/200
 - 1s - loss: 0.0125 - val_loss: 0.0103
 - val_f1: 0.9918
Epoch 41/200
 - 1s - loss: 0.0123 - val_loss: 0.0104
2019-12-24 01:36:04,243 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep3/ann_model_epoch_40.pickle
 - val_f1: 0.9910
Epoch 42/200
 - 1s - loss: 0.0124 - val_loss: 0.0112
 - val_f1: 0.9897
Epoch 43/200
 - 1s - loss: 0.0121 - val_loss: 0.0105
 - val_f1: 0.9918
Epoch 44/200
 - 1s - loss: 0.0121 - val_loss: 0.0101
 - val_f1: 0.9917
Epoch 45/200
 - 1s - loss: 0.0123 - val_loss: 0.0105
 - val_f1: 0.9915
Epoch 46/200
 - 1s - loss: 0.0120 - val_loss: 0.0098
 - val_f1: 0.9928
Epoch 47/200
 - 1s - loss: 0.0117 - val_loss: 0.0099
 - val_f1: 0.9926
Epoch 48/200
 - 1s - loss: 0.0121 - val_loss: 0.0104
 - val_f1: 0.9916
Epoch 49/200
 - 1s - loss: 0.0120 - val_loss: 0.0098
 - val_f1: 0.9921
Epoch 50/200
 - 1s - loss: 0.0118 - val_loss: 0.0106
 - val_f1: 0.9911
Epoch 51/200
 - 1s - loss: 0.0116 - val_loss: 0.0104
 - val_f1: 0.9915
Epoch 52/200
 - 1s - loss: 0.0116 - val_loss: 0.0101
 - val_f1: 0.9920
Epoch 53/200
 - 1s - loss: 0.0117 - val_loss: 0.0096
 - val_f1: 0.9928
Epoch 54/200
 - 1s - loss: 0.0117 - val_loss: 0.0098
 - val_f1: 0.9921
Epoch 55/200
 - 1s - loss: 0.0115 - val_loss: 0.0108
 - val_f1: 0.9917
Epoch 56/200
 - 1s - loss: 0.0114 - val_loss: 0.0101
 - val_f1: 0.9918
Epoch 57/200
 - 1s - loss: 0.0115 - val_loss: 0.0096
 - val_f1: 0.9923
Epoch 58/200
 - 1s - loss: 0.0115 - val_loss: 0.0096
 - val_f1: 0.9929
Epoch 59/200
 - 1s - loss: 0.0116 - val_loss: 0.0098
 - val_f1: 0.9924
Epoch 60/200
 - 1s - loss: 0.0114 - val_loss: 0.0098
 - val_f1: 0.9923
Epoch 61/200
 - 1s - loss: 0.0112 - val_loss: 0.0094
2019-12-24 01:36:35,804 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep3/ann_model_epoch_60.pickle
 - val_f1: 0.9928
Epoch 62/200
 - 1s - loss: 0.0115 - val_loss: 0.0095
 - val_f1: 0.9928
Epoch 63/200
 - 1s - loss: 0.0111 - val_loss: 0.0094
 - val_f1: 0.9929
Epoch 64/200
 - 1s - loss: 0.0111 - val_loss: 0.0098
 - val_f1: 0.9928
Epoch 65/200
 - 1s - loss: 0.0107 - val_loss: 0.0101
 - val_f1: 0.9924
Epoch 66/200
 - 1s - loss: 0.0108 - val_loss: 0.0095
 - val_f1: 0.9922
Epoch 67/200
 - 1s - loss: 0.0110 - val_loss: 0.0099
 - val_f1: 0.9921
Epoch 68/200
 - 1s - loss: 0.0110 - val_loss: 0.0110
 - val_f1: 0.9906
Epoch 69/200
 - 1s - loss: 0.0110 - val_loss: 0.0097
 - val_f1: 0.9927
Epoch 70/200
 - 1s - loss: 0.0107 - val_loss: 0.0096
 - val_f1: 0.9920
Epoch 71/200
 - 1s - loss: 0.0107 - val_loss: 0.0092
 - val_f1: 0.9933
Epoch 72/200
 - 1s - loss: 0.0105 - val_loss: 0.0100
 - val_f1: 0.9919
Epoch 73/200
 - 1s - loss: 0.0109 - val_loss: 0.0092
 - val_f1: 0.9930
Epoch 74/200
 - 1s - loss: 0.0106 - val_loss: 0.0091
 - val_f1: 0.9928
Epoch 75/200
 - 1s - loss: 0.0103 - val_loss: 0.0098
 - val_f1: 0.9920
Epoch 76/200
 - 1s - loss: 0.0105 - val_loss: 0.0092
 - val_f1: 0.9929
Epoch 77/200
 - 1s - loss: 0.0105 - val_loss: 0.0098
 - val_f1: 0.9922
Epoch 78/200
 - 1s - loss: 0.0108 - val_loss: 0.0096
 - val_f1: 0.9924
Epoch 79/200
 - 1s - loss: 0.0104 - val_loss: 0.0096
 - val_f1: 0.9922
Epoch 80/200
 - 1s - loss: 0.0107 - val_loss: 0.0094
 - val_f1: 0.9927
Epoch 81/200
 - 1s - loss: 0.0107 - val_loss: 0.0095
2019-12-24 01:37:07,398 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep3/ann_model_epoch_80.pickle
 - val_f1: 0.9927
Epoch 82/200
 - 1s - loss: 0.0104 - val_loss: 0.0093
 - val_f1: 0.9930
Epoch 83/200
 - 1s - loss: 0.0103 - val_loss: 0.0096
 - val_f1: 0.9928
Epoch 84/200
 - 1s - loss: 0.0105 - val_loss: 0.0093
 - val_f1: 0.9920
Epoch 85/200
 - 1s - loss: 0.0101 - val_loss: 0.0092
 - val_f1: 0.9926
Epoch 86/200
 - 1s - loss: 0.0104 - val_loss: 0.0093
 - val_f1: 0.9927
Epoch 87/200
 - 1s - loss: 0.0103 - val_loss: 0.0091
 - val_f1: 0.9932
Epoch 88/200
 - 1s - loss: 0.0099 - val_loss: 0.0096
 - val_f1: 0.9921
Epoch 89/200
 - 1s - loss: 0.0101 - val_loss: 0.0090
 - val_f1: 0.9933
Epoch 90/200
 - 1s - loss: 0.0101 - val_loss: 0.0090
 - val_f1: 0.9931
Epoch 91/200
 - 1s - loss: 0.0100 - val_loss: 0.0091
 - val_f1: 0.9934
Epoch 92/200
 - 1s - loss: 0.0101 - val_loss: 0.0089
 - val_f1: 0.9936
Epoch 93/200
 - 1s - loss: 0.0097 - val_loss: 0.0092
 - val_f1: 0.9929
Epoch 94/200
 - 1s - loss: 0.0097 - val_loss: 0.0094
 - val_f1: 0.9929
Epoch 95/200
 - 1s - loss: 0.0101 - val_loss: 0.0093
 - val_f1: 0.9931
Epoch 96/200
 - 1s - loss: 0.0098 - val_loss: 0.0091
 - val_f1: 0.9929
Epoch 97/200
 - 1s - loss: 0.0097 - val_loss: 0.0091
 - val_f1: 0.9931
Epoch 98/200
 - 1s - loss: 0.0101 - val_loss: 0.0089
 - val_f1: 0.9928
Epoch 99/200
 - 1s - loss: 0.0099 - val_loss: 0.0097
 - val_f1: 0.9924
Epoch 100/200
 - 1s - loss: 0.0098 - val_loss: 0.0098
 - val_f1: 0.9927
Epoch 101/200
 - 1s - loss: 0.0100 - val_loss: 0.0090
2019-12-24 01:37:38,948 [INFO] epoch = 100. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep3/ann_model_epoch_100.pickle
 - val_f1: 0.9929
Epoch 102/200
 - 1s - loss: 0.0100 - val_loss: 0.0091
 - val_f1: 0.9923
Epoch 103/200
 - 1s - loss: 0.0097 - val_loss: 0.0094
 - val_f1: 0.9925
Epoch 104/200
 - 1s - loss: 0.0098 - val_loss: 0.0101
 - val_f1: 0.9923
Epoch 105/200
 - 1s - loss: 0.0096 - val_loss: 0.0088
 - val_f1: 0.9936
Epoch 106/200
 - 1s - loss: 0.0098 - val_loss: 0.0104
 - val_f1: 0.9919
Epoch 107/200
 - 1s - loss: 0.0096 - val_loss: 0.0090
 - val_f1: 0.9931
Epoch 108/200
 - 1s - loss: 0.0096 - val_loss: 0.0087
 - val_f1: 0.9936
Epoch 109/200
 - 1s - loss: 0.0096 - val_loss: 0.0098
 - val_f1: 0.9922
Epoch 110/200
 - 1s - loss: 0.0099 - val_loss: 0.0094
 - val_f1: 0.9929
Epoch 111/200
 - 1s - loss: 0.0098 - val_loss: 0.0097
 - val_f1: 0.9926
Epoch 112/200
 - 1s - loss: 0.0099 - val_loss: 0.0100
 - val_f1: 0.9922
Epoch 113/200
 - 1s - loss: 0.0094 - val_loss: 0.0100
 - val_f1: 0.9923
Epoch 114/200
 - 1s - loss: 0.0099 - val_loss: 0.0098
 - val_f1: 0.9923
Epoch 115/200
 - 1s - loss: 0.0093 - val_loss: 0.0090
 - val_f1: 0.9930
Epoch 116/200
 - 1s - loss: 0.0096 - val_loss: 0.0085
 - val_f1: 0.9931
Epoch 117/200
 - 1s - loss: 0.0097 - val_loss: 0.0094
 - val_f1: 0.9921
Epoch 118/200
 - 1s - loss: 0.0099 - val_loss: 0.0091
 - val_f1: 0.9933
Epoch 119/200
 - 1s - loss: 0.0098 - val_loss: 0.0100
 - val_f1: 0.9917
Epoch 120/200
 - 1s - loss: 0.0095 - val_loss: 0.0090
 - val_f1: 0.9931
Epoch 121/200
 - 1s - loss: 0.0096 - val_loss: 0.0089
2019-12-24 01:38:10,479 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep3/ann_model_epoch_120.pickle
 - val_f1: 0.9930
Epoch 122/200
 - 1s - loss: 0.0098 - val_loss: 0.0089
 - val_f1: 0.9933
Epoch 123/200
 - 1s - loss: 0.0095 - val_loss: 0.0090
 - val_f1: 0.9932
Epoch 124/200
 - 1s - loss: 0.0095 - val_loss: 0.0094
 - val_f1: 0.9932
Epoch 125/200
 - 1s - loss: 0.0090 - val_loss: 0.0092
 - val_f1: 0.9934
Epoch 126/200
 - 1s - loss: 0.0095 - val_loss: 0.0103
 - val_f1: 0.9923
Epoch 127/200
 - 1s - loss: 0.0096 - val_loss: 0.0096
 - val_f1: 0.9923
Epoch 128/200
 - 1s - loss: 0.0096 - val_loss: 0.0097
 - val_f1: 0.9915
Epoch 129/200
 - 1s - loss: 0.0092 - val_loss: 0.0094
 - val_f1: 0.9926
Epoch 130/200
 - 1s - loss: 0.0096 - val_loss: 0.0094
 - val_f1: 0.9932
Epoch 131/200
 - 1s - loss: 0.0095 - val_loss: 0.0092
 - val_f1: 0.9930
Epoch 132/200
 - 1s - loss: 0.0096 - val_loss: 0.0090
 - val_f1: 0.9934
Epoch 133/200
 - 1s - loss: 0.0091 - val_loss: 0.0095
 - val_f1: 0.9930
Epoch 134/200
 - 1s - loss: 0.0098 - val_loss: 0.0096
 - val_f1: 0.9929
Epoch 135/200
 - 1s - loss: 0.0096 - val_loss: 0.0095
 - val_f1: 0.9935
Epoch 136/200
 - 1s - loss: 0.0095 - val_loss: 0.0093
 - val_f1: 0.9931
Epoch 137/200
 - 1s - loss: 0.0092 - val_loss: 0.0092
 - val_f1: 0.9929
Epoch 138/200
 - 1s - loss: 0.0093 - val_loss: 0.0092
 - val_f1: 0.9930
Epoch 139/200
 - 1s - loss: 0.0092 - val_loss: 0.0094
 - val_f1: 0.9929
Epoch 140/200
 - 1s - loss: 0.0094 - val_loss: 0.0093
 - val_f1: 0.9931
Epoch 141/200
 - 1s - loss: 0.0089 - val_loss: 0.0092
2019-12-24 01:38:42,127 [INFO] epoch = 140. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep3/ann_model_epoch_140.pickle
 - val_f1: 0.9935
Epoch 142/200
 - 1s - loss: 0.0092 - val_loss: 0.0097
 - val_f1: 0.9930
Epoch 143/200
 - 1s - loss: 0.0092 - val_loss: 0.0096
 - val_f1: 0.9927
Epoch 144/200
 - 1s - loss: 0.0091 - val_loss: 0.0095
 - val_f1: 0.9932
Epoch 145/200
 - 1s - loss: 0.0097 - val_loss: 0.0095
 - val_f1: 0.9922
Epoch 146/200
 - 1s - loss: 0.0090 - val_loss: 0.0095
 - val_f1: 0.9928
Epoch 147/200
 - 1s - loss: 0.0092 - val_loss: 0.0091
 - val_f1: 0.9932
Epoch 148/200
 - 1s - loss: 0.0092 - val_loss: 0.0090
 - val_f1: 0.9934
Epoch 149/200
 - 1s - loss: 0.0091 - val_loss: 0.0096
 - val_f1: 0.9931
Epoch 150/200
 - 1s - loss: 0.0089 - val_loss: 0.0089
 - val_f1: 0.9937
Epoch 151/200
 - 1s - loss: 0.0091 - val_loss: 0.0092
 - val_f1: 0.9934
Epoch 152/200
 - 1s - loss: 0.0088 - val_loss: 0.0092
 - val_f1: 0.9931
Epoch 153/200
 - 1s - loss: 0.0090 - val_loss: 0.0092
 - val_f1: 0.9933
Epoch 154/200
 - 1s - loss: 0.0084 - val_loss: 0.0095
 - val_f1: 0.9937
Epoch 155/200
 - 1s - loss: 0.0091 - val_loss: 0.0090
 - val_f1: 0.9935
Epoch 156/200
 - 1s - loss: 0.0090 - val_loss: 0.0089
 - val_f1: 0.9934
Epoch 157/200
 - 1s - loss: 0.0087 - val_loss: 0.0093
 - val_f1: 0.9936
Epoch 158/200
 - 1s - loss: 0.0089 - val_loss: 0.0090
 - val_f1: 0.9934
Epoch 159/200
 - 1s - loss: 0.0091 - val_loss: 0.0094
 - val_f1: 0.9928
Epoch 160/200
 - 1s - loss: 0.0089 - val_loss: 0.0092
 - val_f1: 0.9933
Epoch 161/200
 - 1s - loss: 0.0091 - val_loss: 0.0092
2019-12-24 01:39:13,755 [INFO] epoch = 160. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep3/ann_model_epoch_160.pickle
 - val_f1: 0.9932
Epoch 162/200
 - 1s - loss: 0.0087 - val_loss: 0.0090
 - val_f1: 0.9933
Epoch 163/200
 - 1s - loss: 0.0089 - val_loss: 0.0087
 - val_f1: 0.9934
Epoch 164/200
 - 1s - loss: 0.0085 - val_loss: 0.0092
 - val_f1: 0.9929
Epoch 165/200
 - 1s - loss: 0.0086 - val_loss: 0.0092
 - val_f1: 0.9928
Epoch 166/200
 - 1s - loss: 0.0090 - val_loss: 0.0091
2019-12-24 01:39:22,303 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-24 01:39:25,893 [INFO] Last epoch loss evaluation: train_loss = 0.006552, val_loss = 0.008470
2019-12-24 01:39:25,894 [INFO] Training complete. time_to_train = 522.44 sec, 8.71 min
2019-12-24 01:39:25,916 [INFO] Model saved to results_selected_models/selected_nsl_ae_ann_deep_rep3/best_model.pickle
2019-12-24 01:39:26,098 [INFO] Plot saved to: results_selected_models/selected_nsl_ae_ann_deep_rep3/training_error_history.png
2019-12-24 01:39:26,263 [INFO] Plot saved to: results_selected_models/selected_nsl_ae_ann_deep_rep3/training_f1_history.png
2019-12-24 01:39:26,263 [INFO] Making predictions on training, validation, testing data
2019-12-24 01:39:35,399 [INFO] Evaluating predictions (results)
2019-12-24 01:39:35,954 [INFO] Dataset: Testing. Classification report below
2019-12-24 01:39:35,954 [INFO] 
              precision    recall  f1-score   support

         dos       0.94      0.83      0.88      7458
      normal       0.69      0.96      0.80      9711
       probe       0.82      0.67      0.74      2421
         r2l       0.95      0.16      0.27      2421
         u2r       0.62      0.02      0.05       533

    accuracy                           0.78     22544
   macro avg       0.80      0.53      0.55     22544
weighted avg       0.81      0.78      0.75     22544

2019-12-24 01:39:35,954 [INFO] Overall accuracy (micro avg): 0.778388928317956
2019-12-24 01:39:36,555 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.7784         0.7784                       0.7784                0.0554                   0.2216  0.7784
1     Macro avg        0.9114         0.8032                       0.5294                0.0748                   0.4706  0.5484
2  Weighted avg        0.8720         0.8127                       0.7784                0.1524                   0.2216  0.7470
2019-12-24 01:39:37,226 [INFO] Dataset: Validation. Classification report below
2019-12-24 01:39:37,226 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00      9186
      normal       0.99      0.99      0.99     13469
       probe       0.98      0.98      0.98      2331
         r2l       0.93      0.81      0.86       199
         u2r       0.75      0.30      0.43        10

    accuracy                           0.99     25195
   macro avg       0.93      0.82      0.85     25195
weighted avg       0.99      0.99      0.99     25195

2019-12-24 01:39:37,226 [INFO] Overall accuracy (micro avg): 0.9930938678309188
2019-12-24 01:39:37,897 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9931         0.9931                       0.9931                0.0017                   0.0069  0.9931
1     Macro avg        0.9972         0.9297                       0.8161                0.0024                   0.1839  0.8528
2  Weighted avg        0.9957         0.9930                       0.9931                0.0049                   0.0069  0.9930
2019-12-24 01:39:40,739 [INFO] Dataset: Training. Classification report below
2019-12-24 01:39:40,739 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00     36741
      normal       0.99      1.00      0.99     53874
       probe       0.99      0.98      0.98      9325
         r2l       0.90      0.79      0.84       796
         u2r       0.77      0.48      0.59        42

    accuracy                           0.99    100778
   macro avg       0.93      0.85      0.88    100778
weighted avg       0.99      0.99      0.99    100778

2019-12-24 01:39:40,742 [INFO] Overall accuracy (micro avg): 0.9938974776240846
2019-12-24 01:39:43,791 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9939         0.9939                       0.9939                0.0015                   0.0061  0.9939
1     Macro avg        0.9976         0.9303                       0.8489                0.0021                   0.1511  0.8820
2  Weighted avg        0.9963         0.9938                       0.9939                0.0046                   0.0061  0.9938
2019-12-24 01:39:43,838 [INFO] Results saved to: results_selected_models/selected_nsl_ae_ann_deep_rep3/selected_nsl_ae_ann_deep_rep3_results.xlsx
2019-12-24 01:39:43,839 [INFO] ================= Finished running experiment no. 3 ================= 

2019-12-24 01:39:43,840 [INFO] Created directory: results_selected_models/selected_nsl_ae_ann_deep_rep4
2019-12-24 01:39:43,840 [INFO] Initialized logging. log_filename = results_selected_models/selected_nsl_ae_ann_deep_rep4/run_log.log
2019-12-24 01:39:43,840 [INFO] ================= Running experiment no. 4  ================= 

2019-12-24 01:39:43,840 [INFO] Experiment parameters given below
2019-12-24 01:39:43,840 [INFO] 
{'experiment_num': 4, 'results_dir': 'results_selected_models/selected_nsl_ae_ann_deep_rep4', 'model_type': 'classifier', 'model': 'ae_ann', 'normal_label': 'normal', 'scaling_type': 'NA', 'unsupervised_ratio': 0.25, 'ae_encoder_units': [128, 64, 32], 'ae_encoder_activations': ['relu', 'relu', 'relu'], 'ae_encoder_dropout_rates': [0.2, 0.2, 0.2], 'ae_encoder_l1_param': -1, 'ae_decoder_units': [64, 128], 'ae_decoder_activations': ['relu', 'relu'], 'ae_decoder_dropout_rates': [0.2, 0.2], 'output_activation': 'sigmoid', 'loss_function': 'binary_crossentropy', 'ae_epochs': 200, 'ann_layer_units': [64], 'ann_layer_activations': ['relu'], 'ann_layer_dropout_rates': [0.2], 'ann_epochs': 200, 'early_stop_patience': 50, 'batch_size': 256, 'dataset_dir': '../Datasets/small_datasets/nsl_kdd_five_classes', 'description': 'selected_nsl_ae_ann_deep_rep4'}
2019-12-24 01:39:43,840 [INFO] Created tensorboard log directory: results_selected_models/selected_nsl_ae_ann_deep_rep4/tf_logs_run_2019_12_24-01_39_43
2019-12-24 01:39:43,840 [INFO] Loading datsets from: ../Datasets/small_datasets/nsl_kdd_five_classes
2019-12-24 01:39:43,840 [INFO] Reading X, y files
2019-12-24 01:39:43,840 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_train.h5
2019-12-24 01:39:44,075 [INFO] Reading complete. time_to_read=0.23 seconds
2019-12-24 01:39:44,075 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_val.h5
2019-12-24 01:39:44,139 [INFO] Reading complete. time_to_read=0.06 seconds
2019-12-24 01:39:44,139 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_test.h5
2019-12-24 01:39:44,196 [INFO] Reading complete. time_to_read=0.06 seconds
2019-12-24 01:39:44,197 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_train.h5
2019-12-24 01:39:44,205 [INFO] Reading complete. time_to_read=0.01 seconds
2019-12-24 01:39:44,206 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_val.h5
2019-12-24 01:39:44,210 [INFO] Reading complete. time_to_read=0.00 seconds
2019-12-24 01:39:44,210 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_test.h5
2019-12-24 01:39:44,214 [INFO] Reading complete. time_to_read=0.00 seconds
2019-12-24 01:39:44,324 [INFO] Initializing model
2019-12-24 01:39:44,837 [INFO] _________________________________________________________________
2019-12-24 01:39:44,837 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-24 01:39:44,838 [INFO] =================================================================
2019-12-24 01:39:44,838 [INFO] dense_45 (Dense)             (None, 128)               15744     
2019-12-24 01:39:44,838 [INFO] _________________________________________________________________
2019-12-24 01:39:44,838 [INFO] batch_normalization_29 (Batc (None, 128)               512       
2019-12-24 01:39:44,838 [INFO] _________________________________________________________________
2019-12-24 01:39:44,838 [INFO] dropout_29 (Dropout)         (None, 128)               0         
2019-12-24 01:39:44,838 [INFO] _________________________________________________________________
2019-12-24 01:39:44,838 [INFO] dense_46 (Dense)             (None, 64)                8256      
2019-12-24 01:39:44,838 [INFO] _________________________________________________________________
2019-12-24 01:39:44,838 [INFO] batch_normalization_30 (Batc (None, 64)                256       
2019-12-24 01:39:44,838 [INFO] _________________________________________________________________
2019-12-24 01:39:44,838 [INFO] dropout_30 (Dropout)         (None, 64)                0         
2019-12-24 01:39:44,839 [INFO] _________________________________________________________________
2019-12-24 01:39:44,839 [INFO] dense_47 (Dense)             (None, 32)                2080      
2019-12-24 01:39:44,839 [INFO] _________________________________________________________________
2019-12-24 01:39:44,839 [INFO] batch_normalization_31 (Batc (None, 32)                128       
2019-12-24 01:39:44,839 [INFO] _________________________________________________________________
2019-12-24 01:39:44,839 [INFO] dropout_31 (Dropout)         (None, 32)                0         
2019-12-24 01:39:44,839 [INFO] _________________________________________________________________
2019-12-24 01:39:44,839 [INFO] dense_48 (Dense)             (None, 64)                2112      
2019-12-24 01:39:44,839 [INFO] _________________________________________________________________
2019-12-24 01:39:44,839 [INFO] batch_normalization_32 (Batc (None, 64)                256       
2019-12-24 01:39:44,839 [INFO] _________________________________________________________________
2019-12-24 01:39:44,839 [INFO] dropout_32 (Dropout)         (None, 64)                0         
2019-12-24 01:39:44,840 [INFO] _________________________________________________________________
2019-12-24 01:39:44,840 [INFO] dense_49 (Dense)             (None, 128)               8320      
2019-12-24 01:39:44,840 [INFO] _________________________________________________________________
2019-12-24 01:39:44,840 [INFO] batch_normalization_33 (Batc (None, 128)               512       
2019-12-24 01:39:44,840 [INFO] _________________________________________________________________
2019-12-24 01:39:44,840 [INFO] dropout_33 (Dropout)         (None, 128)               0         
2019-12-24 01:39:44,840 [INFO] _________________________________________________________________
2019-12-24 01:39:44,840 [INFO] dense_50 (Dense)             (None, 122)               15738     
2019-12-24 01:39:44,840 [INFO] =================================================================
2019-12-24 01:39:44,841 [INFO] Total params: 53,914
2019-12-24 01:39:44,841 [INFO] Trainable params: 53,082
2019-12-24 01:39:44,841 [INFO] Non-trainable params: 832
2019-12-24 01:39:44,841 [INFO] _________________________________________________________________
2019-12-24 01:39:44,972 [INFO] _________________________________________________________________
2019-12-24 01:39:44,972 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-24 01:39:44,972 [INFO] =================================================================
2019-12-24 01:39:44,972 [INFO] dense_51 (Dense)             (None, 64)                2112      
2019-12-24 01:39:44,972 [INFO] _________________________________________________________________
2019-12-24 01:39:44,972 [INFO] batch_normalization_34 (Batc (None, 64)                256       
2019-12-24 01:39:44,972 [INFO] _________________________________________________________________
2019-12-24 01:39:44,972 [INFO] dropout_34 (Dropout)         (None, 64)                0         
2019-12-24 01:39:44,972 [INFO] _________________________________________________________________
2019-12-24 01:39:44,972 [INFO] dense_52 (Dense)             (None, 5)                 325       
2019-12-24 01:39:44,973 [INFO] =================================================================
2019-12-24 01:39:44,973 [INFO] Total params: 2,693
2019-12-24 01:39:44,973 [INFO] Trainable params: 2,565
2019-12-24 01:39:44,973 [INFO] Non-trainable params: 128
2019-12-24 01:39:44,973 [INFO] _________________________________________________________________
2019-12-24 01:39:44,973 [INFO] Training model
2019-12-24 01:39:44,973 [INFO] Splitting train set into 2 sets (unsupervised, supervised), random_seed = None
2019-12-24 01:39:46,137 [INFO] Split sizes (instances). total = 100778, unsupervised = 25194, supervised = 75584, unsupervised dataset hash = d65a87e7b6c11d8ceda0220f3a9bba5aeaa7c8fd
2019-12-24 01:39:46,141 [INFO] Training autoencoder
 - val_f1: 0.9928
Epoch 00166: early stopping
Train on 25194 samples, validate on 25195 samples
Epoch 1/200
 - 4s - loss: 0.4429 - val_loss: -3.1716e-01
Epoch 2/200
 - 1s - loss: -5.3829e-01 - val_loss: -1.2478e+00
Epoch 3/200
 - 1s - loss: -1.3165e+00 - val_loss: -1.7821e+00
Epoch 4/200
 - 1s - loss: -1.8145e+00 - val_loss: -2.1978e+00
Epoch 5/200
 - 1s - loss: -2.1428e+00 - val_loss: -2.4915e+00
Epoch 6/200
 - 1s - loss: -2.3721e+00 - val_loss: -2.6774e+00
Epoch 7/200
 - 1s - loss: -2.5266e+00 - val_loss: -2.7977e+00
Epoch 8/200
 - 1s - loss: -2.6328e+00 - val_loss: -2.8738e+00
Epoch 9/200
 - 1s - loss: -2.7063e+00 - val_loss: -2.9261e+00
Epoch 10/200
 - 1s - loss: -2.7560e+00 - val_loss: -2.9639e+00
Epoch 11/200
 - 1s - loss: -2.7980e+00 - val_loss: -2.9868e+00
Epoch 12/200
 - 1s - loss: -2.8278e+00 - val_loss: -3.0051e+00
Epoch 13/200
 - 1s - loss: -2.8535e+00 - val_loss: -3.0234e+00
Epoch 14/200
 - 1s - loss: -2.8809e+00 - val_loss: -3.0346e+00
Epoch 15/200
 - 1s - loss: -2.8940e+00 - val_loss: -3.0450e+00
Epoch 16/200
 - 1s - loss: -2.9113e+00 - val_loss: -3.0573e+00
Epoch 17/200
 - 1s - loss: -2.9231e+00 - val_loss: -3.0635e+00
Epoch 18/200
 - 1s - loss: -2.9365e+00 - val_loss: -3.0684e+00
Epoch 19/200
 - 1s - loss: -2.9469e+00 - val_loss: -3.0760e+00
Epoch 20/200
 - 1s - loss: -2.9547e+00 - val_loss: -3.0833e+00
Epoch 21/200
 - 1s - loss: -2.9664e+00 - val_loss: -3.0843e+00
2019-12-24 01:40:22,400 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep4/ae_model_epoch_20.pickle
Epoch 22/200
 - 1s - loss: -2.9731e+00 - val_loss: -3.0907e+00
Epoch 23/200
 - 1s - loss: -2.9814e+00 - val_loss: -3.0939e+00
Epoch 24/200
 - 1s - loss: -2.9906e+00 - val_loss: -3.0998e+00
Epoch 25/200
 - 1s - loss: -2.9928e+00 - val_loss: -3.1008e+00
Epoch 26/200
 - 1s - loss: -3.0028e+00 - val_loss: -3.1058e+00
Epoch 27/200
 - 1s - loss: -3.0046e+00 - val_loss: -3.1111e+00
Epoch 28/200
 - 1s - loss: -3.0082e+00 - val_loss: -3.1138e+00
Epoch 29/200
 - 1s - loss: -3.0141e+00 - val_loss: -3.1189e+00
Epoch 30/200
 - 1s - loss: -3.0192e+00 - val_loss: -3.1203e+00
Epoch 31/200
 - 1s - loss: -3.0231e+00 - val_loss: -3.1256e+00
Epoch 32/200
 - 1s - loss: -3.0286e+00 - val_loss: -3.1289e+00
Epoch 33/200
 - 1s - loss: -3.0313e+00 - val_loss: -3.1315e+00
Epoch 34/200
 - 1s - loss: -3.0355e+00 - val_loss: -3.1340e+00
Epoch 35/200
 - 1s - loss: -3.0419e+00 - val_loss: -3.1361e+00
Epoch 36/200
 - 1s - loss: -3.0471e+00 - val_loss: -3.1378e+00
Epoch 37/200
 - 1s - loss: -3.0460e+00 - val_loss: -3.1400e+00
Epoch 38/200
 - 1s - loss: -3.0544e+00 - val_loss: -3.1428e+00
Epoch 39/200
 - 1s - loss: -3.0517e+00 - val_loss: -3.1449e+00
Epoch 40/200
 - 1s - loss: -3.0539e+00 - val_loss: -3.1449e+00
Epoch 41/200
 - 1s - loss: -3.0595e+00 - val_loss: -3.1471e+00
2019-12-24 01:40:46,428 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep4/ae_model_epoch_40.pickle
Epoch 42/200
 - 1s - loss: -3.0598e+00 - val_loss: -3.1465e+00
Epoch 43/200
 - 1s - loss: -3.0646e+00 - val_loss: -3.1493e+00
Epoch 44/200
 - 1s - loss: -3.0651e+00 - val_loss: -3.1518e+00
Epoch 45/200
 - 1s - loss: -3.0670e+00 - val_loss: -3.1528e+00
Epoch 46/200
 - 1s - loss: -3.0687e+00 - val_loss: -3.1531e+00
Epoch 47/200
 - 1s - loss: -3.0720e+00 - val_loss: -3.1545e+00
Epoch 48/200
 - 1s - loss: -3.0753e+00 - val_loss: -3.1558e+00
Epoch 49/200
 - 1s - loss: -3.0775e+00 - val_loss: -3.1584e+00
Epoch 50/200
 - 1s - loss: -3.0775e+00 - val_loss: -3.1582e+00
Epoch 51/200
 - 1s - loss: -3.0822e+00 - val_loss: -3.1594e+00
Epoch 52/200
 - 1s - loss: -3.0805e+00 - val_loss: -3.1597e+00
Epoch 53/200
 - 1s - loss: -3.0828e+00 - val_loss: -3.1613e+00
Epoch 54/200
 - 1s - loss: -3.0858e+00 - val_loss: -3.1626e+00
Epoch 55/200
 - 1s - loss: -3.0864e+00 - val_loss: -3.1643e+00
Epoch 56/200
 - 1s - loss: -3.0874e+00 - val_loss: -3.1640e+00
Epoch 57/200
 - 1s - loss: -3.0903e+00 - val_loss: -3.1656e+00
Epoch 58/200
 - 1s - loss: -3.0913e+00 - val_loss: -3.1655e+00
Epoch 59/200
 - 1s - loss: -3.0927e+00 - val_loss: -3.1658e+00
Epoch 60/200
 - 1s - loss: -3.0934e+00 - val_loss: -3.1669e+00
Epoch 61/200
 - 1s - loss: -3.0936e+00 - val_loss: -3.1679e+00
2019-12-24 01:41:10,449 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep4/ae_model_epoch_60.pickle
Epoch 62/200
 - 1s - loss: -3.0952e+00 - val_loss: -3.1670e+00
Epoch 63/200
 - 1s - loss: -3.0976e+00 - val_loss: -3.1693e+00
Epoch 64/200
 - 1s - loss: -3.1008e+00 - val_loss: -3.1707e+00
Epoch 65/200
 - 1s - loss: -3.1015e+00 - val_loss: -3.1696e+00
Epoch 66/200
 - 1s - loss: -3.1037e+00 - val_loss: -3.1711e+00
Epoch 67/200
 - 1s - loss: -3.1026e+00 - val_loss: -3.1708e+00
Epoch 68/200
 - 1s - loss: -3.1060e+00 - val_loss: -3.1725e+00
Epoch 69/200
 - 1s - loss: -3.1057e+00 - val_loss: -3.1731e+00
Epoch 70/200
 - 1s - loss: -3.1095e+00 - val_loss: -3.1734e+00
Epoch 71/200
 - 1s - loss: -3.1110e+00 - val_loss: -3.1726e+00
Epoch 72/200
 - 1s - loss: -3.1109e+00 - val_loss: -3.1755e+00
Epoch 73/200
 - 1s - loss: -3.1047e+00 - val_loss: -3.1741e+00
Epoch 74/200
 - 1s - loss: -3.1110e+00 - val_loss: -3.1759e+00
Epoch 75/200
 - 1s - loss: -3.1086e+00 - val_loss: -3.1753e+00
Epoch 76/200
 - 1s - loss: -3.1117e+00 - val_loss: -3.1772e+00
Epoch 77/200
 - 1s - loss: -3.1124e+00 - val_loss: -3.1777e+00
Epoch 78/200
 - 1s - loss: -3.1140e+00 - val_loss: -3.1759e+00
Epoch 79/200
 - 1s - loss: -3.1144e+00 - val_loss: -3.1778e+00
Epoch 80/200
 - 1s - loss: -3.1159e+00 - val_loss: -3.1776e+00
Epoch 81/200
 - 1s - loss: -3.1151e+00 - val_loss: -3.1779e+00
2019-12-24 01:41:34,460 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep4/ae_model_epoch_80.pickle
Epoch 82/200
 - 1s - loss: -3.1159e+00 - val_loss: -3.1783e+00
Epoch 83/200
 - 1s - loss: -3.1194e+00 - val_loss: -3.1806e+00
Epoch 84/200
 - 1s - loss: -3.1159e+00 - val_loss: -3.1810e+00
Epoch 85/200
 - 1s - loss: -3.1191e+00 - val_loss: -3.1808e+00
Epoch 86/200
 - 1s - loss: -3.1213e+00 - val_loss: -3.1811e+00
Epoch 87/200
 - 1s - loss: -3.1228e+00 - val_loss: -3.1812e+00
Epoch 88/200
 - 1s - loss: -3.1200e+00 - val_loss: -3.1807e+00
Epoch 89/200
 - 1s - loss: -3.1262e+00 - val_loss: -3.1836e+00
Epoch 90/200
 - 1s - loss: -3.1248e+00 - val_loss: -3.1835e+00
Epoch 91/200
 - 1s - loss: -3.1300e+00 - val_loss: -3.1852e+00
Epoch 92/200
 - 1s - loss: -3.1265e+00 - val_loss: -3.1841e+00
Epoch 93/200
 - 1s - loss: -3.1290e+00 - val_loss: -3.1847e+00
Epoch 94/200
 - 1s - loss: -3.1261e+00 - val_loss: -3.1849e+00
Epoch 95/200
 - 1s - loss: -3.1254e+00 - val_loss: -3.1853e+00
Epoch 96/200
 - 1s - loss: -3.1266e+00 - val_loss: -3.1852e+00
Epoch 97/200
 - 1s - loss: -3.1281e+00 - val_loss: -3.1862e+00
Epoch 98/200
 - 1s - loss: -3.1291e+00 - val_loss: -3.1861e+00
Epoch 99/200
 - 1s - loss: -3.1321e+00 - val_loss: -3.1861e+00
Epoch 100/200
 - 1s - loss: -3.1319e+00 - val_loss: -3.1876e+00
Epoch 101/200
 - 1s - loss: -3.1271e+00 - val_loss: -3.1873e+00
2019-12-24 01:41:58,444 [INFO] epoch = 100. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep4/ae_model_epoch_100.pickle
Epoch 102/200
 - 1s - loss: -3.1311e+00 - val_loss: -3.1887e+00
Epoch 103/200
 - 1s - loss: -3.1303e+00 - val_loss: -3.1882e+00
Epoch 104/200
 - 1s - loss: -3.1322e+00 - val_loss: -3.1885e+00
Epoch 105/200
 - 1s - loss: -3.1324e+00 - val_loss: -3.1890e+00
Epoch 106/200
 - 1s - loss: -3.1328e+00 - val_loss: -3.1903e+00
Epoch 107/200
 - 1s - loss: -3.1343e+00 - val_loss: -3.1889e+00
Epoch 108/200
 - 1s - loss: -3.1340e+00 - val_loss: -3.1884e+00
Epoch 109/200
 - 1s - loss: -3.1375e+00 - val_loss: -3.1911e+00
Epoch 110/200
 - 1s - loss: -3.1364e+00 - val_loss: -3.1902e+00
Epoch 111/200
 - 1s - loss: -3.1360e+00 - val_loss: -3.1900e+00
Epoch 112/200
 - 1s - loss: -3.1381e+00 - val_loss: -3.1910e+00
Epoch 113/200
 - 1s - loss: -3.1382e+00 - val_loss: -3.1905e+00
Epoch 114/200
 - 1s - loss: -3.1384e+00 - val_loss: -3.1912e+00
Epoch 115/200
 - 1s - loss: -3.1395e+00 - val_loss: -3.1917e+00
Epoch 116/200
 - 1s - loss: -3.1389e+00 - val_loss: -3.1909e+00
Epoch 117/200
 - 1s - loss: -3.1434e+00 - val_loss: -3.1923e+00
Epoch 118/200
 - 1s - loss: -3.1387e+00 - val_loss: -3.1914e+00
Epoch 119/200
 - 1s - loss: -3.1391e+00 - val_loss: -3.1915e+00
Epoch 120/200
 - 1s - loss: -3.1424e+00 - val_loss: -3.1916e+00
Epoch 121/200
 - 1s - loss: -3.1424e+00 - val_loss: -3.1919e+00
2019-12-24 01:42:22,497 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep4/ae_model_epoch_120.pickle
Epoch 122/200
 - 1s - loss: -3.1420e+00 - val_loss: -3.1927e+00
Epoch 123/200
 - 1s - loss: -3.1439e+00 - val_loss: -3.1935e+00
Epoch 124/200
 - 1s - loss: -3.1415e+00 - val_loss: -3.1933e+00
Epoch 125/200
 - 1s - loss: -3.1443e+00 - val_loss: -3.1938e+00
Epoch 126/200
 - 1s - loss: -3.1423e+00 - val_loss: -3.1934e+00
Epoch 127/200
 - 1s - loss: -3.1427e+00 - val_loss: -3.1936e+00
Epoch 128/200
 - 1s - loss: -3.1439e+00 - val_loss: -3.1932e+00
Epoch 129/200
 - 1s - loss: -3.1473e+00 - val_loss: -3.1944e+00
Epoch 130/200
 - 1s - loss: -3.1443e+00 - val_loss: -3.1931e+00
Epoch 131/200
 - 1s - loss: -3.1438e+00 - val_loss: -3.1942e+00
Epoch 132/200
 - 1s - loss: -3.1451e+00 - val_loss: -3.1951e+00
Epoch 133/200
 - 1s - loss: -3.1462e+00 - val_loss: -3.1949e+00
Epoch 134/200
 - 1s - loss: -3.1447e+00 - val_loss: -3.1957e+00
Epoch 135/200
 - 1s - loss: -3.1454e+00 - val_loss: -3.1958e+00
Epoch 136/200
 - 1s - loss: -3.1462e+00 - val_loss: -3.1964e+00
Epoch 137/200
 - 1s - loss: -3.1476e+00 - val_loss: -3.1968e+00
Epoch 138/200
 - 1s - loss: -3.1463e+00 - val_loss: -3.1960e+00
Epoch 139/200
 - 1s - loss: -3.1473e+00 - val_loss: -3.1963e+00
Epoch 140/200
 - 1s - loss: -3.1479e+00 - val_loss: -3.1973e+00
Epoch 141/200
 - 1s - loss: -3.1493e+00 - val_loss: -3.1972e+00
2019-12-24 01:42:46,508 [INFO] epoch = 140. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep4/ae_model_epoch_140.pickle
Epoch 142/200
 - 1s - loss: -3.1477e+00 - val_loss: -3.1969e+00
Epoch 143/200
 - 1s - loss: -3.1473e+00 - val_loss: -3.1973e+00
Epoch 144/200
 - 1s - loss: -3.1502e+00 - val_loss: -3.1973e+00
Epoch 145/200
 - 1s - loss: -3.1491e+00 - val_loss: -3.1976e+00
Epoch 146/200
 - 1s - loss: -3.1482e+00 - val_loss: -3.1973e+00
Epoch 147/200
 - 1s - loss: -3.1510e+00 - val_loss: -3.1976e+00
Epoch 148/200
 - 1s - loss: -3.1486e+00 - val_loss: -3.1970e+00
Epoch 149/200
 - 1s - loss: -3.1469e+00 - val_loss: -3.1978e+00
Epoch 150/200
 - 1s - loss: -3.1506e+00 - val_loss: -3.1982e+00
Epoch 151/200
 - 1s - loss: -3.1520e+00 - val_loss: -3.1984e+00
Epoch 152/200
 - 1s - loss: -3.1532e+00 - val_loss: -3.1990e+00
Epoch 153/200
 - 1s - loss: -3.1505e+00 - val_loss: -3.1995e+00
Epoch 154/200
 - 1s - loss: -3.1506e+00 - val_loss: -3.1992e+00
Epoch 155/200
 - 1s - loss: -3.1546e+00 - val_loss: -3.1995e+00
Epoch 156/200
 - 1s - loss: -3.1532e+00 - val_loss: -3.1990e+00
Epoch 157/200
 - 1s - loss: -3.1525e+00 - val_loss: -3.1998e+00
Epoch 158/200
 - 1s - loss: -3.1513e+00 - val_loss: -3.1987e+00
Epoch 159/200
 - 1s - loss: -3.1514e+00 - val_loss: -3.1991e+00
Epoch 160/200
 - 1s - loss: -3.1519e+00 - val_loss: -3.1996e+00
Epoch 161/200
 - 1s - loss: -3.1527e+00 - val_loss: -3.1999e+00
2019-12-24 01:43:10,549 [INFO] epoch = 160. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep4/ae_model_epoch_160.pickle
Epoch 162/200
 - 1s - loss: -3.1571e+00 - val_loss: -3.2006e+00
Epoch 163/200
 - 1s - loss: -3.1537e+00 - val_loss: -3.2002e+00
Epoch 164/200
 - 1s - loss: -3.1526e+00 - val_loss: -3.2001e+00
Epoch 165/200
 - 1s - loss: -3.1575e+00 - val_loss: -3.1999e+00
Epoch 166/200
 - 1s - loss: -3.1540e+00 - val_loss: -3.1998e+00
Epoch 167/200
 - 1s - loss: -3.1542e+00 - val_loss: -3.2008e+00
Epoch 168/200
 - 1s - loss: -3.1560e+00 - val_loss: -3.2011e+00
Epoch 169/200
 - 1s - loss: -3.1546e+00 - val_loss: -3.2014e+00
Epoch 170/200
 - 1s - loss: -3.1572e+00 - val_loss: -3.2016e+00
Epoch 171/200
 - 1s - loss: -3.1547e+00 - val_loss: -3.2013e+00
Epoch 172/200
 - 1s - loss: -3.1590e+00 - val_loss: -3.2013e+00
Epoch 173/200
 - 1s - loss: -3.1570e+00 - val_loss: -3.2019e+00
Epoch 174/200
 - 1s - loss: -3.1575e+00 - val_loss: -3.2016e+00
Epoch 175/200
 - 1s - loss: -3.1565e+00 - val_loss: -3.2012e+00
Epoch 176/200
 - 1s - loss: -3.1566e+00 - val_loss: -3.2017e+00
Epoch 177/200
 - 1s - loss: -3.1570e+00 - val_loss: -3.2011e+00
Epoch 178/200
 - 1s - loss: -3.1564e+00 - val_loss: -3.2016e+00
Epoch 179/200
 - 1s - loss: -3.1541e+00 - val_loss: -3.2018e+00
Epoch 180/200
 - 1s - loss: -3.1557e+00 - val_loss: -3.2014e+00
Epoch 181/200
 - 1s - loss: -3.1572e+00 - val_loss: -3.2010e+00
2019-12-24 01:43:34,550 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep4/ae_model_epoch_180.pickle
Epoch 182/200
 - 1s - loss: -3.1568e+00 - val_loss: -3.2026e+00
Epoch 183/200
 - 1s - loss: -3.1569e+00 - val_loss: -3.2027e+00
Epoch 184/200
 - 1s - loss: -3.1579e+00 - val_loss: -3.2016e+00
Epoch 185/200
 - 1s - loss: -3.1575e+00 - val_loss: -3.2032e+00
Epoch 186/200
 - 1s - loss: -3.1611e+00 - val_loss: -3.2023e+00
Epoch 187/200
 - 1s - loss: -3.1595e+00 - val_loss: -3.2018e+00
Epoch 188/200
 - 1s - loss: -3.1609e+00 - val_loss: -3.2024e+00
Epoch 189/200
 - 1s - loss: -3.1591e+00 - val_loss: -3.2020e+00
Epoch 190/200
 - 1s - loss: -3.1621e+00 - val_loss: -3.2026e+00
Epoch 191/200
 - 1s - loss: -3.1574e+00 - val_loss: -3.2029e+00
Epoch 192/200
 - 1s - loss: -3.1590e+00 - val_loss: -3.2025e+00
Epoch 193/200
 - 1s - loss: -3.1597e+00 - val_loss: -3.2029e+00
Epoch 194/200
 - 1s - loss: -3.1597e+00 - val_loss: -3.2028e+00
Epoch 195/200
 - 1s - loss: -3.1605e+00 - val_loss: -3.2021e+00
Epoch 196/200
 - 1s - loss: -3.1594e+00 - val_loss: -3.2033e+00
Epoch 197/200
 - 1s - loss: -3.1607e+00 - val_loss: -3.2029e+00
Epoch 198/200
 - 1s - loss: -3.1605e+00 - val_loss: -3.2031e+00
Epoch 199/200
 - 1s - loss: -3.1645e+00 - val_loss: -3.2037e+00
Epoch 200/200
 - 1s - loss: -3.1631e+00 - val_loss: -3.2040e+00
2019-12-24 01:43:57,384 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-24 01:44:00,977 [INFO] Last epoch loss evaluation: train_loss = -3.232089, val_loss = -3.204011
2019-12-24 01:44:00,978 [INFO] Training autoencoder complete
2019-12-24 01:44:00,978 [INFO] Encoding data for supervised training
2019-12-24 01:44:05,594 [INFO] Encoding complete
2019-12-24 01:44:05,594 [INFO] Training neural network layers (after autoencoder)
Train on 75584 samples, validate on 25195 samples
Epoch 1/200
 - 2s - loss: 0.1447 - val_loss: 0.0459
 - val_f1: 0.9714
Epoch 2/200
 - 1s - loss: 0.0469 - val_loss: 0.0328
 - val_f1: 0.9769
Epoch 3/200
 - 1s - loss: 0.0382 - val_loss: 0.0276
 - val_f1: 0.9783
Epoch 4/200
 - 1s - loss: 0.0328 - val_loss: 0.0240
 - val_f1: 0.9793
Epoch 5/200
 - 1s - loss: 0.0298 - val_loss: 0.0222
 - val_f1: 0.9800
Epoch 6/200
 - 1s - loss: 0.0273 - val_loss: 0.0203
 - val_f1: 0.9813
Epoch 7/200
 - 1s - loss: 0.0258 - val_loss: 0.0194
 - val_f1: 0.9816
Epoch 8/200
 - 1s - loss: 0.0240 - val_loss: 0.0183
 - val_f1: 0.9842
Epoch 9/200
 - 1s - loss: 0.0231 - val_loss: 0.0176
 - val_f1: 0.9849
Epoch 10/200
 - 1s - loss: 0.0226 - val_loss: 0.0174
 - val_f1: 0.9834
Epoch 11/200
 - 1s - loss: 0.0221 - val_loss: 0.0173
 - val_f1: 0.9831
Epoch 12/200
 - 1s - loss: 0.0214 - val_loss: 0.0161
 - val_f1: 0.9840
Epoch 13/200
 - 1s - loss: 0.0205 - val_loss: 0.0181
 - val_f1: 0.9824
Epoch 14/200
 - 1s - loss: 0.0199 - val_loss: 0.0153
 - val_f1: 0.9850
Epoch 15/200
 - 1s - loss: 0.0190 - val_loss: 0.0161
 - val_f1: 0.9857
Epoch 16/200
 - 1s - loss: 0.0184 - val_loss: 0.0145
 - val_f1: 0.9881
Epoch 17/200
 - 1s - loss: 0.0185 - val_loss: 0.0145
 - val_f1: 0.9862
Epoch 18/200
 - 1s - loss: 0.0181 - val_loss: 0.0145
 - val_f1: 0.9880
Epoch 19/200
 - 1s - loss: 0.0175 - val_loss: 0.0130
 - val_f1: 0.9904
Epoch 20/200
 - 1s - loss: 0.0174 - val_loss: 0.0161
 - val_f1: 0.9838
Epoch 21/200
 - 1s - loss: 0.0174 - val_loss: 0.0130
2019-12-24 01:44:47,476 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep4/ann_model_epoch_20.pickle
 - val_f1: 0.9903
Epoch 22/200
 - 1s - loss: 0.0171 - val_loss: 0.0131
 - val_f1: 0.9891
Epoch 23/200
 - 1s - loss: 0.0163 - val_loss: 0.0132
 - val_f1: 0.9882
Epoch 24/200
 - 1s - loss: 0.0152 - val_loss: 0.0116
 - val_f1: 0.9919
Epoch 25/200
 - 1s - loss: 0.0145 - val_loss: 0.0115
 - val_f1: 0.9909
Epoch 26/200
 - 1s - loss: 0.0150 - val_loss: 0.0130
 - val_f1: 0.9872
Epoch 27/200
 - 1s - loss: 0.0153 - val_loss: 0.0119
 - val_f1: 0.9903
Epoch 28/200
 - 1s - loss: 0.0156 - val_loss: 0.0122
 - val_f1: 0.9900
Epoch 29/200
 - 1s - loss: 0.0148 - val_loss: 0.0112
 - val_f1: 0.9914
Epoch 30/200
 - 1s - loss: 0.0146 - val_loss: 0.0114
 - val_f1: 0.9919
Epoch 31/200
 - 1s - loss: 0.0148 - val_loss: 0.0117
 - val_f1: 0.9913
Epoch 32/200
 - 1s - loss: 0.0147 - val_loss: 0.0114
 - val_f1: 0.9906
Epoch 33/200
 - 1s - loss: 0.0146 - val_loss: 0.0110
 - val_f1: 0.9920
Epoch 34/200
 - 1s - loss: 0.0140 - val_loss: 0.0117
 - val_f1: 0.9897
Epoch 35/200
 - 1s - loss: 0.0142 - val_loss: 0.0108
 - val_f1: 0.9916
Epoch 36/200
 - 1s - loss: 0.0137 - val_loss: 0.0122
 - val_f1: 0.9882
Epoch 37/200
 - 1s - loss: 0.0136 - val_loss: 0.0105
 - val_f1: 0.9910
Epoch 38/200
 - 1s - loss: 0.0130 - val_loss: 0.0102
 - val_f1: 0.9909
Epoch 39/200
 - 1s - loss: 0.0136 - val_loss: 0.0100
 - val_f1: 0.9929
Epoch 40/200
 - 1s - loss: 0.0134 - val_loss: 0.0108
 - val_f1: 0.9914
Epoch 41/200
 - 1s - loss: 0.0130 - val_loss: 0.0108
2019-12-24 01:45:20,890 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep4/ann_model_epoch_40.pickle
 - val_f1: 0.9914
Epoch 42/200
 - 1s - loss: 0.0135 - val_loss: 0.0107
 - val_f1: 0.9904
Epoch 43/200
 - 1s - loss: 0.0133 - val_loss: 0.0100
 - val_f1: 0.9930
Epoch 44/200
 - 1s - loss: 0.0127 - val_loss: 0.0098
 - val_f1: 0.9929
Epoch 45/200
 - 1s - loss: 0.0127 - val_loss: 0.0100
 - val_f1: 0.9926
Epoch 46/200
 - 1s - loss: 0.0129 - val_loss: 0.0111
 - val_f1: 0.9902
Epoch 47/200
 - 1s - loss: 0.0128 - val_loss: 0.0102
 - val_f1: 0.9923
Epoch 48/200
 - 1s - loss: 0.0131 - val_loss: 0.0116
 - val_f1: 0.9906
Epoch 49/200
 - 1s - loss: 0.0124 - val_loss: 0.0108
 - val_f1: 0.9917
Epoch 50/200
 - 1s - loss: 0.0131 - val_loss: 0.0107
 - val_f1: 0.9932
Epoch 51/200
 - 1s - loss: 0.0127 - val_loss: 0.0140
 - val_f1: 0.9876
Epoch 52/200
 - 1s - loss: 0.0127 - val_loss: 0.0099
 - val_f1: 0.9926
Epoch 53/200
 - 1s - loss: 0.0123 - val_loss: 0.0105
 - val_f1: 0.9926
Epoch 54/200
 - 1s - loss: 0.0131 - val_loss: 0.0112
 - val_f1: 0.9912
Epoch 55/200
 - 1s - loss: 0.0129 - val_loss: 0.0113
 - val_f1: 0.9917
Epoch 56/200
 - 1s - loss: 0.0131 - val_loss: 0.0104
 - val_f1: 0.9920
Epoch 57/200
 - 1s - loss: 0.0128 - val_loss: 0.0104
 - val_f1: 0.9922
Epoch 58/200
 - 1s - loss: 0.0125 - val_loss: 0.0114
 - val_f1: 0.9905
Epoch 59/200
 - 1s - loss: 0.0130 - val_loss: 0.0110
 - val_f1: 0.9907
Epoch 60/200
 - 1s - loss: 0.0125 - val_loss: 0.0110
 - val_f1: 0.9904
Epoch 61/200
 - 1s - loss: 0.0127 - val_loss: 0.0128
2019-12-24 01:45:54,315 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep4/ann_model_epoch_60.pickle
 - val_f1: 0.9878
Epoch 62/200
 - 1s - loss: 0.0126 - val_loss: 0.0105
 - val_f1: 0.9909
Epoch 63/200
 - 1s - loss: 0.0123 - val_loss: 0.0113
 - val_f1: 0.9913
Epoch 64/200
 - 1s - loss: 0.0122 - val_loss: 0.0112
 - val_f1: 0.9926
Epoch 65/200
 - 1s - loss: 0.0123 - val_loss: 0.0101
 - val_f1: 0.9920
Epoch 66/200
 - 1s - loss: 0.0121 - val_loss: 0.0103
 - val_f1: 0.9929
Epoch 67/200
 - 1s - loss: 0.0119 - val_loss: 0.0098
 - val_f1: 0.9923
Epoch 68/200
 - 1s - loss: 0.0121 - val_loss: 0.0098
 - val_f1: 0.9927
Epoch 69/200
 - 1s - loss: 0.0116 - val_loss: 0.0098
 - val_f1: 0.9934
Epoch 70/200
 - 1s - loss: 0.0125 - val_loss: 0.0099
 - val_f1: 0.9925
Epoch 71/200
 - 1s - loss: 0.0123 - val_loss: 0.0096
 - val_f1: 0.9926
Epoch 72/200
 - 1s - loss: 0.0120 - val_loss: 0.0102
 - val_f1: 0.9927
Epoch 73/200
 - 1s - loss: 0.0120 - val_loss: 0.0103
 - val_f1: 0.9918
Epoch 74/200
 - 1s - loss: 0.0113 - val_loss: 0.0102
 - val_f1: 0.9921
Epoch 75/200
 - 1s - loss: 0.0120 - val_loss: 0.0095
 - val_f1: 0.9923
Epoch 76/200
 - 1s - loss: 0.0117 - val_loss: 0.0095
 - val_f1: 0.9917
Epoch 77/200
 - 1s - loss: 0.0116 - val_loss: 0.0100
 - val_f1: 0.9920
Epoch 78/200
 - 1s - loss: 0.0121 - val_loss: 0.0094
 - val_f1: 0.9935
Epoch 79/200
 - 1s - loss: 0.0118 - val_loss: 0.0097
 - val_f1: 0.9935
Epoch 80/200
 - 1s - loss: 0.0116 - val_loss: 0.0096
 - val_f1: 0.9920
Epoch 81/200
 - 1s - loss: 0.0113 - val_loss: 0.0103
2019-12-24 01:46:27,608 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep4/ann_model_epoch_80.pickle
 - val_f1: 0.9907
Epoch 82/200
 - 1s - loss: 0.0115 - val_loss: 0.0091
 - val_f1: 0.9938
Epoch 83/200
 - 1s - loss: 0.0114 - val_loss: 0.0096
 - val_f1: 0.9928
Epoch 84/200
 - 1s - loss: 0.0112 - val_loss: 0.0094
 - val_f1: 0.9927
Epoch 85/200
 - 1s - loss: 0.0112 - val_loss: 0.0094
 - val_f1: 0.9927
Epoch 86/200
 - 1s - loss: 0.0118 - val_loss: 0.0087
 - val_f1: 0.9927
Epoch 87/200
 - 1s - loss: 0.0109 - val_loss: 0.0092
 - val_f1: 0.9933
Epoch 88/200
 - 1s - loss: 0.0105 - val_loss: 0.0090
 - val_f1: 0.9932
Epoch 89/200
 - 1s - loss: 0.0111 - val_loss: 0.0095
 - val_f1: 0.9930
Epoch 90/200
 - 1s - loss: 0.0106 - val_loss: 0.0111
 - val_f1: 0.9920
Epoch 91/200
 - 1s - loss: 0.0113 - val_loss: 0.0098
 - val_f1: 0.9927
Epoch 92/200
 - 1s - loss: 0.0109 - val_loss: 0.0099
 - val_f1: 0.9927
Epoch 93/200
 - 1s - loss: 0.0106 - val_loss: 0.0091
 - val_f1: 0.9931
Epoch 94/200
 - 1s - loss: 0.0106 - val_loss: 0.0088
 - val_f1: 0.9939
Epoch 95/200
 - 1s - loss: 0.0108 - val_loss: 0.0091
 - val_f1: 0.9936
Epoch 96/200
 - 1s - loss: 0.0108 - val_loss: 0.0086
 - val_f1: 0.9937
Epoch 97/200
 - 1s - loss: 0.0106 - val_loss: 0.0086
 - val_f1: 0.9930
Epoch 98/200
 - 1s - loss: 0.0104 - val_loss: 0.0096
 - val_f1: 0.9930
Epoch 99/200
 - 1s - loss: 0.0106 - val_loss: 0.0111
 - val_f1: 0.9903
Epoch 100/200
 - 1s - loss: 0.0106 - val_loss: 0.0091
 - val_f1: 0.9931
Epoch 101/200
 - 1s - loss: 0.0111 - val_loss: 0.0100
2019-12-24 01:47:01,049 [INFO] epoch = 100. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep4/ann_model_epoch_100.pickle
 - val_f1: 0.9933
Epoch 102/200
 - 1s - loss: 0.0108 - val_loss: 0.0103
 - val_f1: 0.9910
Epoch 103/200
 - 1s - loss: 0.0113 - val_loss: 0.0102
 - val_f1: 0.9919
Epoch 104/200
 - 1s - loss: 0.0111 - val_loss: 0.0128
 - val_f1: 0.9887
Epoch 105/200
 - 1s - loss: 0.0106 - val_loss: 0.0088
 - val_f1: 0.9933
Epoch 106/200
 - 1s - loss: 0.0112 - val_loss: 0.0088
 - val_f1: 0.9937
Epoch 107/200
 - 1s - loss: 0.0113 - val_loss: 0.0091
 - val_f1: 0.9930
Epoch 108/200
 - 1s - loss: 0.0111 - val_loss: 0.0096
 - val_f1: 0.9931
Epoch 109/200
 - 1s - loss: 0.0106 - val_loss: 0.0099
 - val_f1: 0.9907
Epoch 110/200
 - 1s - loss: 0.0105 - val_loss: 0.0091
 - val_f1: 0.9936
Epoch 111/200
 - 1s - loss: 0.0112 - val_loss: 0.0096
 - val_f1: 0.9929
Epoch 112/200
 - 1s - loss: 0.0106 - val_loss: 0.0097
 - val_f1: 0.9932
Epoch 113/200
 - 1s - loss: 0.0106 - val_loss: 0.0092
 - val_f1: 0.9936
Epoch 114/200
 - 1s - loss: 0.0109 - val_loss: 0.0093
 - val_f1: 0.9920
Epoch 115/200
 - 1s - loss: 0.0107 - val_loss: 0.0099
 - val_f1: 0.9922
Epoch 116/200
 - 1s - loss: 0.0109 - val_loss: 0.0098
 - val_f1: 0.9922
Epoch 117/200
 - 1s - loss: 0.0109 - val_loss: 0.0096
 - val_f1: 0.9921
Epoch 118/200
 - 1s - loss: 0.0107 - val_loss: 0.0097
 - val_f1: 0.9928
Epoch 119/200
 - 1s - loss: 0.0108 - val_loss: 0.0088
 - val_f1: 0.9936
Epoch 120/200
 - 1s - loss: 0.0111 - val_loss: 0.0088
 - val_f1: 0.9936
Epoch 121/200
 - 1s - loss: 0.0104 - val_loss: 0.0093
2019-12-24 01:47:34,472 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep4/ann_model_epoch_120.pickle
 - val_f1: 0.9917
Epoch 122/200
 - 1s - loss: 0.0101 - val_loss: 0.0095
 - val_f1: 0.9933
Epoch 123/200
 - 1s - loss: 0.0107 - val_loss: 0.0089
 - val_f1: 0.9932
Epoch 124/200
 - 1s - loss: 0.0107 - val_loss: 0.0091
 - val_f1: 0.9931
Epoch 125/200
 - 1s - loss: 0.0101 - val_loss: 0.0091
 - val_f1: 0.9932
Epoch 126/200
 - 1s - loss: 0.0107 - val_loss: 0.0085
 - val_f1: 0.9941
Epoch 127/200
 - 1s - loss: 0.0105 - val_loss: 0.0088
 - val_f1: 0.9928
Epoch 128/200
 - 1s - loss: 0.0104 - val_loss: 0.0108
 - val_f1: 0.9902
Epoch 129/200
 - 1s - loss: 0.0107 - val_loss: 0.0110
 - val_f1: 0.9884
Epoch 130/200
 - 1s - loss: 0.0108 - val_loss: 0.0096
 - val_f1: 0.9924
Epoch 131/200
 - 1s - loss: 0.0101 - val_loss: 0.0087
 - val_f1: 0.9932
Epoch 132/200
 - 1s - loss: 0.0100 - val_loss: 0.0089
 - val_f1: 0.9932
Epoch 133/200
 - 1s - loss: 0.0100 - val_loss: 0.0088
 - val_f1: 0.9932
Epoch 134/200
 - 1s - loss: 0.0100 - val_loss: 0.0090
 - val_f1: 0.9930
Epoch 135/200
 - 1s - loss: 0.0105 - val_loss: 0.0086
 - val_f1: 0.9931
Epoch 136/200
 - 1s - loss: 0.0102 - val_loss: 0.0086
 - val_f1: 0.9938
Epoch 137/200
 - 1s - loss: 0.0101 - val_loss: 0.0086
 - val_f1: 0.9934
Epoch 138/200
 - 1s - loss: 0.0098 - val_loss: 0.0084
 - val_f1: 0.9939
Epoch 139/200
 - 1s - loss: 0.0100 - val_loss: 0.0104
 - val_f1: 0.9916
Epoch 140/200
 - 1s - loss: 0.0109 - val_loss: 0.0086
 - val_f1: 0.9940
Epoch 141/200
 - 1s - loss: 0.0103 - val_loss: 0.0095
2019-12-24 01:48:07,905 [INFO] epoch = 140. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep4/ann_model_epoch_140.pickle
 - val_f1: 0.9928
Epoch 142/200
 - 1s - loss: 0.0102 - val_loss: 0.0084
 - val_f1: 0.9941
Epoch 143/200
 - 1s - loss: 0.0102 - val_loss: 0.0081
 - val_f1: 0.9939
Epoch 144/200
 - 1s - loss: 0.0096 - val_loss: 0.0090
 - val_f1: 0.9925
Epoch 145/200
 - 1s - loss: 0.0103 - val_loss: 0.0086
 - val_f1: 0.9938
Epoch 146/200
 - 1s - loss: 0.0101 - val_loss: 0.0084
 - val_f1: 0.9939
Epoch 147/200
 - 1s - loss: 0.0097 - val_loss: 0.0088
 - val_f1: 0.9940
Epoch 148/200
 - 1s - loss: 0.0099 - val_loss: 0.0086
 - val_f1: 0.9937
Epoch 149/200
 - 1s - loss: 0.0098 - val_loss: 0.0095
 - val_f1: 0.9920
Epoch 150/200
 - 1s - loss: 0.0103 - val_loss: 0.0097
 - val_f1: 0.9924
Epoch 151/200
 - 1s - loss: 0.0099 - val_loss: 0.0091
 - val_f1: 0.9933
Epoch 152/200
 - 1s - loss: 0.0098 - val_loss: 0.0085
 - val_f1: 0.9941
Epoch 153/200
 - 1s - loss: 0.0101 - val_loss: 0.0091
 - val_f1: 0.9930
Epoch 154/200
 - 1s - loss: 0.0105 - val_loss: 0.0087
 - val_f1: 0.9934
Epoch 155/200
 - 1s - loss: 0.0100 - val_loss: 0.0080
 - val_f1: 0.9942
Epoch 156/200
 - 1s - loss: 0.0099 - val_loss: 0.0090
 - val_f1: 0.9934
Epoch 157/200
 - 1s - loss: 0.0099 - val_loss: 0.0087
 - val_f1: 0.9932
Epoch 158/200
 - 1s - loss: 0.0102 - val_loss: 0.0087
 - val_f1: 0.9939
Epoch 159/200
 - 1s - loss: 0.0102 - val_loss: 0.0082
 - val_f1: 0.9941
Epoch 160/200
 - 1s - loss: 0.0095 - val_loss: 0.0082
 - val_f1: 0.9939
Epoch 161/200
 - 1s - loss: 0.0099 - val_loss: 0.0086
2019-12-24 01:48:41,316 [INFO] epoch = 160. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep4/ann_model_epoch_160.pickle
 - val_f1: 0.9932
Epoch 162/200
 - 1s - loss: 0.0101 - val_loss: 0.0098
 - val_f1: 0.9925
Epoch 163/200
 - 1s - loss: 0.0100 - val_loss: 0.0092
 - val_f1: 0.9924
Epoch 164/200
 - 1s - loss: 0.0102 - val_loss: 0.0091
 - val_f1: 0.9934
Epoch 165/200
 - 1s - loss: 0.0098 - val_loss: 0.0085
 - val_f1: 0.9932
Epoch 166/200
 - 1s - loss: 0.0098 - val_loss: 0.0090
 - val_f1: 0.9917
Epoch 167/200
 - 1s - loss: 0.0096 - val_loss: 0.0101
 - val_f1: 0.9918
Epoch 168/200
 - 1s - loss: 0.0100 - val_loss: 0.0092
 - val_f1: 0.9934
Epoch 169/200
 - 1s - loss: 0.0106 - val_loss: 0.0090
 - val_f1: 0.9935
Epoch 170/200
 - 1s - loss: 0.0102 - val_loss: 0.0085
 - val_f1: 0.9938
Epoch 171/200
 - 1s - loss: 0.0099 - val_loss: 0.0083
 - val_f1: 0.9936
Epoch 172/200
 - 1s - loss: 0.0102 - val_loss: 0.0099
 - val_f1: 0.9913
Epoch 173/200
 - 1s - loss: 0.0101 - val_loss: 0.0087
 - val_f1: 0.9931
Epoch 174/200
 - 1s - loss: 0.0100 - val_loss: 0.0086
 - val_f1: 0.9934
Epoch 175/200
 - 1s - loss: 0.0099 - val_loss: 0.0083
 - val_f1: 0.9930
Epoch 176/200
 - 1s - loss: 0.0109 - val_loss: 0.0088
 - val_f1: 0.9931
Epoch 177/200
 - 1s - loss: 0.0106 - val_loss: 0.0083
 - val_f1: 0.9943
Epoch 178/200
 - 1s - loss: 0.0103 - val_loss: 0.0082
 - val_f1: 0.9938
Epoch 179/200
 - 1s - loss: 0.0096 - val_loss: 0.0081
 - val_f1: 0.9939
Epoch 180/200
 - 1s - loss: 0.0098 - val_loss: 0.0088
 - val_f1: 0.9933
Epoch 181/200
 - 1s - loss: 0.0098 - val_loss: 0.0080
2019-12-24 01:49:14,964 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep4/ann_model_epoch_180.pickle
 - val_f1: 0.9943
Epoch 182/200
 - 1s - loss: 0.0100 - val_loss: 0.0082
 - val_f1: 0.9935
Epoch 183/200
 - 1s - loss: 0.0098 - val_loss: 0.0081
 - val_f1: 0.9941
Epoch 184/200
 - 1s - loss: 0.0093 - val_loss: 0.0081
 - val_f1: 0.9940
Epoch 185/200
 - 1s - loss: 0.0100 - val_loss: 0.0090
 - val_f1: 0.9931
Epoch 186/200
 - 1s - loss: 0.0101 - val_loss: 0.0084
 - val_f1: 0.9943
Epoch 187/200
 - 1s - loss: 0.0104 - val_loss: 0.0083
 - val_f1: 0.9941
Epoch 188/200
 - 1s - loss: 0.0096 - val_loss: 0.0085
 - val_f1: 0.9937
Epoch 189/200
 - 1s - loss: 0.0099 - val_loss: 0.0088
 - val_f1: 0.9925
Epoch 190/200
 - 1s - loss: 0.0099 - val_loss: 0.0080
 - val_f1: 0.9933
Epoch 191/200
 - 1s - loss: 0.0096 - val_loss: 0.0080
 - val_f1: 0.9943
Epoch 192/200
 - 1s - loss: 0.0098 - val_loss: 0.0084
 - val_f1: 0.9937
Epoch 193/200
 - 1s - loss: 0.0096 - val_loss: 0.0099
 - val_f1: 0.9919
Epoch 194/200
 - 1s - loss: 0.0097 - val_loss: 0.0082
 - val_f1: 0.9937
Epoch 195/200
 - 1s - loss: 0.0100 - val_loss: 0.0081
 - val_f1: 0.9937
Epoch 196/200
 - 1s - loss: 0.0092 - val_loss: 0.0081
 - val_f1: 0.9937
Epoch 197/200
 - 1s - loss: 0.0096 - val_loss: 0.0079
 - val_f1: 0.9940
Epoch 198/200
 - 1s - loss: 0.0093 - val_loss: 0.0079
 - val_f1: 0.9942
Epoch 199/200
 - 1s - loss: 0.0090 - val_loss: 0.0090
 - val_f1: 0.9927
Epoch 200/200
 - 1s - loss: 0.0097 - val_loss: 0.0079
2019-12-24 01:49:47,393 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-24 01:49:51,165 [INFO] Last epoch loss evaluation: train_loss = 0.006237, val_loss = 0.007858
2019-12-24 01:49:51,166 [INFO] Training complete. time_to_train = 606.19 sec, 10.10 min
2019-12-24 01:49:51,188 [INFO] Model saved to results_selected_models/selected_nsl_ae_ann_deep_rep4/best_model.pickle
2019-12-24 01:49:51,379 [INFO] Plot saved to: results_selected_models/selected_nsl_ae_ann_deep_rep4/training_error_history.png
2019-12-24 01:49:51,555 [INFO] Plot saved to: results_selected_models/selected_nsl_ae_ann_deep_rep4/training_f1_history.png
2019-12-24 01:49:51,555 [INFO] Making predictions on training, validation, testing data
2019-12-24 01:50:01,226 [INFO] Evaluating predictions (results)
2019-12-24 01:50:01,780 [INFO] Dataset: Testing. Classification report below
2019-12-24 01:50:01,780 [INFO] 
              precision    recall  f1-score   support

         dos       0.96      0.83      0.89      7458
      normal       0.68      0.93      0.79      9711
       probe       0.72      0.75      0.74      2421
         r2l       0.92      0.12      0.22      2421
         u2r       0.56      0.01      0.02       533

    accuracy                           0.77     22544
   macro avg       0.77      0.53      0.53     22544
weighted avg       0.80      0.77      0.74     22544

2019-12-24 01:50:01,780 [INFO] Overall accuracy (micro avg): 0.7684971611071683
2019-12-24 01:50:02,381 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.7685         0.7685                       0.7685                0.0579                   0.2315  0.7685
1     Macro avg        0.9074         0.7673                       0.5286                0.0766                   0.4714  0.5295
2  Weighted avg        0.8665         0.8005                       0.7685                0.1515                   0.2315  0.7356
2019-12-24 01:50:03,052 [INFO] Dataset: Validation. Classification report below
2019-12-24 01:50:03,052 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00      9186
      normal       0.99      1.00      0.99     13469
       probe       0.99      0.98      0.98      2331
         r2l       0.91      0.80      0.85       199
         u2r       1.00      0.30      0.46        10

    accuracy                           0.99     25195
   macro avg       0.98      0.81      0.86     25195
weighted avg       0.99      0.99      0.99     25195

2019-12-24 01:50:03,052 [INFO] Overall accuracy (micro avg): 0.9935701528080968
2019-12-24 01:50:03,723 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9936         0.9936                       0.9936                0.0016                   0.0064  0.9936
1     Macro avg        0.9974         0.9774                       0.8147                0.0023                   0.1853  0.8576
2  Weighted avg        0.9960         0.9935                       0.9936                0.0051                   0.0064  0.9934
2019-12-24 01:50:06,582 [INFO] Dataset: Training. Classification report below
2019-12-24 01:50:06,582 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00     36741
      normal       0.99      1.00      0.99     53874
       probe       0.99      0.98      0.99      9325
         r2l       0.89      0.77      0.82       796
         u2r       0.82      0.43      0.56        42

    accuracy                           0.99    100778
   macro avg       0.94      0.83      0.87    100778
weighted avg       0.99      0.99      0.99    100778

2019-12-24 01:50:06,582 [INFO] Overall accuracy (micro avg): 0.9936791760106373
2019-12-24 01:50:09,645 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9937         0.9937                       0.9937                0.0016                   0.0063  0.9937
1     Macro avg        0.9975         0.9378                       0.8344                0.0023                   0.1656  0.8730
2  Weighted avg        0.9961         0.9935                       0.9937                0.0054                   0.0063  0.9935
2019-12-24 01:50:09,691 [INFO] Results saved to: results_selected_models/selected_nsl_ae_ann_deep_rep4/selected_nsl_ae_ann_deep_rep4_results.xlsx
2019-12-24 01:50:09,691 [INFO] ================= Finished running experiment no. 4 ================= 

2019-12-24 01:50:09,692 [INFO] Created directory: results_selected_models/selected_nsl_ae_ann_deep_rep5
2019-12-24 01:50:09,693 [INFO] Initialized logging. log_filename = results_selected_models/selected_nsl_ae_ann_deep_rep5/run_log.log
2019-12-24 01:50:09,693 [INFO] ================= Running experiment no. 5  ================= 

2019-12-24 01:50:09,693 [INFO] Experiment parameters given below
2019-12-24 01:50:09,693 [INFO] 
{'experiment_num': 5, 'results_dir': 'results_selected_models/selected_nsl_ae_ann_deep_rep5', 'model_type': 'classifier', 'model': 'ae_ann', 'normal_label': 'normal', 'scaling_type': 'NA', 'unsupervised_ratio': 0.25, 'ae_encoder_units': [128, 64, 32], 'ae_encoder_activations': ['relu', 'relu', 'relu'], 'ae_encoder_dropout_rates': [0.2, 0.2, 0.2], 'ae_encoder_l1_param': -1, 'ae_decoder_units': [64, 128], 'ae_decoder_activations': ['relu', 'relu'], 'ae_decoder_dropout_rates': [0.2, 0.2], 'output_activation': 'sigmoid', 'loss_function': 'binary_crossentropy', 'ae_epochs': 200, 'ann_layer_units': [64], 'ann_layer_activations': ['relu'], 'ann_layer_dropout_rates': [0.2], 'ann_epochs': 200, 'early_stop_patience': 50, 'batch_size': 256, 'dataset_dir': '../Datasets/small_datasets/nsl_kdd_five_classes', 'description': 'selected_nsl_ae_ann_deep_rep5'}
2019-12-24 01:50:09,693 [INFO] Created tensorboard log directory: results_selected_models/selected_nsl_ae_ann_deep_rep5/tf_logs_run_2019_12_24-01_50_09
2019-12-24 01:50:09,693 [INFO] Loading datsets from: ../Datasets/small_datasets/nsl_kdd_five_classes
2019-12-24 01:50:09,693 [INFO] Reading X, y files
2019-12-24 01:50:09,693 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_train.h5
2019-12-24 01:50:09,928 [INFO] Reading complete. time_to_read=0.23 seconds
2019-12-24 01:50:09,928 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_val.h5
2019-12-24 01:50:09,992 [INFO] Reading complete. time_to_read=0.06 seconds
2019-12-24 01:50:09,992 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_test.h5
2019-12-24 01:50:10,049 [INFO] Reading complete. time_to_read=0.06 seconds
2019-12-24 01:50:10,049 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_train.h5
2019-12-24 01:50:10,058 [INFO] Reading complete. time_to_read=0.01 seconds
2019-12-24 01:50:10,058 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_val.h5
2019-12-24 01:50:10,063 [INFO] Reading complete. time_to_read=0.00 seconds
2019-12-24 01:50:10,063 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_test.h5
2019-12-24 01:50:10,067 [INFO] Reading complete. time_to_read=0.00 seconds
2019-12-24 01:50:10,183 [INFO] Initializing model
2019-12-24 01:50:10,714 [INFO] _________________________________________________________________
2019-12-24 01:50:10,715 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-24 01:50:10,715 [INFO] =================================================================
2019-12-24 01:50:10,715 [INFO] dense_53 (Dense)             (None, 128)               15744     
2019-12-24 01:50:10,715 [INFO] _________________________________________________________________
2019-12-24 01:50:10,715 [INFO] batch_normalization_35 (Batc (None, 128)               512       
2019-12-24 01:50:10,715 [INFO] _________________________________________________________________
2019-12-24 01:50:10,715 [INFO] dropout_35 (Dropout)         (None, 128)               0         
2019-12-24 01:50:10,715 [INFO] _________________________________________________________________
2019-12-24 01:50:10,715 [INFO] dense_54 (Dense)             (None, 64)                8256      
2019-12-24 01:50:10,715 [INFO] _________________________________________________________________
2019-12-24 01:50:10,715 [INFO] batch_normalization_36 (Batc (None, 64)                256       
2019-12-24 01:50:10,716 [INFO] _________________________________________________________________
2019-12-24 01:50:10,716 [INFO] dropout_36 (Dropout)         (None, 64)                0         
2019-12-24 01:50:10,716 [INFO] _________________________________________________________________
2019-12-24 01:50:10,716 [INFO] dense_55 (Dense)             (None, 32)                2080      
2019-12-24 01:50:10,716 [INFO] _________________________________________________________________
2019-12-24 01:50:10,716 [INFO] batch_normalization_37 (Batc (None, 32)                128       
2019-12-24 01:50:10,716 [INFO] _________________________________________________________________
2019-12-24 01:50:10,716 [INFO] dropout_37 (Dropout)         (None, 32)                0         
2019-12-24 01:50:10,716 [INFO] _________________________________________________________________
2019-12-24 01:50:10,716 [INFO] dense_56 (Dense)             (None, 64)                2112      
2019-12-24 01:50:10,716 [INFO] _________________________________________________________________
2019-12-24 01:50:10,716 [INFO] batch_normalization_38 (Batc (None, 64)                256       
2019-12-24 01:50:10,717 [INFO] _________________________________________________________________
2019-12-24 01:50:10,717 [INFO] dropout_38 (Dropout)         (None, 64)                0         
2019-12-24 01:50:10,717 [INFO] _________________________________________________________________
2019-12-24 01:50:10,717 [INFO] dense_57 (Dense)             (None, 128)               8320      
2019-12-24 01:50:10,717 [INFO] _________________________________________________________________
2019-12-24 01:50:10,717 [INFO] batch_normalization_39 (Batc (None, 128)               512       
2019-12-24 01:50:10,717 [INFO] _________________________________________________________________
2019-12-24 01:50:10,717 [INFO] dropout_39 (Dropout)         (None, 128)               0         
2019-12-24 01:50:10,717 [INFO] _________________________________________________________________
2019-12-24 01:50:10,717 [INFO] dense_58 (Dense)             (None, 122)               15738     
2019-12-24 01:50:10,717 [INFO] =================================================================
2019-12-24 01:50:10,718 [INFO] Total params: 53,914
2019-12-24 01:50:10,718 [INFO] Trainable params: 53,082
2019-12-24 01:50:10,718 [INFO] Non-trainable params: 832
2019-12-24 01:50:10,718 [INFO] _________________________________________________________________
2019-12-24 01:50:10,852 [INFO] _________________________________________________________________
2019-12-24 01:50:10,852 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-24 01:50:10,852 [INFO] =================================================================
2019-12-24 01:50:10,852 [INFO] dense_59 (Dense)             (None, 64)                2112      
2019-12-24 01:50:10,852 [INFO] _________________________________________________________________
2019-12-24 01:50:10,852 [INFO] batch_normalization_40 (Batc (None, 64)                256       
2019-12-24 01:50:10,852 [INFO] _________________________________________________________________
2019-12-24 01:50:10,852 [INFO] dropout_40 (Dropout)         (None, 64)                0         
2019-12-24 01:50:10,852 [INFO] _________________________________________________________________
2019-12-24 01:50:10,852 [INFO] dense_60 (Dense)             (None, 5)                 325       
2019-12-24 01:50:10,852 [INFO] =================================================================
2019-12-24 01:50:10,853 [INFO] Total params: 2,693
2019-12-24 01:50:10,853 [INFO] Trainable params: 2,565
2019-12-24 01:50:10,853 [INFO] Non-trainable params: 128
2019-12-24 01:50:10,853 [INFO] _________________________________________________________________
2019-12-24 01:50:10,853 [INFO] Training model
2019-12-24 01:50:10,853 [INFO] Splitting train set into 2 sets (unsupervised, supervised), random_seed = None
2019-12-24 01:50:12,198 [INFO] Split sizes (instances). total = 100778, unsupervised = 25194, supervised = 75584, unsupervised dataset hash = 35573998083c57e49c93a6268dc533d19d341f80
2019-12-24 01:50:12,201 [INFO] Training autoencoder
 - val_f1: 0.9936
Train on 25194 samples, validate on 25195 samples
Epoch 1/200
 - 4s - loss: 0.3988 - val_loss: -4.9266e-01
Epoch 2/200
 - 1s - loss: -6.2790e-01 - val_loss: -1.2802e+00
Epoch 3/200
 - 1s - loss: -1.3555e+00 - val_loss: -1.7911e+00
Epoch 4/200
 - 1s - loss: -1.8277e+00 - val_loss: -2.1882e+00
Epoch 5/200
 - 1s - loss: -2.1452e+00 - val_loss: -2.4714e+00
Epoch 6/200
 - 1s - loss: -2.3641e+00 - val_loss: -2.6563e+00
Epoch 7/200
 - 1s - loss: -2.5123e+00 - val_loss: -2.7936e+00
Epoch 8/200
 - 1s - loss: -2.6229e+00 - val_loss: -2.8754e+00
Epoch 9/200
 - 1s - loss: -2.7004e+00 - val_loss: -2.9308e+00
Epoch 10/200
 - 1s - loss: -2.7507e+00 - val_loss: -2.9556e+00
Epoch 11/200
 - 1s - loss: -2.7941e+00 - val_loss: -2.9871e+00
Epoch 12/200
 - 1s - loss: -2.8254e+00 - val_loss: -3.0022e+00
Epoch 13/200
 - 1s - loss: -2.8546e+00 - val_loss: -3.0208e+00
Epoch 14/200
 - 1s - loss: -2.8740e+00 - val_loss: -3.0370e+00
Epoch 15/200
 - 1s - loss: -2.8920e+00 - val_loss: -3.0449e+00
Epoch 16/200
 - 1s - loss: -2.9121e+00 - val_loss: -3.0553e+00
Epoch 17/200
 - 1s - loss: -2.9235e+00 - val_loss: -3.0633e+00
Epoch 18/200
 - 1s - loss: -2.9346e+00 - val_loss: -3.0734e+00
Epoch 19/200
 - 1s - loss: -2.9494e+00 - val_loss: -3.0790e+00
Epoch 20/200
 - 1s - loss: -2.9580e+00 - val_loss: -3.0850e+00
Epoch 21/200
 - 1s - loss: -2.9658e+00 - val_loss: -3.0949e+00
2019-12-24 01:50:50,798 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep5/ae_model_epoch_20.pickle
Epoch 22/200
 - 1s - loss: -2.9760e+00 - val_loss: -3.1012e+00
Epoch 23/200
 - 1s - loss: -2.9846e+00 - val_loss: -3.1077e+00
Epoch 24/200
 - 1s - loss: -2.9866e+00 - val_loss: -3.1065e+00
Epoch 25/200
 - 1s - loss: -3.0025e+00 - val_loss: -3.1144e+00
Epoch 26/200
 - 1s - loss: -3.0021e+00 - val_loss: -3.1138e+00
Epoch 27/200
 - 1s - loss: -3.0088e+00 - val_loss: -3.1182e+00
Epoch 28/200
 - 1s - loss: -3.0143e+00 - val_loss: -3.1232e+00
Epoch 29/200
 - 1s - loss: -3.0186e+00 - val_loss: -3.1258e+00
Epoch 30/200
 - 1s - loss: -3.0225e+00 - val_loss: -3.1284e+00
Epoch 31/200
 - 1s - loss: -3.0255e+00 - val_loss: -3.1300e+00
Epoch 32/200
 - 1s - loss: -3.0329e+00 - val_loss: -3.1330e+00
Epoch 33/200
 - 1s - loss: -3.0347e+00 - val_loss: -3.1340e+00
Epoch 34/200
 - 1s - loss: -3.0377e+00 - val_loss: -3.1368e+00
Epoch 35/200
 - 1s - loss: -3.0393e+00 - val_loss: -3.1395e+00
Epoch 36/200
 - 1s - loss: -3.0447e+00 - val_loss: -3.1406e+00
Epoch 37/200
 - 1s - loss: -3.0449e+00 - val_loss: -3.1414e+00
Epoch 38/200
 - 1s - loss: -3.0482e+00 - val_loss: -3.1441e+00
Epoch 39/200
 - 1s - loss: -3.0547e+00 - val_loss: -3.1461e+00
Epoch 40/200
 - 1s - loss: -3.0554e+00 - val_loss: -3.1482e+00
Epoch 41/200
 - 1s - loss: -3.0603e+00 - val_loss: -3.1498e+00
2019-12-24 01:51:15,662 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep5/ae_model_epoch_40.pickle
Epoch 42/200
 - 1s - loss: -3.0576e+00 - val_loss: -3.1485e+00
Epoch 43/200
 - 1s - loss: -3.0606e+00 - val_loss: -3.1511e+00
Epoch 44/200
 - 1s - loss: -3.0678e+00 - val_loss: -3.1518e+00
Epoch 45/200
 - 1s - loss: -3.0657e+00 - val_loss: -3.1522e+00
Epoch 46/200
 - 1s - loss: -3.0707e+00 - val_loss: -3.1549e+00
Epoch 47/200
 - 1s - loss: -3.0709e+00 - val_loss: -3.1548e+00
Epoch 48/200
 - 1s - loss: -3.0775e+00 - val_loss: -3.1547e+00
Epoch 49/200
 - 1s - loss: -3.0762e+00 - val_loss: -3.1557e+00
Epoch 50/200
 - 1s - loss: -3.0760e+00 - val_loss: -3.1565e+00
Epoch 51/200
 - 1s - loss: -3.0771e+00 - val_loss: -3.1581e+00
Epoch 52/200
 - 1s - loss: -3.0814e+00 - val_loss: -3.1588e+00
Epoch 53/200
 - 1s - loss: -3.0825e+00 - val_loss: -3.1583e+00
Epoch 54/200
 - 1s - loss: -3.0821e+00 - val_loss: -3.1602e+00
Epoch 55/200
 - 1s - loss: -3.0858e+00 - val_loss: -3.1605e+00
Epoch 56/200
 - 1s - loss: -3.0886e+00 - val_loss: -3.1617e+00
Epoch 57/200
 - 1s - loss: -3.0855e+00 - val_loss: -3.1613e+00
Epoch 58/200
 - 1s - loss: -3.0886e+00 - val_loss: -3.1628e+00
Epoch 59/200
 - 1s - loss: -3.0875e+00 - val_loss: -3.1640e+00
Epoch 60/200
 - 1s - loss: -3.0941e+00 - val_loss: -3.1651e+00
Epoch 61/200
 - 1s - loss: -3.0903e+00 - val_loss: -3.1645e+00
2019-12-24 01:51:40,490 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep5/ae_model_epoch_60.pickle
Epoch 62/200
 - 1s - loss: -3.0937e+00 - val_loss: -3.1648e+00
Epoch 63/200
 - 1s - loss: -3.0948e+00 - val_loss: -3.1661e+00
Epoch 64/200
 - 1s - loss: -3.0980e+00 - val_loss: -3.1653e+00
Epoch 65/200
 - 1s - loss: -3.0962e+00 - val_loss: -3.1676e+00
Epoch 66/200
 - 1s - loss: -3.0954e+00 - val_loss: -3.1683e+00
Epoch 67/200
 - 1s - loss: -3.0967e+00 - val_loss: -3.1689e+00
Epoch 68/200
 - 1s - loss: -3.1015e+00 - val_loss: -3.1683e+00
Epoch 69/200
 - 1s - loss: -3.0984e+00 - val_loss: -3.1680e+00
Epoch 70/200
 - 1s - loss: -3.1026e+00 - val_loss: -3.1700e+00
Epoch 71/200
 - 1s - loss: -3.1023e+00 - val_loss: -3.1707e+00
Epoch 72/200
 - 1s - loss: -3.1027e+00 - val_loss: -3.1699e+00
Epoch 73/200
 - 1s - loss: -3.1035e+00 - val_loss: -3.1711e+00
Epoch 74/200
 - 1s - loss: -3.1060e+00 - val_loss: -3.1723e+00
Epoch 75/200
 - 1s - loss: -3.1085e+00 - val_loss: -3.1729e+00
Epoch 76/200
 - 1s - loss: -3.1083e+00 - val_loss: -3.1733e+00
Epoch 77/200
 - 1s - loss: -3.1104e+00 - val_loss: -3.1731e+00
Epoch 78/200
 - 1s - loss: -3.1093e+00 - val_loss: -3.1760e+00
Epoch 79/200
 - 1s - loss: -3.1120e+00 - val_loss: -3.1747e+00
Epoch 80/200
 - 1s - loss: -3.1118e+00 - val_loss: -3.1758e+00
Epoch 81/200
 - 1s - loss: -3.1116e+00 - val_loss: -3.1748e+00
2019-12-24 01:52:05,307 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep5/ae_model_epoch_80.pickle
Epoch 82/200
 - 1s - loss: -3.1126e+00 - val_loss: -3.1750e+00
Epoch 83/200
 - 1s - loss: -3.1100e+00 - val_loss: -3.1752e+00
Epoch 84/200
 - 1s - loss: -3.1111e+00 - val_loss: -3.1752e+00
Epoch 85/200
 - 1s - loss: -3.1156e+00 - val_loss: -3.1772e+00
Epoch 86/200
 - 1s - loss: -3.1162e+00 - val_loss: -3.1786e+00
Epoch 87/200
 - 1s - loss: -3.1184e+00 - val_loss: -3.1796e+00
Epoch 88/200
 - 1s - loss: -3.1143e+00 - val_loss: -3.1788e+00
Epoch 89/200
 - 1s - loss: -3.1169e+00 - val_loss: -3.1787e+00
Epoch 90/200
 - 1s - loss: -3.1175e+00 - val_loss: -3.1797e+00
Epoch 91/200
 - 1s - loss: -3.1193e+00 - val_loss: -3.1792e+00
Epoch 92/200
 - 1s - loss: -3.1188e+00 - val_loss: -3.1796e+00
Epoch 93/200
 - 1s - loss: -3.1142e+00 - val_loss: -3.1808e+00
Epoch 94/200
 - 1s - loss: -3.1196e+00 - val_loss: -3.1791e+00
Epoch 95/200
 - 1s - loss: -3.1197e+00 - val_loss: -3.1801e+00
Epoch 96/200
 - 1s - loss: -3.1206e+00 - val_loss: -3.1816e+00
Epoch 97/200
 - 1s - loss: -3.1228e+00 - val_loss: -3.1824e+00
Epoch 98/200
 - 1s - loss: -3.1267e+00 - val_loss: -3.1833e+00
Epoch 99/200
 - 1s - loss: -3.1245e+00 - val_loss: -3.1819e+00
Epoch 100/200
 - 1s - loss: -3.1228e+00 - val_loss: -3.1818e+00
Epoch 101/200
 - 1s - loss: -3.1243e+00 - val_loss: -3.1839e+00
2019-12-24 01:52:30,116 [INFO] epoch = 100. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep5/ae_model_epoch_100.pickle
Epoch 102/200
 - 1s - loss: -3.1271e+00 - val_loss: -3.1832e+00
Epoch 103/200
 - 1s - loss: -3.1274e+00 - val_loss: -3.1840e+00
Epoch 104/200
 - 1s - loss: -3.1259e+00 - val_loss: -3.1844e+00
Epoch 105/200
 - 1s - loss: -3.1276e+00 - val_loss: -3.1857e+00
Epoch 106/200
 - 1s - loss: -3.1269e+00 - val_loss: -3.1855e+00
Epoch 107/200
 - 1s - loss: -3.1296e+00 - val_loss: -3.1848e+00
Epoch 108/200
 - 1s - loss: -3.1301e+00 - val_loss: -3.1857e+00
Epoch 109/200
 - 1s - loss: -3.1325e+00 - val_loss: -3.1869e+00
Epoch 110/200
 - 1s - loss: -3.1298e+00 - val_loss: -3.1864e+00
Epoch 111/200
 - 1s - loss: -3.1319e+00 - val_loss: -3.1871e+00
Epoch 112/200
 - 1s - loss: -3.1298e+00 - val_loss: -3.1882e+00
Epoch 113/200
 - 1s - loss: -3.1304e+00 - val_loss: -3.1860e+00
Epoch 114/200
 - 1s - loss: -3.1313e+00 - val_loss: -3.1870e+00
Epoch 115/200
 - 1s - loss: -3.1333e+00 - val_loss: -3.1868e+00
Epoch 116/200
 - 1s - loss: -3.1359e+00 - val_loss: -3.1868e+00
Epoch 117/200
 - 1s - loss: -3.1295e+00 - val_loss: -3.1877e+00
Epoch 118/200
 - 1s - loss: -3.1326e+00 - val_loss: -3.1887e+00
Epoch 119/200
 - 1s - loss: -3.1343e+00 - val_loss: -3.1883e+00
Epoch 120/200
 - 1s - loss: -3.1326e+00 - val_loss: -3.1880e+00
Epoch 121/200
 - 1s - loss: -3.1352e+00 - val_loss: -3.1880e+00
2019-12-24 01:52:54,908 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep5/ae_model_epoch_120.pickle
Epoch 122/200
 - 1s - loss: -3.1321e+00 - val_loss: -3.1881e+00
Epoch 123/200
 - 1s - loss: -3.1370e+00 - val_loss: -3.1896e+00
Epoch 124/200
 - 1s - loss: -3.1371e+00 - val_loss: -3.1891e+00
Epoch 125/200
 - 1s - loss: -3.1373e+00 - val_loss: -3.1900e+00
Epoch 126/200
 - 1s - loss: -3.1341e+00 - val_loss: -3.1892e+00
Epoch 127/200
 - 1s - loss: -3.1357e+00 - val_loss: -3.1907e+00
Epoch 128/200
 - 1s - loss: -3.1372e+00 - val_loss: -3.1907e+00
Epoch 129/200
 - 1s - loss: -3.1355e+00 - val_loss: -3.1901e+00
Epoch 130/200
 - 1s - loss: -3.1370e+00 - val_loss: -3.1897e+00
Epoch 131/200
 - 1s - loss: -3.1367e+00 - val_loss: -3.1900e+00
Epoch 132/200
 - 1s - loss: -3.1413e+00 - val_loss: -3.1908e+00
Epoch 133/200
 - 1s - loss: -3.1345e+00 - val_loss: -3.1906e+00
Epoch 134/200
 - 1s - loss: -3.1408e+00 - val_loss: -3.1905e+00
Epoch 135/200
 - 1s - loss: -3.1391e+00 - val_loss: -3.1910e+00
Epoch 136/200
 - 1s - loss: -3.1392e+00 - val_loss: -3.1911e+00
Epoch 137/200
 - 1s - loss: -3.1385e+00 - val_loss: -3.1916e+00
Epoch 138/200
 - 1s - loss: -3.1405e+00 - val_loss: -3.1921e+00
Epoch 139/200
 - 1s - loss: -3.1416e+00 - val_loss: -3.1922e+00
Epoch 140/200
 - 1s - loss: -3.1387e+00 - val_loss: -3.1915e+00
Epoch 141/200
 - 1s - loss: -3.1390e+00 - val_loss: -3.1919e+00
2019-12-24 01:53:19,738 [INFO] epoch = 140. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep5/ae_model_epoch_140.pickle
Epoch 142/200
 - 1s - loss: -3.1426e+00 - val_loss: -3.1923e+00
Epoch 143/200
 - 1s - loss: -3.1446e+00 - val_loss: -3.1931e+00
Epoch 144/200
 - 1s - loss: -3.1413e+00 - val_loss: -3.1930e+00
Epoch 145/200
 - 1s - loss: -3.1464e+00 - val_loss: -3.1941e+00
Epoch 146/200
 - 1s - loss: -3.1427e+00 - val_loss: -3.1938e+00
Epoch 147/200
 - 1s - loss: -3.1439e+00 - val_loss: -3.1932e+00
Epoch 148/200
 - 1s - loss: -3.1435e+00 - val_loss: -3.1926e+00
Epoch 149/200
 - 1s - loss: -3.1440e+00 - val_loss: -3.1935e+00
Epoch 150/200
 - 1s - loss: -3.1442e+00 - val_loss: -3.1946e+00
Epoch 151/200
 - 1s - loss: -3.1469e+00 - val_loss: -3.1946e+00
Epoch 152/200
 - 1s - loss: -3.1446e+00 - val_loss: -3.1942e+00
Epoch 153/200
 - 1s - loss: -3.1436e+00 - val_loss: -3.1954e+00
Epoch 154/200
 - 1s - loss: -3.1466e+00 - val_loss: -3.1948e+00
Epoch 155/200
 - 1s - loss: -3.1435e+00 - val_loss: -3.1946e+00
Epoch 156/200
 - 1s - loss: -3.1450e+00 - val_loss: -3.1950e+00
Epoch 157/200
 - 1s - loss: -3.1473e+00 - val_loss: -3.1946e+00
Epoch 158/200
 - 1s - loss: -3.1471e+00 - val_loss: -3.1941e+00
Epoch 159/200
 - 1s - loss: -3.1448e+00 - val_loss: -3.1940e+00
Epoch 160/200
 - 1s - loss: -3.1487e+00 - val_loss: -3.1946e+00
Epoch 161/200
 - 1s - loss: -3.1460e+00 - val_loss: -3.1955e+00
2019-12-24 01:53:44,583 [INFO] epoch = 160. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep5/ae_model_epoch_160.pickle
Epoch 162/200
 - 1s - loss: -3.1472e+00 - val_loss: -3.1951e+00
Epoch 163/200
 - 1s - loss: -3.1479e+00 - val_loss: -3.1960e+00
Epoch 164/200
 - 1s - loss: -3.1488e+00 - val_loss: -3.1956e+00
Epoch 165/200
 - 1s - loss: -3.1475e+00 - val_loss: -3.1962e+00
Epoch 166/200
 - 1s - loss: -3.1462e+00 - val_loss: -3.1956e+00
Epoch 167/200
 - 1s - loss: -3.1465e+00 - val_loss: -3.1962e+00
Epoch 168/200
 - 1s - loss: -3.1501e+00 - val_loss: -3.1967e+00
Epoch 169/200
 - 1s - loss: -3.1511e+00 - val_loss: -3.1964e+00
Epoch 170/200
 - 1s - loss: -3.1492e+00 - val_loss: -3.1965e+00
Epoch 171/200
 - 1s - loss: -3.1485e+00 - val_loss: -3.1968e+00
Epoch 172/200
 - 1s - loss: -3.1534e+00 - val_loss: -3.1962e+00
Epoch 173/200
 - 1s - loss: -3.1512e+00 - val_loss: -3.1964e+00
Epoch 174/200
 - 1s - loss: -3.1501e+00 - val_loss: -3.1966e+00
Epoch 175/200
 - 1s - loss: -3.1475e+00 - val_loss: -3.1965e+00
Epoch 176/200
 - 1s - loss: -3.1533e+00 - val_loss: -3.1966e+00
Epoch 177/200
 - 1s - loss: -3.1514e+00 - val_loss: -3.1970e+00
Epoch 178/200
 - 1s - loss: -3.1503e+00 - val_loss: -3.1964e+00
Epoch 179/200
 - 1s - loss: -3.1507e+00 - val_loss: -3.1973e+00
Epoch 180/200
 - 1s - loss: -3.1525e+00 - val_loss: -3.1973e+00
Epoch 181/200
 - 1s - loss: -3.1530e+00 - val_loss: -3.1975e+00
2019-12-24 01:54:09,434 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep5/ae_model_epoch_180.pickle
Epoch 182/200
 - 1s - loss: -3.1484e+00 - val_loss: -3.1960e+00
Epoch 183/200
 - 1s - loss: -3.1521e+00 - val_loss: -3.1977e+00
Epoch 184/200
 - 1s - loss: -3.1519e+00 - val_loss: -3.1971e+00
Epoch 185/200
 - 1s - loss: -3.1541e+00 - val_loss: -3.1981e+00
Epoch 186/200
 - 1s - loss: -3.1537e+00 - val_loss: -3.1968e+00
Epoch 187/200
 - 1s - loss: -3.1526e+00 - val_loss: -3.1967e+00
Epoch 188/200
 - 1s - loss: -3.1503e+00 - val_loss: -3.1980e+00
Epoch 189/200
 - 1s - loss: -3.1542e+00 - val_loss: -3.1977e+00
Epoch 190/200
 - 1s - loss: -3.1527e+00 - val_loss: -3.1979e+00
Epoch 191/200
 - 1s - loss: -3.1544e+00 - val_loss: -3.1977e+00
Epoch 192/200
 - 1s - loss: -3.1529e+00 - val_loss: -3.1988e+00
Epoch 193/200
 - 1s - loss: -3.1531e+00 - val_loss: -3.1994e+00
Epoch 194/200
 - 1s - loss: -3.1528e+00 - val_loss: -3.1986e+00
Epoch 195/200
 - 1s - loss: -3.1582e+00 - val_loss: -3.1990e+00
Epoch 196/200
 - 1s - loss: -3.1565e+00 - val_loss: -3.1994e+00
Epoch 197/200
 - 1s - loss: -3.1550e+00 - val_loss: -3.1983e+00
Epoch 198/200
 - 1s - loss: -3.1571e+00 - val_loss: -3.1989e+00
Epoch 199/200
 - 1s - loss: -3.1574e+00 - val_loss: -3.1989e+00
Epoch 200/200
 - 1s - loss: -3.1558e+00 - val_loss: -3.1988e+00
2019-12-24 01:54:33,032 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-24 01:54:36,957 [INFO] Last epoch loss evaluation: train_loss = -3.225103, val_loss = -3.199440
2019-12-24 01:54:36,958 [INFO] Training autoencoder complete
2019-12-24 01:54:36,958 [INFO] Encoding data for supervised training
2019-12-24 01:54:41,776 [INFO] Encoding complete
2019-12-24 01:54:41,776 [INFO] Training neural network layers (after autoencoder)
Train on 75584 samples, validate on 25195 samples
Epoch 1/200
 - 2s - loss: 0.1371 - val_loss: 0.0427
 - val_f1: 0.9692
Epoch 2/200
 - 1s - loss: 0.0433 - val_loss: 0.0293
 - val_f1: 0.9742
Epoch 3/200
 - 1s - loss: 0.0335 - val_loss: 0.0237
 - val_f1: 0.9782
Epoch 4/200
 - 1s - loss: 0.0292 - val_loss: 0.0216
 - val_f1: 0.9805
Epoch 5/200
 - 1s - loss: 0.0259 - val_loss: 0.0195
 - val_f1: 0.9829
Epoch 6/200
 - 1s - loss: 0.0240 - val_loss: 0.0171
 - val_f1: 0.9860
Epoch 7/200
 - 1s - loss: 0.0215 - val_loss: 0.0157
 - val_f1: 0.9867
Epoch 8/200
 - 1s - loss: 0.0206 - val_loss: 0.0147
 - val_f1: 0.9890
Epoch 9/200
 - 1s - loss: 0.0189 - val_loss: 0.0142
 - val_f1: 0.9888
Epoch 10/200
 - 1s - loss: 0.0189 - val_loss: 0.0137
 - val_f1: 0.9902
Epoch 11/200
 - 1s - loss: 0.0180 - val_loss: 0.0141
 - val_f1: 0.9888
Epoch 12/200
 - 1s - loss: 0.0178 - val_loss: 0.0133
 - val_f1: 0.9891
Epoch 13/200
 - 1s - loss: 0.0172 - val_loss: 0.0134
 - val_f1: 0.9901
Epoch 14/200
 - 1s - loss: 0.0170 - val_loss: 0.0147
 - val_f1: 0.9889
Epoch 15/200
 - 1s - loss: 0.0165 - val_loss: 0.0136
 - val_f1: 0.9899
Epoch 16/200
 - 1s - loss: 0.0159 - val_loss: 0.0143
 - val_f1: 0.9898
Epoch 17/200
 - 1s - loss: 0.0155 - val_loss: 0.0140
 - val_f1: 0.9906
Epoch 18/200
 - 1s - loss: 0.0149 - val_loss: 0.0137
 - val_f1: 0.9891
Epoch 19/200
 - 1s - loss: 0.0153 - val_loss: 0.0134
 - val_f1: 0.9900
Epoch 20/200
 - 1s - loss: 0.0145 - val_loss: 0.0134
 - val_f1: 0.9901
Epoch 21/200
 - 1s - loss: 0.0145 - val_loss: 0.0136
2019-12-24 01:55:26,726 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep5/ann_model_epoch_20.pickle
 - val_f1: 0.9898
Epoch 22/200
 - 1s - loss: 0.0139 - val_loss: 0.0123
 - val_f1: 0.9905
Epoch 23/200
 - 1s - loss: 0.0138 - val_loss: 0.0124
 - val_f1: 0.9897
Epoch 24/200
 - 1s - loss: 0.0145 - val_loss: 0.0128
 - val_f1: 0.9912
Epoch 25/200
 - 1s - loss: 0.0140 - val_loss: 0.0129
 - val_f1: 0.9900
Epoch 26/200
 - 1s - loss: 0.0139 - val_loss: 0.0125
 - val_f1: 0.9913
Epoch 27/200
 - 1s - loss: 0.0136 - val_loss: 0.0117
 - val_f1: 0.9916
Epoch 28/200
 - 1s - loss: 0.0134 - val_loss: 0.0114
 - val_f1: 0.9909
Epoch 29/200
 - 1s - loss: 0.0131 - val_loss: 0.0118
 - val_f1: 0.9909
Epoch 30/200
 - 1s - loss: 0.0130 - val_loss: 0.0112
 - val_f1: 0.9912
Epoch 31/200
 - 1s - loss: 0.0129 - val_loss: 0.0121
 - val_f1: 0.9907
Epoch 32/200
 - 1s - loss: 0.0128 - val_loss: 0.0109
 - val_f1: 0.9917
Epoch 33/200
 - 1s - loss: 0.0121 - val_loss: 0.0113
 - val_f1: 0.9927
Epoch 34/200
 - 1s - loss: 0.0122 - val_loss: 0.0111
 - val_f1: 0.9910
Epoch 35/200
 - 1s - loss: 0.0120 - val_loss: 0.0124
 - val_f1: 0.9902
Epoch 36/200
 - 1s - loss: 0.0121 - val_loss: 0.0108
 - val_f1: 0.9926
Epoch 37/200
 - 1s - loss: 0.0118 - val_loss: 0.0105
 - val_f1: 0.9924
Epoch 38/200
 - 1s - loss: 0.0125 - val_loss: 0.0104
 - val_f1: 0.9925
Epoch 39/200
 - 1s - loss: 0.0122 - val_loss: 0.0111
 - val_f1: 0.9918
Epoch 40/200
 - 1s - loss: 0.0120 - val_loss: 0.0108
 - val_f1: 0.9921
Epoch 41/200
 - 1s - loss: 0.0124 - val_loss: 0.0109
2019-12-24 01:56:01,680 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep5/ann_model_epoch_40.pickle
 - val_f1: 0.9927
Epoch 42/200
 - 1s - loss: 0.0114 - val_loss: 0.0107
 - val_f1: 0.9927
Epoch 43/200
 - 1s - loss: 0.0114 - val_loss: 0.0116
 - val_f1: 0.9916
Epoch 44/200
 - 1s - loss: 0.0119 - val_loss: 0.0105
 - val_f1: 0.9923
Epoch 45/200
 - 1s - loss: 0.0113 - val_loss: 0.0110
 - val_f1: 0.9915
Epoch 46/200
 - 1s - loss: 0.0113 - val_loss: 0.0104
 - val_f1: 0.9928
Epoch 47/200
 - 1s - loss: 0.0114 - val_loss: 0.0107
 - val_f1: 0.9919
Epoch 48/200
 - 1s - loss: 0.0112 - val_loss: 0.0108
 - val_f1: 0.9924
Epoch 49/200
 - 1s - loss: 0.0113 - val_loss: 0.0106
 - val_f1: 0.9926
Epoch 50/200
 - 1s - loss: 0.0109 - val_loss: 0.0102
 - val_f1: 0.9929
Epoch 51/200
 - 1s - loss: 0.0109 - val_loss: 0.0100
 - val_f1: 0.9931
Epoch 52/200
 - 1s - loss: 0.0109 - val_loss: 0.0104
 - val_f1: 0.9926
Epoch 53/200
 - 1s - loss: 0.0106 - val_loss: 0.0097
 - val_f1: 0.9924
Epoch 54/200
 - 1s - loss: 0.0108 - val_loss: 0.0102
 - val_f1: 0.9929
Epoch 55/200
 - 1s - loss: 0.0106 - val_loss: 0.0103
 - val_f1: 0.9925
Epoch 56/200
 - 1s - loss: 0.0109 - val_loss: 0.0098
 - val_f1: 0.9929
Epoch 57/200
 - 1s - loss: 0.0104 - val_loss: 0.0107
 - val_f1: 0.9921
Epoch 58/200
 - 1s - loss: 0.0106 - val_loss: 0.0108
 - val_f1: 0.9916
Epoch 59/200
 - 1s - loss: 0.0107 - val_loss: 0.0103
 - val_f1: 0.9929
Epoch 60/200
 - 1s - loss: 0.0106 - val_loss: 0.0100
 - val_f1: 0.9925
Epoch 61/200
 - 1s - loss: 0.0102 - val_loss: 0.0099
2019-12-24 01:56:36,640 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep5/ann_model_epoch_60.pickle
 - val_f1: 0.9929
Epoch 62/200
 - 1s - loss: 0.0103 - val_loss: 0.0099
 - val_f1: 0.9932
Epoch 63/200
 - 1s - loss: 0.0102 - val_loss: 0.0099
 - val_f1: 0.9932
Epoch 64/200
 - 1s - loss: 0.0106 - val_loss: 0.0105
 - val_f1: 0.9920
Epoch 65/200
 - 1s - loss: 0.0104 - val_loss: 0.0101
 - val_f1: 0.9926
Epoch 66/200
 - 1s - loss: 0.0104 - val_loss: 0.0110
 - val_f1: 0.9915
Epoch 67/200
 - 1s - loss: 0.0107 - val_loss: 0.0101
 - val_f1: 0.9924
Epoch 68/200
 - 1s - loss: 0.0106 - val_loss: 0.0103
 - val_f1: 0.9928
Epoch 69/200
 - 1s - loss: 0.0098 - val_loss: 0.0098
 - val_f1: 0.9927
Epoch 70/200
 - 1s - loss: 0.0102 - val_loss: 0.0101
 - val_f1: 0.9927
Epoch 71/200
 - 1s - loss: 0.0103 - val_loss: 0.0106
 - val_f1: 0.9929
Epoch 72/200
 - 1s - loss: 0.0100 - val_loss: 0.0103
 - val_f1: 0.9922
Epoch 73/200
 - 1s - loss: 0.0103 - val_loss: 0.0100
 - val_f1: 0.9927
Epoch 74/200
 - 1s - loss: 0.0098 - val_loss: 0.0099
 - val_f1: 0.9928
Epoch 75/200
 - 1s - loss: 0.0101 - val_loss: 0.0096
 - val_f1: 0.9933
Epoch 76/200
 - 1s - loss: 0.0097 - val_loss: 0.0100
 - val_f1: 0.9928
Epoch 77/200
 - 1s - loss: 0.0102 - val_loss: 0.0100
 - val_f1: 0.9930
Epoch 78/200
 - 1s - loss: 0.0098 - val_loss: 0.0098
 - val_f1: 0.9930
Epoch 79/200
 - 1s - loss: 0.0099 - val_loss: 0.0099
 - val_f1: 0.9930
Epoch 80/200
 - 1s - loss: 0.0097 - val_loss: 0.0103
 - val_f1: 0.9919
Epoch 81/200
 - 1s - loss: 0.0098 - val_loss: 0.0107
2019-12-24 01:57:11,661 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep5/ann_model_epoch_80.pickle
 - val_f1: 0.9924
Epoch 82/200
 - 1s - loss: 0.0100 - val_loss: 0.0100
 - val_f1: 0.9929
Epoch 83/200
 - 1s - loss: 0.0093 - val_loss: 0.0099
 - val_f1: 0.9930
Epoch 84/200
 - 1s - loss: 0.0093 - val_loss: 0.0097
 - val_f1: 0.9929
Epoch 85/200
 - 1s - loss: 0.0098 - val_loss: 0.0097
 - val_f1: 0.9936
Epoch 86/200
 - 1s - loss: 0.0097 - val_loss: 0.0100
 - val_f1: 0.9923
Epoch 87/200
 - 1s - loss: 0.0096 - val_loss: 0.0103
 - val_f1: 0.9929
Epoch 88/200
 - 1s - loss: 0.0098 - val_loss: 0.0097
 - val_f1: 0.9925
Epoch 89/200
 - 1s - loss: 0.0095 - val_loss: 0.0096
 - val_f1: 0.9926
Epoch 90/200
 - 1s - loss: 0.0095 - val_loss: 0.0096
 - val_f1: 0.9927
Epoch 91/200
 - 1s - loss: 0.0093 - val_loss: 0.0108
 - val_f1: 0.9914
Epoch 92/200
 - 1s - loss: 0.0095 - val_loss: 0.0103
 - val_f1: 0.9930
Epoch 93/200
 - 1s - loss: 0.0096 - val_loss: 0.0094
 - val_f1: 0.9934
Epoch 94/200
 - 1s - loss: 0.0093 - val_loss: 0.0102
 - val_f1: 0.9926
Epoch 95/200
 - 1s - loss: 0.0093 - val_loss: 0.0094
 - val_f1: 0.9936
Epoch 96/200
 - 1s - loss: 0.0092 - val_loss: 0.0091
 - val_f1: 0.9930
Epoch 97/200
 - 1s - loss: 0.0093 - val_loss: 0.0094
 - val_f1: 0.9927
Epoch 98/200
 - 1s - loss: 0.0089 - val_loss: 0.0101
 - val_f1: 0.9917
Epoch 99/200
 - 1s - loss: 0.0090 - val_loss: 0.0094
 - val_f1: 0.9931
Epoch 100/200
 - 1s - loss: 0.0089 - val_loss: 0.0094
 - val_f1: 0.9928
Epoch 101/200
 - 1s - loss: 0.0092 - val_loss: 0.0097
2019-12-24 01:57:46,715 [INFO] epoch = 100. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep5/ann_model_epoch_100.pickle
 - val_f1: 0.9930
Epoch 102/200
 - 1s - loss: 0.0091 - val_loss: 0.0093
 - val_f1: 0.9935
Epoch 103/200
 - 1s - loss: 0.0090 - val_loss: 0.0093
 - val_f1: 0.9941
Epoch 104/200
 - 1s - loss: 0.0095 - val_loss: 0.0097
 - val_f1: 0.9930
Epoch 105/200
 - 1s - loss: 0.0096 - val_loss: 0.0099
 - val_f1: 0.9927
Epoch 106/200
 - 1s - loss: 0.0091 - val_loss: 0.0095
 - val_f1: 0.9935
Epoch 107/200
 - 1s - loss: 0.0091 - val_loss: 0.0097
 - val_f1: 0.9926
Epoch 108/200
 - 1s - loss: 0.0097 - val_loss: 0.0104
 - val_f1: 0.9931
Epoch 109/200
 - 1s - loss: 0.0092 - val_loss: 0.0096
 - val_f1: 0.9934
Epoch 110/200
 - 1s - loss: 0.0093 - val_loss: 0.0096
 - val_f1: 0.9929
Epoch 111/200
 - 1s - loss: 0.0093 - val_loss: 0.0099
 - val_f1: 0.9924
Epoch 112/200
 - 1s - loss: 0.0090 - val_loss: 0.0093
 - val_f1: 0.9932
Epoch 113/200
 - 1s - loss: 0.0090 - val_loss: 0.0095
 - val_f1: 0.9932
Epoch 114/200
 - 1s - loss: 0.0087 - val_loss: 0.0095
 - val_f1: 0.9934
Epoch 115/200
 - 1s - loss: 0.0089 - val_loss: 0.0099
 - val_f1: 0.9925
Epoch 116/200
 - 1s - loss: 0.0089 - val_loss: 0.0103
 - val_f1: 0.9928
Epoch 117/200
 - 1s - loss: 0.0089 - val_loss: 0.0097
 - val_f1: 0.9931
Epoch 118/200
 - 1s - loss: 0.0090 - val_loss: 0.0104
 - val_f1: 0.9919
Epoch 119/200
 - 1s - loss: 0.0093 - val_loss: 0.0099
 - val_f1: 0.9930
Epoch 120/200
 - 1s - loss: 0.0091 - val_loss: 0.0094
 - val_f1: 0.9932
Epoch 121/200
 - 1s - loss: 0.0087 - val_loss: 0.0096
2019-12-24 01:58:21,585 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep5/ann_model_epoch_120.pickle
 - val_f1: 0.9930
Epoch 122/200
 - 1s - loss: 0.0086 - val_loss: 0.0098
 - val_f1: 0.9929
Epoch 123/200
 - 1s - loss: 0.0091 - val_loss: 0.0103
 - val_f1: 0.9931
Epoch 124/200
 - 1s - loss: 0.0091 - val_loss: 0.0096
 - val_f1: 0.9935
Epoch 125/200
 - 1s - loss: 0.0089 - val_loss: 0.0093
 - val_f1: 0.9932
Epoch 126/200
 - 1s - loss: 0.0087 - val_loss: 0.0093
 - val_f1: 0.9931
Epoch 127/200
 - 1s - loss: 0.0092 - val_loss: 0.0093
 - val_f1: 0.9937
Epoch 128/200
 - 1s - loss: 0.0089 - val_loss: 0.0105
 - val_f1: 0.9917
Epoch 129/200
 - 1s - loss: 0.0085 - val_loss: 0.0096
 - val_f1: 0.9930
Epoch 130/200
 - 1s - loss: 0.0086 - val_loss: 0.0090
 - val_f1: 0.9939
Epoch 131/200
 - 1s - loss: 0.0087 - val_loss: 0.0094
 - val_f1: 0.9934
Epoch 132/200
 - 1s - loss: 0.0087 - val_loss: 0.0096
 - val_f1: 0.9931
Epoch 133/200
 - 1s - loss: 0.0088 - val_loss: 0.0099
 - val_f1: 0.9927
Epoch 134/200
 - 1s - loss: 0.0088 - val_loss: 0.0099
 - val_f1: 0.9925
Epoch 135/200
 - 1s - loss: 0.0090 - val_loss: 0.0094
 - val_f1: 0.9928
Epoch 136/200
 - 1s - loss: 0.0086 - val_loss: 0.0093
 - val_f1: 0.9938
Epoch 137/200
 - 1s - loss: 0.0086 - val_loss: 0.0094
 - val_f1: 0.9932
Epoch 138/200
 - 1s - loss: 0.0085 - val_loss: 0.0100
 - val_f1: 0.9933
Epoch 139/200
 - 1s - loss: 0.0087 - val_loss: 0.0094
 - val_f1: 0.9940
Epoch 140/200
 - 1s - loss: 0.0087 - val_loss: 0.0095
 - val_f1: 0.9935
Epoch 141/200
 - 1s - loss: 0.0084 - val_loss: 0.0096
2019-12-24 01:58:56,548 [INFO] epoch = 140. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep5/ann_model_epoch_140.pickle
 - val_f1: 0.9932
Epoch 142/200
 - 1s - loss: 0.0090 - val_loss: 0.0095
 - val_f1: 0.9932
Epoch 143/200
 - 1s - loss: 0.0088 - val_loss: 0.0096
 - val_f1: 0.9940
Epoch 144/200
 - 1s - loss: 0.0083 - val_loss: 0.0093
 - val_f1: 0.9937
Epoch 145/200
 - 1s - loss: 0.0083 - val_loss: 0.0098
 - val_f1: 0.9933
Epoch 146/200
 - 1s - loss: 0.0088 - val_loss: 0.0099
 - val_f1: 0.9919
Epoch 147/200
 - 1s - loss: 0.0086 - val_loss: 0.0091
 - val_f1: 0.9940
Epoch 148/200
 - 1s - loss: 0.0084 - val_loss: 0.0091
 - val_f1: 0.9932
Epoch 149/200
 - 1s - loss: 0.0083 - val_loss: 0.0095
 - val_f1: 0.9936
Epoch 150/200
 - 1s - loss: 0.0086 - val_loss: 0.0101
 - val_f1: 0.9922
Epoch 151/200
 - 1s - loss: 0.0082 - val_loss: 0.0094
 - val_f1: 0.9936
Epoch 152/200
 - 1s - loss: 0.0084 - val_loss: 0.0097
 - val_f1: 0.9931
Epoch 153/200
 - 1s - loss: 0.0084 - val_loss: 0.0094
 - val_f1: 0.9930
Epoch 154/200
 - 1s - loss: 0.0083 - val_loss: 0.0090
 - val_f1: 0.9935
Epoch 155/200
 - 1s - loss: 0.0083 - val_loss: 0.0092
 - val_f1: 0.9938
Epoch 156/200
 - 1s - loss: 0.0079 - val_loss: 0.0096
 - val_f1: 0.9933
Epoch 157/200
 - 1s - loss: 0.0083 - val_loss: 0.0095
 - val_f1: 0.9930
Epoch 158/200
 - 1s - loss: 0.0081 - val_loss: 0.0091
 - val_f1: 0.9934
Epoch 159/200
 - 1s - loss: 0.0085 - val_loss: 0.0089
 - val_f1: 0.9933
Epoch 160/200
 - 1s - loss: 0.0087 - val_loss: 0.0101
 - val_f1: 0.9929
Epoch 161/200
 - 1s - loss: 0.0081 - val_loss: 0.0100
2019-12-24 01:59:31,541 [INFO] epoch = 160. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep5/ann_model_epoch_160.pickle
 - val_f1: 0.9922
Epoch 162/200
 - 1s - loss: 0.0082 - val_loss: 0.0100
 - val_f1: 0.9928
Epoch 163/200
 - 1s - loss: 0.0086 - val_loss: 0.0087
 - val_f1: 0.9942
Epoch 164/200
 - 1s - loss: 0.0081 - val_loss: 0.0090
 - val_f1: 0.9935
Epoch 165/200
 - 1s - loss: 0.0083 - val_loss: 0.0092
 - val_f1: 0.9936
Epoch 166/200
 - 1s - loss: 0.0082 - val_loss: 0.0097
 - val_f1: 0.9928
Epoch 167/200
 - 1s - loss: 0.0084 - val_loss: 0.0090
 - val_f1: 0.9928
Epoch 168/200
 - 1s - loss: 0.0085 - val_loss: 0.0094
 - val_f1: 0.9931
Epoch 169/200
 - 1s - loss: 0.0080 - val_loss: 0.0109
 - val_f1: 0.9915
Epoch 170/200
 - 1s - loss: 0.0086 - val_loss: 0.0096
 - val_f1: 0.9938
Epoch 171/200
 - 1s - loss: 0.0083 - val_loss: 0.0090
 - val_f1: 0.9932
Epoch 172/200
 - 1s - loss: 0.0084 - val_loss: 0.0088
 - val_f1: 0.9935
Epoch 173/200
 - 1s - loss: 0.0085 - val_loss: 0.0089
 - val_f1: 0.9938
Epoch 174/200
 - 1s - loss: 0.0083 - val_loss: 0.0094
 - val_f1: 0.9935
Epoch 175/200
 - 1s - loss: 0.0084 - val_loss: 0.0093
 - val_f1: 0.9931
Epoch 176/200
 - 1s - loss: 0.0087 - val_loss: 0.0097
 - val_f1: 0.9919
Epoch 177/200
 - 1s - loss: 0.0086 - val_loss: 0.0092
 - val_f1: 0.9940
Epoch 178/200
 - 1s - loss: 0.0086 - val_loss: 0.0093
 - val_f1: 0.9938
Epoch 179/200
 - 1s - loss: 0.0087 - val_loss: 0.0094
 - val_f1: 0.9936
Epoch 180/200
 - 1s - loss: 0.0082 - val_loss: 0.0092
 - val_f1: 0.9938
Epoch 181/200
 - 1s - loss: 0.0087 - val_loss: 0.0093
2019-12-24 02:00:06,539 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_nsl_ae_ann_deep_rep5/ann_model_epoch_180.pickle
 - val_f1: 0.9935
Epoch 182/200
 - 1s - loss: 0.0088 - val_loss: 0.0097
 - val_f1: 0.9934
Epoch 183/200
 - 1s - loss: 0.0091 - val_loss: 0.0094
 - val_f1: 0.9934
Epoch 184/200
 - 1s - loss: 0.0084 - val_loss: 0.0092
 - val_f1: 0.9932
Epoch 185/200
 - 1s - loss: 0.0084 - val_loss: 0.0094
 - val_f1: 0.9932
Epoch 186/200
 - 1s - loss: 0.0088 - val_loss: 0.0100
 - val_f1: 0.9929
Epoch 187/200
 - 1s - loss: 0.0086 - val_loss: 0.0093
 - val_f1: 0.9938
Epoch 188/200
 - 1s - loss: 0.0087 - val_loss: 0.0091
 - val_f1: 0.9933
Epoch 189/200
 - 1s - loss: 0.0084 - val_loss: 0.0095
 - val_f1: 0.9935
Epoch 190/200
 - 1s - loss: 0.0089 - val_loss: 0.0096
 - val_f1: 0.9926
Epoch 191/200
 - 1s - loss: 0.0086 - val_loss: 0.0093
 - val_f1: 0.9921
Epoch 192/200
 - 1s - loss: 0.0087 - val_loss: 0.0093
 - val_f1: 0.9936
Epoch 193/200
 - 1s - loss: 0.0089 - val_loss: 0.0092
 - val_f1: 0.9934
Epoch 194/200
 - 1s - loss: 0.0082 - val_loss: 0.0092
 - val_f1: 0.9934
Epoch 195/200
 - 1s - loss: 0.0088 - val_loss: 0.0098
 - val_f1: 0.9934
Epoch 196/200
 - 1s - loss: 0.0083 - val_loss: 0.0094
 - val_f1: 0.9936
Epoch 197/200
 - 1s - loss: 0.0084 - val_loss: 0.0094
 - val_f1: 0.9934
Epoch 198/200
 - 1s - loss: 0.0085 - val_loss: 0.0091
 - val_f1: 0.9941
Epoch 199/200
 - 1s - loss: 0.0083 - val_loss: 0.0096
 - val_f1: 0.9916
Epoch 200/200
 - 1s - loss: 0.0081 - val_loss: 0.0090
2019-12-24 02:00:40,471 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-24 02:00:44,541 [INFO] Last epoch loss evaluation: train_loss = 0.005857, val_loss = 0.008709
2019-12-24 02:00:44,542 [INFO] Training complete. time_to_train = 633.69 sec, 10.56 min
2019-12-24 02:00:44,564 [INFO] Model saved to results_selected_models/selected_nsl_ae_ann_deep_rep5/best_model.pickle
2019-12-24 02:00:44,755 [INFO] Plot saved to: results_selected_models/selected_nsl_ae_ann_deep_rep5/training_error_history.png
2019-12-24 02:00:44,934 [INFO] Plot saved to: results_selected_models/selected_nsl_ae_ann_deep_rep5/training_f1_history.png
2019-12-24 02:00:44,934 [INFO] Making predictions on training, validation, testing data
2019-12-24 02:00:55,113 [INFO] Evaluating predictions (results)
2019-12-24 02:00:55,665 [INFO] Dataset: Testing. Classification report below
2019-12-24 02:00:55,665 [INFO] 
              precision    recall  f1-score   support

         dos       0.94      0.83      0.88      7458
      normal       0.68      0.97      0.80      9711
       probe       0.81      0.71      0.76      2421
         r2l       0.90      0.01      0.02      2421
         u2r       0.63      0.03      0.06       533

    accuracy                           0.77     22544
   macro avg       0.79      0.51      0.50     22544
weighted avg       0.80      0.77      0.72     22544

2019-12-24 02:00:55,665 [INFO] Overall accuracy (micro avg): 0.7696504613200852
2019-12-24 02:00:56,264 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.7697         0.7697                       0.7697                0.0576                   0.2303  0.7697
1     Macro avg        0.9079         0.7910                       0.5100                0.0776                   0.4900  0.5038
2  Weighted avg        0.8689         0.8022                       0.7697                0.1575                   0.2303  0.7210
2019-12-24 02:00:56,935 [INFO] Dataset: Validation. Classification report below
2019-12-24 02:00:56,935 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00      9186
      normal       0.99      1.00      1.00     13469
       probe       0.98      0.99      0.99      2331
         r2l       0.93      0.80      0.86       199
         u2r       0.67      0.20      0.31        10

    accuracy                           0.99     25195
   macro avg       0.91      0.80      0.83     25195
weighted avg       0.99      0.99      0.99     25195

2019-12-24 02:00:56,935 [INFO] Overall accuracy (micro avg): 0.994244889859099
2019-12-24 02:00:57,605 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9942         0.9942                       0.9942                0.0014                   0.0058  0.9942
1     Macro avg        0.9977         0.9144                       0.7976                0.0019                   0.2024  0.8299
2  Weighted avg        0.9965         0.9941                       0.9942                0.0038                   0.0058  0.9941
2019-12-24 02:01:00,457 [INFO] Dataset: Training. Classification report below
2019-12-24 02:01:00,460 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00     36741
      normal       0.99      1.00      1.00     53874
       probe       0.99      0.99      0.99      9325
         r2l       0.90      0.79      0.84       796
         u2r       0.79      0.45      0.58        42

    accuracy                           0.99    100778
   macro avg       0.94      0.85      0.88    100778
weighted avg       0.99      0.99      0.99    100778

2019-12-24 02:01:00,460 [INFO] Overall accuracy (micro avg): 0.9947309928754292
2019-12-24 02:01:03,519 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9947         0.9947                       0.9947                0.0013                   0.0053  0.9947
1     Macro avg        0.9979         0.9356                       0.8450                0.0018                   0.1550  0.8803
2  Weighted avg        0.9968         0.9946                       0.9947                0.0038                   0.0053  0.9946
2019-12-24 02:01:03,565 [INFO] Results saved to: results_selected_models/selected_nsl_ae_ann_deep_rep5/selected_nsl_ae_ann_deep_rep5_results.xlsx
2019-12-24 02:01:03,566 [INFO] ================= Finished running experiment no. 5 ================= 

2019-12-24 02:01:03,566 [INFO] Created directory: results_selected_models/selected_ids17_ae_ann_deep_rep1
2019-12-24 02:01:03,567 [INFO] Initialized logging. log_filename = results_selected_models/selected_ids17_ae_ann_deep_rep1/run_log.log
2019-12-24 02:01:03,567 [INFO] ================= Running experiment no. 1  ================= 

2019-12-24 02:01:03,567 [INFO] Experiment parameters given below
2019-12-24 02:01:03,567 [INFO] 
{'experiment_num': 1, 'results_dir': 'results_selected_models/selected_ids17_ae_ann_deep_rep1', 'model_type': 'classifier', 'model': 'ae_ann', 'normal_label': 'BENIGN', 'scaling_type': 'NA', 'unsupervised_ratio': 0.25, 'ae_encoder_units': [128, 64, 32], 'ae_encoder_activations': ['relu', 'relu', 'relu'], 'ae_encoder_dropout_rates': [0.2, 0.2, 0.2], 'ae_encoder_l1_param': -1, 'ae_decoder_units': [64, 128], 'ae_decoder_activations': ['relu', 'relu'], 'ae_decoder_dropout_rates': [0.2, 0.2], 'output_activation': 'sigmoid', 'loss_function': 'binary_crossentropy', 'ae_epochs': 200, 'ann_layer_units': [64], 'ann_layer_activations': ['relu'], 'ann_layer_dropout_rates': [0.2], 'ann_epochs': 200, 'early_stop_patience': 50, 'batch_size': 256, 'dataset_dir': '../Datasets/full_datasets/ids2017', 'description': 'selected_ids17_ae_ann_deep_rep1'}
2019-12-24 02:01:03,567 [INFO] Created tensorboard log directory: results_selected_models/selected_ids17_ae_ann_deep_rep1/tf_logs_run_2019_12_24-02_01_03
2019-12-24 02:01:03,567 [INFO] Loading datsets from: ../Datasets/full_datasets/ids2017
2019-12-24 02:01:03,567 [INFO] Reading X, y files
2019-12-24 02:01:03,567 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_train.h5
2019-12-24 02:01:07,372 [INFO] Reading complete. time_to_read=3.80 seconds
2019-12-24 02:01:07,372 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_val.h5
2019-12-24 02:01:08,666 [INFO] Reading complete. time_to_read=1.29 seconds
2019-12-24 02:01:08,666 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_test.h5
2019-12-24 02:01:09,958 [INFO] Reading complete. time_to_read=1.29 seconds
2019-12-24 02:01:09,958 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_train.h5
2019-12-24 02:01:10,210 [INFO] Reading complete. time_to_read=0.25 seconds
2019-12-24 02:01:10,210 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_val.h5
2019-12-24 02:01:10,295 [INFO] Reading complete. time_to_read=0.08 seconds
2019-12-24 02:01:10,295 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_test.h5
2019-12-24 02:01:10,376 [INFO] Reading complete. time_to_read=0.08 seconds
2019-12-24 02:01:13,311 [INFO] Initializing model
2019-12-24 02:01:13,847 [INFO] _________________________________________________________________
2019-12-24 02:01:13,847 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-24 02:01:13,847 [INFO] =================================================================
2019-12-24 02:01:13,847 [INFO] dense_61 (Dense)             (None, 128)               10112     
2019-12-24 02:01:13,847 [INFO] _________________________________________________________________
2019-12-24 02:01:13,847 [INFO] batch_normalization_41 (Batc (None, 128)               512       
2019-12-24 02:01:13,847 [INFO] _________________________________________________________________
2019-12-24 02:01:13,847 [INFO] dropout_41 (Dropout)         (None, 128)               0         
2019-12-24 02:01:13,847 [INFO] _________________________________________________________________
2019-12-24 02:01:13,848 [INFO] dense_62 (Dense)             (None, 64)                8256      
2019-12-24 02:01:13,848 [INFO] _________________________________________________________________
2019-12-24 02:01:13,848 [INFO] batch_normalization_42 (Batc (None, 64)                256       
2019-12-24 02:01:13,848 [INFO] _________________________________________________________________
2019-12-24 02:01:13,848 [INFO] dropout_42 (Dropout)         (None, 64)                0         
2019-12-24 02:01:13,848 [INFO] _________________________________________________________________
2019-12-24 02:01:13,848 [INFO] dense_63 (Dense)             (None, 32)                2080      
2019-12-24 02:01:13,848 [INFO] _________________________________________________________________
2019-12-24 02:01:13,848 [INFO] batch_normalization_43 (Batc (None, 32)                128       
2019-12-24 02:01:13,848 [INFO] _________________________________________________________________
2019-12-24 02:01:13,848 [INFO] dropout_43 (Dropout)         (None, 32)                0         
2019-12-24 02:01:13,848 [INFO] _________________________________________________________________
2019-12-24 02:01:13,849 [INFO] dense_64 (Dense)             (None, 64)                2112      
2019-12-24 02:01:13,849 [INFO] _________________________________________________________________
2019-12-24 02:01:13,849 [INFO] batch_normalization_44 (Batc (None, 64)                256       
2019-12-24 02:01:13,849 [INFO] _________________________________________________________________
2019-12-24 02:01:13,849 [INFO] dropout_44 (Dropout)         (None, 64)                0         
2019-12-24 02:01:13,849 [INFO] _________________________________________________________________
2019-12-24 02:01:13,849 [INFO] dense_65 (Dense)             (None, 128)               8320      
2019-12-24 02:01:13,849 [INFO] _________________________________________________________________
2019-12-24 02:01:13,849 [INFO] batch_normalization_45 (Batc (None, 128)               512       
2019-12-24 02:01:13,849 [INFO] _________________________________________________________________
2019-12-24 02:01:13,849 [INFO] dropout_45 (Dropout)         (None, 128)               0         
2019-12-24 02:01:13,849 [INFO] _________________________________________________________________
2019-12-24 02:01:13,850 [INFO] dense_66 (Dense)             (None, 78)                10062     
2019-12-24 02:01:13,850 [INFO] =================================================================
2019-12-24 02:01:13,850 [INFO] Total params: 42,606
2019-12-24 02:01:13,850 [INFO] Trainable params: 41,774
2019-12-24 02:01:13,850 [INFO] Non-trainable params: 832
2019-12-24 02:01:13,850 [INFO] _________________________________________________________________
2019-12-24 02:01:13,984 [INFO] _________________________________________________________________
2019-12-24 02:01:13,985 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-24 02:01:13,985 [INFO] =================================================================
2019-12-24 02:01:13,985 [INFO] dense_67 (Dense)             (None, 64)                2112      
2019-12-24 02:01:13,985 [INFO] _________________________________________________________________
2019-12-24 02:01:13,985 [INFO] batch_normalization_46 (Batc (None, 64)                256       
2019-12-24 02:01:13,985 [INFO] _________________________________________________________________
2019-12-24 02:01:13,985 [INFO] dropout_46 (Dropout)         (None, 64)                0         
2019-12-24 02:01:13,985 [INFO] _________________________________________________________________
2019-12-24 02:01:13,985 [INFO] dense_68 (Dense)             (None, 12)                780       
2019-12-24 02:01:13,985 [INFO] =================================================================
2019-12-24 02:01:13,986 [INFO] Total params: 3,148
2019-12-24 02:01:13,986 [INFO] Trainable params: 3,020
2019-12-24 02:01:13,986 [INFO] Non-trainable params: 128
2019-12-24 02:01:13,986 [INFO] _________________________________________________________________
2019-12-24 02:01:13,986 [INFO] Training model
2019-12-24 02:01:13,986 [INFO] Splitting train set into 2 sets (unsupervised, supervised), random_seed = None
2019-12-24 02:01:40,544 [INFO] Split sizes (instances). total = 1696684, unsupervised = 424171, supervised = 1272513, unsupervised dataset hash = 5a1efcfd91c543924205577d7493fd94be7f8114
2019-12-24 02:01:40,544 [INFO] Training autoencoder
 - val_f1: 0.9935
Train on 424171 samples, validate on 565562 samples
Epoch 1/200
 - 25s - loss: -3.2020e+00 - val_loss: -3.9933e+00
Epoch 2/200
 - 22s - loss: -3.9144e+00 - val_loss: -4.0614e+00
Epoch 3/200
 - 22s - loss: -3.9687e+00 - val_loss: -4.0812e+00
Epoch 4/200
 - 22s - loss: -3.9967e+00 - val_loss: -4.0974e+00
Epoch 5/200
 - 22s - loss: -4.0167e+00 - val_loss: -4.0984e+00
Epoch 6/200
 - 22s - loss: -4.0284e+00 - val_loss: -4.1078e+00
Epoch 7/200
 - 22s - loss: -4.0380e+00 - val_loss: -4.1137e+00
Epoch 8/200
 - 22s - loss: -4.0469e+00 - val_loss: -4.1171e+00
Epoch 9/200
 - 22s - loss: -4.0545e+00 - val_loss: -4.1236e+00
Epoch 10/200
 - 22s - loss: -4.0601e+00 - val_loss: -4.1258e+00
Epoch 11/200
 - 22s - loss: -4.0634e+00 - val_loss: -4.1287e+00
Epoch 12/200
 - 22s - loss: -4.0681e+00 - val_loss: -4.1295e+00
Epoch 13/200
 - 22s - loss: -4.0730e+00 - val_loss: -4.1328e+00
Epoch 14/200
 - 22s - loss: -4.0743e+00 - val_loss: -4.1329e+00
Epoch 15/200
 - 22s - loss: -4.0762e+00 - val_loss: -4.1337e+00
Epoch 16/200
 - 22s - loss: -4.0810e+00 - val_loss: -4.1334e+00
Epoch 17/200
 - 22s - loss: -4.0812e+00 - val_loss: -4.1360e+00
Epoch 18/200
 - 22s - loss: -4.0833e+00 - val_loss: -4.1373e+00
Epoch 19/200
 - 22s - loss: -4.0848e+00 - val_loss: -4.1367e+00
Epoch 20/200
 - 22s - loss: -4.0862e+00 - val_loss: -4.1374e+00
Epoch 21/200
 - 22s - loss: -4.0883e+00 - val_loss: -4.1389e+00
2019-12-24 02:09:42,424 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep1/ae_model_epoch_20.pickle
Epoch 22/200
 - 22s - loss: -4.0900e+00 - val_loss: -4.1396e+00
Epoch 23/200
 - 22s - loss: -4.0907e+00 - val_loss: -4.1374e+00
Epoch 24/200
 - 22s - loss: -4.0910e+00 - val_loss: -4.1370e+00
Epoch 25/200
 - 22s - loss: -4.0922e+00 - val_loss: -4.1400e+00
Epoch 26/200
 - 22s - loss: -4.0930e+00 - val_loss: -4.1377e+00
Epoch 27/200
 - 22s - loss: -4.0954e+00 - val_loss: -4.1403e+00
Epoch 28/200
 - 22s - loss: -4.0952e+00 - val_loss: -4.1418e+00
Epoch 29/200
 - 22s - loss: -4.0964e+00 - val_loss: -4.1366e+00
Epoch 30/200
 - 22s - loss: -4.0963e+00 - val_loss: -4.1400e+00
Epoch 31/200
 - 22s - loss: -4.0977e+00 - val_loss: -4.1423e+00
Epoch 32/200
 - 22s - loss: -4.0989e+00 - val_loss: -4.1428e+00
Epoch 33/200
 - 22s - loss: -4.0994e+00 - val_loss: -4.1435e+00
Epoch 34/200
 - 22s - loss: -4.1003e+00 - val_loss: -4.1431e+00
Epoch 35/200
 - 22s - loss: -4.0993e+00 - val_loss: -4.1444e+00
Epoch 36/200
 - 22s - loss: -4.1005e+00 - val_loss: -4.1435e+00
Epoch 37/200
 - 22s - loss: -4.1017e+00 - val_loss: -4.1448e+00
Epoch 38/200
 - 22s - loss: -4.1015e+00 - val_loss: -4.1408e+00
Epoch 39/200
 - 22s - loss: -4.1017e+00 - val_loss: -4.1449e+00
Epoch 40/200
 - 22s - loss: -4.1036e+00 - val_loss: -4.1464e+00
Epoch 41/200
 - 22s - loss: -4.1038e+00 - val_loss: -4.1457e+00
2019-12-24 02:17:08,538 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep1/ae_model_epoch_40.pickle
Epoch 42/200
 - 22s - loss: -4.1042e+00 - val_loss: -4.1450e+00
Epoch 43/200
 - 22s - loss: -4.1050e+00 - val_loss: -4.1461e+00
Epoch 44/200
 - 22s - loss: -4.1041e+00 - val_loss: -4.1461e+00
Epoch 45/200
 - 22s - loss: -4.1052e+00 - val_loss: -4.1470e+00
Epoch 46/200
 - 22s - loss: -4.1037e+00 - val_loss: -4.1457e+00
Epoch 47/200
 - 22s - loss: -4.1058e+00 - val_loss: -4.1441e+00
Epoch 48/200
 - 22s - loss: -4.1066e+00 - val_loss: -4.1449e+00
Epoch 49/200
 - 22s - loss: -4.1062e+00 - val_loss: -4.1459e+00
Epoch 50/200
 - 22s - loss: -4.1067e+00 - val_loss: -4.1471e+00
Epoch 51/200
 - 22s - loss: -4.1078e+00 - val_loss: -4.1473e+00
Epoch 52/200
 - 22s - loss: -4.1084e+00 - val_loss: -4.1465e+00
Epoch 53/200
 - 22s - loss: -4.1085e+00 - val_loss: -4.1462e+00
Epoch 54/200
 - 22s - loss: -4.1079e+00 - val_loss: -4.1477e+00
Epoch 55/200
 - 22s - loss: -4.1081e+00 - val_loss: -4.1455e+00
Epoch 56/200
 - 22s - loss: -4.1095e+00 - val_loss: -4.1479e+00
Epoch 57/200
 - 22s - loss: -4.1083e+00 - val_loss: -4.1481e+00
Epoch 58/200
 - 22s - loss: -4.1089e+00 - val_loss: -4.1484e+00
Epoch 59/200
 - 22s - loss: -4.1106e+00 - val_loss: -4.1460e+00
Epoch 60/200
 - 22s - loss: -4.1096e+00 - val_loss: -4.1489e+00
Epoch 61/200
 - 22s - loss: -4.1082e+00 - val_loss: -4.1486e+00
2019-12-24 02:24:34,599 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep1/ae_model_epoch_60.pickle
Epoch 62/200
 - 22s - loss: -4.1112e+00 - val_loss: -4.1490e+00
Epoch 63/200
 - 22s - loss: -4.1101e+00 - val_loss: -4.1490e+00
Epoch 64/200
 - 22s - loss: -4.1108e+00 - val_loss: -4.1494e+00
Epoch 65/200
 - 22s - loss: -4.1116e+00 - val_loss: -4.1498e+00
Epoch 66/200
 - 22s - loss: -4.1116e+00 - val_loss: -4.1486e+00
Epoch 67/200
 - 22s - loss: -4.1128e+00 - val_loss: -4.1496e+00
Epoch 68/200
 - 22s - loss: -4.1116e+00 - val_loss: -4.1502e+00
Epoch 69/200
 - 22s - loss: -4.1123e+00 - val_loss: -4.1498e+00
Epoch 70/200
 - 22s - loss: -4.1125e+00 - val_loss: -4.1502e+00
Epoch 71/200
 - 22s - loss: -4.1132e+00 - val_loss: -4.1499e+00
Epoch 72/200
 - 22s - loss: -4.1116e+00 - val_loss: -4.1504e+00
Epoch 73/200
 - 22s - loss: -4.1120e+00 - val_loss: -4.1501e+00
Epoch 74/200
 - 22s - loss: -4.1130e+00 - val_loss: -4.1492e+00
Epoch 75/200
 - 22s - loss: -4.1132e+00 - val_loss: -4.1488e+00
Epoch 76/200
 - 22s - loss: -4.1131e+00 - val_loss: -4.1486e+00
Epoch 77/200
 - 22s - loss: -4.1138e+00 - val_loss: -4.1501e+00
Epoch 78/200
 - 22s - loss: -4.1129e+00 - val_loss: -4.1500e+00
Epoch 79/200
 - 22s - loss: -4.1131e+00 - val_loss: -4.1499e+00
Epoch 80/200
 - 22s - loss: -4.1138e+00 - val_loss: -4.1502e+00
Epoch 81/200
 - 22s - loss: -4.1136e+00 - val_loss: -4.1498e+00
2019-12-24 02:32:00,856 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep1/ae_model_epoch_80.pickle
Epoch 82/200
 - 22s - loss: -4.1143e+00 - val_loss: -4.1489e+00
Epoch 83/200
 - 22s - loss: -4.1149e+00 - val_loss: -4.1506e+00
Epoch 84/200
 - 22s - loss: -4.1128e+00 - val_loss: -4.1506e+00
Epoch 85/200
 - 22s - loss: -4.1151e+00 - val_loss: -4.1496e+00
Epoch 86/200
 - 22s - loss: -4.1155e+00 - val_loss: -4.1495e+00
Epoch 87/200
 - 22s - loss: -4.1149e+00 - val_loss: -4.1502e+00
Epoch 88/200
 - 22s - loss: -4.1153e+00 - val_loss: -4.1518e+00
Epoch 89/200
 - 22s - loss: -4.1144e+00 - val_loss: -4.1512e+00
Epoch 90/200
 - 22s - loss: -4.1158e+00 - val_loss: -4.1505e+00
Epoch 91/200
 - 22s - loss: -4.1131e+00 - val_loss: -4.1507e+00
Epoch 92/200
 - 22s - loss: -4.1156e+00 - val_loss: -4.1499e+00
Epoch 93/200
 - 22s - loss: -4.1154e+00 - val_loss: -4.1499e+00
Epoch 94/200
 - 22s - loss: -4.1162e+00 - val_loss: -4.1511e+00
Epoch 95/200
 - 22s - loss: -4.1175e+00 - val_loss: -4.1513e+00
Epoch 96/200
 - 22s - loss: -4.1161e+00 - val_loss: -4.1506e+00
Epoch 97/200
 - 22s - loss: -4.1168e+00 - val_loss: -4.1513e+00
Epoch 98/200
 - 22s - loss: -4.1144e+00 - val_loss: -4.1519e+00
Epoch 99/200
 - 22s - loss: -4.1173e+00 - val_loss: -4.1513e+00
Epoch 100/200
 - 22s - loss: -4.1164e+00 - val_loss: -4.1519e+00
Epoch 101/200
 - 22s - loss: -4.1159e+00 - val_loss: -4.1490e+00
2019-12-24 02:39:27,095 [INFO] epoch = 100. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep1/ae_model_epoch_100.pickle
Epoch 102/200
 - 22s - loss: -4.1166e+00 - val_loss: -4.1494e+00
Epoch 103/200
 - 22s - loss: -4.1159e+00 - val_loss: -4.1506e+00
Epoch 104/200
 - 22s - loss: -4.1171e+00 - val_loss: -4.1508e+00
Epoch 105/200
 - 22s - loss: -4.1163e+00 - val_loss: -4.1518e+00
Epoch 106/200
 - 22s - loss: -4.1175e+00 - val_loss: -4.1503e+00
Epoch 107/200
 - 22s - loss: -4.1172e+00 - val_loss: -4.1514e+00
Epoch 108/200
 - 22s - loss: -4.1155e+00 - val_loss: -4.1513e+00
Epoch 109/200
 - 22s - loss: -4.1168e+00 - val_loss: -4.1508e+00
Epoch 110/200
 - 22s - loss: -4.1176e+00 - val_loss: -4.1506e+00
Epoch 111/200
 - 22s - loss: -4.1174e+00 - val_loss: -4.1516e+00
Epoch 112/200
 - 22s - loss: -4.1159e+00 - val_loss: -4.1520e+00
Epoch 113/200
 - 22s - loss: -4.1163e+00 - val_loss: -4.1516e+00
Epoch 114/200
 - 22s - loss: -4.1173e+00 - val_loss: -4.1525e+00
Epoch 115/200
 - 22s - loss: -4.1167e+00 - val_loss: -4.1512e+00
Epoch 116/200
 - 22s - loss: -4.1182e+00 - val_loss: -4.1518e+00
Epoch 117/200
 - 22s - loss: -4.1185e+00 - val_loss: -4.1518e+00
Epoch 118/200
 - 22s - loss: -4.1183e+00 - val_loss: -4.1520e+00
Epoch 119/200
 - 22s - loss: -4.1187e+00 - val_loss: -4.1512e+00
Epoch 120/200
 - 22s - loss: -4.1183e+00 - val_loss: -4.1523e+00
Epoch 121/200
 - 22s - loss: -4.1183e+00 - val_loss: -4.1508e+00
2019-12-24 02:46:53,228 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep1/ae_model_epoch_120.pickle
Epoch 122/200
 - 22s - loss: -4.1182e+00 - val_loss: -4.1515e+00
Epoch 123/200
 - 22s - loss: -4.1189e+00 - val_loss: -4.1500e+00
Epoch 124/200
 - 22s - loss: -4.1188e+00 - val_loss: -4.1520e+00
Epoch 125/200
 - 22s - loss: -4.1187e+00 - val_loss: -4.1524e+00
Epoch 126/200
 - 22s - loss: -4.1189e+00 - val_loss: -4.1516e+00
Epoch 127/200
 - 22s - loss: -4.1193e+00 - val_loss: -4.1531e+00
Epoch 128/200
 - 22s - loss: -4.1187e+00 - val_loss: -4.1525e+00
Epoch 129/200
 - 22s - loss: -4.1194e+00 - val_loss: -4.1529e+00
Epoch 130/200
 - 22s - loss: -4.1195e+00 - val_loss: -4.1524e+00
Epoch 131/200
 - 22s - loss: -4.1181e+00 - val_loss: -4.1510e+00
Epoch 132/200
 - 22s - loss: -4.1194e+00 - val_loss: -4.1520e+00
Epoch 133/200
 - 22s - loss: -4.1200e+00 - val_loss: -4.1521e+00
Epoch 134/200
 - 22s - loss: -4.1186e+00 - val_loss: -4.1523e+00
Epoch 135/200
 - 22s - loss: -4.1189e+00 - val_loss: -4.1525e+00
Epoch 136/200
 - 22s - loss: -4.1196e+00 - val_loss: -4.1528e+00
Epoch 137/200
 - 22s - loss: -4.1177e+00 - val_loss: -4.1530e+00
Epoch 138/200
 - 22s - loss: -4.1200e+00 - val_loss: -4.1533e+00
Epoch 139/200
 - 22s - loss: -4.1201e+00 - val_loss: -4.1526e+00
Epoch 140/200
 - 22s - loss: -4.1193e+00 - val_loss: -4.1530e+00
Epoch 141/200
 - 22s - loss: -4.1195e+00 - val_loss: -4.1518e+00
2019-12-24 02:54:19,295 [INFO] epoch = 140. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep1/ae_model_epoch_140.pickle
Epoch 142/200
 - 22s - loss: -4.1202e+00 - val_loss: -4.1528e+00
Epoch 143/200
 - 22s - loss: -4.1201e+00 - val_loss: -4.1521e+00
Epoch 144/200
 - 22s - loss: -4.1201e+00 - val_loss: -4.1530e+00
Epoch 145/200
 - 22s - loss: -4.1207e+00 - val_loss: -4.1526e+00
Epoch 146/200
 - 22s - loss: -4.1201e+00 - val_loss: -4.1527e+00
Epoch 147/200
 - 22s - loss: -4.1205e+00 - val_loss: -4.1534e+00
Epoch 148/200
 - 22s - loss: -4.1201e+00 - val_loss: -4.1529e+00
Epoch 149/200
 - 23s - loss: -4.1200e+00 - val_loss: -4.1523e+00
Epoch 150/200
 - 22s - loss: -4.1207e+00 - val_loss: -4.1532e+00
Epoch 151/200
 - 22s - loss: -4.1217e+00 - val_loss: -4.1534e+00
Epoch 152/200
 - 22s - loss: -4.1213e+00 - val_loss: -4.1527e+00
Epoch 153/200
 - 22s - loss: -4.1209e+00 - val_loss: -4.1526e+00
Epoch 154/200
 - 22s - loss: -4.1213e+00 - val_loss: -4.1533e+00
Epoch 155/200
 - 22s - loss: -4.1211e+00 - val_loss: -4.1540e+00
Epoch 156/200
 - 22s - loss: -4.1206e+00 - val_loss: -4.1537e+00
Epoch 157/200
 - 22s - loss: -4.1211e+00 - val_loss: -4.1537e+00
Epoch 158/200
 - 22s - loss: -4.1215e+00 - val_loss: -4.1531e+00
Epoch 159/200
 - 22s - loss: -4.1212e+00 - val_loss: -4.1507e+00
Epoch 160/200
 - 22s - loss: -4.1212e+00 - val_loss: -4.1532e+00
Epoch 161/200
 - 22s - loss: -4.1209e+00 - val_loss: -4.1530e+00
2019-12-24 03:01:45,518 [INFO] epoch = 160. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep1/ae_model_epoch_160.pickle
Epoch 162/200
 - 22s - loss: -4.1206e+00 - val_loss: -4.1530e+00
Epoch 163/200
 - 22s - loss: -4.1218e+00 - val_loss: -4.1535e+00
Epoch 164/200
 - 22s - loss: -4.1203e+00 - val_loss: -4.1531e+00
Epoch 165/200
 - 22s - loss: -4.1203e+00 - val_loss: -4.1537e+00
Epoch 166/200
 - 22s - loss: -4.1200e+00 - val_loss: -4.1531e+00
Epoch 167/200
 - 22s - loss: -4.1214e+00 - val_loss: -4.1535e+00
Epoch 168/200
 - 22s - loss: -4.1218e+00 - val_loss: -4.1538e+00
Epoch 169/200
 - 22s - loss: -4.1210e+00 - val_loss: -4.1542e+00
Epoch 170/200
 - 22s - loss: -4.1210e+00 - val_loss: -4.1536e+00
Epoch 171/200
 - 22s - loss: -4.1199e+00 - val_loss: -4.1539e+00
Epoch 172/200
 - 22s - loss: -4.1224e+00 - val_loss: -4.1542e+00
Epoch 173/200
 - 22s - loss: -4.1219e+00 - val_loss: -4.1537e+00
Epoch 174/200
 - 22s - loss: -4.1223e+00 - val_loss: -4.1533e+00
Epoch 175/200
 - 22s - loss: -4.1216e+00 - val_loss: -4.1544e+00
Epoch 176/200
 - 22s - loss: -4.1229e+00 - val_loss: -4.1536e+00
Epoch 177/200
 - 22s - loss: -4.1222e+00 - val_loss: -4.1538e+00
Epoch 178/200
 - 22s - loss: -4.1221e+00 - val_loss: -4.1539e+00
Epoch 179/200
 - 22s - loss: -4.1225e+00 - val_loss: -4.1534e+00
Epoch 180/200
 - 22s - loss: -4.1212e+00 - val_loss: -4.1532e+00
Epoch 181/200
 - 22s - loss: -4.1220e+00 - val_loss: -4.1538e+00
2019-12-24 03:09:11,614 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep1/ae_model_epoch_180.pickle
Epoch 182/200
 - 22s - loss: -4.1231e+00 - val_loss: -4.1544e+00
Epoch 183/200
 - 22s - loss: -4.1223e+00 - val_loss: -4.1539e+00
Epoch 184/200
 - 22s - loss: -4.1227e+00 - val_loss: -4.1544e+00
Epoch 185/200
 - 22s - loss: -4.1219e+00 - val_loss: -4.1544e+00
Epoch 186/200
 - 22s - loss: -4.1232e+00 - val_loss: -4.1551e+00
Epoch 187/200
 - 22s - loss: -4.1226e+00 - val_loss: -4.1539e+00
Epoch 188/200
 - 22s - loss: -4.1233e+00 - val_loss: -4.1544e+00
Epoch 189/200
 - 22s - loss: -4.1224e+00 - val_loss: -4.1547e+00
Epoch 190/200
 - 22s - loss: -4.1229e+00 - val_loss: -4.1540e+00
Epoch 191/200
 - 22s - loss: -4.1233e+00 - val_loss: -4.1545e+00
Epoch 192/200
 - 22s - loss: -4.1229e+00 - val_loss: -4.1536e+00
Epoch 193/200
 - 22s - loss: -4.1223e+00 - val_loss: -4.1540e+00
Epoch 194/200
 - 22s - loss: -4.1238e+00 - val_loss: -4.1538e+00
Epoch 195/200
 - 22s - loss: -4.1229e+00 - val_loss: -4.1552e+00
Epoch 196/200
 - 22s - loss: -4.1211e+00 - val_loss: -4.1541e+00
Epoch 197/200
 - 22s - loss: -4.1234e+00 - val_loss: -4.1547e+00
Epoch 198/200
 - 22s - loss: -4.1207e+00 - val_loss: -4.1543e+00
Epoch 199/200
 - 22s - loss: -4.1229e+00 - val_loss: -4.1541e+00
Epoch 200/200
 - 22s - loss: -4.1230e+00 - val_loss: -4.1547e+00
2019-12-24 03:16:15,096 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-24 03:17:21,893 [INFO] Last epoch loss evaluation: train_loss = -4.164541, val_loss = -4.155164
2019-12-24 03:17:21,893 [INFO] Training autoencoder complete
2019-12-24 03:17:21,893 [INFO] Encoding data for supervised training
2019-12-24 03:18:35,719 [INFO] Encoding complete
2019-12-24 03:18:35,719 [INFO] Training neural network layers (after autoencoder)
Train on 1272513 samples, validate on 565562 samples
Epoch 1/200
 - 20s - loss: 0.0236 - val_loss: 0.0132
 - val_f1: 0.9648
Epoch 2/200
 - 19s - loss: 0.0132 - val_loss: 0.0121
 - val_f1: 0.9715
Epoch 3/200
 - 19s - loss: 0.0121 - val_loss: 0.0116
 - val_f1: 0.9694
Epoch 4/200
 - 19s - loss: 0.0115 - val_loss: 0.0101
 - val_f1: 0.9745
Epoch 5/200
 - 19s - loss: 0.0111 - val_loss: 0.0102
 - val_f1: 0.9731
Epoch 6/200
 - 19s - loss: 0.0109 - val_loss: 0.0102
 - val_f1: 0.9724
Epoch 7/200
 - 19s - loss: 0.0107 - val_loss: 0.0095
 - val_f1: 0.9750
Epoch 8/200
 - 19s - loss: 0.0105 - val_loss: 0.0114
 - val_f1: 0.9745
Epoch 9/200
 - 19s - loss: 0.0104 - val_loss: 0.0091
 - val_f1: 0.9751
Epoch 10/200
 - 19s - loss: 0.0103 - val_loss: 0.0101
 - val_f1: 0.9745
Epoch 11/200
 - 19s - loss: 0.0102 - val_loss: 0.0090
 - val_f1: 0.9753
Epoch 12/200
 - 19s - loss: 0.0101 - val_loss: 0.0088
 - val_f1: 0.9787
Epoch 13/200
 - 19s - loss: 0.0102 - val_loss: 0.0104
 - val_f1: 0.9746
Epoch 14/200
 - 19s - loss: 0.0104 - val_loss: 0.0088
 - val_f1: 0.9760
Epoch 15/200
 - 19s - loss: 0.0101 - val_loss: 0.0088
 - val_f1: 0.9755
Epoch 16/200
 - 19s - loss: 0.0101 - val_loss: 0.0093
 - val_f1: 0.9757
Epoch 17/200
 - 19s - loss: 0.0100 - val_loss: 0.0096
 - val_f1: 0.9744
Epoch 18/200
 - 19s - loss: 0.0100 - val_loss: 0.0086
 - val_f1: 0.9770
Epoch 19/200
 - 19s - loss: 0.0101 - val_loss: 0.0087
 - val_f1: 0.9775
Epoch 20/200
 - 19s - loss: 0.0100 - val_loss: 0.0085
 - val_f1: 0.9767
Epoch 21/200
 - 19s - loss: 0.0100 - val_loss: 0.0095
2019-12-24 03:31:14,402 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep1/ann_model_epoch_20.pickle
 - val_f1: 0.9724
Epoch 22/200
 - 19s - loss: 0.0099 - val_loss: 0.0104
 - val_f1: 0.9738
Epoch 23/200
 - 19s - loss: 0.0099 - val_loss: 0.0094
 - val_f1: 0.9753
Epoch 24/200
 - 19s - loss: 0.0098 - val_loss: 0.0092
 - val_f1: 0.9746
Epoch 25/200
 - 19s - loss: 0.0098 - val_loss: 0.0089
 - val_f1: 0.9769
Epoch 26/200
 - 19s - loss: 0.0097 - val_loss: 0.0096
 - val_f1: 0.9741
Epoch 27/200
 - 19s - loss: 0.0097 - val_loss: 0.0089
 - val_f1: 0.9780
Epoch 28/200
 - 19s - loss: 0.0097 - val_loss: 0.0089
 - val_f1: 0.9766
Epoch 29/200
 - 19s - loss: 0.0097 - val_loss: 0.0102
 - val_f1: 0.9722
Epoch 30/200
 - 19s - loss: 0.0096 - val_loss: 0.0084
 - val_f1: 0.9765
Epoch 31/200
 - 19s - loss: 0.0096 - val_loss: 0.0088
 - val_f1: 0.9771
Epoch 32/200
 - 19s - loss: 0.0096 - val_loss: 0.0083
 - val_f1: 0.9782
Epoch 33/200
 - 19s - loss: 0.0096 - val_loss: 0.0085
 - val_f1: 0.9788
Epoch 34/200
 - 19s - loss: 0.0096 - val_loss: 0.0085
 - val_f1: 0.9754
Epoch 35/200
 - 19s - loss: 0.0096 - val_loss: 0.0088
 - val_f1: 0.9761
Epoch 36/200
 - 19s - loss: 0.0096 - val_loss: 0.0084
 - val_f1: 0.9786
Epoch 37/200
 - 19s - loss: 0.0095 - val_loss: 0.0086
 - val_f1: 0.9753
Epoch 38/200
 - 19s - loss: 0.0095 - val_loss: 0.0086
 - val_f1: 0.9765
Epoch 39/200
 - 19s - loss: 0.0095 - val_loss: 0.0088
 - val_f1: 0.9766
Epoch 40/200
 - 19s - loss: 0.0095 - val_loss: 0.0082
 - val_f1: 0.9774
Epoch 41/200
 - 19s - loss: 0.0095 - val_loss: 0.0090
2019-12-24 03:43:24,857 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep1/ann_model_epoch_40.pickle
 - val_f1: 0.9779
Epoch 42/200
 - 19s - loss: 0.0095 - val_loss: 0.0085
 - val_f1: 0.9749
Epoch 43/200
 - 19s - loss: 0.0094 - val_loss: 0.0092
 - val_f1: 0.9769
Epoch 44/200
 - 19s - loss: 0.0094 - val_loss: 0.0088
 - val_f1: 0.9779
Epoch 45/200
 - 19s - loss: 0.0093 - val_loss: 0.0082
 - val_f1: 0.9767
Epoch 46/200
 - 19s - loss: 0.0093 - val_loss: 0.0081
 - val_f1: 0.9785
Epoch 47/200
 - 19s - loss: 0.0093 - val_loss: 0.0088
 - val_f1: 0.9773
Epoch 48/200
 - 19s - loss: 0.0094 - val_loss: 0.0091
 - val_f1: 0.9775
Epoch 49/200
 - 19s - loss: 0.0093 - val_loss: 0.0082
 - val_f1: 0.9795
Epoch 50/200
 - 19s - loss: 0.0093 - val_loss: 0.0085
 - val_f1: 0.9763
Epoch 51/200
 - 19s - loss: 0.0093 - val_loss: 0.0083
 - val_f1: 0.9765
Epoch 52/200
 - 19s - loss: 0.0093 - val_loss: 0.0087
 - val_f1: 0.9760
Epoch 53/200
 - 19s - loss: 0.0093 - val_loss: 0.0086
 - val_f1: 0.9756
Epoch 54/200
 - 19s - loss: 0.0093 - val_loss: 0.0083
 - val_f1: 0.9782
Epoch 55/200
 - 19s - loss: 0.0093 - val_loss: 0.0087
 - val_f1: 0.9777
Epoch 56/200
 - 19s - loss: 0.0092 - val_loss: 0.0089
 - val_f1: 0.9762
Epoch 57/200
 - 19s - loss: 0.0093 - val_loss: 0.0083
 - val_f1: 0.9776
Epoch 58/200
 - 19s - loss: 0.0092 - val_loss: 0.0082
 - val_f1: 0.9800
Epoch 59/200
 - 19s - loss: 0.0092 - val_loss: 0.0081
 - val_f1: 0.9765
Epoch 60/200
 - 19s - loss: 0.0092 - val_loss: 0.0103
 - val_f1: 0.9748
Epoch 61/200
 - 19s - loss: 0.0092 - val_loss: 0.0083
2019-12-24 03:55:35,965 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep1/ann_model_epoch_60.pickle
 - val_f1: 0.9769
Epoch 62/200
 - 19s - loss: 0.0092 - val_loss: 0.0084
 - val_f1: 0.9799
Epoch 63/200
 - 19s - loss: 0.0092 - val_loss: 0.0091
 - val_f1: 0.9771
Epoch 64/200
 - 19s - loss: 0.0092 - val_loss: 0.0079
 - val_f1: 0.9772
Epoch 65/200
 - 19s - loss: 0.0092 - val_loss: 0.0080
 - val_f1: 0.9781
Epoch 66/200
 - 19s - loss: 0.0091 - val_loss: 0.0079
 - val_f1: 0.9812
Epoch 67/200
 - 19s - loss: 0.0091 - val_loss: 0.0080
 - val_f1: 0.9791
Epoch 68/200
 - 19s - loss: 0.0092 - val_loss: 0.0080
 - val_f1: 0.9800
Epoch 69/200
 - 19s - loss: 0.0091 - val_loss: 0.0086
 - val_f1: 0.9762
Epoch 70/200
 - 19s - loss: 0.0091 - val_loss: 0.0078
 - val_f1: 0.9774
Epoch 71/200
 - 19s - loss: 0.0091 - val_loss: 0.0082
 - val_f1: 0.9786
Epoch 72/200
 - 19s - loss: 0.0091 - val_loss: 0.0080
 - val_f1: 0.9808
Epoch 73/200
 - 19s - loss: 0.0091 - val_loss: 0.0080
 - val_f1: 0.9799
Epoch 74/200
 - 19s - loss: 0.0091 - val_loss: 0.0091
 - val_f1: 0.9772
Epoch 75/200
 - 19s - loss: 0.0091 - val_loss: 0.0086
 - val_f1: 0.9767
Epoch 76/200
 - 19s - loss: 0.0091 - val_loss: 0.0084
 - val_f1: 0.9801
Epoch 77/200
 - 19s - loss: 0.0091 - val_loss: 0.0081
 - val_f1: 0.9788
Epoch 78/200
 - 19s - loss: 0.0091 - val_loss: 0.0080
 - val_f1: 0.9793
Epoch 79/200
 - 19s - loss: 0.0091 - val_loss: 0.0088
 - val_f1: 0.9779
Epoch 80/200
 - 19s - loss: 0.0090 - val_loss: 0.0078
 - val_f1: 0.9797
Epoch 81/200
 - 19s - loss: 0.0091 - val_loss: 0.0084
2019-12-24 04:07:46,402 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep1/ann_model_epoch_80.pickle
 - val_f1: 0.9779
Epoch 82/200
 - 19s - loss: 0.0090 - val_loss: 0.0080
 - val_f1: 0.9770
Epoch 83/200
 - 19s - loss: 0.0091 - val_loss: 0.0080
 - val_f1: 0.9785
Epoch 84/200
 - 19s - loss: 0.0090 - val_loss: 0.0082
 - val_f1: 0.9773
Epoch 85/200
 - 19s - loss: 0.0090 - val_loss: 0.0077
 - val_f1: 0.9808
Epoch 86/200
 - 19s - loss: 0.0090 - val_loss: 0.0085
 - val_f1: 0.9779
Epoch 87/200
 - 19s - loss: 0.0090 - val_loss: 0.0079
 - val_f1: 0.9800
Epoch 88/200
 - 19s - loss: 0.0090 - val_loss: 0.0079
 - val_f1: 0.9785
Epoch 89/200
 - 19s - loss: 0.0090 - val_loss: 0.0080
 - val_f1: 0.9772
Epoch 90/200
 - 19s - loss: 0.0090 - val_loss: 0.0082
 - val_f1: 0.9788
Epoch 91/200
 - 19s - loss: 0.0090 - val_loss: 0.0079
 - val_f1: 0.9811
Epoch 92/200
 - 19s - loss: 0.0089 - val_loss: 0.0084
 - val_f1: 0.9768
Epoch 93/200
 - 19s - loss: 0.0090 - val_loss: 0.0079
 - val_f1: 0.9792
Epoch 94/200
 - 19s - loss: 0.0090 - val_loss: 0.0078
 - val_f1: 0.9800
Epoch 95/200
 - 19s - loss: 0.0090 - val_loss: 0.0080
 - val_f1: 0.9808
Epoch 96/200
 - 19s - loss: 0.0090 - val_loss: 0.0089
 - val_f1: 0.9771
Epoch 97/200
 - 19s - loss: 0.0089 - val_loss: 0.0081
 - val_f1: 0.9775
Epoch 98/200
 - 19s - loss: 0.0090 - val_loss: 0.0081
 - val_f1: 0.9798
Epoch 99/200
 - 19s - loss: 0.0089 - val_loss: 0.0083
 - val_f1: 0.9773
Epoch 100/200
 - 19s - loss: 0.0089 - val_loss: 0.0099
 - val_f1: 0.9760
Epoch 101/200
 - 19s - loss: 0.0090 - val_loss: 0.0077
2019-12-24 04:19:56,992 [INFO] epoch = 100. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep1/ann_model_epoch_100.pickle
 - val_f1: 0.9796
Epoch 102/200
 - 19s - loss: 0.0089 - val_loss: 0.0079
 - val_f1: 0.9800
Epoch 103/200
 - 19s - loss: 0.0089 - val_loss: 0.0089
 - val_f1: 0.9780
Epoch 104/200
 - 19s - loss: 0.0089 - val_loss: 0.0082
 - val_f1: 0.9780
Epoch 105/200
 - 19s - loss: 0.0089 - val_loss: 0.0081
 - val_f1: 0.9779
Epoch 106/200
 - 19s - loss: 0.0089 - val_loss: 0.0096
 - val_f1: 0.9773
Epoch 107/200
 - 19s - loss: 0.0089 - val_loss: 0.0089
 - val_f1: 0.9762
Epoch 108/200
 - 19s - loss: 0.0089 - val_loss: 0.0082
 - val_f1: 0.9778
Epoch 109/200
 - 19s - loss: 0.0088 - val_loss: 0.0078
 - val_f1: 0.9805
Epoch 110/200
 - 19s - loss: 0.0089 - val_loss: 0.0089
 - val_f1: 0.9778
Epoch 111/200
 - 19s - loss: 0.0088 - val_loss: 0.0079
 - val_f1: 0.9778
Epoch 112/200
 - 19s - loss: 0.0089 - val_loss: 0.0082
 - val_f1: 0.9782
Epoch 113/200
 - 19s - loss: 0.0088 - val_loss: 0.0081
 - val_f1: 0.9780
Epoch 114/200
 - 19s - loss: 0.0089 - val_loss: 0.0078
 - val_f1: 0.9817
Epoch 115/200
 - 19s - loss: 0.0089 - val_loss: 0.0079
 - val_f1: 0.9797
Epoch 116/200
 - 19s - loss: 0.0088 - val_loss: 0.0087
 - val_f1: 0.9765
Epoch 117/200
 - 19s - loss: 0.0088 - val_loss: 0.0089
 - val_f1: 0.9774
Epoch 118/200
 - 19s - loss: 0.0088 - val_loss: 0.0094
 - val_f1: 0.9778
Epoch 119/200
 - 19s - loss: 0.0088 - val_loss: 0.0077
 - val_f1: 0.9792
Epoch 120/200
 - 19s - loss: 0.0088 - val_loss: 0.0079
 - val_f1: 0.9800
Epoch 121/200
 - 19s - loss: 0.0088 - val_loss: 0.0078
2019-12-24 04:32:07,736 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep1/ann_model_epoch_120.pickle
 - val_f1: 0.9787
Epoch 122/200
 - 19s - loss: 0.0088 - val_loss: 0.0089
 - val_f1: 0.9777
Epoch 123/200
 - 19s - loss: 0.0088 - val_loss: 0.0094
 - val_f1: 0.9774
Epoch 124/200
 - 19s - loss: 0.0088 - val_loss: 0.0085
 - val_f1: 0.9784
Epoch 125/200
 - 19s - loss: 0.0088 - val_loss: 0.0081
 - val_f1: 0.9783
Epoch 126/200
 - 19s - loss: 0.0088 - val_loss: 0.0082
 - val_f1: 0.9788
Epoch 127/200
 - 19s - loss: 0.0088 - val_loss: 0.0083
 - val_f1: 0.9796
Epoch 128/200
 - 19s - loss: 0.0088 - val_loss: 0.0078
 - val_f1: 0.9771
Epoch 129/200
 - 19s - loss: 0.0088 - val_loss: 0.0081
 - val_f1: 0.9774
Epoch 130/200
 - 19s - loss: 0.0088 - val_loss: 0.0076
 - val_f1: 0.9814
Epoch 131/200
 - 19s - loss: 0.0088 - val_loss: 0.0077
 - val_f1: 0.9808
Epoch 132/200
 - 19s - loss: 0.0087 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 133/200
 - 19s - loss: 0.0088 - val_loss: 0.0090
 - val_f1: 0.9752
Epoch 134/200
 - 19s - loss: 0.0087 - val_loss: 0.0077
 - val_f1: 0.9825
Epoch 135/200
 - 19s - loss: 0.0088 - val_loss: 0.0077
 - val_f1: 0.9806
Epoch 136/200
 - 19s - loss: 0.0088 - val_loss: 0.0083
 - val_f1: 0.9783
Epoch 137/200
 - 19s - loss: 0.0088 - val_loss: 0.0080
 - val_f1: 0.9797
Epoch 138/200
 - 19s - loss: 0.0087 - val_loss: 0.0086
 - val_f1: 0.9773
Epoch 139/200
 - 19s - loss: 0.0088 - val_loss: 0.0081
 - val_f1: 0.9794
Epoch 140/200
 - 19s - loss: 0.0087 - val_loss: 0.0079
 - val_f1: 0.9791
Epoch 141/200
 - 19s - loss: 0.0087 - val_loss: 0.0078
2019-12-24 04:44:17,610 [INFO] epoch = 140. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep1/ann_model_epoch_140.pickle
 - val_f1: 0.9815
Epoch 142/200
 - 19s - loss: 0.0087 - val_loss: 0.0079
 - val_f1: 0.9799
Epoch 143/200
 - 19s - loss: 0.0087 - val_loss: 0.0082
 - val_f1: 0.9779
Epoch 144/200
 - 19s - loss: 0.0087 - val_loss: 0.0089
 - val_f1: 0.9778
Epoch 145/200
 - 19s - loss: 0.0087 - val_loss: 0.0077
 - val_f1: 0.9796
Epoch 146/200
 - 19s - loss: 0.0087 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 147/200
 - 19s - loss: 0.0087 - val_loss: 0.0088
 - val_f1: 0.9780
Epoch 148/200
 - 19s - loss: 0.0087 - val_loss: 0.0079
 - val_f1: 0.9794
Epoch 149/200
 - 19s - loss: 0.0087 - val_loss: 0.0081
 - val_f1: 0.9784
Epoch 150/200
 - 19s - loss: 0.0087 - val_loss: 0.0076
 - val_f1: 0.9823
Epoch 151/200
 - 19s - loss: 0.0087 - val_loss: 0.0082
 - val_f1: 0.9777
Epoch 152/200
 - 19s - loss: 0.0087 - val_loss: 0.0082
 - val_f1: 0.9787
Epoch 153/200
 - 19s - loss: 0.0087 - val_loss: 0.0081
 - val_f1: 0.9790
Epoch 154/200
 - 19s - loss: 0.0087 - val_loss: 0.0076
 - val_f1: 0.9809
Epoch 155/200
 - 19s - loss: 0.0087 - val_loss: 0.0083
 - val_f1: 0.9782
Epoch 156/200
 - 19s - loss: 0.0087 - val_loss: 0.0089
 - val_f1: 0.9758
Epoch 157/200
 - 19s - loss: 0.0087 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 158/200
 - 19s - loss: 0.0087 - val_loss: 0.0089
 - val_f1: 0.9785
Epoch 159/200
 - 19s - loss: 0.0087 - val_loss: 0.0084
 - val_f1: 0.9780
Epoch 160/200
 - 19s - loss: 0.0087 - val_loss: 0.0076
 - val_f1: 0.9778
Epoch 161/200
 - 19s - loss: 0.0087 - val_loss: 0.0077
2019-12-24 04:56:27,291 [INFO] epoch = 160. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep1/ann_model_epoch_160.pickle
 - val_f1: 0.9811
Epoch 162/200
 - 19s - loss: 0.0087 - val_loss: 0.0077
 - val_f1: 0.9818
Epoch 163/200
 - 19s - loss: 0.0086 - val_loss: 0.0081
 - val_f1: 0.9783
Epoch 164/200
 - 19s - loss: 0.0087 - val_loss: 0.0078
 - val_f1: 0.9787
Epoch 165/200
 - 19s - loss: 0.0086 - val_loss: 0.0081
 - val_f1: 0.9781
Epoch 166/200
 - 19s - loss: 0.0086 - val_loss: 0.0077
 - val_f1: 0.9783
Epoch 167/200
 - 19s - loss: 0.0086 - val_loss: 0.0081
 - val_f1: 0.9780
Epoch 168/200
 - 19s - loss: 0.0087 - val_loss: 0.0086
 - val_f1: 0.9784
Epoch 169/200
 - 19s - loss: 0.0087 - val_loss: 0.0078
 - val_f1: 0.9804
Epoch 170/200
 - 19s - loss: 0.0086 - val_loss: 0.0089
 - val_f1: 0.9777
Epoch 171/200
 - 19s - loss: 0.0087 - val_loss: 0.0084
 - val_f1: 0.9780
Epoch 172/200
 - 19s - loss: 0.0086 - val_loss: 0.0077
 - val_f1: 0.9796
Epoch 173/200
 - 19s - loss: 0.0086 - val_loss: 0.0076
 - val_f1: 0.9823
Epoch 174/200
 - 19s - loss: 0.0086 - val_loss: 0.0093
 - val_f1: 0.9775
Epoch 175/200
 - 19s - loss: 0.0086 - val_loss: 0.0076
 - val_f1: 0.9792
Epoch 176/200
 - 19s - loss: 0.0086 - val_loss: 0.0090
 - val_f1: 0.9785
Epoch 177/200
 - 19s - loss: 0.0086 - val_loss: 0.0077
 - val_f1: 0.9811
Epoch 178/200
 - 19s - loss: 0.0086 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 179/200
 - 19s - loss: 0.0086 - val_loss: 0.0076
 - val_f1: 0.9807
Epoch 180/200
 - 19s - loss: 0.0086 - val_loss: 0.0089
 - val_f1: 0.9741
Epoch 181/200
 - 19s - loss: 0.0086 - val_loss: 0.0081
2019-12-24 05:08:38,565 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep1/ann_model_epoch_180.pickle
 - val_f1: 0.9791
Epoch 182/200
 - 19s - loss: 0.0086 - val_loss: 0.0080
 - val_f1: 0.9786
Epoch 183/200
 - 19s - loss: 0.0086 - val_loss: 0.0075
 - val_f1: 0.9795
Epoch 184/200
 - 19s - loss: 0.0086 - val_loss: 0.0085
 - val_f1: 0.9780
Epoch 185/200
 - 19s - loss: 0.0086 - val_loss: 0.0080
 - val_f1: 0.9781
Epoch 186/200
 - 19s - loss: 0.0086 - val_loss: 0.0075
 - val_f1: 0.9820
Epoch 187/200
 - 19s - loss: 0.0086 - val_loss: 0.0076
 - val_f1: 0.9782
Epoch 188/200
 - 19s - loss: 0.0086 - val_loss: 0.0076
 - val_f1: 0.9820
Epoch 189/200
 - 19s - loss: 0.0086 - val_loss: 0.0078
 - val_f1: 0.9794
Epoch 190/200
 - 19s - loss: 0.0086 - val_loss: 0.0076
 - val_f1: 0.9802
Epoch 191/200
 - 19s - loss: 0.0086 - val_loss: 0.0079
 - val_f1: 0.9797
Epoch 192/200
 - 19s - loss: 0.0085 - val_loss: 0.0083
 - val_f1: 0.9779
Epoch 193/200
 - 19s - loss: 0.0086 - val_loss: 0.0081
 - val_f1: 0.9783
Epoch 194/200
 - 19s - loss: 0.0085 - val_loss: 0.0086
 - val_f1: 0.9781
Epoch 195/200
 - 19s - loss: 0.0086 - val_loss: 0.0076
 - val_f1: 0.9800
Epoch 196/200
 - 19s - loss: 0.0085 - val_loss: 0.0079
 - val_f1: 0.9790
Epoch 197/200
 - 19s - loss: 0.0086 - val_loss: 0.0080
 - val_f1: 0.9796
Epoch 198/200
 - 19s - loss: 0.0086 - val_loss: 0.0075
 - val_f1: 0.9816
Epoch 199/200
 - 19s - loss: 0.0086 - val_loss: 0.0078
 - val_f1: 0.9811
Epoch 200/200
 - 19s - loss: 0.0085 - val_loss: 0.0089
2019-12-24 05:20:29,565 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-24 05:21:32,317 [INFO] Last epoch loss evaluation: train_loss = 0.007423, val_loss = 0.007527
2019-12-24 05:21:32,325 [INFO] Training complete. time_to_train = 12018.34 sec, 200.31 min
2019-12-24 05:21:32,349 [INFO] Model saved to results_selected_models/selected_ids17_ae_ann_deep_rep1/best_model.pickle
/home/sunanda/research/ids_experiments/utility.py:268: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
2019-12-24 05:21:32,552 [INFO] Plot saved to: results_selected_models/selected_ids17_ae_ann_deep_rep1/training_error_history.png
/home/sunanda/research/ids_experiments/utility.py:285: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
2019-12-24 05:21:32,744 [INFO] Plot saved to: results_selected_models/selected_ids17_ae_ann_deep_rep1/training_f1_history.png
2019-12-24 05:21:32,744 [INFO] Making predictions on training, validation, testing data
2019-12-24 05:24:46,462 [INFO] Evaluating predictions (results)
/home/sunanda/anaconda3/envs/ml_env_cpu/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2019-12-24 05:25:06,350 [INFO] Dataset: Testing. Classification report below
2019-12-24 05:25:06,350 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       0.99      0.98      0.99    454265
                   Bot       0.97      0.35      0.52       391
                  DDoS       1.00      0.98      0.99     25605
         DoS GoldenEye       0.99      0.95      0.97      2058
              DoS Hulk       0.96      0.97      0.97     46025
      DoS Slowhttptest       0.88      0.93      0.90      1100
         DoS slowloris       0.96      0.95      0.95      1159
           FTP-Patator       0.98      0.99      0.99      1587
              PortScan       0.86      0.97      0.91     31761
           SSH-Patator       0.93      0.98      0.95      1179
Web Attack Brute Force       0.00      0.00      0.00       302
        Web Attack XSS       0.00      0.00      0.00       130

              accuracy                           0.98    565562
             macro avg       0.79      0.76      0.76    565562
          weighted avg       0.98      0.98      0.98    565562

2019-12-24 05:25:06,350 [INFO] Overall accuracy (micro avg): 0.9812929440096754
/home/sunanda/research/ids_experiments/utility.py:307: RuntimeWarning: invalid value encountered in true_divide
  PPV = TP / (TP + FP)    # Precision or positive predictive value
/home/sunanda/research/ids_experiments/utility.py:311: RuntimeWarning: invalid value encountered in true_divide
  FDR = FP / (TP + FP)    # False discovery rate
2019-12-24 05:25:27,735 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9813         0.9813                       0.9813                0.0017                   0.0187  0.9813
1     Macro avg        0.9969         0.7935                       0.7553                0.0038                   0.2447  0.7619
2  Weighted avg        0.9841         0.9815                       0.9813                0.0266                   0.0187  0.9811
2019-12-24 05:25:47,829 [INFO] Dataset: Validation. Classification report below
2019-12-24 05:25:47,830 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       0.99      0.98      0.99    454264
                   Bot       0.98      0.31      0.47       391
                  DDoS       1.00      0.98      0.99     25605
         DoS GoldenEye       0.98      0.94      0.96      2059
              DoS Hulk       0.97      0.97      0.97     46025
      DoS Slowhttptest       0.88      0.92      0.90      1099
         DoS slowloris       0.94      0.96      0.95      1159
           FTP-Patator       0.98      0.99      0.99      1587
              PortScan       0.86      0.97      0.91     31761
           SSH-Patator       0.94      0.97      0.95      1180
Web Attack Brute Force       0.00      0.00      0.00       301
        Web Attack XSS       0.00      0.00      0.00       131

              accuracy                           0.98    565562
             macro avg       0.79      0.75      0.76    565562
          weighted avg       0.98      0.98      0.98    565562

2019-12-24 05:25:47,830 [INFO] Overall accuracy (micro avg): 0.9815475580042506
2019-12-24 05:26:09,482 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9815         0.9815                       0.9815                0.0017                   0.0185  0.9815
1     Macro avg        0.9969         0.7943                       0.7499                0.0037                   0.2501  0.7569
2  Weighted avg        0.9843         0.9818                       0.9815                0.0257                   0.0185  0.9813
2019-12-24 05:27:16,368 [INFO] Dataset: Training. Classification report below
2019-12-24 05:27:16,368 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       0.99      0.98      0.99   1362791
                   Bot       0.99      0.35      0.51      1174
                  DDoS       1.00      0.98      0.99     76815
         DoS GoldenEye       0.98      0.95      0.97      6176
              DoS Hulk       0.97      0.97      0.97    138074
      DoS Slowhttptest       0.89      0.93      0.91      3300
         DoS slowloris       0.95      0.96      0.95      3478
           FTP-Patator       0.98      0.99      0.99      4761
              PortScan       0.86      0.97      0.91     95282
           SSH-Patator       0.95      0.97      0.96      3538
Web Attack Brute Force       0.00      0.00      0.00       904
        Web Attack XSS       0.00      0.00      0.00       391

              accuracy                           0.98   1696684
             macro avg       0.80      0.76      0.76   1696684
          weighted avg       0.98      0.98      0.98   1696684

2019-12-24 05:27:16,368 [INFO] Overall accuracy (micro avg): 0.9815699328808428
2019-12-24 05:28:28,429 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9816         0.9816                       0.9816                0.0017                   0.0184  0.9816
1     Macro avg        0.9969         0.7963                       0.7556                0.0037                   0.2444  0.7628
2  Weighted avg        0.9843         0.9818                       0.9816                0.0259                   0.0184  0.9814
2019-12-24 05:28:28,493 [INFO] Results saved to: results_selected_models/selected_ids17_ae_ann_deep_rep1/selected_ids17_ae_ann_deep_rep1_results.xlsx
2019-12-24 05:28:28,497 [INFO] ================= Finished running experiment no. 1 ================= 

2019-12-24 05:28:28,538 [INFO] Created directory: results_selected_models/selected_ids17_ae_ann_deep_rep2
2019-12-24 05:28:28,539 [INFO] Initialized logging. log_filename = results_selected_models/selected_ids17_ae_ann_deep_rep2/run_log.log
2019-12-24 05:28:28,539 [INFO] ================= Running experiment no. 2  ================= 

2019-12-24 05:28:28,539 [INFO] Experiment parameters given below
2019-12-24 05:28:28,539 [INFO] 
{'experiment_num': 2, 'results_dir': 'results_selected_models/selected_ids17_ae_ann_deep_rep2', 'model_type': 'classifier', 'model': 'ae_ann', 'normal_label': 'BENIGN', 'scaling_type': 'NA', 'unsupervised_ratio': 0.25, 'ae_encoder_units': [128, 64, 32], 'ae_encoder_activations': ['relu', 'relu', 'relu'], 'ae_encoder_dropout_rates': [0.2, 0.2, 0.2], 'ae_encoder_l1_param': -1, 'ae_decoder_units': [64, 128], 'ae_decoder_activations': ['relu', 'relu'], 'ae_decoder_dropout_rates': [0.2, 0.2], 'output_activation': 'sigmoid', 'loss_function': 'binary_crossentropy', 'ae_epochs': 200, 'ann_layer_units': [64], 'ann_layer_activations': ['relu'], 'ann_layer_dropout_rates': [0.2], 'ann_epochs': 200, 'early_stop_patience': 50, 'batch_size': 256, 'dataset_dir': '../Datasets/full_datasets/ids2017', 'description': 'selected_ids17_ae_ann_deep_rep2'}
2019-12-24 05:28:28,539 [INFO] Created tensorboard log directory: results_selected_models/selected_ids17_ae_ann_deep_rep2/tf_logs_run_2019_12_24-05_28_28
2019-12-24 05:28:28,539 [INFO] Loading datsets from: ../Datasets/full_datasets/ids2017
2019-12-24 05:28:28,539 [INFO] Reading X, y files
2019-12-24 05:28:28,540 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_train.h5
2019-12-24 05:28:32,373 [INFO] Reading complete. time_to_read=3.83 seconds
2019-12-24 05:28:32,373 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_val.h5
2019-12-24 05:28:33,663 [INFO] Reading complete. time_to_read=1.29 seconds
2019-12-24 05:28:33,667 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_test.h5
2019-12-24 05:28:34,960 [INFO] Reading complete. time_to_read=1.29 seconds
2019-12-24 05:28:34,960 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_train.h5
2019-12-24 05:28:35,217 [INFO] Reading complete. time_to_read=0.26 seconds
2019-12-24 05:28:35,217 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_val.h5
2019-12-24 05:28:35,299 [INFO] Reading complete. time_to_read=0.08 seconds
2019-12-24 05:28:35,299 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_test.h5
2019-12-24 05:28:35,381 [INFO] Reading complete. time_to_read=0.08 seconds
2019-12-24 05:28:38,328 [INFO] Initializing model
2019-12-24 05:28:38,874 [INFO] _________________________________________________________________
2019-12-24 05:28:38,874 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-24 05:28:38,877 [INFO] =================================================================
2019-12-24 05:28:38,877 [INFO] dense_69 (Dense)             (None, 128)               10112     
2019-12-24 05:28:38,877 [INFO] _________________________________________________________________
2019-12-24 05:28:38,877 [INFO] batch_normalization_47 (Batc (None, 128)               512       
2019-12-24 05:28:38,877 [INFO] _________________________________________________________________
2019-12-24 05:28:38,877 [INFO] dropout_47 (Dropout)         (None, 128)               0         
2019-12-24 05:28:38,877 [INFO] _________________________________________________________________
2019-12-24 05:28:38,877 [INFO] dense_70 (Dense)             (None, 64)                8256      
2019-12-24 05:28:38,877 [INFO] _________________________________________________________________
2019-12-24 05:28:38,878 [INFO] batch_normalization_48 (Batc (None, 64)                256       
2019-12-24 05:28:38,878 [INFO] _________________________________________________________________
2019-12-24 05:28:38,878 [INFO] dropout_48 (Dropout)         (None, 64)                0         
2019-12-24 05:28:38,878 [INFO] _________________________________________________________________
2019-12-24 05:28:38,878 [INFO] dense_71 (Dense)             (None, 32)                2080      
2019-12-24 05:28:38,878 [INFO] _________________________________________________________________
2019-12-24 05:28:38,878 [INFO] batch_normalization_49 (Batc (None, 32)                128       
2019-12-24 05:28:38,878 [INFO] _________________________________________________________________
2019-12-24 05:28:38,878 [INFO] dropout_49 (Dropout)         (None, 32)                0         
2019-12-24 05:28:38,878 [INFO] _________________________________________________________________
2019-12-24 05:28:38,878 [INFO] dense_72 (Dense)             (None, 64)                2112      
2019-12-24 05:28:38,878 [INFO] _________________________________________________________________
2019-12-24 05:28:38,879 [INFO] batch_normalization_50 (Batc (None, 64)                256       
2019-12-24 05:28:38,879 [INFO] _________________________________________________________________
2019-12-24 05:28:38,879 [INFO] dropout_50 (Dropout)         (None, 64)                0         
2019-12-24 05:28:38,879 [INFO] _________________________________________________________________
2019-12-24 05:28:38,879 [INFO] dense_73 (Dense)             (None, 128)               8320      
2019-12-24 05:28:38,879 [INFO] _________________________________________________________________
2019-12-24 05:28:38,879 [INFO] batch_normalization_51 (Batc (None, 128)               512       
2019-12-24 05:28:38,879 [INFO] _________________________________________________________________
2019-12-24 05:28:38,879 [INFO] dropout_51 (Dropout)         (None, 128)               0         
2019-12-24 05:28:38,879 [INFO] _________________________________________________________________
2019-12-24 05:28:38,879 [INFO] dense_74 (Dense)             (None, 78)                10062     
2019-12-24 05:28:38,879 [INFO] =================================================================
2019-12-24 05:28:38,880 [INFO] Total params: 42,606
2019-12-24 05:28:38,880 [INFO] Trainable params: 41,774
2019-12-24 05:28:38,880 [INFO] Non-trainable params: 832
2019-12-24 05:28:38,880 [INFO] _________________________________________________________________
2019-12-24 05:28:39,018 [INFO] _________________________________________________________________
2019-12-24 05:28:39,018 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-24 05:28:39,018 [INFO] =================================================================
2019-12-24 05:28:39,018 [INFO] dense_75 (Dense)             (None, 64)                2112      
2019-12-24 05:28:39,018 [INFO] _________________________________________________________________
2019-12-24 05:28:39,018 [INFO] batch_normalization_52 (Batc (None, 64)                256       
2019-12-24 05:28:39,018 [INFO] _________________________________________________________________
2019-12-24 05:28:39,018 [INFO] dropout_52 (Dropout)         (None, 64)                0         
2019-12-24 05:28:39,018 [INFO] _________________________________________________________________
2019-12-24 05:28:39,018 [INFO] dense_76 (Dense)             (None, 12)                780       
2019-12-24 05:28:39,019 [INFO] =================================================================
2019-12-24 05:28:39,019 [INFO] Total params: 3,148
2019-12-24 05:28:39,019 [INFO] Trainable params: 3,020
2019-12-24 05:28:39,019 [INFO] Non-trainable params: 128
2019-12-24 05:28:39,019 [INFO] _________________________________________________________________
2019-12-24 05:28:39,019 [INFO] Training model
2019-12-24 05:28:39,019 [INFO] Splitting train set into 2 sets (unsupervised, supervised), random_seed = None
2019-12-24 05:29:05,726 [INFO] Split sizes (instances). total = 1696684, unsupervised = 424171, supervised = 1272513, unsupervised dataset hash = 73a92d67cc53a3356ac3456afcacfb7ba63c09e2
2019-12-24 05:29:05,727 [INFO] Training autoencoder
 - val_f1: 0.9778
Train on 424171 samples, validate on 565562 samples
Epoch 1/200
 - 26s - loss: -3.1872e+00 - val_loss: -3.9953e+00
Epoch 2/200
 - 23s - loss: -3.9042e+00 - val_loss: -4.0600e+00
Epoch 3/200
 - 23s - loss: -3.9633e+00 - val_loss: -4.0860e+00
Epoch 4/200
 - 23s - loss: -3.9904e+00 - val_loss: -4.0962e+00
Epoch 5/200
 - 23s - loss: -4.0101e+00 - val_loss: -4.1026e+00
Epoch 6/200
 - 23s - loss: -4.0232e+00 - val_loss: -4.1096e+00
Epoch 7/200
 - 23s - loss: -4.0330e+00 - val_loss: -4.1080e+00
Epoch 8/200
 - 23s - loss: -4.0412e+00 - val_loss: -4.1187e+00
Epoch 9/200
 - 23s - loss: -4.0476e+00 - val_loss: -4.1230e+00
Epoch 10/200
 - 23s - loss: -4.0541e+00 - val_loss: -4.1244e+00
Epoch 11/200
 - 23s - loss: -4.0578e+00 - val_loss: -4.1277e+00
Epoch 12/200
 - 23s - loss: -4.0628e+00 - val_loss: -4.1304e+00
Epoch 13/200
 - 23s - loss: -4.0652e+00 - val_loss: -4.1322e+00
Epoch 14/200
 - 23s - loss: -4.0685e+00 - val_loss: -4.1318e+00
Epoch 15/200
 - 23s - loss: -4.0719e+00 - val_loss: -4.1333e+00
Epoch 16/200
 - 23s - loss: -4.0725e+00 - val_loss: -4.1349e+00
Epoch 17/200
 - 23s - loss: -4.0747e+00 - val_loss: -4.1363e+00
Epoch 18/200
 - 23s - loss: -4.0781e+00 - val_loss: -4.1362e+00
Epoch 19/200
 - 23s - loss: -4.0804e+00 - val_loss: -4.1367e+00
Epoch 20/200
 - 23s - loss: -4.0807e+00 - val_loss: -4.1386e+00
Epoch 21/200
 - 23s - loss: -4.0813e+00 - val_loss: -4.1394e+00
2019-12-24 05:37:25,079 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep2/ae_model_epoch_20.pickle
Epoch 22/200
 - 23s - loss: -4.0829e+00 - val_loss: -4.1371e+00
Epoch 23/200
 - 23s - loss: -4.0842e+00 - val_loss: -4.1377e+00
Epoch 24/200
 - 23s - loss: -4.0860e+00 - val_loss: -4.1408e+00
Epoch 25/200
 - 23s - loss: -4.0856e+00 - val_loss: -4.1400e+00
Epoch 26/200
 - 23s - loss: -4.0874e+00 - val_loss: -4.1411e+00
Epoch 27/200
 - 23s - loss: -4.0883e+00 - val_loss: -4.1417e+00
Epoch 28/200
 - 23s - loss: -4.0891e+00 - val_loss: -4.1407e+00
Epoch 29/200
 - 23s - loss: -4.0888e+00 - val_loss: -4.1413e+00
Epoch 30/200
 - 23s - loss: -4.0897e+00 - val_loss: -4.1423e+00
Epoch 31/200
 - 23s - loss: -4.0885e+00 - val_loss: -4.1421e+00
Epoch 32/200
 - 23s - loss: -4.0903e+00 - val_loss: -4.1430e+00
Epoch 33/200
 - 23s - loss: -4.0916e+00 - val_loss: -4.1431e+00
Epoch 34/200
 - 23s - loss: -4.0917e+00 - val_loss: -4.1435e+00
Epoch 35/200
 - 23s - loss: -4.0924e+00 - val_loss: -4.1442e+00
Epoch 36/200
 - 23s - loss: -4.0935e+00 - val_loss: -4.1449e+00
Epoch 37/200
 - 23s - loss: -4.0933e+00 - val_loss: -4.1436e+00
Epoch 38/200
 - 23s - loss: -4.0948e+00 - val_loss: -4.1455e+00
Epoch 39/200
 - 23s - loss: -4.0942e+00 - val_loss: -4.1462e+00
Epoch 40/200
 - 23s - loss: -4.0953e+00 - val_loss: -4.1447e+00
Epoch 41/200
 - 23s - loss: -4.0959e+00 - val_loss: -4.1453e+00
2019-12-24 05:45:08,537 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep2/ae_model_epoch_40.pickle
Epoch 42/200
 - 23s - loss: -4.0959e+00 - val_loss: -4.1461e+00
Epoch 43/200
 - 23s - loss: -4.0971e+00 - val_loss: -4.1457e+00
Epoch 44/200
 - 23s - loss: -4.0952e+00 - val_loss: -4.1453e+00
Epoch 45/200
 - 23s - loss: -4.0971e+00 - val_loss: -4.1459e+00
Epoch 46/200
 - 23s - loss: -4.0975e+00 - val_loss: -4.1460e+00
Epoch 47/200
 - 23s - loss: -4.0984e+00 - val_loss: -4.1458e+00
Epoch 48/200
 - 23s - loss: -4.0982e+00 - val_loss: -4.1469e+00
Epoch 49/200
 - 23s - loss: -4.0992e+00 - val_loss: -4.1449e+00
Epoch 50/200
 - 23s - loss: -4.0969e+00 - val_loss: -4.1473e+00
Epoch 51/200
 - 23s - loss: -4.0983e+00 - val_loss: -4.1482e+00
Epoch 52/200
 - 23s - loss: -4.0998e+00 - val_loss: -4.1477e+00
Epoch 53/200
 - 23s - loss: -4.0985e+00 - val_loss: -4.1474e+00
Epoch 54/200
 - 23s - loss: -4.0994e+00 - val_loss: -4.1464e+00
Epoch 55/200
 - 23s - loss: -4.1014e+00 - val_loss: -4.1480e+00
Epoch 56/200
 - 23s - loss: -4.0989e+00 - val_loss: -4.1482e+00
Epoch 57/200
 - 23s - loss: -4.1007e+00 - val_loss: -4.1482e+00
Epoch 58/200
 - 23s - loss: -4.1002e+00 - val_loss: -4.1479e+00
Epoch 59/200
 - 23s - loss: -4.1009e+00 - val_loss: -4.1478e+00
Epoch 60/200
 - 23s - loss: -4.1015e+00 - val_loss: -4.1475e+00
Epoch 61/200
 - 23s - loss: -4.1021e+00 - val_loss: -4.1476e+00
2019-12-24 05:52:51,644 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep2/ae_model_epoch_60.pickle
Epoch 62/200
 - 23s - loss: -4.1018e+00 - val_loss: -4.1479e+00
Epoch 63/200
 - 23s - loss: -4.1017e+00 - val_loss: -4.1481e+00
Epoch 64/200
 - 23s - loss: -4.1016e+00 - val_loss: -4.1494e+00
Epoch 65/200
 - 23s - loss: -4.1026e+00 - val_loss: -4.1480e+00
Epoch 66/200
 - 23s - loss: -4.1031e+00 - val_loss: -4.1493e+00
Epoch 67/200
 - 23s - loss: -4.1029e+00 - val_loss: -4.1496e+00
Epoch 68/200
 - 23s - loss: -4.1033e+00 - val_loss: -4.1499e+00
Epoch 69/200
 - 23s - loss: -4.1036e+00 - val_loss: -4.1479e+00
Epoch 70/200
 - 23s - loss: -4.1028e+00 - val_loss: -4.1499e+00
Epoch 71/200
 - 23s - loss: -4.1017e+00 - val_loss: -4.1480e+00
Epoch 72/200
 - 23s - loss: -4.1037e+00 - val_loss: -4.1496e+00
Epoch 73/200
 - 23s - loss: -4.1036e+00 - val_loss: -4.1502e+00
Epoch 74/200
 - 23s - loss: -4.1029e+00 - val_loss: -4.1493e+00
Epoch 75/200
 - 23s - loss: -4.1044e+00 - val_loss: -4.1481e+00
Epoch 76/200
 - 23s - loss: -4.1053e+00 - val_loss: -4.1499e+00
Epoch 77/200
 - 23s - loss: -4.1054e+00 - val_loss: -4.1504e+00
Epoch 78/200
 - 23s - loss: -4.1051e+00 - val_loss: -4.1499e+00
Epoch 79/200
 - 23s - loss: -4.1049e+00 - val_loss: -4.1499e+00
Epoch 80/200
 - 23s - loss: -4.1043e+00 - val_loss: -4.1486e+00
Epoch 81/200
 - 23s - loss: -4.1059e+00 - val_loss: -4.1507e+00
2019-12-24 06:00:34,563 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep2/ae_model_epoch_80.pickle
Epoch 82/200
 - 23s - loss: -4.1059e+00 - val_loss: -4.1499e+00
Epoch 83/200
 - 23s - loss: -4.1055e+00 - val_loss: -4.1481e+00
Epoch 84/200
 - 23s - loss: -4.1030e+00 - val_loss: -4.1498e+00
Epoch 85/200
 - 23s - loss: -4.1058e+00 - val_loss: -4.1484e+00
Epoch 86/200
 - 23s - loss: -4.1066e+00 - val_loss: -4.1499e+00
Epoch 87/200
 - 23s - loss: -4.1061e+00 - val_loss: -4.1507e+00
Epoch 88/200
 - 23s - loss: -4.1071e+00 - val_loss: -4.1520e+00
Epoch 89/200
 - 23s - loss: -4.1068e+00 - val_loss: -4.1489e+00
Epoch 90/200
 - 23s - loss: -4.1064e+00 - val_loss: -4.1508e+00
Epoch 91/200
 - 23s - loss: -4.1073e+00 - val_loss: -4.1509e+00
Epoch 92/200
 - 23s - loss: -4.1070e+00 - val_loss: -4.1505e+00
Epoch 93/200
 - 23s - loss: -4.1070e+00 - val_loss: -4.1516e+00
Epoch 94/200
 - 23s - loss: -4.1081e+00 - val_loss: -4.1516e+00
Epoch 95/200
 - 23s - loss: -4.1078e+00 - val_loss: -4.1506e+00
Epoch 96/200
 - 23s - loss: -4.1077e+00 - val_loss: -4.1503e+00
Epoch 97/200
 - 23s - loss: -4.1074e+00 - val_loss: -4.1502e+00
Epoch 98/200
 - 23s - loss: -4.1082e+00 - val_loss: -4.1511e+00
Epoch 99/200
 - 23s - loss: -4.1078e+00 - val_loss: -4.1503e+00
Epoch 100/200
 - 23s - loss: -4.1087e+00 - val_loss: -4.1508e+00
Epoch 101/200
 - 23s - loss: -4.1085e+00 - val_loss: -4.1520e+00
2019-12-24 06:08:17,422 [INFO] epoch = 100. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep2/ae_model_epoch_100.pickle
Epoch 102/200
 - 23s - loss: -4.1081e+00 - val_loss: -4.1516e+00
Epoch 103/200
 - 23s - loss: -4.1066e+00 - val_loss: -4.1514e+00
Epoch 104/200
 - 23s - loss: -4.1086e+00 - val_loss: -4.1520e+00
Epoch 105/200
 - 23s - loss: -4.1084e+00 - val_loss: -4.1514e+00
Epoch 106/200
 - 23s - loss: -4.1088e+00 - val_loss: -4.1509e+00
Epoch 107/200
 - 23s - loss: -4.1096e+00 - val_loss: -4.1495e+00
Epoch 108/200
 - 23s - loss: -4.1086e+00 - val_loss: -4.1512e+00
Epoch 109/200
 - 23s - loss: -4.1091e+00 - val_loss: -4.1512e+00
Epoch 110/200
 - 23s - loss: -4.1090e+00 - val_loss: -4.1515e+00
Epoch 111/200
 - 23s - loss: -4.1096e+00 - val_loss: -4.1514e+00
Epoch 112/200
 - 23s - loss: -4.1091e+00 - val_loss: -4.1510e+00
Epoch 113/200
 - 23s - loss: -4.1094e+00 - val_loss: -4.1499e+00
Epoch 114/200
 - 23s - loss: -4.1089e+00 - val_loss: -4.1518e+00
Epoch 115/200
 - 23s - loss: -4.1082e+00 - val_loss: -4.1519e+00
Epoch 116/200
 - 23s - loss: -4.1093e+00 - val_loss: -4.1519e+00
Epoch 117/200
 - 23s - loss: -4.1089e+00 - val_loss: -4.1509e+00
Epoch 118/200
 - 23s - loss: -4.1092e+00 - val_loss: -4.1514e+00
Epoch 119/200
 - 23s - loss: -4.1091e+00 - val_loss: -4.1517e+00
Epoch 120/200
 - 23s - loss: -4.1094e+00 - val_loss: -4.1523e+00
Epoch 121/200
 - 23s - loss: -4.1102e+00 - val_loss: -4.1519e+00
2019-12-24 06:16:00,354 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep2/ae_model_epoch_120.pickle
Epoch 122/200
 - 23s - loss: -4.1091e+00 - val_loss: -4.1515e+00
Epoch 123/200
 - 23s - loss: -4.1093e+00 - val_loss: -4.1513e+00
Epoch 124/200
 - 23s - loss: -4.1100e+00 - val_loss: -4.1511e+00
Epoch 125/200
 - 23s - loss: -4.1097e+00 - val_loss: -4.1521e+00
Epoch 126/200
 - 23s - loss: -4.1094e+00 - val_loss: -4.1507e+00
Epoch 127/200
 - 21s - loss: -4.1098e+00 - val_loss: -4.1515e+00
Epoch 128/200
 - 23s - loss: -4.1103e+00 - val_loss: -4.1519e+00
Epoch 129/200
 - 23s - loss: -4.1102e+00 - val_loss: -4.1512e+00
Epoch 130/200
 - 23s - loss: -4.1101e+00 - val_loss: -4.1511e+00
Epoch 131/200
 - 23s - loss: -4.1096e+00 - val_loss: -4.1515e+00
Epoch 132/200
 - 23s - loss: -4.1104e+00 - val_loss: -4.1517e+00
Epoch 133/200
 - 23s - loss: -4.1090e+00 - val_loss: -4.1515e+00
Epoch 134/200
 - 23s - loss: -4.1108e+00 - val_loss: -4.1519e+00
Epoch 135/200
 - 23s - loss: -4.1108e+00 - val_loss: -4.1523e+00
Epoch 136/200
 - 23s - loss: -4.1107e+00 - val_loss: -4.1519e+00
Epoch 137/200
 - 23s - loss: -4.1105e+00 - val_loss: -4.1508e+00
Epoch 138/200
 - 23s - loss: -4.1112e+00 - val_loss: -4.1512e+00
Epoch 139/200
 - 23s - loss: -4.1106e+00 - val_loss: -4.1517e+00
Epoch 140/200
 - 23s - loss: -4.1111e+00 - val_loss: -4.1515e+00
Epoch 141/200
 - 23s - loss: -4.1103e+00 - val_loss: -4.1512e+00
2019-12-24 06:23:41,708 [INFO] epoch = 140. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep2/ae_model_epoch_140.pickle
Epoch 142/200
 - 23s - loss: -4.1107e+00 - val_loss: -4.1529e+00
Epoch 143/200
 - 23s - loss: -4.1101e+00 - val_loss: -4.1524e+00
Epoch 144/200
 - 23s - loss: -4.1118e+00 - val_loss: -4.1530e+00
Epoch 145/200
 - 23s - loss: -4.1110e+00 - val_loss: -4.1519e+00
Epoch 146/200
 - 23s - loss: -4.1111e+00 - val_loss: -4.1508e+00
Epoch 147/200
 - 23s - loss: -4.1107e+00 - val_loss: -4.1527e+00
Epoch 148/200
 - 23s - loss: -4.1108e+00 - val_loss: -4.1530e+00
Epoch 149/200
 - 23s - loss: -4.1106e+00 - val_loss: -4.1521e+00
Epoch 150/200
 - 23s - loss: -4.1115e+00 - val_loss: -4.1526e+00
Epoch 151/200
 - 23s - loss: -4.1116e+00 - val_loss: -4.1510e+00
Epoch 152/200
 - 23s - loss: -4.1110e+00 - val_loss: -4.1521e+00
Epoch 153/200
 - 23s - loss: -4.1111e+00 - val_loss: -4.1529e+00
Epoch 154/200
 - 23s - loss: -4.1116e+00 - val_loss: -4.1520e+00
Epoch 155/200
 - 23s - loss: -4.1111e+00 - val_loss: -4.1513e+00
Epoch 156/200
 - 23s - loss: -4.1110e+00 - val_loss: -4.1531e+00
Epoch 157/200
 - 23s - loss: -4.1115e+00 - val_loss: -4.1535e+00
Epoch 158/200
 - 23s - loss: -4.1122e+00 - val_loss: -4.1522e+00
Epoch 159/200
 - 23s - loss: -4.1115e+00 - val_loss: -4.1531e+00
Epoch 160/200
 - 23s - loss: -4.1121e+00 - val_loss: -4.1521e+00
Epoch 161/200
 - 23s - loss: -4.1121e+00 - val_loss: -4.1530e+00
2019-12-24 06:31:25,275 [INFO] epoch = 160. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep2/ae_model_epoch_160.pickle
Epoch 162/200
 - 23s - loss: -4.1121e+00 - val_loss: -4.1523e+00
Epoch 163/200
 - 23s - loss: -4.1125e+00 - val_loss: -4.1532e+00
Epoch 164/200
 - 23s - loss: -4.1119e+00 - val_loss: -4.1531e+00
Epoch 165/200
 - 23s - loss: -4.1115e+00 - val_loss: -4.1534e+00
Epoch 166/200
 - 23s - loss: -4.1114e+00 - val_loss: -4.1521e+00
Epoch 167/200
 - 23s - loss: -4.1118e+00 - val_loss: -4.1523e+00
Epoch 168/200
 - 23s - loss: -4.1111e+00 - val_loss: -4.1530e+00
Epoch 169/200
 - 23s - loss: -4.1124e+00 - val_loss: -4.1531e+00
Epoch 170/200
 - 23s - loss: -4.1124e+00 - val_loss: -4.1537e+00
Epoch 171/200
 - 23s - loss: -4.1125e+00 - val_loss: -4.1530e+00
Epoch 172/200
 - 23s - loss: -4.1112e+00 - val_loss: -4.1535e+00
Epoch 173/200
 - 23s - loss: -4.1122e+00 - val_loss: -4.1525e+00
Epoch 174/200
 - 23s - loss: -4.1128e+00 - val_loss: -4.1517e+00
Epoch 175/200
 - 23s - loss: -4.1135e+00 - val_loss: -4.1534e+00
Epoch 176/200
 - 23s - loss: -4.1120e+00 - val_loss: -4.1538e+00
Epoch 177/200
 - 23s - loss: -4.1127e+00 - val_loss: -4.1537e+00
Epoch 178/200
 - 23s - loss: -4.1134e+00 - val_loss: -4.1530e+00
Epoch 179/200
 - 23s - loss: -4.1128e+00 - val_loss: -4.1516e+00
Epoch 180/200
 - 23s - loss: -4.1129e+00 - val_loss: -4.1533e+00
Epoch 181/200
 - 23s - loss: -4.1116e+00 - val_loss: -4.1521e+00
2019-12-24 06:39:08,686 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep2/ae_model_epoch_180.pickle
Epoch 182/200
 - 23s - loss: -4.1118e+00 - val_loss: -4.1529e+00
Epoch 183/200
 - 23s - loss: -4.1128e+00 - val_loss: -4.1518e+00
Epoch 184/200
 - 23s - loss: -4.1124e+00 - val_loss: -4.1532e+00
Epoch 185/200
 - 23s - loss: -4.1143e+00 - val_loss: -4.1521e+00
Epoch 186/200
 - 23s - loss: -4.1129e+00 - val_loss: -4.1532e+00
Epoch 187/200
 - 23s - loss: -4.1132e+00 - val_loss: -4.1534e+00
Epoch 188/200
 - 23s - loss: -4.1126e+00 - val_loss: -4.1535e+00
Epoch 189/200
 - 23s - loss: -4.1137e+00 - val_loss: -4.1539e+00
Epoch 190/200
 - 23s - loss: -4.1130e+00 - val_loss: -4.1503e+00
Epoch 191/200
 - 23s - loss: -4.1131e+00 - val_loss: -4.1531e+00
Epoch 192/200
 - 23s - loss: -4.1125e+00 - val_loss: -4.1519e+00
Epoch 193/200
 - 23s - loss: -4.1140e+00 - val_loss: -4.1523e+00
Epoch 194/200
 - 23s - loss: -4.1143e+00 - val_loss: -4.1531e+00
Epoch 195/200
 - 23s - loss: -4.1138e+00 - val_loss: -4.1519e+00
Epoch 196/200
 - 23s - loss: -4.1116e+00 - val_loss: -4.1529e+00
Epoch 197/200
 - 23s - loss: -4.1132e+00 - val_loss: -4.1539e+00
Epoch 198/200
 - 23s - loss: -4.1139e+00 - val_loss: -4.1543e+00
Epoch 199/200
 - 23s - loss: -4.1127e+00 - val_loss: -4.1530e+00
Epoch 200/200
 - 23s - loss: -4.1132e+00 - val_loss: -4.1537e+00
2019-12-24 06:46:28,905 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-24 06:47:37,955 [INFO] Last epoch loss evaluation: train_loss = -4.155492, val_loss = -4.154310
2019-12-24 06:47:37,955 [INFO] Training autoencoder complete
2019-12-24 06:47:37,955 [INFO] Encoding data for supervised training
2019-12-24 06:48:53,450 [INFO] Encoding complete
2019-12-24 06:48:53,451 [INFO] Training neural network layers (after autoencoder)
Train on 1272513 samples, validate on 565562 samples
Epoch 1/200
 - 21s - loss: 0.0239 - val_loss: 0.0128
 - val_f1: 0.9660
Epoch 2/200
 - 20s - loss: 0.0136 - val_loss: 0.0108
 - val_f1: 0.9727
Epoch 3/200
 - 20s - loss: 0.0125 - val_loss: 0.0120
 - val_f1: 0.9700
Epoch 4/200
 - 20s - loss: 0.0117 - val_loss: 0.0095
 - val_f1: 0.9777
Epoch 5/200
 - 20s - loss: 0.0115 - val_loss: 0.0104
 - val_f1: 0.9733
Epoch 6/200
 - 20s - loss: 0.0112 - val_loss: 0.0095
 - val_f1: 0.9760
Epoch 7/200
 - 20s - loss: 0.0110 - val_loss: 0.0103
 - val_f1: 0.9743
Epoch 8/200
 - 20s - loss: 0.0109 - val_loss: 0.0103
 - val_f1: 0.9724
Epoch 9/200
 - 20s - loss: 0.0108 - val_loss: 0.0103
 - val_f1: 0.9738
Epoch 10/200
 - 20s - loss: 0.0108 - val_loss: 0.0089
 - val_f1: 0.9774
Epoch 11/200
 - 20s - loss: 0.0106 - val_loss: 0.0097
 - val_f1: 0.9753
Epoch 12/200
 - 20s - loss: 0.0105 - val_loss: 0.0089
 - val_f1: 0.9751
Epoch 13/200
 - 20s - loss: 0.0104 - val_loss: 0.0085
 - val_f1: 0.9765
Epoch 14/200
 - 20s - loss: 0.0103 - val_loss: 0.0104
 - val_f1: 0.9724
Epoch 15/200
 - 20s - loss: 0.0102 - val_loss: 0.0089
 - val_f1: 0.9782
Epoch 16/200
 - 20s - loss: 0.0102 - val_loss: 0.0089
 - val_f1: 0.9754
Epoch 17/200
 - 20s - loss: 0.0102 - val_loss: 0.0091
 - val_f1: 0.9753
Epoch 18/200
 - 20s - loss: 0.0100 - val_loss: 0.0102
 - val_f1: 0.9739
Epoch 19/200
 - 20s - loss: 0.0100 - val_loss: 0.0103
 - val_f1: 0.9756
Epoch 20/200
 - 20s - loss: 0.0100 - val_loss: 0.0092
 - val_f1: 0.9762
Epoch 21/200
 - 20s - loss: 0.0100 - val_loss: 0.0092
2019-12-24 07:02:03,934 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep2/ann_model_epoch_20.pickle
 - val_f1: 0.9792
Epoch 22/200
 - 20s - loss: 0.0099 - val_loss: 0.0095
 - val_f1: 0.9743
Epoch 23/200
 - 20s - loss: 0.0099 - val_loss: 0.0090
 - val_f1: 0.9759
Epoch 24/200
 - 20s - loss: 0.0099 - val_loss: 0.0085
 - val_f1: 0.9776
Epoch 25/200
 - 20s - loss: 0.0098 - val_loss: 0.0091
 - val_f1: 0.9758
Epoch 26/200
 - 20s - loss: 0.0097 - val_loss: 0.0089
 - val_f1: 0.9767
Epoch 27/200
 - 20s - loss: 0.0097 - val_loss: 0.0082
 - val_f1: 0.9763
Epoch 28/200
 - 20s - loss: 0.0098 - val_loss: 0.0104
 - val_f1: 0.9755
Epoch 29/200
 - 20s - loss: 0.0097 - val_loss: 0.0086
 - val_f1: 0.9788
Epoch 30/200
 - 20s - loss: 0.0096 - val_loss: 0.0103
 - val_f1: 0.9738
Epoch 31/200
 - 20s - loss: 0.0096 - val_loss: 0.0101
 - val_f1: 0.9716
Epoch 32/200
 - 20s - loss: 0.0096 - val_loss: 0.0100
 - val_f1: 0.9761
Epoch 33/200
 - 20s - loss: 0.0096 - val_loss: 0.0082
 - val_f1: 0.9766
Epoch 34/200
 - 20s - loss: 0.0096 - val_loss: 0.0085
 - val_f1: 0.9766
Epoch 35/200
 - 20s - loss: 0.0096 - val_loss: 0.0127
 - val_f1: 0.9631
Epoch 36/200
 - 20s - loss: 0.0096 - val_loss: 0.0082
 - val_f1: 0.9799
Epoch 37/200
 - 20s - loss: 0.0095 - val_loss: 0.0088
 - val_f1: 0.9763
Epoch 38/200
 - 20s - loss: 0.0095 - val_loss: 0.0088
 - val_f1: 0.9776
Epoch 39/200
 - 20s - loss: 0.0095 - val_loss: 0.0086
 - val_f1: 0.9748
Epoch 40/200
 - 20s - loss: 0.0094 - val_loss: 0.0093
 - val_f1: 0.9743
Epoch 41/200
 - 20s - loss: 0.0094 - val_loss: 0.0083
2019-12-24 07:14:43,108 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep2/ann_model_epoch_40.pickle
 - val_f1: 0.9801
Epoch 42/200
 - 20s - loss: 0.0094 - val_loss: 0.0083
 - val_f1: 0.9769
Epoch 43/200
 - 20s - loss: 0.0093 - val_loss: 0.0102
 - val_f1: 0.9740
Epoch 44/200
 - 20s - loss: 0.0094 - val_loss: 0.0090
 - val_f1: 0.9770
Epoch 45/200
 - 20s - loss: 0.0094 - val_loss: 0.0099
 - val_f1: 0.9760
Epoch 46/200
 - 20s - loss: 0.0094 - val_loss: 0.0082
 - val_f1: 0.9788
Epoch 47/200
 - 20s - loss: 0.0093 - val_loss: 0.0095
 - val_f1: 0.9745
Epoch 48/200
 - 20s - loss: 0.0093 - val_loss: 0.0087
 - val_f1: 0.9788
Epoch 49/200
 - 20s - loss: 0.0093 - val_loss: 0.0106
 - val_f1: 0.9752
Epoch 50/200
 - 20s - loss: 0.0093 - val_loss: 0.0080
 - val_f1: 0.9772
Epoch 51/200
 - 20s - loss: 0.0093 - val_loss: 0.0079
 - val_f1: 0.9822
Epoch 52/200
 - 20s - loss: 0.0093 - val_loss: 0.0103
 - val_f1: 0.9760
Epoch 53/200
 - 20s - loss: 0.0093 - val_loss: 0.0089
 - val_f1: 0.9776
Epoch 54/200
 - 20s - loss: 0.0092 - val_loss: 0.0088
 - val_f1: 0.9764
Epoch 55/200
 - 20s - loss: 0.0093 - val_loss: 0.0101
 - val_f1: 0.9761
Epoch 56/200
 - 20s - loss: 0.0092 - val_loss: 0.0082
 - val_f1: 0.9811
Epoch 57/200
 - 20s - loss: 0.0093 - val_loss: 0.0080
 - val_f1: 0.9819
Epoch 58/200
 - 20s - loss: 0.0092 - val_loss: 0.0082
 - val_f1: 0.9810
Epoch 59/200
 - 20s - loss: 0.0092 - val_loss: 0.0082
 - val_f1: 0.9803
Epoch 60/200
 - 20s - loss: 0.0092 - val_loss: 0.0087
 - val_f1: 0.9773
Epoch 61/200
 - 20s - loss: 0.0093 - val_loss: 0.0082
2019-12-24 07:27:23,266 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep2/ann_model_epoch_60.pickle
 - val_f1: 0.9757
Epoch 62/200
 - 20s - loss: 0.0093 - val_loss: 0.0087
 - val_f1: 0.9780
Epoch 63/200
 - 20s - loss: 0.0092 - val_loss: 0.0085
 - val_f1: 0.9776
Epoch 64/200
 - 20s - loss: 0.0092 - val_loss: 0.0081
 - val_f1: 0.9804
Epoch 65/200
 - 20s - loss: 0.0093 - val_loss: 0.0083
 - val_f1: 0.9759
Epoch 66/200
 - 20s - loss: 0.0093 - val_loss: 0.0085
 - val_f1: 0.9770
Epoch 67/200
 - 20s - loss: 0.0092 - val_loss: 0.0078
 - val_f1: 0.9824
Epoch 68/200
 - 20s - loss: 0.0092 - val_loss: 0.0078
 - val_f1: 0.9823
Epoch 69/200
 - 20s - loss: 0.0092 - val_loss: 0.0094
 - val_f1: 0.9748
Epoch 70/200
 - 20s - loss: 0.0092 - val_loss: 0.0086
 - val_f1: 0.9764
Epoch 71/200
 - 20s - loss: 0.0092 - val_loss: 0.0083
 - val_f1: 0.9771
Epoch 72/200
 - 20s - loss: 0.0093 - val_loss: 0.0083
 - val_f1: 0.9802
Epoch 73/200
 - 20s - loss: 0.0091 - val_loss: 0.0077
 - val_f1: 0.9808
Epoch 74/200
 - 20s - loss: 0.0091 - val_loss: 0.0091
 - val_f1: 0.9754
Epoch 75/200
 - 20s - loss: 0.0092 - val_loss: 0.0080
 - val_f1: 0.9810
Epoch 76/200
 - 20s - loss: 0.0092 - val_loss: 0.0082
 - val_f1: 0.9765
Epoch 77/200
 - 20s - loss: 0.0091 - val_loss: 0.0083
 - val_f1: 0.9795
Epoch 78/200
 - 20s - loss: 0.0091 - val_loss: 0.0083
 - val_f1: 0.9768
Epoch 79/200
 - 20s - loss: 0.0092 - val_loss: 0.0088
 - val_f1: 0.9765
Epoch 80/200
 - 20s - loss: 0.0091 - val_loss: 0.0083
 - val_f1: 0.9767
Epoch 81/200
 - 20s - loss: 0.0091 - val_loss: 0.0110
2019-12-24 07:40:02,882 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep2/ann_model_epoch_80.pickle
 - val_f1: 0.9758
Epoch 82/200
 - 20s - loss: 0.0091 - val_loss: 0.0084
 - val_f1: 0.9780
Epoch 83/200
 - 20s - loss: 0.0090 - val_loss: 0.0082
 - val_f1: 0.9794
Epoch 84/200
 - 20s - loss: 0.0091 - val_loss: 0.0088
 - val_f1: 0.9741
Epoch 85/200
 - 20s - loss: 0.0090 - val_loss: 0.0081
 - val_f1: 0.9807
Epoch 86/200
 - 20s - loss: 0.0090 - val_loss: 0.0091
 - val_f1: 0.9749
Epoch 87/200
 - 20s - loss: 0.0090 - val_loss: 0.0076
 - val_f1: 0.9816
Epoch 88/200
 - 20s - loss: 0.0091 - val_loss: 0.0085
 - val_f1: 0.9772
Epoch 89/200
 - 20s - loss: 0.0090 - val_loss: 0.0080
 - val_f1: 0.9789
Epoch 90/200
 - 20s - loss: 0.0091 - val_loss: 0.0081
 - val_f1: 0.9783
Epoch 91/200
 - 20s - loss: 0.0090 - val_loss: 0.0087
 - val_f1: 0.9774
Epoch 92/200
 - 20s - loss: 0.0091 - val_loss: 0.0089
 - val_f1: 0.9765
Epoch 93/200
 - 20s - loss: 0.0090 - val_loss: 0.0078
 - val_f1: 0.9787
Epoch 94/200
 - 20s - loss: 0.0090 - val_loss: 0.0081
 - val_f1: 0.9780
Epoch 95/200
 - 20s - loss: 0.0091 - val_loss: 0.0078
 - val_f1: 0.9817
Epoch 96/200
 - 20s - loss: 0.0091 - val_loss: 0.0079
 - val_f1: 0.9804
Epoch 97/200
 - 20s - loss: 0.0089 - val_loss: 0.0080
 - val_f1: 0.9757
Epoch 98/200
 - 20s - loss: 0.0090 - val_loss: 0.0086
 - val_f1: 0.9778
Epoch 99/200
 - 20s - loss: 0.0090 - val_loss: 0.0086
 - val_f1: 0.9774
Epoch 100/200
 - 20s - loss: 0.0089 - val_loss: 0.0082
 - val_f1: 0.9788
Epoch 101/200
 - 20s - loss: 0.0090 - val_loss: 0.0078
2019-12-24 07:52:43,062 [INFO] epoch = 100. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep2/ann_model_epoch_100.pickle
 - val_f1: 0.9806
Epoch 102/200
 - 20s - loss: 0.0090 - val_loss: 0.0079
 - val_f1: 0.9797
Epoch 103/200
 - 20s - loss: 0.0091 - val_loss: 0.0084
 - val_f1: 0.9775
Epoch 104/200
 - 20s - loss: 0.0090 - val_loss: 0.0078
 - val_f1: 0.9792
Epoch 105/200
 - 20s - loss: 0.0090 - val_loss: 0.0083
 - val_f1: 0.9767
Epoch 106/200
 - 20s - loss: 0.0090 - val_loss: 0.0078
 - val_f1: 0.9815
Epoch 107/200
 - 20s - loss: 0.0089 - val_loss: 0.0080
 - val_f1: 0.9773
Epoch 108/200
 - 20s - loss: 0.0089 - val_loss: 0.0082
 - val_f1: 0.9796
Epoch 109/200
 - 20s - loss: 0.0089 - val_loss: 0.0086
 - val_f1: 0.9803
Epoch 110/200
 - 20s - loss: 0.0089 - val_loss: 0.0080
 - val_f1: 0.9800
Epoch 111/200
 - 20s - loss: 0.0090 - val_loss: 0.0104
 - val_f1: 0.9755
Epoch 112/200
 - 20s - loss: 0.0090 - val_loss: 0.0076
 - val_f1: 0.9821
Epoch 113/200
 - 20s - loss: 0.0089 - val_loss: 0.0094
 - val_f1: 0.9774
Epoch 114/200
 - 20s - loss: 0.0088 - val_loss: 0.0082
 - val_f1: 0.9779
Epoch 115/200
 - 20s - loss: 0.0089 - val_loss: 0.0079
 - val_f1: 0.9768
Epoch 116/200
 - 20s - loss: 0.0089 - val_loss: 0.0082
 - val_f1: 0.9785
Epoch 117/200
 - 20s - loss: 0.0089 - val_loss: 0.0081
 - val_f1: 0.9776
Epoch 118/200
 - 20s - loss: 0.0089 - val_loss: 0.0077
 - val_f1: 0.9797
Epoch 119/200
 - 20s - loss: 0.0089 - val_loss: 0.0083
 - val_f1: 0.9782
Epoch 120/200
 - 20s - loss: 0.0089 - val_loss: 0.0078
 - val_f1: 0.9813
Epoch 121/200
 - 20s - loss: 0.0089 - val_loss: 0.0082
2019-12-24 08:05:23,487 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep2/ann_model_epoch_120.pickle
 - val_f1: 0.9809
Epoch 122/200
 - 20s - loss: 0.0088 - val_loss: 0.0077
 - val_f1: 0.9780
Epoch 123/200
 - 20s - loss: 0.0089 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 124/200
 - 20s - loss: 0.0088 - val_loss: 0.0083
 - val_f1: 0.9793
Epoch 125/200
 - 20s - loss: 0.0088 - val_loss: 0.0093
 - val_f1: 0.9771
Epoch 126/200
 - 20s - loss: 0.0089 - val_loss: 0.0081
 - val_f1: 0.9765
Epoch 127/200
 - 20s - loss: 0.0089 - val_loss: 0.0079
 - val_f1: 0.9798
Epoch 128/200
 - 20s - loss: 0.0089 - val_loss: 0.0084
 - val_f1: 0.9786
Epoch 129/200
 - 20s - loss: 0.0089 - val_loss: 0.0078
 - val_f1: 0.9778
Epoch 130/200
 - 20s - loss: 0.0088 - val_loss: 0.0085
 - val_f1: 0.9777
Epoch 131/200
 - 20s - loss: 0.0088 - val_loss: 0.0079
 - val_f1: 0.9784
Epoch 132/200
 - 20s - loss: 0.0089 - val_loss: 0.0085
 - val_f1: 0.9779
Epoch 133/200
 - 20s - loss: 0.0088 - val_loss: 0.0075
 - val_f1: 0.9781
Epoch 134/200
 - 20s - loss: 0.0089 - val_loss: 0.0090
 - val_f1: 0.9765
Epoch 135/200
 - 20s - loss: 0.0088 - val_loss: 0.0083
 - val_f1: 0.9782
Epoch 136/200
 - 20s - loss: 0.0089 - val_loss: 0.0083
 - val_f1: 0.9804
Epoch 137/200
 - 20s - loss: 0.0089 - val_loss: 0.0082
 - val_f1: 0.9775
Epoch 138/200
 - 20s - loss: 0.0089 - val_loss: 0.0076
 - val_f1: 0.9810
Epoch 139/200
 - 20s - loss: 0.0089 - val_loss: 0.0081
 - val_f1: 0.9783
Epoch 140/200
 - 20s - loss: 0.0089 - val_loss: 0.0076
 - val_f1: 0.9806
Epoch 141/200
 - 20s - loss: 0.0089 - val_loss: 0.0077
2019-12-24 08:18:03,520 [INFO] epoch = 140. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep2/ann_model_epoch_140.pickle
 - val_f1: 0.9817
Epoch 142/200
 - 20s - loss: 0.0088 - val_loss: 0.0077
 - val_f1: 0.9807
Epoch 143/200
 - 20s - loss: 0.0088 - val_loss: 0.0086
 - val_f1: 0.9779
Epoch 144/200
 - 20s - loss: 0.0089 - val_loss: 0.0077
 - val_f1: 0.9796
Epoch 145/200
 - 20s - loss: 0.0088 - val_loss: 0.0081
 - val_f1: 0.9779
Epoch 146/200
 - 20s - loss: 0.0089 - val_loss: 0.0091
 - val_f1: 0.9770
Epoch 147/200
 - 20s - loss: 0.0089 - val_loss: 0.0088
 - val_f1: 0.9778
Epoch 148/200
 - 20s - loss: 0.0088 - val_loss: 0.0078
 - val_f1: 0.9823
Epoch 149/200
 - 20s - loss: 0.0088 - val_loss: 0.0080
 - val_f1: 0.9786
Epoch 150/200
 - 20s - loss: 0.0088 - val_loss: 0.0074
 - val_f1: 0.9817
Epoch 151/200
 - 20s - loss: 0.0088 - val_loss: 0.0079
 - val_f1: 0.9810
Epoch 152/200
 - 20s - loss: 0.0088 - val_loss: 0.0079
 - val_f1: 0.9770
Epoch 153/200
 - 20s - loss: 0.0088 - val_loss: 0.0090
 - val_f1: 0.9759
Epoch 154/200
 - 20s - loss: 0.0088 - val_loss: 0.0102
 - val_f1: 0.9766
Epoch 155/200
 - 20s - loss: 0.0088 - val_loss: 0.0090
 - val_f1: 0.9720
Epoch 156/200
 - 20s - loss: 0.0089 - val_loss: 0.0083
 - val_f1: 0.9770
Epoch 157/200
 - 20s - loss: 0.0088 - val_loss: 0.0085
 - val_f1: 0.9768
Epoch 158/200
 - 20s - loss: 0.0088 - val_loss: 0.0078
 - val_f1: 0.9812
Epoch 159/200
 - 20s - loss: 0.0088 - val_loss: 0.0080
 - val_f1: 0.9791
Epoch 160/200
 - 20s - loss: 0.0088 - val_loss: 0.0077
 - val_f1: 0.9785
Epoch 161/200
 - 20s - loss: 0.0088 - val_loss: 0.0079
2019-12-24 08:30:43,345 [INFO] epoch = 160. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep2/ann_model_epoch_160.pickle
 - val_f1: 0.9777
Epoch 162/200
 - 20s - loss: 0.0088 - val_loss: 0.0079
 - val_f1: 0.9769
Epoch 163/200
 - 20s - loss: 0.0089 - val_loss: 0.0080
 - val_f1: 0.9790
Epoch 164/200
 - 20s - loss: 0.0088 - val_loss: 0.0076
 - val_f1: 0.9798
Epoch 165/200
 - 20s - loss: 0.0087 - val_loss: 0.0074
 - val_f1: 0.9815
Epoch 166/200
 - 20s - loss: 0.0087 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 167/200
 - 20s - loss: 0.0088 - val_loss: 0.0077
 - val_f1: 0.9791
Epoch 168/200
 - 20s - loss: 0.0087 - val_loss: 0.0088
 - val_f1: 0.9760
Epoch 169/200
 - 20s - loss: 0.0088 - val_loss: 0.0080
 - val_f1: 0.9794
Epoch 170/200
 - 20s - loss: 0.0088 - val_loss: 0.0078
 - val_f1: 0.9777
Epoch 171/200
 - 20s - loss: 0.0087 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 172/200
 - 20s - loss: 0.0087 - val_loss: 0.0082
 - val_f1: 0.9786
Epoch 173/200
 - 20s - loss: 0.0088 - val_loss: 0.0084
 - val_f1: 0.9810
Epoch 174/200
 - 20s - loss: 0.0088 - val_loss: 0.0081
 - val_f1: 0.9770
Epoch 175/200
 - 20s - loss: 0.0087 - val_loss: 0.0078
 - val_f1: 0.9778
Epoch 176/200
 - 20s - loss: 0.0088 - val_loss: 0.0077
 - val_f1: 0.9777
Epoch 177/200
 - 20s - loss: 0.0088 - val_loss: 0.0075
 - val_f1: 0.9809
Epoch 178/200
 - 20s - loss: 0.0087 - val_loss: 0.0073
 - val_f1: 0.9815
Epoch 179/200
 - 20s - loss: 0.0087 - val_loss: 0.0097
 - val_f1: 0.9759
Epoch 180/200
 - 20s - loss: 0.0088 - val_loss: 0.0079
 - val_f1: 0.9771
Epoch 181/200
 - 20s - loss: 0.0087 - val_loss: 0.0077
2019-12-24 08:43:23,857 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep2/ann_model_epoch_180.pickle
 - val_f1: 0.9795
Epoch 182/200
 - 20s - loss: 0.0087 - val_loss: 0.0080
 - val_f1: 0.9770
Epoch 183/200
 - 20s - loss: 0.0087 - val_loss: 0.0086
 - val_f1: 0.9771
Epoch 184/200
 - 20s - loss: 0.0087 - val_loss: 0.0079
 - val_f1: 0.9803
Epoch 185/200
 - 20s - loss: 0.0088 - val_loss: 0.0091
 - val_f1: 0.9752
Epoch 186/200
 - 20s - loss: 0.0087 - val_loss: 0.0078
 - val_f1: 0.9800
Epoch 187/200
 - 20s - loss: 0.0087 - val_loss: 0.0074
 - val_f1: 0.9807
Epoch 188/200
 - 20s - loss: 0.0087 - val_loss: 0.0092
 - val_f1: 0.9770
Epoch 189/200
 - 20s - loss: 0.0088 - val_loss: 0.0099
 - val_f1: 0.9749
Epoch 190/200
 - 20s - loss: 0.0088 - val_loss: 0.0089
 - val_f1: 0.9753
Epoch 191/200
 - 20s - loss: 0.0088 - val_loss: 0.0076
 - val_f1: 0.9795
Epoch 192/200
 - 20s - loss: 0.0087 - val_loss: 0.0082
 - val_f1: 0.9782
Epoch 193/200
 - 20s - loss: 0.0087 - val_loss: 0.0074
 - val_f1: 0.9809
Epoch 194/200
 - 20s - loss: 0.0087 - val_loss: 0.0077
 - val_f1: 0.9814
Epoch 195/200
 - 20s - loss: 0.0087 - val_loss: 0.0081
 - val_f1: 0.9810
Epoch 196/200
 - 20s - loss: 0.0087 - val_loss: 0.0088
 - val_f1: 0.9775
Epoch 197/200
 - 20s - loss: 0.0087 - val_loss: 0.0085
 - val_f1: 0.9774
Epoch 198/200
 - 20s - loss: 0.0087 - val_loss: 0.0082
 - val_f1: 0.9773
Epoch 199/200
 - 20s - loss: 0.0087 - val_loss: 0.0079
 - val_f1: 0.9795
Epoch 200/200
 - 20s - loss: 0.0086 - val_loss: 0.0086
2019-12-24 08:55:44,890 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-24 08:56:50,822 [INFO] Last epoch loss evaluation: train_loss = 0.007279, val_loss = 0.007349
2019-12-24 08:56:50,830 [INFO] Training complete. time_to_train = 12491.81 sec, 208.20 min
2019-12-24 08:56:50,855 [INFO] Model saved to results_selected_models/selected_ids17_ae_ann_deep_rep2/best_model.pickle
/home/sunanda/research/ids_experiments/utility.py:268: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
2019-12-24 08:56:51,052 [INFO] Plot saved to: results_selected_models/selected_ids17_ae_ann_deep_rep2/training_error_history.png
/home/sunanda/research/ids_experiments/utility.py:285: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
2019-12-24 08:56:51,249 [INFO] Plot saved to: results_selected_models/selected_ids17_ae_ann_deep_rep2/training_f1_history.png
2019-12-24 08:56:51,249 [INFO] Making predictions on training, validation, testing data
2019-12-24 09:00:13,085 [INFO] Evaluating predictions (results)
/home/sunanda/anaconda3/envs/ml_env_cpu/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2019-12-24 09:00:32,948 [INFO] Dataset: Testing. Classification report below
2019-12-24 09:00:32,948 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       0.99      0.99      0.99    454265
                   Bot       0.97      0.35      0.52       391
                  DDoS       1.00      0.98      0.99     25605
         DoS GoldenEye       0.98      0.95      0.96      2058
              DoS Hulk       0.96      0.97      0.97     46025
      DoS Slowhttptest       0.87      0.96      0.92      1100
         DoS slowloris       0.97      0.93      0.95      1159
           FTP-Patator       0.98      0.99      0.98      1587
              PortScan       0.90      0.93      0.91     31761
           SSH-Patator       0.87      0.98      0.92      1179
Web Attack Brute Force       0.00      0.00      0.00       302
        Web Attack XSS       0.00      0.00      0.00       130

              accuracy                           0.98    565562
             macro avg       0.79      0.75      0.76    565562
          weighted avg       0.98      0.98      0.98    565562

2019-12-24 09:00:32,948 [INFO] Overall accuracy (micro avg): 0.9814043376323021
/home/sunanda/research/ids_experiments/utility.py:307: RuntimeWarning: invalid value encountered in true_divide
  PPV = TP / (TP + FP)    # Precision or positive predictive value
/home/sunanda/research/ids_experiments/utility.py:311: RuntimeWarning: invalid value encountered in true_divide
  FDR = FP / (TP + FP)    # False discovery rate
/home/sunanda/research/ids_experiments/utility.py:313: RuntimeWarning: invalid value encountered in true_divide
  F1 = (2 * PPV * TPR) / (PPV + TPR)  # F1 score
2019-12-24 09:00:54,304 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9814         0.9814                       0.9814                0.0017                   0.0186  0.9814
1     Macro avg        0.9969         0.7912                       0.7532                0.0045                   0.2468  0.7596
2  Weighted avg        0.9842         0.9809                       0.9814                0.0358                   0.0186  0.9810
2019-12-24 09:01:14,378 [INFO] Dataset: Validation. Classification report below
2019-12-24 09:01:14,378 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       0.99      0.99      0.99    454264
                   Bot       0.98      0.31      0.47       391
                  DDoS       1.00      0.98      0.99     25605
         DoS GoldenEye       0.97      0.94      0.96      2059
              DoS Hulk       0.96      0.97      0.97     46025
      DoS Slowhttptest       0.88      0.96      0.92      1099
         DoS slowloris       0.96      0.93      0.95      1159
           FTP-Patator       0.97      0.99      0.98      1587
              PortScan       0.90      0.93      0.91     31761
           SSH-Patator       0.88      0.97      0.93      1180
Web Attack Brute Force       0.00      0.00      0.00       301
        Web Attack XSS       0.00      0.00      0.00       131

              accuracy                           0.98    565562
             macro avg       0.79      0.75      0.75    565562
          weighted avg       0.98      0.98      0.98    565562

2019-12-24 09:01:14,378 [INFO] Overall accuracy (micro avg): 0.9815174994076689
2019-12-24 09:01:35,994 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9815         0.9815                       0.9815                0.0017                   0.0185  0.9815
1     Macro avg        0.9969         0.7919                       0.7475                0.0045                   0.2525  0.7545
2  Weighted avg        0.9843         0.9810                       0.9815                0.0357                   0.0185  0.9811
2019-12-24 09:02:42,899 [INFO] Dataset: Training. Classification report below
2019-12-24 09:02:42,899 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       0.99      0.99      0.99   1362791
                   Bot       0.99      0.35      0.51      1174
                  DDoS       1.00      0.98      0.99     76815
         DoS GoldenEye       0.97      0.94      0.96      6176
              DoS Hulk       0.96      0.97      0.97    138074
      DoS Slowhttptest       0.89      0.96      0.92      3300
         DoS slowloris       0.97      0.94      0.96      3478
           FTP-Patator       0.98      0.99      0.98      4761
              PortScan       0.90      0.93      0.92     95282
           SSH-Patator       0.89      0.98      0.93      3538
Web Attack Brute Force       0.00      0.00      0.00       904
        Web Attack XSS       0.00      0.00      0.00       391

              accuracy                           0.98   1696684
             macro avg       0.79      0.75      0.76   1696684
          weighted avg       0.98      0.98      0.98   1696684

2019-12-24 09:02:42,899 [INFO] Overall accuracy (micro avg): 0.9818610890419194
2019-12-24 09:03:55,012 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9819         0.9819                       0.9819                0.0016                   0.0181  0.9819
1     Macro avg        0.9970         0.7947                       0.7529                0.0044                   0.2471  0.7607
2  Weighted avg        0.9846         0.9813                       0.9819                0.0352                   0.0181  0.9815
2019-12-24 09:03:55,077 [INFO] Results saved to: results_selected_models/selected_ids17_ae_ann_deep_rep2/selected_ids17_ae_ann_deep_rep2_results.xlsx
2019-12-24 09:03:55,080 [INFO] ================= Finished running experiment no. 2 ================= 

2019-12-24 09:03:55,122 [INFO] Created directory: results_selected_models/selected_ids17_ae_ann_deep_rep3
2019-12-24 09:03:55,123 [INFO] Initialized logging. log_filename = results_selected_models/selected_ids17_ae_ann_deep_rep3/run_log.log
2019-12-24 09:03:55,123 [INFO] ================= Running experiment no. 3  ================= 

2019-12-24 09:03:55,123 [INFO] Experiment parameters given below
2019-12-24 09:03:55,123 [INFO] 
{'experiment_num': 3, 'results_dir': 'results_selected_models/selected_ids17_ae_ann_deep_rep3', 'model_type': 'classifier', 'model': 'ae_ann', 'normal_label': 'BENIGN', 'scaling_type': 'NA', 'unsupervised_ratio': 0.25, 'ae_encoder_units': [128, 64, 32], 'ae_encoder_activations': ['relu', 'relu', 'relu'], 'ae_encoder_dropout_rates': [0.2, 0.2, 0.2], 'ae_encoder_l1_param': -1, 'ae_decoder_units': [64, 128], 'ae_decoder_activations': ['relu', 'relu'], 'ae_decoder_dropout_rates': [0.2, 0.2], 'output_activation': 'sigmoid', 'loss_function': 'binary_crossentropy', 'ae_epochs': 200, 'ann_layer_units': [64], 'ann_layer_activations': ['relu'], 'ann_layer_dropout_rates': [0.2], 'ann_epochs': 200, 'early_stop_patience': 50, 'batch_size': 256, 'dataset_dir': '../Datasets/full_datasets/ids2017', 'description': 'selected_ids17_ae_ann_deep_rep3'}
2019-12-24 09:03:55,123 [INFO] Created tensorboard log directory: results_selected_models/selected_ids17_ae_ann_deep_rep3/tf_logs_run_2019_12_24-09_03_55
2019-12-24 09:03:55,123 [INFO] Loading datsets from: ../Datasets/full_datasets/ids2017
2019-12-24 09:03:55,124 [INFO] Reading X, y files
2019-12-24 09:03:55,124 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_train.h5
2019-12-24 09:03:58,923 [INFO] Reading complete. time_to_read=3.80 seconds
2019-12-24 09:03:58,923 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_val.h5
2019-12-24 09:04:00,210 [INFO] Reading complete. time_to_read=1.29 seconds
2019-12-24 09:04:00,211 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_test.h5
2019-12-24 09:04:01,499 [INFO] Reading complete. time_to_read=1.29 seconds
2019-12-24 09:04:01,499 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_train.h5
2019-12-24 09:04:01,751 [INFO] Reading complete. time_to_read=0.25 seconds
2019-12-24 09:04:01,751 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_val.h5
2019-12-24 09:04:01,834 [INFO] Reading complete. time_to_read=0.08 seconds
2019-12-24 09:04:01,834 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_test.h5
2019-12-24 09:04:01,917 [INFO] Reading complete. time_to_read=0.08 seconds
2019-12-24 09:04:04,843 [INFO] Initializing model
2019-12-24 09:04:05,403 [INFO] _________________________________________________________________
2019-12-24 09:04:05,405 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-24 09:04:05,405 [INFO] =================================================================
2019-12-24 09:04:05,405 [INFO] dense_77 (Dense)             (None, 128)               10112     
2019-12-24 09:04:05,405 [INFO] _________________________________________________________________
2019-12-24 09:04:05,405 [INFO] batch_normalization_53 (Batc (None, 128)               512       
2019-12-24 09:04:05,406 [INFO] _________________________________________________________________
2019-12-24 09:04:05,406 [INFO] dropout_53 (Dropout)         (None, 128)               0         
2019-12-24 09:04:05,406 [INFO] _________________________________________________________________
2019-12-24 09:04:05,406 [INFO] dense_78 (Dense)             (None, 64)                8256      
2019-12-24 09:04:05,406 [INFO] _________________________________________________________________
2019-12-24 09:04:05,406 [INFO] batch_normalization_54 (Batc (None, 64)                256       
2019-12-24 09:04:05,406 [INFO] _________________________________________________________________
2019-12-24 09:04:05,406 [INFO] dropout_54 (Dropout)         (None, 64)                0         
2019-12-24 09:04:05,406 [INFO] _________________________________________________________________
2019-12-24 09:04:05,406 [INFO] dense_79 (Dense)             (None, 32)                2080      
2019-12-24 09:04:05,406 [INFO] _________________________________________________________________
2019-12-24 09:04:05,406 [INFO] batch_normalization_55 (Batc (None, 32)                128       
2019-12-24 09:04:05,407 [INFO] _________________________________________________________________
2019-12-24 09:04:05,407 [INFO] dropout_55 (Dropout)         (None, 32)                0         
2019-12-24 09:04:05,407 [INFO] _________________________________________________________________
2019-12-24 09:04:05,407 [INFO] dense_80 (Dense)             (None, 64)                2112      
2019-12-24 09:04:05,407 [INFO] _________________________________________________________________
2019-12-24 09:04:05,407 [INFO] batch_normalization_56 (Batc (None, 64)                256       
2019-12-24 09:04:05,407 [INFO] _________________________________________________________________
2019-12-24 09:04:05,407 [INFO] dropout_56 (Dropout)         (None, 64)                0         
2019-12-24 09:04:05,407 [INFO] _________________________________________________________________
2019-12-24 09:04:05,407 [INFO] dense_81 (Dense)             (None, 128)               8320      
2019-12-24 09:04:05,407 [INFO] _________________________________________________________________
2019-12-24 09:04:05,407 [INFO] batch_normalization_57 (Batc (None, 128)               512       
2019-12-24 09:04:05,408 [INFO] _________________________________________________________________
2019-12-24 09:04:05,408 [INFO] dropout_57 (Dropout)         (None, 128)               0         
2019-12-24 09:04:05,408 [INFO] _________________________________________________________________
2019-12-24 09:04:05,408 [INFO] dense_82 (Dense)             (None, 78)                10062     
2019-12-24 09:04:05,408 [INFO] =================================================================
2019-12-24 09:04:05,408 [INFO] Total params: 42,606
2019-12-24 09:04:05,408 [INFO] Trainable params: 41,774
2019-12-24 09:04:05,408 [INFO] Non-trainable params: 832
2019-12-24 09:04:05,408 [INFO] _________________________________________________________________
2019-12-24 09:04:05,548 [INFO] _________________________________________________________________
2019-12-24 09:04:05,548 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-24 09:04:05,548 [INFO] =================================================================
2019-12-24 09:04:05,548 [INFO] dense_83 (Dense)             (None, 64)                2112      
2019-12-24 09:04:05,548 [INFO] _________________________________________________________________
2019-12-24 09:04:05,549 [INFO] batch_normalization_58 (Batc (None, 64)                256       
2019-12-24 09:04:05,549 [INFO] _________________________________________________________________
2019-12-24 09:04:05,549 [INFO] dropout_58 (Dropout)         (None, 64)                0         
2019-12-24 09:04:05,549 [INFO] _________________________________________________________________
2019-12-24 09:04:05,549 [INFO] dense_84 (Dense)             (None, 12)                780       
2019-12-24 09:04:05,549 [INFO] =================================================================
2019-12-24 09:04:05,549 [INFO] Total params: 3,148
2019-12-24 09:04:05,549 [INFO] Trainable params: 3,020
2019-12-24 09:04:05,549 [INFO] Non-trainable params: 128
2019-12-24 09:04:05,549 [INFO] _________________________________________________________________
2019-12-24 09:04:05,549 [INFO] Training model
2019-12-24 09:04:05,549 [INFO] Splitting train set into 2 sets (unsupervised, supervised), random_seed = None
2019-12-24 09:04:31,061 [INFO] Split sizes (instances). total = 1696684, unsupervised = 424171, supervised = 1272513, unsupervised dataset hash = 78927a36ceaf06d160eabca1790864a1b15c893a
2019-12-24 09:04:31,061 [INFO] Training autoencoder
 - val_f1: 0.9769
Train on 424171 samples, validate on 565562 samples
Epoch 1/200
 - 27s - loss: -3.1901e+00 - val_loss: -3.9980e+00
Epoch 2/200
 - 24s - loss: -3.9029e+00 - val_loss: -4.0561e+00
Epoch 3/200
 - 24s - loss: -3.9601e+00 - val_loss: -4.0851e+00
Epoch 4/200
 - 24s - loss: -3.9908e+00 - val_loss: -4.0974e+00
Epoch 5/200
 - 24s - loss: -4.0102e+00 - val_loss: -4.1070e+00
Epoch 6/200
 - 24s - loss: -4.0221e+00 - val_loss: -4.1123e+00
Epoch 7/200
 - 24s - loss: -4.0345e+00 - val_loss: -4.1142e+00
Epoch 8/200
 - 24s - loss: -4.0423e+00 - val_loss: -4.1177e+00
Epoch 9/200
 - 24s - loss: -4.0493e+00 - val_loss: -4.1229e+00
Epoch 10/200
 - 24s - loss: -4.0543e+00 - val_loss: -4.1265e+00
Epoch 11/200
 - 24s - loss: -4.0595e+00 - val_loss: -4.1272e+00
Epoch 12/200
 - 24s - loss: -4.0644e+00 - val_loss: -4.1305e+00
Epoch 13/200
 - 24s - loss: -4.0685e+00 - val_loss: -4.1317e+00
Epoch 14/200
 - 24s - loss: -4.0719e+00 - val_loss: -4.1353e+00
Epoch 15/200
 - 24s - loss: -4.0740e+00 - val_loss: -4.1359e+00
Epoch 16/200
 - 24s - loss: -4.0773e+00 - val_loss: -4.1357e+00
Epoch 17/200
 - 24s - loss: -4.0788e+00 - val_loss: -4.1381e+00
Epoch 18/200
 - 24s - loss: -4.0788e+00 - val_loss: -4.1394e+00
Epoch 19/200
 - 24s - loss: -4.0821e+00 - val_loss: -4.1401e+00
Epoch 20/200
 - 24s - loss: -4.0833e+00 - val_loss: -4.1388e+00
Epoch 21/200
 - 24s - loss: -4.0839e+00 - val_loss: -4.1408e+00
2019-12-24 09:13:06,862 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep3/ae_model_epoch_20.pickle
Epoch 22/200
 - 24s - loss: -4.0855e+00 - val_loss: -4.1410e+00
Epoch 23/200
 - 24s - loss: -4.0872e+00 - val_loss: -4.1411e+00
Epoch 24/200
 - 24s - loss: -4.0871e+00 - val_loss: -4.1421e+00
Epoch 25/200
 - 24s - loss: -4.0890e+00 - val_loss: -4.1410e+00
Epoch 26/200
 - 24s - loss: -4.0901e+00 - val_loss: -4.1428e+00
Epoch 27/200
 - 24s - loss: -4.0900e+00 - val_loss: -4.1431e+00
Epoch 28/200
 - 24s - loss: -4.0918e+00 - val_loss: -4.1437e+00
Epoch 29/200
 - 24s - loss: -4.0920e+00 - val_loss: -4.1442e+00
Epoch 30/200
 - 24s - loss: -4.0931e+00 - val_loss: -4.1451e+00
Epoch 31/200
 - 24s - loss: -4.0929e+00 - val_loss: -4.1453e+00
Epoch 32/200
 - 24s - loss: -4.0942e+00 - val_loss: -4.1444e+00
Epoch 33/200
 - 24s - loss: -4.0943e+00 - val_loss: -4.1449e+00
Epoch 34/200
 - 24s - loss: -4.0952e+00 - val_loss: -4.1459e+00
Epoch 35/200
 - 24s - loss: -4.0957e+00 - val_loss: -4.1466e+00
Epoch 36/200
 - 24s - loss: -4.0956e+00 - val_loss: -4.1454e+00
Epoch 37/200
 - 24s - loss: -4.0959e+00 - val_loss: -4.1456e+00
Epoch 38/200
 - 24s - loss: -4.0960e+00 - val_loss: -4.1470e+00
Epoch 39/200
 - 24s - loss: -4.0972e+00 - val_loss: -4.1458e+00
Epoch 40/200
 - 24s - loss: -4.0975e+00 - val_loss: -4.1465e+00
Epoch 41/200
 - 24s - loss: -4.0989e+00 - val_loss: -4.1468e+00
2019-12-24 09:21:03,199 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep3/ae_model_epoch_40.pickle
Epoch 42/200
 - 24s - loss: -4.0981e+00 - val_loss: -4.1463e+00
Epoch 43/200
 - 24s - loss: -4.0980e+00 - val_loss: -4.1467e+00
Epoch 44/200
 - 24s - loss: -4.0995e+00 - val_loss: -4.1461e+00
Epoch 45/200
 - 24s - loss: -4.0998e+00 - val_loss: -4.1470e+00
Epoch 46/200
 - 24s - loss: -4.0991e+00 - val_loss: -4.1473e+00
Epoch 47/200
 - 24s - loss: -4.0992e+00 - val_loss: -4.1474e+00
Epoch 48/200
 - 24s - loss: -4.1006e+00 - val_loss: -4.1478e+00
Epoch 49/200
 - 24s - loss: -4.1001e+00 - val_loss: -4.1464e+00
Epoch 50/200
 - 24s - loss: -4.1012e+00 - val_loss: -4.1485e+00
Epoch 51/200
 - 24s - loss: -4.1015e+00 - val_loss: -4.1474e+00
Epoch 52/200
 - 24s - loss: -4.1016e+00 - val_loss: -4.1481e+00
Epoch 53/200
 - 24s - loss: -4.1020e+00 - val_loss: -4.1493e+00
Epoch 54/200
 - 24s - loss: -4.1020e+00 - val_loss: -4.1482e+00
Epoch 55/200
 - 24s - loss: -4.1024e+00 - val_loss: -4.1484e+00
Epoch 56/200
 - 24s - loss: -4.1029e+00 - val_loss: -4.1491e+00
Epoch 57/200
 - 24s - loss: -4.1025e+00 - val_loss: -4.1488e+00
Epoch 58/200
 - 24s - loss: -4.1020e+00 - val_loss: -4.1485e+00
Epoch 59/200
 - 24s - loss: -4.1032e+00 - val_loss: -4.1496e+00
Epoch 60/200
 - 24s - loss: -4.1044e+00 - val_loss: -4.1493e+00
Epoch 61/200
 - 24s - loss: -4.1043e+00 - val_loss: -4.1494e+00
2019-12-24 09:28:59,293 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep3/ae_model_epoch_60.pickle
Epoch 62/200
 - 24s - loss: -4.1034e+00 - val_loss: -4.1493e+00
Epoch 63/200
 - 24s - loss: -4.1040e+00 - val_loss: -4.1499e+00
Epoch 64/200
 - 24s - loss: -4.1040e+00 - val_loss: -4.1493e+00
Epoch 65/200
 - 24s - loss: -4.1042e+00 - val_loss: -4.1490e+00
Epoch 66/200
 - 24s - loss: -4.1052e+00 - val_loss: -4.1497e+00
Epoch 67/200
 - 24s - loss: -4.1048e+00 - val_loss: -4.1496e+00
Epoch 68/200
 - 24s - loss: -4.1044e+00 - val_loss: -4.1494e+00
Epoch 69/200
 - 24s - loss: -4.1055e+00 - val_loss: -4.1503e+00
Epoch 70/200
 - 24s - loss: -4.1059e+00 - val_loss: -4.1503e+00
Epoch 71/200
 - 24s - loss: -4.1057e+00 - val_loss: -4.1497e+00
Epoch 72/200
 - 24s - loss: -4.1055e+00 - val_loss: -4.1492e+00
Epoch 73/200
 - 24s - loss: -4.1050e+00 - val_loss: -4.1503e+00
Epoch 74/200
 - 24s - loss: -4.1057e+00 - val_loss: -4.1503e+00
Epoch 75/200
 - 24s - loss: -4.1051e+00 - val_loss: -4.1494e+00
Epoch 76/200
 - 24s - loss: -4.1061e+00 - val_loss: -4.1503e+00
Epoch 77/200
 - 24s - loss: -4.1049e+00 - val_loss: -4.1500e+00
Epoch 78/200
 - 24s - loss: -4.1060e+00 - val_loss: -4.1506e+00
Epoch 79/200
 - 24s - loss: -4.1066e+00 - val_loss: -4.1508e+00
Epoch 80/200
 - 24s - loss: -4.1071e+00 - val_loss: -4.1506e+00
Epoch 81/200
 - 24s - loss: -4.1066e+00 - val_loss: -4.1510e+00
2019-12-24 09:36:55,892 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep3/ae_model_epoch_80.pickle
Epoch 82/200
 - 24s - loss: -4.1056e+00 - val_loss: -4.1506e+00
Epoch 83/200
 - 24s - loss: -4.1066e+00 - val_loss: -4.1513e+00
Epoch 84/200
 - 24s - loss: -4.1075e+00 - val_loss: -4.1510e+00
Epoch 85/200
 - 24s - loss: -4.1070e+00 - val_loss: -4.1511e+00
Epoch 86/200
 - 24s - loss: -4.1075e+00 - val_loss: -4.1494e+00
Epoch 87/200
 - 24s - loss: -4.1076e+00 - val_loss: -4.1516e+00
Epoch 88/200
 - 24s - loss: -4.1072e+00 - val_loss: -4.1510e+00
Epoch 89/200
 - 24s - loss: -4.1069e+00 - val_loss: -4.1514e+00
Epoch 90/200
 - 24s - loss: -4.1080e+00 - val_loss: -4.1513e+00
Epoch 91/200
 - 24s - loss: -4.1082e+00 - val_loss: -4.1514e+00
Epoch 92/200
 - 24s - loss: -4.1075e+00 - val_loss: -4.1511e+00
Epoch 93/200
 - 24s - loss: -4.1076e+00 - val_loss: -4.1512e+00
Epoch 94/200
 - 24s - loss: -4.1085e+00 - val_loss: -4.1515e+00
Epoch 95/200
 - 24s - loss: -4.1084e+00 - val_loss: -4.1516e+00
Epoch 96/200
 - 24s - loss: -4.1077e+00 - val_loss: -4.1515e+00
Epoch 97/200
 - 24s - loss: -4.1086e+00 - val_loss: -4.1504e+00
Epoch 98/200
 - 24s - loss: -4.1087e+00 - val_loss: -4.1513e+00
Epoch 99/200
 - 24s - loss: -4.1083e+00 - val_loss: -4.1511e+00
Epoch 100/200
 - 24s - loss: -4.1079e+00 - val_loss: -4.1512e+00
Epoch 101/200
 - 24s - loss: -4.1074e+00 - val_loss: -4.1511e+00
2019-12-24 09:44:52,025 [INFO] epoch = 100. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep3/ae_model_epoch_100.pickle
Epoch 102/200
 - 24s - loss: -4.1080e+00 - val_loss: -4.1522e+00
Epoch 103/200
 - 24s - loss: -4.1089e+00 - val_loss: -4.1512e+00
Epoch 104/200
 - 25s - loss: -4.1085e+00 - val_loss: -4.1511e+00
Epoch 105/200
 - 24s - loss: -4.1080e+00 - val_loss: -4.1524e+00
Epoch 106/200
 - 24s - loss: -4.1087e+00 - val_loss: -4.1515e+00
Epoch 107/200
 - 24s - loss: -4.1092e+00 - val_loss: -4.1510e+00
Epoch 108/200
 - 24s - loss: -4.1091e+00 - val_loss: -4.1517e+00
Epoch 109/200
 - 24s - loss: -4.1084e+00 - val_loss: -4.1516e+00
Epoch 110/200
 - 24s - loss: -4.1106e+00 - val_loss: -4.1508e+00
Epoch 111/200
 - 24s - loss: -4.1100e+00 - val_loss: -4.1519e+00
Epoch 112/200
 - 24s - loss: -4.1085e+00 - val_loss: -4.1522e+00
Epoch 113/200
 - 24s - loss: -4.1087e+00 - val_loss: -4.1519e+00
Epoch 114/200
 - 24s - loss: -4.1102e+00 - val_loss: -4.1514e+00
Epoch 115/200
 - 24s - loss: -4.1095e+00 - val_loss: -4.1519e+00
Epoch 116/200
 - 24s - loss: -4.1093e+00 - val_loss: -4.1517e+00
Epoch 117/200
 - 24s - loss: -4.1103e+00 - val_loss: -4.1520e+00
Epoch 118/200
 - 24s - loss: -4.1096e+00 - val_loss: -4.1518e+00
Epoch 119/200
 - 24s - loss: -4.1094e+00 - val_loss: -4.1517e+00
Epoch 120/200
 - 24s - loss: -4.1102e+00 - val_loss: -4.1522e+00
Epoch 121/200
 - 24s - loss: -4.1101e+00 - val_loss: -4.1526e+00
2019-12-24 09:52:52,007 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep3/ae_model_epoch_120.pickle
Epoch 122/200
 - 24s - loss: -4.1099e+00 - val_loss: -4.1515e+00
Epoch 123/200
 - 24s - loss: -4.1108e+00 - val_loss: -4.1524e+00
Epoch 124/200
 - 24s - loss: -4.1103e+00 - val_loss: -4.1523e+00
Epoch 125/200
 - 24s - loss: -4.1108e+00 - val_loss: -4.1516e+00
Epoch 126/200
 - 24s - loss: -4.1103e+00 - val_loss: -4.1525e+00
Epoch 127/200
 - 24s - loss: -4.1104e+00 - val_loss: -4.1519e+00
Epoch 128/200
 - 24s - loss: -4.1110e+00 - val_loss: -4.1520e+00
Epoch 129/200
 - 24s - loss: -4.1098e+00 - val_loss: -4.1520e+00
Epoch 130/200
 - 24s - loss: -4.1103e+00 - val_loss: -4.1522e+00
Epoch 131/200
 - 24s - loss: -4.1102e+00 - val_loss: -4.1518e+00
Epoch 132/200
 - 24s - loss: -4.1102e+00 - val_loss: -4.1523e+00
Epoch 133/200
 - 24s - loss: -4.1107e+00 - val_loss: -4.1528e+00
Epoch 134/200
 - 24s - loss: -4.1107e+00 - val_loss: -4.1523e+00
Epoch 135/200
 - 24s - loss: -4.1107e+00 - val_loss: -4.1520e+00
Epoch 136/200
 - 24s - loss: -4.1106e+00 - val_loss: -4.1517e+00
Epoch 137/200
 - 24s - loss: -4.1109e+00 - val_loss: -4.1525e+00
Epoch 138/200
 - 24s - loss: -4.1106e+00 - val_loss: -4.1526e+00
Epoch 139/200
 - 24s - loss: -4.1116e+00 - val_loss: -4.1530e+00
Epoch 140/200
 - 24s - loss: -4.1112e+00 - val_loss: -4.1537e+00
Epoch 141/200
 - 24s - loss: -4.1116e+00 - val_loss: -4.1528e+00
2019-12-24 10:00:51,740 [INFO] epoch = 140. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep3/ae_model_epoch_140.pickle
Epoch 142/200
 - 24s - loss: -4.1105e+00 - val_loss: -4.1519e+00
Epoch 143/200
 - 24s - loss: -4.1109e+00 - val_loss: -4.1525e+00
Epoch 144/200
 - 24s - loss: -4.1111e+00 - val_loss: -4.1522e+00
Epoch 145/200
 - 24s - loss: -4.1120e+00 - val_loss: -4.1524e+00
Epoch 146/200
 - 24s - loss: -4.1110e+00 - val_loss: -4.1527e+00
Epoch 147/200
 - 24s - loss: -4.1116e+00 - val_loss: -4.1523e+00
Epoch 148/200
 - 24s - loss: -4.1120e+00 - val_loss: -4.1529e+00
Epoch 149/200
 - 24s - loss: -4.1110e+00 - val_loss: -4.1521e+00
Epoch 150/200
 - 24s - loss: -4.1121e+00 - val_loss: -4.1524e+00
Epoch 151/200
 - 24s - loss: -4.1116e+00 - val_loss: -4.1520e+00
Epoch 152/200
 - 24s - loss: -4.1117e+00 - val_loss: -4.1533e+00
Epoch 153/200
 - 24s - loss: -4.1109e+00 - val_loss: -4.1527e+00
Epoch 154/200
 - 24s - loss: -4.1114e+00 - val_loss: -4.1522e+00
Epoch 155/200
 - 24s - loss: -4.1118e+00 - val_loss: -4.1530e+00
Epoch 156/200
 - 24s - loss: -4.1120e+00 - val_loss: -4.1523e+00
Epoch 157/200
 - 24s - loss: -4.1120e+00 - val_loss: -4.1521e+00
Epoch 158/200
 - 24s - loss: -4.1118e+00 - val_loss: -4.1533e+00
Epoch 159/200
 - 24s - loss: -4.1120e+00 - val_loss: -4.1532e+00
Epoch 160/200
 - 24s - loss: -4.1124e+00 - val_loss: -4.1525e+00
Epoch 161/200
 - 24s - loss: -4.1121e+00 - val_loss: -4.1530e+00
2019-12-24 10:08:51,744 [INFO] epoch = 160. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep3/ae_model_epoch_160.pickle
Epoch 162/200
 - 24s - loss: -4.1118e+00 - val_loss: -4.1533e+00
Epoch 163/200
 - 24s - loss: -4.1128e+00 - val_loss: -4.1525e+00
Epoch 164/200
 - 24s - loss: -4.1125e+00 - val_loss: -4.1531e+00
Epoch 165/200
 - 24s - loss: -4.1124e+00 - val_loss: -4.1538e+00
Epoch 166/200
 - 24s - loss: -4.1116e+00 - val_loss: -4.1531e+00
Epoch 167/200
 - 24s - loss: -4.1130e+00 - val_loss: -4.1523e+00
Epoch 168/200
 - 24s - loss: -4.1122e+00 - val_loss: -4.1524e+00
Epoch 169/200
 - 24s - loss: -4.1131e+00 - val_loss: -4.1514e+00
Epoch 170/200
 - 24s - loss: -4.1127e+00 - val_loss: -4.1531e+00
Epoch 171/200
 - 24s - loss: -4.1124e+00 - val_loss: -4.1535e+00
Epoch 172/200
 - 24s - loss: -4.1123e+00 - val_loss: -4.1520e+00
Epoch 173/200
 - 24s - loss: -4.1115e+00 - val_loss: -4.1531e+00
Epoch 174/200
 - 24s - loss: -4.1132e+00 - val_loss: -4.1538e+00
Epoch 175/200
 - 24s - loss: -4.1126e+00 - val_loss: -4.1533e+00
Epoch 176/200
 - 24s - loss: -4.1127e+00 - val_loss: -4.1529e+00
Epoch 177/200
 - 24s - loss: -4.1127e+00 - val_loss: -4.1540e+00
Epoch 178/200
 - 24s - loss: -4.1127e+00 - val_loss: -4.1532e+00
Epoch 179/200
 - 24s - loss: -4.1127e+00 - val_loss: -4.1535e+00
Epoch 180/200
 - 24s - loss: -4.1143e+00 - val_loss: -4.1538e+00
Epoch 181/200
 - 24s - loss: -4.1128e+00 - val_loss: -4.1532e+00
2019-12-24 10:16:51,844 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep3/ae_model_epoch_180.pickle
Epoch 182/200
 - 24s - loss: -4.1130e+00 - val_loss: -4.1538e+00
Epoch 183/200
 - 24s - loss: -4.1132e+00 - val_loss: -4.1536e+00
Epoch 184/200
 - 24s - loss: -4.1136e+00 - val_loss: -4.1538e+00
Epoch 185/200
 - 24s - loss: -4.1132e+00 - val_loss: -4.1534e+00
Epoch 186/200
 - 24s - loss: -4.1129e+00 - val_loss: -4.1535e+00
Epoch 187/200
 - 24s - loss: -4.1115e+00 - val_loss: -4.1540e+00
Epoch 188/200
 - 24s - loss: -4.1134e+00 - val_loss: -4.1539e+00
Epoch 189/200
 - 24s - loss: -4.1140e+00 - val_loss: -4.1537e+00
Epoch 190/200
 - 24s - loss: -4.1130e+00 - val_loss: -4.1540e+00
Epoch 191/200
 - 24s - loss: -4.1136e+00 - val_loss: -4.1543e+00
Epoch 192/200
 - 24s - loss: -4.1141e+00 - val_loss: -4.1539e+00
Epoch 193/200
 - 24s - loss: -4.1136e+00 - val_loss: -4.1542e+00
Epoch 194/200
 - 24s - loss: -4.1130e+00 - val_loss: -4.1534e+00
Epoch 195/200
 - 24s - loss: -4.1135e+00 - val_loss: -4.1544e+00
Epoch 196/200
 - 24s - loss: -4.1130e+00 - val_loss: -4.1532e+00
Epoch 197/200
 - 24s - loss: -4.1136e+00 - val_loss: -4.1543e+00
Epoch 198/200
 - 24s - loss: -4.1137e+00 - val_loss: -4.1528e+00
Epoch 199/200
 - 24s - loss: -4.1139e+00 - val_loss: -4.1528e+00
Epoch 200/200
 - 24s - loss: -4.1136e+00 - val_loss: -4.1539e+00
2019-12-24 10:24:28,147 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-24 10:25:38,909 [INFO] Last epoch loss evaluation: train_loss = -4.155657, val_loss = -4.154366
2019-12-24 10:25:38,909 [INFO] Training autoencoder complete
2019-12-24 10:25:38,910 [INFO] Encoding data for supervised training
2019-12-24 10:26:57,523 [INFO] Encoding complete
2019-12-24 10:26:57,523 [INFO] Training neural network layers (after autoencoder)
Train on 1272513 samples, validate on 565562 samples
Epoch 1/200
 - 22s - loss: 0.0254 - val_loss: 0.0140
 - val_f1: 0.9666
Epoch 2/200
 - 21s - loss: 0.0145 - val_loss: 0.0130
 - val_f1: 0.9663
Epoch 3/200
 - 21s - loss: 0.0130 - val_loss: 0.0118
 - val_f1: 0.9709
Epoch 4/200
 - 21s - loss: 0.0126 - val_loss: 0.0122
 - val_f1: 0.9707
Epoch 5/200
 - 21s - loss: 0.0123 - val_loss: 0.0121
 - val_f1: 0.9706
Epoch 6/200
 - 21s - loss: 0.0120 - val_loss: 0.0101
 - val_f1: 0.9733
Epoch 7/200
 - 21s - loss: 0.0119 - val_loss: 0.0116
 - val_f1: 0.9725
Epoch 8/200
 - 21s - loss: 0.0116 - val_loss: 0.0115
 - val_f1: 0.9720
Epoch 9/200
 - 21s - loss: 0.0116 - val_loss: 0.0134
 - val_f1: 0.9686
Epoch 10/200
 - 21s - loss: 0.0114 - val_loss: 0.0123
 - val_f1: 0.9715
Epoch 11/200
 - 21s - loss: 0.0113 - val_loss: 0.0104
 - val_f1: 0.9764
Epoch 12/200
 - 21s - loss: 0.0112 - val_loss: 0.0099
 - val_f1: 0.9733
Epoch 13/200
 - 21s - loss: 0.0111 - val_loss: 0.0104
 - val_f1: 0.9725
Epoch 14/200
 - 21s - loss: 0.0110 - val_loss: 0.0116
 - val_f1: 0.9726
Epoch 15/200
 - 21s - loss: 0.0110 - val_loss: 0.0100
 - val_f1: 0.9731
Epoch 16/200
 - 21s - loss: 0.0109 - val_loss: 0.0111
 - val_f1: 0.9684
Epoch 17/200
 - 21s - loss: 0.0108 - val_loss: 0.0111
 - val_f1: 0.9721
Epoch 18/200
 - 21s - loss: 0.0108 - val_loss: 0.0105
 - val_f1: 0.9742
Epoch 19/200
 - 21s - loss: 0.0107 - val_loss: 0.0106
 - val_f1: 0.9740
Epoch 20/200
 - 21s - loss: 0.0106 - val_loss: 0.0104
 - val_f1: 0.9681
Epoch 21/200
 - 21s - loss: 0.0107 - val_loss: 0.0101
2019-12-24 10:40:57,127 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep3/ann_model_epoch_20.pickle
 - val_f1: 0.9764
Epoch 22/200
 - 21s - loss: 0.0108 - val_loss: 0.0100
 - val_f1: 0.9751
Epoch 23/200
 - 21s - loss: 0.0107 - val_loss: 0.0093
 - val_f1: 0.9745
Epoch 24/200
 - 21s - loss: 0.0106 - val_loss: 0.0096
 - val_f1: 0.9737
Epoch 25/200
 - 21s - loss: 0.0106 - val_loss: 0.0092
 - val_f1: 0.9792
Epoch 26/200
 - 21s - loss: 0.0106 - val_loss: 0.0095
 - val_f1: 0.9776
Epoch 27/200
 - 21s - loss: 0.0106 - val_loss: 0.0092
 - val_f1: 0.9770
Epoch 28/200
 - 21s - loss: 0.0105 - val_loss: 0.0092
 - val_f1: 0.9741
Epoch 29/200
 - 21s - loss: 0.0104 - val_loss: 0.0096
 - val_f1: 0.9760
Epoch 30/200
 - 21s - loss: 0.0104 - val_loss: 0.0094
 - val_f1: 0.9769
Epoch 31/200
 - 21s - loss: 0.0105 - val_loss: 0.0109
 - val_f1: 0.9731
Epoch 32/200
 - 21s - loss: 0.0105 - val_loss: 0.0104
 - val_f1: 0.9755
Epoch 33/200
 - 21s - loss: 0.0105 - val_loss: 0.0095
 - val_f1: 0.9716
Epoch 34/200
 - 21s - loss: 0.0104 - val_loss: 0.0093
 - val_f1: 0.9769
Epoch 35/200
 - 21s - loss: 0.0105 - val_loss: 0.0091
 - val_f1: 0.9776
Epoch 36/200
 - 21s - loss: 0.0103 - val_loss: 0.0101
 - val_f1: 0.9739
Epoch 37/200
 - 21s - loss: 0.0103 - val_loss: 0.0090
 - val_f1: 0.9768
Epoch 38/200
 - 21s - loss: 0.0103 - val_loss: 0.0086
 - val_f1: 0.9768
Epoch 39/200
 - 21s - loss: 0.0103 - val_loss: 0.0090
 - val_f1: 0.9764
Epoch 40/200
 - 21s - loss: 0.0103 - val_loss: 0.0092
 - val_f1: 0.9738
Epoch 41/200
 - 21s - loss: 0.0103 - val_loss: 0.0092
2019-12-24 10:54:23,973 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep3/ann_model_epoch_40.pickle
 - val_f1: 0.9726
Epoch 42/200
 - 21s - loss: 0.0102 - val_loss: 0.0108
 - val_f1: 0.9687
Epoch 43/200
 - 21s - loss: 0.0102 - val_loss: 0.0099
 - val_f1: 0.9730
Epoch 44/200
 - 21s - loss: 0.0102 - val_loss: 0.0091
 - val_f1: 0.9764
Epoch 45/200
 - 21s - loss: 0.0102 - val_loss: 0.0091
 - val_f1: 0.9758
Epoch 46/200
 - 21s - loss: 0.0102 - val_loss: 0.0092
 - val_f1: 0.9752
Epoch 47/200
 - 21s - loss: 0.0101 - val_loss: 0.0088
 - val_f1: 0.9801
Epoch 48/200
 - 21s - loss: 0.0101 - val_loss: 0.0086
 - val_f1: 0.9770
Epoch 49/200
 - 21s - loss: 0.0101 - val_loss: 0.0086
 - val_f1: 0.9785
Epoch 50/200
 - 21s - loss: 0.0101 - val_loss: 0.0090
 - val_f1: 0.9780
Epoch 51/200
 - 21s - loss: 0.0101 - val_loss: 0.0121
 - val_f1: 0.9691
Epoch 52/200
 - 21s - loss: 0.0101 - val_loss: 0.0092
 - val_f1: 0.9762
Epoch 53/200
 - 21s - loss: 0.0101 - val_loss: 0.0087
 - val_f1: 0.9766
Epoch 54/200
 - 21s - loss: 0.0100 - val_loss: 0.0100
 - val_f1: 0.9743
Epoch 55/200
 - 21s - loss: 0.0100 - val_loss: 0.0094
 - val_f1: 0.9725
Epoch 56/200
 - 21s - loss: 0.0100 - val_loss: 0.0089
 - val_f1: 0.9782
Epoch 57/200
 - 21s - loss: 0.0100 - val_loss: 0.0091
 - val_f1: 0.9767
Epoch 58/200
 - 21s - loss: 0.0100 - val_loss: 0.0090
 - val_f1: 0.9764
Epoch 59/200
 - 21s - loss: 0.0100 - val_loss: 0.0083
 - val_f1: 0.9815
Epoch 60/200
 - 21s - loss: 0.0100 - val_loss: 0.0086
 - val_f1: 0.9815
Epoch 61/200
 - 21s - loss: 0.0100 - val_loss: 0.0091
2019-12-24 11:07:50,123 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep3/ann_model_epoch_60.pickle
 - val_f1: 0.9768
Epoch 62/200
 - 21s - loss: 0.0099 - val_loss: 0.0099
 - val_f1: 0.9758
Epoch 63/200
 - 21s - loss: 0.0099 - val_loss: 0.0091
 - val_f1: 0.9774
Epoch 64/200
 - 21s - loss: 0.0099 - val_loss: 0.0087
 - val_f1: 0.9780
Epoch 65/200
 - 21s - loss: 0.0100 - val_loss: 0.0086
 - val_f1: 0.9808
Epoch 66/200
 - 21s - loss: 0.0099 - val_loss: 0.0087
 - val_f1: 0.9789
Epoch 67/200
 - 21s - loss: 0.0099 - val_loss: 0.0084
 - val_f1: 0.9797
Epoch 68/200
 - 21s - loss: 0.0099 - val_loss: 0.0089
 - val_f1: 0.9765
Epoch 69/200
 - 21s - loss: 0.0099 - val_loss: 0.0086
 - val_f1: 0.9777
Epoch 70/200
 - 21s - loss: 0.0099 - val_loss: 0.0101
 - val_f1: 0.9774
Epoch 71/200
 - 21s - loss: 0.0099 - val_loss: 0.0092
 - val_f1: 0.9747
Epoch 72/200
 - 21s - loss: 0.0098 - val_loss: 0.0085
 - val_f1: 0.9780
Epoch 73/200
 - 21s - loss: 0.0098 - val_loss: 0.0089
 - val_f1: 0.9780
Epoch 74/200
 - 21s - loss: 0.0098 - val_loss: 0.0089
 - val_f1: 0.9752
Epoch 75/200
 - 21s - loss: 0.0098 - val_loss: 0.0101
 - val_f1: 0.9712
Epoch 76/200
 - 21s - loss: 0.0098 - val_loss: 0.0087
 - val_f1: 0.9769
Epoch 77/200
 - 21s - loss: 0.0098 - val_loss: 0.0093
 - val_f1: 0.9766
Epoch 78/200
 - 21s - loss: 0.0098 - val_loss: 0.0085
 - val_f1: 0.9772
Epoch 79/200
 - 21s - loss: 0.0098 - val_loss: 0.0085
 - val_f1: 0.9808
Epoch 80/200
 - 21s - loss: 0.0098 - val_loss: 0.0101
 - val_f1: 0.9682
Epoch 81/200
 - 21s - loss: 0.0098 - val_loss: 0.0089
2019-12-24 11:21:17,133 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep3/ann_model_epoch_80.pickle
 - val_f1: 0.9757
Epoch 82/200
 - 21s - loss: 0.0097 - val_loss: 0.0089
 - val_f1: 0.9793
Epoch 83/200
 - 21s - loss: 0.0098 - val_loss: 0.0088
 - val_f1: 0.9760
Epoch 84/200
 - 21s - loss: 0.0098 - val_loss: 0.0089
 - val_f1: 0.9762
Epoch 85/200
 - 21s - loss: 0.0097 - val_loss: 0.0083
 - val_f1: 0.9803
Epoch 86/200
 - 21s - loss: 0.0098 - val_loss: 0.0094
 - val_f1: 0.9729
Epoch 87/200
 - 21s - loss: 0.0097 - val_loss: 0.0085
 - val_f1: 0.9777
Epoch 88/200
 - 21s - loss: 0.0097 - val_loss: 0.0086
 - val_f1: 0.9774
Epoch 89/200
 - 21s - loss: 0.0097 - val_loss: 0.0096
 - val_f1: 0.9747
Epoch 90/200
 - 21s - loss: 0.0097 - val_loss: 0.0093
 - val_f1: 0.9757
Epoch 91/200
 - 21s - loss: 0.0097 - val_loss: 0.0084
 - val_f1: 0.9797
Epoch 92/200
 - 21s - loss: 0.0097 - val_loss: 0.0090
 - val_f1: 0.9769
Epoch 93/200
 - 21s - loss: 0.0097 - val_loss: 0.0086
 - val_f1: 0.9811
Epoch 94/200
 - 21s - loss: 0.0097 - val_loss: 0.0092
 - val_f1: 0.9735
Epoch 95/200
 - 21s - loss: 0.0096 - val_loss: 0.0084
 - val_f1: 0.9815
Epoch 96/200
 - 21s - loss: 0.0096 - val_loss: 0.0096
 - val_f1: 0.9743
Epoch 97/200
 - 21s - loss: 0.0097 - val_loss: 0.0083
 - val_f1: 0.9784
Epoch 98/200
 - 21s - loss: 0.0096 - val_loss: 0.0081
 - val_f1: 0.9803
Epoch 99/200
 - 21s - loss: 0.0096 - val_loss: 0.0090
 - val_f1: 0.9747
Epoch 100/200
 - 21s - loss: 0.0096 - val_loss: 0.0084
 - val_f1: 0.9776
Epoch 101/200
 - 21s - loss: 0.0096 - val_loss: 0.0082
2019-12-24 11:34:43,965 [INFO] epoch = 100. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep3/ann_model_epoch_100.pickle
 - val_f1: 0.9778
Epoch 102/200
 - 21s - loss: 0.0096 - val_loss: 0.0092
 - val_f1: 0.9747
Epoch 103/200
 - 21s - loss: 0.0096 - val_loss: 0.0093
 - val_f1: 0.9725
Epoch 104/200
 - 21s - loss: 0.0096 - val_loss: 0.0085
 - val_f1: 0.9773
Epoch 105/200
 - 21s - loss: 0.0095 - val_loss: 0.0101
 - val_f1: 0.9731
Epoch 106/200
 - 21s - loss: 0.0096 - val_loss: 0.0082
 - val_f1: 0.9783
Epoch 107/200
 - 21s - loss: 0.0096 - val_loss: 0.0093
 - val_f1: 0.9764
Epoch 108/200
 - 21s - loss: 0.0096 - val_loss: 0.0082
 - val_f1: 0.9812
Epoch 109/200
 - 21s - loss: 0.0096 - val_loss: 0.0083
 - val_f1: 0.9806
Epoch 110/200
 - 21s - loss: 0.0095 - val_loss: 0.0104
 - val_f1: 0.9757
Epoch 111/200
 - 21s - loss: 0.0095 - val_loss: 0.0089
 - val_f1: 0.9766
Epoch 112/200
 - 21s - loss: 0.0095 - val_loss: 0.0126
 - val_f1: 0.9718
Epoch 113/200
 - 21s - loss: 0.0096 - val_loss: 0.0083
 - val_f1: 0.9809
Epoch 114/200
 - 21s - loss: 0.0096 - val_loss: 0.0087
 - val_f1: 0.9776
Epoch 115/200
 - 21s - loss: 0.0095 - val_loss: 0.0083
 - val_f1: 0.9776
Epoch 116/200
 - 21s - loss: 0.0095 - val_loss: 0.0079
 - val_f1: 0.9788
Epoch 117/200
 - 21s - loss: 0.0095 - val_loss: 0.0081
 - val_f1: 0.9800
Epoch 118/200
 - 21s - loss: 0.0095 - val_loss: 0.0085
 - val_f1: 0.9779
Epoch 119/200
 - 21s - loss: 0.0095 - val_loss: 0.0086
 - val_f1: 0.9772
Epoch 120/200
 - 21s - loss: 0.0095 - val_loss: 0.0084
 - val_f1: 0.9804
Epoch 121/200
 - 21s - loss: 0.0095 - val_loss: 0.0086
2019-12-24 11:48:10,251 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep3/ann_model_epoch_120.pickle
 - val_f1: 0.9780
Epoch 122/200
 - 21s - loss: 0.0094 - val_loss: 0.0086
 - val_f1: 0.9778
Epoch 123/200
 - 21s - loss: 0.0094 - val_loss: 0.0080
 - val_f1: 0.9784
Epoch 124/200
 - 21s - loss: 0.0095 - val_loss: 0.0086
 - val_f1: 0.9772
Epoch 125/200
 - 21s - loss: 0.0095 - val_loss: 0.0080
 - val_f1: 0.9778
Epoch 126/200
 - 21s - loss: 0.0095 - val_loss: 0.0093
 - val_f1: 0.9770
Epoch 127/200
 - 21s - loss: 0.0095 - val_loss: 0.0116
 - val_f1: 0.9700
Epoch 128/200
 - 21s - loss: 0.0094 - val_loss: 0.0110
 - val_f1: 0.9752
Epoch 129/200
 - 21s - loss: 0.0095 - val_loss: 0.0086
 - val_f1: 0.9761
Epoch 130/200
 - 21s - loss: 0.0094 - val_loss: 0.0082
 - val_f1: 0.9813
Epoch 131/200
 - 21s - loss: 0.0094 - val_loss: 0.0092
 - val_f1: 0.9742
Epoch 132/200
 - 21s - loss: 0.0094 - val_loss: 0.0094
 - val_f1: 0.9732
Epoch 133/200
 - 21s - loss: 0.0095 - val_loss: 0.0093
 - val_f1: 0.9756
Epoch 134/200
 - 21s - loss: 0.0094 - val_loss: 0.0084
 - val_f1: 0.9805
Epoch 135/200
 - 21s - loss: 0.0094 - val_loss: 0.0082
 - val_f1: 0.9805
Epoch 136/200
 - 21s - loss: 0.0094 - val_loss: 0.0086
 - val_f1: 0.9776
Epoch 137/200
 - 21s - loss: 0.0094 - val_loss: 0.0098
 - val_f1: 0.9767
Epoch 138/200
 - 21s - loss: 0.0094 - val_loss: 0.0095
 - val_f1: 0.9773
Epoch 139/200
 - 21s - loss: 0.0094 - val_loss: 0.0081
 - val_f1: 0.9823
Epoch 140/200
 - 21s - loss: 0.0093 - val_loss: 0.0084
 - val_f1: 0.9783
Epoch 141/200
 - 21s - loss: 0.0094 - val_loss: 0.0095
2019-12-24 12:01:37,270 [INFO] epoch = 140. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep3/ann_model_epoch_140.pickle
 - val_f1: 0.9765
Epoch 142/200
 - 21s - loss: 0.0093 - val_loss: 0.0086
 - val_f1: 0.9794
Epoch 143/200
 - 21s - loss: 0.0094 - val_loss: 0.0086
 - val_f1: 0.9768
Epoch 144/200
 - 21s - loss: 0.0094 - val_loss: 0.0095
 - val_f1: 0.9770
Epoch 145/200
 - 21s - loss: 0.0094 - val_loss: 0.0082
 - val_f1: 0.9780
Epoch 146/200
 - 21s - loss: 0.0094 - val_loss: 0.0086
 - val_f1: 0.9772
Epoch 147/200
 - 21s - loss: 0.0093 - val_loss: 0.0086
 - val_f1: 0.9776
Epoch 148/200
 - 21s - loss: 0.0093 - val_loss: 0.0081
 - val_f1: 0.9783
Epoch 149/200
 - 21s - loss: 0.0093 - val_loss: 0.0096
 - val_f1: 0.9768
Epoch 150/200
 - 21s - loss: 0.0093 - val_loss: 0.0081
 - val_f1: 0.9812
Epoch 151/200
 - 21s - loss: 0.0093 - val_loss: 0.0090
 - val_f1: 0.9772
Epoch 152/200
 - 21s - loss: 0.0093 - val_loss: 0.0084
 - val_f1: 0.9796
Epoch 153/200
 - 21s - loss: 0.0093 - val_loss: 0.0090
 - val_f1: 0.9751
Epoch 154/200
 - 21s - loss: 0.0093 - val_loss: 0.0084
 - val_f1: 0.9770
Epoch 155/200
 - 21s - loss: 0.0093 - val_loss: 0.0085
 - val_f1: 0.9796
Epoch 156/200
 - 21s - loss: 0.0094 - val_loss: 0.0087
 - val_f1: 0.9753
Epoch 157/200
 - 21s - loss: 0.0093 - val_loss: 0.0087
 - val_f1: 0.9778
Epoch 158/200
 - 21s - loss: 0.0093 - val_loss: 0.0081
 - val_f1: 0.9797
Epoch 159/200
 - 21s - loss: 0.0093 - val_loss: 0.0081
 - val_f1: 0.9780
Epoch 160/200
 - 21s - loss: 0.0093 - val_loss: 0.0104
 - val_f1: 0.9757
Epoch 161/200
 - 21s - loss: 0.0093 - val_loss: 0.0081
2019-12-24 12:15:04,865 [INFO] epoch = 160. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep3/ann_model_epoch_160.pickle
 - val_f1: 0.9810
Epoch 162/200
 - 21s - loss: 0.0093 - val_loss: 0.0086
 - val_f1: 0.9790
Epoch 163/200
 - 21s - loss: 0.0093 - val_loss: 0.0083
 - val_f1: 0.9774
Epoch 164/200
 - 21s - loss: 0.0093 - val_loss: 0.0085
 - val_f1: 0.9779
Epoch 165/200
 - 21s - loss: 0.0093 - val_loss: 0.0085
 - val_f1: 0.9782
Epoch 166/200
 - 21s - loss: 0.0093 - val_loss: 0.0082
2019-12-24 12:18:46,158 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-24 12:19:55,658 [INFO] Last epoch loss evaluation: train_loss = 0.007839, val_loss = 0.007939
2019-12-24 12:19:55,665 [INFO] Training complete. time_to_train = 11750.12 sec, 195.84 min
2019-12-24 12:19:55,692 [INFO] Model saved to results_selected_models/selected_ids17_ae_ann_deep_rep3/best_model.pickle
/home/sunanda/research/ids_experiments/utility.py:268: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
2019-12-24 12:19:55,884 [INFO] Plot saved to: results_selected_models/selected_ids17_ae_ann_deep_rep3/training_error_history.png
/home/sunanda/research/ids_experiments/utility.py:285: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
2019-12-24 12:19:56,073 [INFO] Plot saved to: results_selected_models/selected_ids17_ae_ann_deep_rep3/training_f1_history.png
2019-12-24 12:19:56,073 [INFO] Making predictions on training, validation, testing data
2019-12-24 12:23:26,737 [INFO] Evaluating predictions (results)
/home/sunanda/anaconda3/envs/ml_env_cpu/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2019-12-24 12:23:46,581 [INFO] Dataset: Testing. Classification report below
2019-12-24 12:23:46,582 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       0.98      0.99      0.99    454265
                   Bot       1.00      0.35      0.52       391
                  DDoS       1.00      0.98      0.99     25605
         DoS GoldenEye       0.99      0.94      0.96      2058
              DoS Hulk       0.98      0.91      0.94     46025
      DoS Slowhttptest       0.87      0.92      0.90      1100
         DoS slowloris       0.98      0.91      0.94      1159
           FTP-Patator       0.99      0.97      0.98      1587
              PortScan       0.89      0.95      0.92     31761
           SSH-Patator       0.93      0.96      0.94      1179
Web Attack Brute Force       0.00      0.00      0.00       302
        Web Attack XSS       0.00      0.00      0.00       130

              accuracy                           0.98    565562
             macro avg       0.80      0.74      0.76    565562
          weighted avg       0.98      0.98      0.98    565562

2019-12-24 12:23:46,582 [INFO] Overall accuracy (micro avg): 0.9786937594817191
/home/sunanda/research/ids_experiments/utility.py:307: RuntimeWarning: invalid value encountered in true_divide
  PPV = TP / (TP + FP)    # Precision or positive predictive value
/home/sunanda/research/ids_experiments/utility.py:311: RuntimeWarning: invalid value encountered in true_divide
  FDR = FP / (TP + FP)    # False discovery rate
/home/sunanda/research/ids_experiments/utility.py:313: RuntimeWarning: invalid value encountered in true_divide
  F1 = (2 * PPV * TPR) / (PPV + TPR)  # F1 score
2019-12-24 12:24:07,922 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9787         0.9787                       0.9787                0.0019                   0.0213  0.9787
1     Macro avg        0.9964         0.8008                       0.7403                0.0062                   0.2597  0.7572
2  Weighted avg        0.9817         0.9783                       0.9787                0.0534                   0.0213  0.9782
2019-12-24 12:24:27,905 [INFO] Dataset: Validation. Classification report below
2019-12-24 12:24:27,905 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       0.98      0.99      0.99    454264
                   Bot       1.00      0.31      0.47       391
                  DDoS       1.00      0.98      0.99     25605
         DoS GoldenEye       0.98      0.93      0.95      2059
              DoS Hulk       0.98      0.91      0.94     46025
      DoS Slowhttptest       0.88      0.92      0.90      1099
         DoS slowloris       0.98      0.92      0.95      1159
           FTP-Patator       0.99      0.96      0.98      1587
              PortScan       0.89      0.95      0.92     31761
           SSH-Patator       0.93      0.96      0.94      1180
Web Attack Brute Force       0.00      0.00      0.00       301
        Web Attack XSS       0.00      0.00      0.00       131

              accuracy                           0.98    565562
             macro avg       0.80      0.73      0.75    565562
          weighted avg       0.98      0.98      0.98    565562

2019-12-24 12:24:27,905 [INFO] Overall accuracy (micro avg): 0.9787857034242046
2019-12-24 12:24:49,424 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9788         0.9788                       0.9788                0.0019                   0.0212  0.9788
1     Macro avg        0.9965         0.8005                       0.7348                0.0062                   0.2652  0.7519
2  Weighted avg        0.9818         0.9784                       0.9788                0.0527                   0.0212  0.9783
2019-12-24 12:25:56,373 [INFO] Dataset: Training. Classification report below
2019-12-24 12:25:56,374 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       0.98      0.99      0.99   1362791
                   Bot       1.00      0.35      0.52      1174
                  DDoS       1.00      0.98      0.99     76815
         DoS GoldenEye       0.99      0.93      0.96      6176
              DoS Hulk       0.98      0.91      0.94    138074
      DoS Slowhttptest       0.89      0.92      0.90      3300
         DoS slowloris       0.97      0.93      0.95      3478
           FTP-Patator       0.99      0.97      0.98      4761
              PortScan       0.89      0.95      0.92     95282
           SSH-Patator       0.94      0.96      0.95      3538
Web Attack Brute Force       0.00      0.00      0.00       904
        Web Attack XSS       0.00      0.00      0.00       391

              accuracy                           0.98   1696684
             macro avg       0.80      0.74      0.76   1696684
          weighted avg       0.98      0.98      0.98   1696684

2019-12-24 12:25:56,374 [INFO] Overall accuracy (micro avg): 0.9789943206867042
2019-12-24 12:27:08,524 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9790         0.9790                       0.9790                0.0019                   0.0210  0.9790
1     Macro avg        0.9965         0.8032                       0.7403                0.0061                   0.2597  0.7582
2  Weighted avg        0.9820         0.9786                       0.9790                0.0525                   0.0210  0.9785
2019-12-24 12:27:08,589 [INFO] Results saved to: results_selected_models/selected_ids17_ae_ann_deep_rep3/selected_ids17_ae_ann_deep_rep3_results.xlsx
2019-12-24 12:27:08,593 [INFO] ================= Finished running experiment no. 3 ================= 

2019-12-24 12:27:08,634 [INFO] Created directory: results_selected_models/selected_ids17_ae_ann_deep_rep4
2019-12-24 12:27:08,635 [INFO] Initialized logging. log_filename = results_selected_models/selected_ids17_ae_ann_deep_rep4/run_log.log
2019-12-24 12:27:08,635 [INFO] ================= Running experiment no. 4  ================= 

2019-12-24 12:27:08,635 [INFO] Experiment parameters given below
2019-12-24 12:27:08,635 [INFO] 
{'experiment_num': 4, 'results_dir': 'results_selected_models/selected_ids17_ae_ann_deep_rep4', 'model_type': 'classifier', 'model': 'ae_ann', 'normal_label': 'BENIGN', 'scaling_type': 'NA', 'unsupervised_ratio': 0.25, 'ae_encoder_units': [128, 64, 32], 'ae_encoder_activations': ['relu', 'relu', 'relu'], 'ae_encoder_dropout_rates': [0.2, 0.2, 0.2], 'ae_encoder_l1_param': -1, 'ae_decoder_units': [64, 128], 'ae_decoder_activations': ['relu', 'relu'], 'ae_decoder_dropout_rates': [0.2, 0.2], 'output_activation': 'sigmoid', 'loss_function': 'binary_crossentropy', 'ae_epochs': 200, 'ann_layer_units': [64], 'ann_layer_activations': ['relu'], 'ann_layer_dropout_rates': [0.2], 'ann_epochs': 200, 'early_stop_patience': 50, 'batch_size': 256, 'dataset_dir': '../Datasets/full_datasets/ids2017', 'description': 'selected_ids17_ae_ann_deep_rep4'}
2019-12-24 12:27:08,635 [INFO] Created tensorboard log directory: results_selected_models/selected_ids17_ae_ann_deep_rep4/tf_logs_run_2019_12_24-12_27_08
2019-12-24 12:27:08,635 [INFO] Loading datsets from: ../Datasets/full_datasets/ids2017
2019-12-24 12:27:08,635 [INFO] Reading X, y files
2019-12-24 12:27:08,635 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_train.h5
2019-12-24 12:27:12,432 [INFO] Reading complete. time_to_read=3.80 seconds
2019-12-24 12:27:12,432 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_val.h5
2019-12-24 12:27:13,715 [INFO] Reading complete. time_to_read=1.28 seconds
2019-12-24 12:27:13,715 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_test.h5
2019-12-24 12:27:15,001 [INFO] Reading complete. time_to_read=1.29 seconds
2019-12-24 12:27:15,002 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_train.h5
2019-12-24 12:27:15,245 [INFO] Reading complete. time_to_read=0.24 seconds
2019-12-24 12:27:15,245 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_val.h5
2019-12-24 12:27:15,327 [INFO] Reading complete. time_to_read=0.08 seconds
2019-12-24 12:27:15,328 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_test.h5
2019-12-24 12:27:15,410 [INFO] Reading complete. time_to_read=0.08 seconds
2019-12-24 12:27:18,346 [INFO] Initializing model
2019-12-24 12:27:18,918 [INFO] _________________________________________________________________
2019-12-24 12:27:18,921 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-24 12:27:18,921 [INFO] =================================================================
2019-12-24 12:27:18,921 [INFO] dense_85 (Dense)             (None, 128)               10112     
2019-12-24 12:27:18,921 [INFO] _________________________________________________________________
2019-12-24 12:27:18,921 [INFO] batch_normalization_59 (Batc (None, 128)               512       
2019-12-24 12:27:18,921 [INFO] _________________________________________________________________
2019-12-24 12:27:18,921 [INFO] dropout_59 (Dropout)         (None, 128)               0         
2019-12-24 12:27:18,921 [INFO] _________________________________________________________________
2019-12-24 12:27:18,921 [INFO] dense_86 (Dense)             (None, 64)                8256      
2019-12-24 12:27:18,922 [INFO] _________________________________________________________________
2019-12-24 12:27:18,922 [INFO] batch_normalization_60 (Batc (None, 64)                256       
2019-12-24 12:27:18,922 [INFO] _________________________________________________________________
2019-12-24 12:27:18,922 [INFO] dropout_60 (Dropout)         (None, 64)                0         
2019-12-24 12:27:18,922 [INFO] _________________________________________________________________
2019-12-24 12:27:18,922 [INFO] dense_87 (Dense)             (None, 32)                2080      
2019-12-24 12:27:18,922 [INFO] _________________________________________________________________
2019-12-24 12:27:18,922 [INFO] batch_normalization_61 (Batc (None, 32)                128       
2019-12-24 12:27:18,922 [INFO] _________________________________________________________________
2019-12-24 12:27:18,922 [INFO] dropout_61 (Dropout)         (None, 32)                0         
2019-12-24 12:27:18,922 [INFO] _________________________________________________________________
2019-12-24 12:27:18,922 [INFO] dense_88 (Dense)             (None, 64)                2112      
2019-12-24 12:27:18,923 [INFO] _________________________________________________________________
2019-12-24 12:27:18,923 [INFO] batch_normalization_62 (Batc (None, 64)                256       
2019-12-24 12:27:18,923 [INFO] _________________________________________________________________
2019-12-24 12:27:18,923 [INFO] dropout_62 (Dropout)         (None, 64)                0         
2019-12-24 12:27:18,923 [INFO] _________________________________________________________________
2019-12-24 12:27:18,923 [INFO] dense_89 (Dense)             (None, 128)               8320      
2019-12-24 12:27:18,923 [INFO] _________________________________________________________________
2019-12-24 12:27:18,923 [INFO] batch_normalization_63 (Batc (None, 128)               512       
2019-12-24 12:27:18,923 [INFO] _________________________________________________________________
2019-12-24 12:27:18,923 [INFO] dropout_63 (Dropout)         (None, 128)               0         
2019-12-24 12:27:18,923 [INFO] _________________________________________________________________
2019-12-24 12:27:18,923 [INFO] dense_90 (Dense)             (None, 78)                10062     
2019-12-24 12:27:18,924 [INFO] =================================================================
2019-12-24 12:27:18,924 [INFO] Total params: 42,606
2019-12-24 12:27:18,924 [INFO] Trainable params: 41,774
2019-12-24 12:27:18,924 [INFO] Non-trainable params: 832
2019-12-24 12:27:18,924 [INFO] _________________________________________________________________
2019-12-24 12:27:19,066 [INFO] _________________________________________________________________
2019-12-24 12:27:19,066 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-24 12:27:19,066 [INFO] =================================================================
2019-12-24 12:27:19,066 [INFO] dense_91 (Dense)             (None, 64)                2112      
2019-12-24 12:27:19,066 [INFO] _________________________________________________________________
2019-12-24 12:27:19,067 [INFO] batch_normalization_64 (Batc (None, 64)                256       
2019-12-24 12:27:19,067 [INFO] _________________________________________________________________
2019-12-24 12:27:19,067 [INFO] dropout_64 (Dropout)         (None, 64)                0         
2019-12-24 12:27:19,067 [INFO] _________________________________________________________________
2019-12-24 12:27:19,067 [INFO] dense_92 (Dense)             (None, 12)                780       
2019-12-24 12:27:19,067 [INFO] =================================================================
2019-12-24 12:27:19,067 [INFO] Total params: 3,148
2019-12-24 12:27:19,067 [INFO] Trainable params: 3,020
2019-12-24 12:27:19,067 [INFO] Non-trainable params: 128
2019-12-24 12:27:19,067 [INFO] _________________________________________________________________
2019-12-24 12:27:19,067 [INFO] Training model
2019-12-24 12:27:19,067 [INFO] Splitting train set into 2 sets (unsupervised, supervised), random_seed = None
2019-12-24 12:27:45,055 [INFO] Split sizes (instances). total = 1696684, unsupervised = 424171, supervised = 1272513, unsupervised dataset hash = 736af3e14dd484f98e979994152f0d7a9946146b
2019-12-24 12:27:45,055 [INFO] Training autoencoder
 - val_f1: 0.9812
Epoch 00166: early stopping
Train on 424171 samples, validate on 565562 samples
Epoch 1/200
 - 28s - loss: -3.2004e+00 - val_loss: -3.9930e+00
Epoch 2/200
 - 25s - loss: -3.9046e+00 - val_loss: -4.0603e+00
Epoch 3/200
 - 25s - loss: -3.9596e+00 - val_loss: -4.0824e+00
Epoch 4/200
 - 25s - loss: -3.9878e+00 - val_loss: -4.0933e+00
Epoch 5/200
 - 25s - loss: -4.0061e+00 - val_loss: -4.0990e+00
Epoch 6/200
 - 25s - loss: -4.0200e+00 - val_loss: -4.1109e+00
Epoch 7/200
 - 25s - loss: -4.0296e+00 - val_loss: -4.1129e+00
Epoch 8/200
 - 25s - loss: -4.0370e+00 - val_loss: -4.1174e+00
Epoch 9/200
 - 25s - loss: -4.0429e+00 - val_loss: -4.1174e+00
Epoch 10/200
 - 25s - loss: -4.0490e+00 - val_loss: -4.1237e+00
Epoch 11/200
 - 25s - loss: -4.0539e+00 - val_loss: -4.1247e+00
Epoch 12/200
 - 25s - loss: -4.0580e+00 - val_loss: -4.1292e+00
Epoch 13/200
 - 25s - loss: -4.0614e+00 - val_loss: -4.1258e+00
Epoch 14/200
 - 25s - loss: -4.0647e+00 - val_loss: -4.1302e+00
Epoch 15/200
 - 25s - loss: -4.0681e+00 - val_loss: -4.1330e+00
Epoch 16/200
 - 25s - loss: -4.0705e+00 - val_loss: -4.1326e+00
Epoch 17/200
 - 25s - loss: -4.0718e+00 - val_loss: -4.1358e+00
Epoch 18/200
 - 25s - loss: -4.0745e+00 - val_loss: -4.1356e+00
Epoch 19/200
 - 25s - loss: -4.0753e+00 - val_loss: -4.1268e+00
Epoch 20/200
 - 25s - loss: -4.0767e+00 - val_loss: -4.1388e+00
Epoch 21/200
 - 25s - loss: -4.0776e+00 - val_loss: -4.1327e+00
2019-12-24 12:36:39,780 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep4/ae_model_epoch_20.pickle
Epoch 22/200
 - 25s - loss: -4.0802e+00 - val_loss: -4.1392e+00
Epoch 23/200
 - 25s - loss: -4.0819e+00 - val_loss: -4.1399e+00
Epoch 24/200
 - 25s - loss: -4.0831e+00 - val_loss: -4.1392e+00
Epoch 25/200
 - 25s - loss: -4.0821e+00 - val_loss: -4.1412e+00
Epoch 26/200
 - 25s - loss: -4.0839e+00 - val_loss: -4.1397e+00
Epoch 27/200
 - 25s - loss: -4.0860e+00 - val_loss: -4.1411e+00
Epoch 28/200
 - 25s - loss: -4.0838e+00 - val_loss: -4.1413e+00
Epoch 29/200
 - 25s - loss: -4.0874e+00 - val_loss: -4.1410e+00
Epoch 30/200
 - 25s - loss: -4.0886e+00 - val_loss: -4.1419e+00
Epoch 31/200
 - 25s - loss: -4.0888e+00 - val_loss: -4.1428e+00
Epoch 32/200
 - 25s - loss: -4.0887e+00 - val_loss: -4.1427e+00
Epoch 33/200
 - 25s - loss: -4.0893e+00 - val_loss: -4.1415e+00
Epoch 34/200
 - 25s - loss: -4.0898e+00 - val_loss: -4.1425e+00
Epoch 35/200
 - 25s - loss: -4.0913e+00 - val_loss: -4.1439e+00
Epoch 36/200
 - 25s - loss: -4.0903e+00 - val_loss: -4.1441e+00
Epoch 37/200
 - 25s - loss: -4.0926e+00 - val_loss: -4.1449e+00
Epoch 38/200
 - 25s - loss: -4.0919e+00 - val_loss: -4.1436e+00
Epoch 39/200
 - 25s - loss: -4.0931e+00 - val_loss: -4.1460e+00
Epoch 40/200
 - 25s - loss: -4.0922e+00 - val_loss: -4.1457e+00
Epoch 41/200
 - 25s - loss: -4.0952e+00 - val_loss: -4.1451e+00
2019-12-24 12:44:53,321 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep4/ae_model_epoch_40.pickle
Epoch 42/200
 - 25s - loss: -4.0949e+00 - val_loss: -4.1462e+00
Epoch 43/200
 - 25s - loss: -4.0953e+00 - val_loss: -4.1460e+00
Epoch 44/200
 - 25s - loss: -4.0950e+00 - val_loss: -4.1453e+00
Epoch 45/200
 - 25s - loss: -4.0953e+00 - val_loss: -4.1461e+00
Epoch 46/200
 - 25s - loss: -4.0962e+00 - val_loss: -4.1468e+00
Epoch 47/200
 - 25s - loss: -4.0967e+00 - val_loss: -4.1472e+00
Epoch 48/200
 - 25s - loss: -4.0968e+00 - val_loss: -4.1465e+00
Epoch 49/200
 - 25s - loss: -4.0968e+00 - val_loss: -4.1454e+00
Epoch 50/200
 - 25s - loss: -4.0972e+00 - val_loss: -4.1464e+00
Epoch 51/200
 - 25s - loss: -4.0968e+00 - val_loss: -4.1450e+00
Epoch 52/200
 - 25s - loss: -4.0983e+00 - val_loss: -4.1456e+00
Epoch 53/200
 - 25s - loss: -4.0967e+00 - val_loss: -4.1455e+00
Epoch 54/200
 - 25s - loss: -4.0989e+00 - val_loss: -4.1473e+00
Epoch 55/200
 - 25s - loss: -4.0987e+00 - val_loss: -4.1473e+00
Epoch 56/200
 - 25s - loss: -4.0995e+00 - val_loss: -4.1478e+00
Epoch 57/200
 - 25s - loss: -4.0991e+00 - val_loss: -4.1454e+00
Epoch 58/200
 - 25s - loss: -4.1002e+00 - val_loss: -4.1477e+00
Epoch 59/200
 - 25s - loss: -4.0995e+00 - val_loss: -4.1478e+00
Epoch 60/200
 - 25s - loss: -4.0991e+00 - val_loss: -4.1478e+00
Epoch 61/200
 - 25s - loss: -4.1009e+00 - val_loss: -4.1474e+00
2019-12-24 12:53:07,250 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep4/ae_model_epoch_60.pickle
Epoch 62/200
 - 25s - loss: -4.1012e+00 - val_loss: -4.1470e+00
Epoch 63/200
 - 25s - loss: -4.1013e+00 - val_loss: -4.1481e+00
Epoch 64/200
 - 25s - loss: -4.1015e+00 - val_loss: -4.1471e+00
Epoch 65/200
 - 25s - loss: -4.1011e+00 - val_loss: -4.1485e+00
Epoch 66/200
 - 25s - loss: -4.1021e+00 - val_loss: -4.1485e+00
Epoch 67/200
 - 25s - loss: -4.1008e+00 - val_loss: -4.1486e+00
Epoch 68/200
 - 25s - loss: -4.1019e+00 - val_loss: -4.1447e+00
Epoch 69/200
 - 25s - loss: -4.1013e+00 - val_loss: -4.1491e+00
Epoch 70/200
 - 25s - loss: -4.1030e+00 - val_loss: -4.1484e+00
Epoch 71/200
 - 25s - loss: -4.1020e+00 - val_loss: -4.1489e+00
Epoch 72/200
 - 25s - loss: -4.1029e+00 - val_loss: -4.1489e+00
Epoch 73/200
 - 25s - loss: -4.1026e+00 - val_loss: -4.1489e+00
Epoch 74/200
 - 25s - loss: -4.1031e+00 - val_loss: -4.1490e+00
Epoch 75/200
 - 25s - loss: -4.1037e+00 - val_loss: -4.1482e+00
Epoch 76/200
 - 25s - loss: -4.1042e+00 - val_loss: -4.1467e+00
Epoch 77/200
 - 25s - loss: -4.1034e+00 - val_loss: -4.1477e+00
Epoch 78/200
 - 25s - loss: -4.1032e+00 - val_loss: -4.1492e+00
Epoch 79/200
 - 25s - loss: -4.1044e+00 - val_loss: -4.1490e+00
Epoch 80/200
 - 25s - loss: -4.1042e+00 - val_loss: -4.1484e+00
Epoch 81/200
 - 25s - loss: -4.1051e+00 - val_loss: -4.1489e+00
2019-12-24 13:01:21,033 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep4/ae_model_epoch_80.pickle
Epoch 82/200
 - 25s - loss: -4.1044e+00 - val_loss: -4.1471e+00
Epoch 83/200
 - 25s - loss: -4.1052e+00 - val_loss: -4.1494e+00
Epoch 84/200
 - 25s - loss: -4.1039e+00 - val_loss: -4.1505e+00
Epoch 85/200
 - 25s - loss: -4.1055e+00 - val_loss: -4.1493e+00
Epoch 86/200
 - 25s - loss: -4.1042e+00 - val_loss: -4.1504e+00
Epoch 87/200
 - 25s - loss: -4.1049e+00 - val_loss: -4.1486e+00
Epoch 88/200
 - 25s - loss: -4.1053e+00 - val_loss: -4.1497e+00
Epoch 89/200
 - 25s - loss: -4.1033e+00 - val_loss: -4.1500e+00
Epoch 90/200
 - 25s - loss: -4.1055e+00 - val_loss: -4.1501e+00
Epoch 91/200
 - 25s - loss: -4.1052e+00 - val_loss: -4.1504e+00
Epoch 92/200
 - 25s - loss: -4.1055e+00 - val_loss: -4.1507e+00
Epoch 93/200
 - 25s - loss: -4.1060e+00 - val_loss: -4.1506e+00
Epoch 94/200
 - 25s - loss: -4.1064e+00 - val_loss: -4.1497e+00
Epoch 95/200
 - 25s - loss: -4.1051e+00 - val_loss: -4.1508e+00
Epoch 96/200
 - 25s - loss: -4.1065e+00 - val_loss: -4.1509e+00
Epoch 97/200
 - 25s - loss: -4.1064e+00 - val_loss: -4.1511e+00
Epoch 98/200
 - 25s - loss: -4.1061e+00 - val_loss: -4.1489e+00
Epoch 99/200
 - 25s - loss: -4.1071e+00 - val_loss: -4.1507e+00
Epoch 100/200
 - 25s - loss: -4.1071e+00 - val_loss: -4.1494e+00
Epoch 101/200
 - 25s - loss: -4.1065e+00 - val_loss: -4.1516e+00
2019-12-24 13:09:35,074 [INFO] epoch = 100. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep4/ae_model_epoch_100.pickle
Epoch 102/200
 - 25s - loss: -4.1071e+00 - val_loss: -4.1512e+00
Epoch 103/200
 - 25s - loss: -4.1078e+00 - val_loss: -4.1504e+00
Epoch 104/200
 - 25s - loss: -4.1064e+00 - val_loss: -4.1496e+00
Epoch 105/200
 - 25s - loss: -4.1060e+00 - val_loss: -4.1502e+00
Epoch 106/200
 - 25s - loss: -4.1079e+00 - val_loss: -4.1518e+00
Epoch 107/200
 - 25s - loss: -4.1081e+00 - val_loss: -4.1493e+00
Epoch 108/200
 - 25s - loss: -4.1064e+00 - val_loss: -4.1504e+00
Epoch 109/200
 - 25s - loss: -4.1073e+00 - val_loss: -4.1517e+00
Epoch 110/200
 - 25s - loss: -4.1078e+00 - val_loss: -4.1513e+00
Epoch 111/200
 - 25s - loss: -4.1077e+00 - val_loss: -4.1513e+00
Epoch 112/200
 - 25s - loss: -4.1080e+00 - val_loss: -4.1514e+00
Epoch 113/200
 - 25s - loss: -4.1086e+00 - val_loss: -4.1512e+00
Epoch 114/200
 - 25s - loss: -4.1080e+00 - val_loss: -4.1513e+00
Epoch 115/200
 - 25s - loss: -4.1067e+00 - val_loss: -4.1514e+00
Epoch 116/200
 - 25s - loss: -4.1080e+00 - val_loss: -4.1511e+00
Epoch 117/200
 - 25s - loss: -4.1088e+00 - val_loss: -4.1497e+00
Epoch 118/200
 - 25s - loss: -4.1093e+00 - val_loss: -4.1518e+00
Epoch 119/200
 - 25s - loss: -4.1064e+00 - val_loss: -4.1511e+00
Epoch 120/200
 - 25s - loss: -4.1077e+00 - val_loss: -4.1517e+00
Epoch 121/200
 - 25s - loss: -4.1085e+00 - val_loss: -4.1516e+00
2019-12-24 13:17:47,638 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep4/ae_model_epoch_120.pickle
Epoch 122/200
 - 25s - loss: -4.1077e+00 - val_loss: -4.1513e+00
Epoch 123/200
 - 25s - loss: -4.1083e+00 - val_loss: -4.1519e+00
Epoch 124/200
 - 25s - loss: -4.1078e+00 - val_loss: -4.1502e+00
Epoch 125/200
 - 25s - loss: -4.1085e+00 - val_loss: -4.1516e+00
Epoch 126/200
 - 25s - loss: -4.1088e+00 - val_loss: -4.1520e+00
Epoch 127/200
 - 25s - loss: -4.1093e+00 - val_loss: -4.1494e+00
Epoch 128/200
 - 25s - loss: -4.1092e+00 - val_loss: -4.1499e+00
Epoch 129/200
 - 25s - loss: -4.1056e+00 - val_loss: -4.1510e+00
Epoch 130/200
 - 25s - loss: -4.1083e+00 - val_loss: -4.1499e+00
Epoch 131/200
 - 25s - loss: -4.1090e+00 - val_loss: -4.1484e+00
Epoch 132/200
 - 25s - loss: -4.1093e+00 - val_loss: -4.1513e+00
Epoch 133/200
 - 25s - loss: -4.1098e+00 - val_loss: -4.1517e+00
Epoch 134/200
 - 25s - loss: -4.1097e+00 - val_loss: -4.1504e+00
Epoch 135/200
 - 25s - loss: -4.1100e+00 - val_loss: -4.1518e+00
Epoch 136/200
 - 25s - loss: -4.1089e+00 - val_loss: -4.1511e+00
Epoch 137/200
 - 25s - loss: -4.1093e+00 - val_loss: -4.1518e+00
Epoch 138/200
 - 25s - loss: -4.1080e+00 - val_loss: -4.1520e+00
Epoch 139/200
 - 25s - loss: -4.1104e+00 - val_loss: -4.1518e+00
Epoch 140/200
 - 25s - loss: -4.1099e+00 - val_loss: -4.1527e+00
Epoch 141/200
 - 25s - loss: -4.1086e+00 - val_loss: -4.1517e+00
2019-12-24 13:26:00,359 [INFO] epoch = 140. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep4/ae_model_epoch_140.pickle
Epoch 142/200
 - 25s - loss: -4.1099e+00 - val_loss: -4.1517e+00
Epoch 143/200
 - 25s - loss: -4.1103e+00 - val_loss: -4.1525e+00
Epoch 144/200
 - 25s - loss: -4.1098e+00 - val_loss: -4.1523e+00
Epoch 145/200
 - 25s - loss: -4.1104e+00 - val_loss: -4.1529e+00
Epoch 146/200
 - 25s - loss: -4.1082e+00 - val_loss: -4.1528e+00
Epoch 147/200
 - 25s - loss: -4.1099e+00 - val_loss: -4.1519e+00
Epoch 148/200
 - 25s - loss: -4.1085e+00 - val_loss: -4.1523e+00
Epoch 149/200
 - 25s - loss: -4.1102e+00 - val_loss: -4.1507e+00
Epoch 150/200
 - 25s - loss: -4.1109e+00 - val_loss: -4.1530e+00
Epoch 151/200
 - 25s - loss: -4.1111e+00 - val_loss: -4.1532e+00
Epoch 152/200
 - 25s - loss: -4.1107e+00 - val_loss: -4.1494e+00
Epoch 153/200
 - 25s - loss: -4.1098e+00 - val_loss: -4.1531e+00
Epoch 154/200
 - 25s - loss: -4.1108e+00 - val_loss: -4.1510e+00
Epoch 155/200
 - 25s - loss: -4.1109e+00 - val_loss: -4.1531e+00
Epoch 156/200
 - 25s - loss: -4.1097e+00 - val_loss: -4.1523e+00
Epoch 157/200
 - 25s - loss: -4.1119e+00 - val_loss: -4.1524e+00
Epoch 158/200
 - 25s - loss: -4.1104e+00 - val_loss: -4.1528e+00
Epoch 159/200
 - 25s - loss: -4.1091e+00 - val_loss: -4.1517e+00
Epoch 160/200
 - 25s - loss: -4.1093e+00 - val_loss: -4.1519e+00
Epoch 161/200
 - 25s - loss: -4.1110e+00 - val_loss: -4.1530e+00
2019-12-24 13:34:12,724 [INFO] epoch = 160. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep4/ae_model_epoch_160.pickle
Epoch 162/200
 - 25s - loss: -4.1112e+00 - val_loss: -4.1519e+00
Epoch 163/200
 - 25s - loss: -4.1114e+00 - val_loss: -4.1535e+00
Epoch 164/200
 - 25s - loss: -4.1109e+00 - val_loss: -4.1531e+00
Epoch 165/200
 - 25s - loss: -4.1105e+00 - val_loss: -4.1518e+00
Epoch 166/200
 - 25s - loss: -4.1113e+00 - val_loss: -4.1533e+00
Epoch 167/200
 - 25s - loss: -4.1111e+00 - val_loss: -4.1528e+00
Epoch 168/200
 - 25s - loss: -4.1112e+00 - val_loss: -4.1515e+00
Epoch 169/200
 - 25s - loss: -4.1113e+00 - val_loss: -4.1534e+00
Epoch 170/200
 - 25s - loss: -4.1111e+00 - val_loss: -4.1524e+00
Epoch 171/200
 - 25s - loss: -4.1119e+00 - val_loss: -4.1522e+00
Epoch 172/200
 - 25s - loss: -4.1122e+00 - val_loss: -4.1534e+00
Epoch 173/200
 - 25s - loss: -4.1120e+00 - val_loss: -4.1519e+00
Epoch 174/200
 - 25s - loss: -4.1113e+00 - val_loss: -4.1485e+00
Epoch 175/200
 - 25s - loss: -4.1095e+00 - val_loss: -4.1516e+00
Epoch 176/200
 - 25s - loss: -4.1114e+00 - val_loss: -4.1507e+00
Epoch 177/200
 - 25s - loss: -4.1116e+00 - val_loss: -4.1516e+00
Epoch 178/200
 - 25s - loss: -4.1120e+00 - val_loss: -4.1521e+00
Epoch 179/200
 - 25s - loss: -4.1112e+00 - val_loss: -4.1526e+00
Epoch 180/200
 - 25s - loss: -4.1115e+00 - val_loss: -4.1525e+00
Epoch 181/200
 - 25s - loss: -4.1119e+00 - val_loss: -4.1534e+00
2019-12-24 13:42:25,082 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep4/ae_model_epoch_180.pickle
Epoch 182/200
 - 25s - loss: -4.1105e+00 - val_loss: -4.1529e+00
Epoch 183/200
 - 25s - loss: -4.1119e+00 - val_loss: -4.1525e+00
Epoch 184/200
 - 25s - loss: -4.1120e+00 - val_loss: -4.1532e+00
Epoch 185/200
 - 25s - loss: -4.1113e+00 - val_loss: -4.1537e+00
Epoch 186/200
 - 25s - loss: -4.1111e+00 - val_loss: -4.1533e+00
Epoch 187/200
 - 25s - loss: -4.1114e+00 - val_loss: -4.1535e+00
Epoch 188/200
 - 25s - loss: -4.1109e+00 - val_loss: -4.1527e+00
Epoch 189/200
 - 25s - loss: -4.1117e+00 - val_loss: -4.1520e+00
Epoch 190/200
 - 25s - loss: -4.1120e+00 - val_loss: -4.1515e+00
Epoch 191/200
 - 25s - loss: -4.1115e+00 - val_loss: -4.1536e+00
Epoch 192/200
 - 25s - loss: -4.1122e+00 - val_loss: -4.1529e+00
Epoch 193/200
 - 25s - loss: -4.1111e+00 - val_loss: -4.1520e+00
Epoch 194/200
 - 25s - loss: -4.1118e+00 - val_loss: -4.1535e+00
Epoch 195/200
 - 25s - loss: -4.1118e+00 - val_loss: -4.1518e+00
Epoch 196/200
 - 25s - loss: -4.1120e+00 - val_loss: -4.1530e+00
Epoch 197/200
 - 25s - loss: -4.1115e+00 - val_loss: -4.1526e+00
Epoch 198/200
 - 25s - loss: -4.1114e+00 - val_loss: -4.1519e+00
Epoch 199/200
 - 25s - loss: -4.1121e+00 - val_loss: -4.1533e+00
Epoch 200/200
 - 25s - loss: -4.1124e+00 - val_loss: -4.1537e+00
2019-12-24 13:50:12,765 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-24 13:51:24,892 [INFO] Last epoch loss evaluation: train_loss = -4.153029, val_loss = -4.153683
2019-12-24 13:51:24,892 [INFO] Training autoencoder complete
2019-12-24 13:51:24,893 [INFO] Encoding data for supervised training
2019-12-24 13:52:45,780 [INFO] Encoding complete
2019-12-24 13:52:45,780 [INFO] Training neural network layers (after autoencoder)
Train on 1272513 samples, validate on 565562 samples
Epoch 1/200
 - 23s - loss: 0.0240 - val_loss: 0.0125
 - val_f1: 0.9697
Epoch 2/200
 - 22s - loss: 0.0137 - val_loss: 0.0115
 - val_f1: 0.9716
Epoch 3/200
 - 22s - loss: 0.0125 - val_loss: 0.0107
 - val_f1: 0.9733
Epoch 4/200
 - 22s - loss: 0.0119 - val_loss: 0.0107
 - val_f1: 0.9738
Epoch 5/200
 - 22s - loss: 0.0117 - val_loss: 0.0113
 - val_f1: 0.9684
Epoch 6/200
 - 22s - loss: 0.0113 - val_loss: 0.0096
 - val_f1: 0.9746
Epoch 7/200
 - 22s - loss: 0.0112 - val_loss: 0.0097
 - val_f1: 0.9763
Epoch 8/200
 - 22s - loss: 0.0110 - val_loss: 0.0109
 - val_f1: 0.9704
Epoch 9/200
 - 22s - loss: 0.0109 - val_loss: 0.0092
 - val_f1: 0.9746
Epoch 10/200
 - 22s - loss: 0.0108 - val_loss: 0.0102
 - val_f1: 0.9715
Epoch 11/200
 - 22s - loss: 0.0107 - val_loss: 0.0096
 - val_f1: 0.9751
Epoch 12/200
 - 22s - loss: 0.0106 - val_loss: 0.0095
 - val_f1: 0.9737
Epoch 13/200
 - 22s - loss: 0.0105 - val_loss: 0.0089
 - val_f1: 0.9767
Epoch 14/200
 - 22s - loss: 0.0104 - val_loss: 0.0091
 - val_f1: 0.9751
Epoch 15/200
 - 22s - loss: 0.0103 - val_loss: 0.0088
 - val_f1: 0.9763
Epoch 16/200
 - 22s - loss: 0.0103 - val_loss: 0.0094
 - val_f1: 0.9750
Epoch 17/200
 - 22s - loss: 0.0102 - val_loss: 0.0091
 - val_f1: 0.9770
Epoch 18/200
 - 22s - loss: 0.0101 - val_loss: 0.0095
 - val_f1: 0.9709
Epoch 19/200
 - 22s - loss: 0.0101 - val_loss: 0.0087
 - val_f1: 0.9762
Epoch 20/200
 - 22s - loss: 0.0101 - val_loss: 0.0089
 - val_f1: 0.9773
Epoch 21/200
 - 22s - loss: 0.0100 - val_loss: 0.0087
2019-12-24 14:07:32,764 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep4/ann_model_epoch_20.pickle
 - val_f1: 0.9774
Epoch 22/200
 - 22s - loss: 0.0099 - val_loss: 0.0086
 - val_f1: 0.9774
Epoch 23/200
 - 22s - loss: 0.0099 - val_loss: 0.0083
 - val_f1: 0.9799
Epoch 24/200
 - 22s - loss: 0.0099 - val_loss: 0.0103
 - val_f1: 0.9685
Epoch 25/200
 - 22s - loss: 0.0099 - val_loss: 0.0089
 - val_f1: 0.9765
Epoch 26/200
 - 22s - loss: 0.0098 - val_loss: 0.0095
 - val_f1: 0.9745
Epoch 27/200
 - 22s - loss: 0.0098 - val_loss: 0.0096
 - val_f1: 0.9751
Epoch 28/200
 - 22s - loss: 0.0098 - val_loss: 0.0094
 - val_f1: 0.9760
Epoch 29/200
 - 22s - loss: 0.0098 - val_loss: 0.0086
 - val_f1: 0.9761
Epoch 30/200
 - 22s - loss: 0.0097 - val_loss: 0.0098
 - val_f1: 0.9740
Epoch 31/200
 - 22s - loss: 0.0097 - val_loss: 0.0084
 - val_f1: 0.9798
Epoch 32/200
 - 22s - loss: 0.0097 - val_loss: 0.0087
 - val_f1: 0.9776
Epoch 33/200
 - 22s - loss: 0.0097 - val_loss: 0.0084
 - val_f1: 0.9778
Epoch 34/200
 - 22s - loss: 0.0096 - val_loss: 0.0089
 - val_f1: 0.9773
Epoch 35/200
 - 22s - loss: 0.0097 - val_loss: 0.0093
 - val_f1: 0.9766
Epoch 36/200
 - 22s - loss: 0.0097 - val_loss: 0.0100
 - val_f1: 0.9755
Epoch 37/200
 - 22s - loss: 0.0097 - val_loss: 0.0093
 - val_f1: 0.9775
Epoch 38/200
 - 22s - loss: 0.0097 - val_loss: 0.0091
 - val_f1: 0.9750
Epoch 39/200
 - 22s - loss: 0.0096 - val_loss: 0.0089
 - val_f1: 0.9763
Epoch 40/200
 - 22s - loss: 0.0096 - val_loss: 0.0085
 - val_f1: 0.9761
Epoch 41/200
 - 22s - loss: 0.0096 - val_loss: 0.0103
2019-12-24 14:21:43,327 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep4/ann_model_epoch_40.pickle
 - val_f1: 0.9766
Epoch 42/200
 - 22s - loss: 0.0096 - val_loss: 0.0085
 - val_f1: 0.9763
Epoch 43/200
 - 22s - loss: 0.0095 - val_loss: 0.0083
 - val_f1: 0.9782
Epoch 44/200
 - 22s - loss: 0.0095 - val_loss: 0.0090
 - val_f1: 0.9764
Epoch 45/200
 - 22s - loss: 0.0095 - val_loss: 0.0083
 - val_f1: 0.9766
Epoch 46/200
 - 22s - loss: 0.0094 - val_loss: 0.0093
 - val_f1: 0.9766
Epoch 47/200
 - 22s - loss: 0.0095 - val_loss: 0.0081
 - val_f1: 0.9798
Epoch 48/200
 - 22s - loss: 0.0095 - val_loss: 0.0083
 - val_f1: 0.9788
Epoch 49/200
 - 22s - loss: 0.0095 - val_loss: 0.0082
 - val_f1: 0.9775
Epoch 50/200
 - 22s - loss: 0.0095 - val_loss: 0.0082
 - val_f1: 0.9776
Epoch 51/200
 - 22s - loss: 0.0094 - val_loss: 0.0084
 - val_f1: 0.9786
Epoch 52/200
 - 22s - loss: 0.0094 - val_loss: 0.0083
 - val_f1: 0.9785
Epoch 53/200
 - 22s - loss: 0.0094 - val_loss: 0.0084
 - val_f1: 0.9785
Epoch 54/200
 - 22s - loss: 0.0094 - val_loss: 0.0086
 - val_f1: 0.9764
Epoch 55/200
 - 22s - loss: 0.0094 - val_loss: 0.0080
 - val_f1: 0.9777
Epoch 56/200
 - 22s - loss: 0.0094 - val_loss: 0.0087
 - val_f1: 0.9782
Epoch 57/200
 - 22s - loss: 0.0094 - val_loss: 0.0082
 - val_f1: 0.9776
Epoch 58/200
 - 22s - loss: 0.0094 - val_loss: 0.0083
 - val_f1: 0.9779
Epoch 59/200
 - 22s - loss: 0.0093 - val_loss: 0.0082
 - val_f1: 0.9779
Epoch 60/200
 - 22s - loss: 0.0093 - val_loss: 0.0092
 - val_f1: 0.9778
Epoch 61/200
 - 22s - loss: 0.0093 - val_loss: 0.0090
2019-12-24 14:35:54,164 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep4/ann_model_epoch_60.pickle
 - val_f1: 0.9768
Epoch 62/200
 - 22s - loss: 0.0094 - val_loss: 0.0087
 - val_f1: 0.9785
Epoch 63/200
 - 22s - loss: 0.0093 - val_loss: 0.0082
 - val_f1: 0.9776
Epoch 64/200
 - 22s - loss: 0.0094 - val_loss: 0.0085
 - val_f1: 0.9785
Epoch 65/200
 - 22s - loss: 0.0093 - val_loss: 0.0079
 - val_f1: 0.9775
Epoch 66/200
 - 22s - loss: 0.0093 - val_loss: 0.0090
 - val_f1: 0.9764
Epoch 67/200
 - 22s - loss: 0.0094 - val_loss: 0.0081
 - val_f1: 0.9777
Epoch 68/200
 - 22s - loss: 0.0094 - val_loss: 0.0087
 - val_f1: 0.9779
Epoch 69/200
 - 22s - loss: 0.0093 - val_loss: 0.0086
 - val_f1: 0.9770
Epoch 70/200
 - 22s - loss: 0.0093 - val_loss: 0.0086
 - val_f1: 0.9758
Epoch 71/200
 - 22s - loss: 0.0093 - val_loss: 0.0096
 - val_f1: 0.9779
Epoch 72/200
 - 22s - loss: 0.0094 - val_loss: 0.0082
 - val_f1: 0.9763
Epoch 73/200
 - 22s - loss: 0.0093 - val_loss: 0.0083
 - val_f1: 0.9780
Epoch 74/200
 - 22s - loss: 0.0093 - val_loss: 0.0084
 - val_f1: 0.9787
Epoch 75/200
 - 22s - loss: 0.0093 - val_loss: 0.0081
 - val_f1: 0.9780
Epoch 76/200
 - 22s - loss: 0.0093 - val_loss: 0.0082
 - val_f1: 0.9773
Epoch 77/200
 - 22s - loss: 0.0092 - val_loss: 0.0081
 - val_f1: 0.9786
Epoch 78/200
 - 22s - loss: 0.0093 - val_loss: 0.0082
 - val_f1: 0.9796
Epoch 79/200
 - 22s - loss: 0.0092 - val_loss: 0.0096
 - val_f1: 0.9754
Epoch 80/200
 - 22s - loss: 0.0092 - val_loss: 0.0082
 - val_f1: 0.9792
Epoch 81/200
 - 22s - loss: 0.0092 - val_loss: 0.0079
2019-12-24 14:50:03,953 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep4/ann_model_epoch_80.pickle
 - val_f1: 0.9787
Epoch 82/200
 - 22s - loss: 0.0092 - val_loss: 0.0080
 - val_f1: 0.9804
Epoch 83/200
 - 22s - loss: 0.0092 - val_loss: 0.0081
 - val_f1: 0.9805
Epoch 84/200
 - 22s - loss: 0.0092 - val_loss: 0.0083
 - val_f1: 0.9791
Epoch 85/200
 - 22s - loss: 0.0092 - val_loss: 0.0085
 - val_f1: 0.9754
Epoch 86/200
 - 22s - loss: 0.0092 - val_loss: 0.0093
 - val_f1: 0.9772
Epoch 87/200
 - 22s - loss: 0.0092 - val_loss: 0.0083
 - val_f1: 0.9793
Epoch 88/200
 - 22s - loss: 0.0092 - val_loss: 0.0086
 - val_f1: 0.9761
Epoch 89/200
 - 22s - loss: 0.0091 - val_loss: 0.0081
 - val_f1: 0.9767
Epoch 90/200
 - 22s - loss: 0.0091 - val_loss: 0.0089
 - val_f1: 0.9771
Epoch 91/200
 - 22s - loss: 0.0091 - val_loss: 0.0086
 - val_f1: 0.9779
Epoch 92/200
 - 22s - loss: 0.0091 - val_loss: 0.0085
 - val_f1: 0.9784
Epoch 93/200
 - 22s - loss: 0.0091 - val_loss: 0.0080
 - val_f1: 0.9817
Epoch 94/200
 - 22s - loss: 0.0091 - val_loss: 0.0077
 - val_f1: 0.9810
Epoch 95/200
 - 22s - loss: 0.0090 - val_loss: 0.0081
 - val_f1: 0.9771
Epoch 96/200
 - 22s - loss: 0.0090 - val_loss: 0.0078
 - val_f1: 0.9779
Epoch 97/200
 - 22s - loss: 0.0091 - val_loss: 0.0086
 - val_f1: 0.9774
Epoch 98/200
 - 22s - loss: 0.0090 - val_loss: 0.0082
 - val_f1: 0.9788
Epoch 99/200
 - 22s - loss: 0.0091 - val_loss: 0.0094
 - val_f1: 0.9772
Epoch 100/200
 - 22s - loss: 0.0090 - val_loss: 0.0080
 - val_f1: 0.9777
Epoch 101/200
 - 22s - loss: 0.0091 - val_loss: 0.0079
2019-12-24 15:04:14,641 [INFO] epoch = 100. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep4/ann_model_epoch_100.pickle
 - val_f1: 0.9802
Epoch 102/200
 - 22s - loss: 0.0091 - val_loss: 0.0085
 - val_f1: 0.9782
Epoch 103/200
 - 22s - loss: 0.0091 - val_loss: 0.0082
 - val_f1: 0.9786
Epoch 104/200
 - 22s - loss: 0.0090 - val_loss: 0.0081
 - val_f1: 0.9776
Epoch 105/200
 - 22s - loss: 0.0090 - val_loss: 0.0078
 - val_f1: 0.9788
Epoch 106/200
 - 22s - loss: 0.0090 - val_loss: 0.0079
 - val_f1: 0.9794
Epoch 107/200
 - 22s - loss: 0.0090 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 108/200
 - 22s - loss: 0.0090 - val_loss: 0.0078
 - val_f1: 0.9810
Epoch 109/200
 - 22s - loss: 0.0090 - val_loss: 0.0089
 - val_f1: 0.9771
Epoch 110/200
 - 22s - loss: 0.0090 - val_loss: 0.0076
 - val_f1: 0.9797
Epoch 111/200
 - 22s - loss: 0.0090 - val_loss: 0.0082
 - val_f1: 0.9775
Epoch 112/200
 - 22s - loss: 0.0090 - val_loss: 0.0086
 - val_f1: 0.9764
Epoch 113/200
 - 22s - loss: 0.0090 - val_loss: 0.0079
 - val_f1: 0.9795
Epoch 114/200
 - 22s - loss: 0.0090 - val_loss: 0.0075
 - val_f1: 0.9814
Epoch 115/200
 - 22s - loss: 0.0090 - val_loss: 0.0081
 - val_f1: 0.9801
Epoch 116/200
 - 22s - loss: 0.0089 - val_loss: 0.0078
 - val_f1: 0.9820
Epoch 117/200
 - 22s - loss: 0.0090 - val_loss: 0.0096
 - val_f1: 0.9777
Epoch 118/200
 - 22s - loss: 0.0089 - val_loss: 0.0097
 - val_f1: 0.9778
Epoch 119/200
 - 22s - loss: 0.0090 - val_loss: 0.0078
 - val_f1: 0.9792
Epoch 120/200
 - 22s - loss: 0.0089 - val_loss: 0.0078
 - val_f1: 0.9832
Epoch 121/200
 - 22s - loss: 0.0089 - val_loss: 0.0083
2019-12-24 15:18:20,624 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep4/ann_model_epoch_120.pickle
 - val_f1: 0.9779
Epoch 122/200
 - 22s - loss: 0.0089 - val_loss: 0.0077
 - val_f1: 0.9797
Epoch 123/200
 - 22s - loss: 0.0090 - val_loss: 0.0082
 - val_f1: 0.9780
Epoch 124/200
 - 22s - loss: 0.0089 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 125/200
 - 22s - loss: 0.0089 - val_loss: 0.0086
 - val_f1: 0.9776
Epoch 126/200
 - 22s - loss: 0.0089 - val_loss: 0.0085
 - val_f1: 0.9791
Epoch 127/200
 - 22s - loss: 0.0089 - val_loss: 0.0084
 - val_f1: 0.9783
Epoch 128/200
 - 22s - loss: 0.0089 - val_loss: 0.0078
 - val_f1: 0.9803
Epoch 129/200
 - 22s - loss: 0.0089 - val_loss: 0.0084
 - val_f1: 0.9755
Epoch 130/200
 - 22s - loss: 0.0089 - val_loss: 0.0084
 - val_f1: 0.9757
Epoch 131/200
 - 22s - loss: 0.0089 - val_loss: 0.0083
 - val_f1: 0.9773
Epoch 132/200
 - 22s - loss: 0.0089 - val_loss: 0.0083
 - val_f1: 0.9773
Epoch 133/200
 - 22s - loss: 0.0089 - val_loss: 0.0080
 - val_f1: 0.9795
Epoch 134/200
 - 22s - loss: 0.0089 - val_loss: 0.0080
 - val_f1: 0.9794
Epoch 135/200
 - 22s - loss: 0.0088 - val_loss: 0.0081
 - val_f1: 0.9768
Epoch 136/200
 - 22s - loss: 0.0089 - val_loss: 0.0079
 - val_f1: 0.9798
Epoch 137/200
 - 22s - loss: 0.0088 - val_loss: 0.0102
 - val_f1: 0.9781
Epoch 138/200
 - 22s - loss: 0.0089 - val_loss: 0.0079
 - val_f1: 0.9796
Epoch 139/200
 - 22s - loss: 0.0088 - val_loss: 0.0092
 - val_f1: 0.9777
Epoch 140/200
 - 22s - loss: 0.0088 - val_loss: 0.0079
 - val_f1: 0.9792
Epoch 141/200
 - 22s - loss: 0.0088 - val_loss: 0.0079
2019-12-24 15:32:23,529 [INFO] epoch = 140. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep4/ann_model_epoch_140.pickle
 - val_f1: 0.9798
Epoch 142/200
 - 22s - loss: 0.0088 - val_loss: 0.0081
 - val_f1: 0.9780
Epoch 143/200
 - 22s - loss: 0.0088 - val_loss: 0.0080
 - val_f1: 0.9786
Epoch 144/200
 - 22s - loss: 0.0088 - val_loss: 0.0076
 - val_f1: 0.9793
Epoch 145/200
 - 22s - loss: 0.0088 - val_loss: 0.0079
 - val_f1: 0.9789
Epoch 146/200
 - 22s - loss: 0.0088 - val_loss: 0.0079
 - val_f1: 0.9827
Epoch 147/200
 - 22s - loss: 0.0088 - val_loss: 0.0083
 - val_f1: 0.9778
Epoch 148/200
 - 22s - loss: 0.0088 - val_loss: 0.0079
 - val_f1: 0.9794
Epoch 149/200
 - 22s - loss: 0.0088 - val_loss: 0.0078
 - val_f1: 0.9789
Epoch 150/200
 - 22s - loss: 0.0088 - val_loss: 0.0081
 - val_f1: 0.9792
Epoch 151/200
 - 22s - loss: 0.0088 - val_loss: 0.0079
 - val_f1: 0.9810
Epoch 152/200
 - 22s - loss: 0.0088 - val_loss: 0.0101
 - val_f1: 0.9766
Epoch 153/200
 - 22s - loss: 0.0088 - val_loss: 0.0078
 - val_f1: 0.9803
Epoch 154/200
 - 22s - loss: 0.0088 - val_loss: 0.0076
 - val_f1: 0.9777
Epoch 155/200
 - 22s - loss: 0.0088 - val_loss: 0.0077
 - val_f1: 0.9770
Epoch 156/200
 - 22s - loss: 0.0088 - val_loss: 0.0083
 - val_f1: 0.9781
Epoch 157/200
 - 22s - loss: 0.0088 - val_loss: 0.0075
 - val_f1: 0.9801
Epoch 158/200
 - 22s - loss: 0.0088 - val_loss: 0.0082
 - val_f1: 0.9773
Epoch 159/200
 - 22s - loss: 0.0088 - val_loss: 0.0077
 - val_f1: 0.9829
Epoch 160/200
 - 22s - loss: 0.0088 - val_loss: 0.0085
 - val_f1: 0.9781
Epoch 161/200
 - 22s - loss: 0.0088 - val_loss: 0.0079
2019-12-24 15:46:27,384 [INFO] epoch = 160. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep4/ann_model_epoch_160.pickle
 - val_f1: 0.9789
Epoch 162/200
 - 22s - loss: 0.0088 - val_loss: 0.0079
 - val_f1: 0.9810
Epoch 163/200
 - 22s - loss: 0.0088 - val_loss: 0.0099
 - val_f1: 0.9761
Epoch 164/200
 - 22s - loss: 0.0088 - val_loss: 0.0076
 - val_f1: 0.9800
Epoch 165/200
 - 22s - loss: 0.0088 - val_loss: 0.0082
 - val_f1: 0.9768
Epoch 166/200
 - 22s - loss: 0.0087 - val_loss: 0.0079
 - val_f1: 0.9800
Epoch 167/200
 - 22s - loss: 0.0087 - val_loss: 0.0080
 - val_f1: 0.9768
Epoch 168/200
 - 22s - loss: 0.0087 - val_loss: 0.0081
 - val_f1: 0.9788
Epoch 169/200
 - 22s - loss: 0.0087 - val_loss: 0.0084
 - val_f1: 0.9785
Epoch 170/200
 - 22s - loss: 0.0087 - val_loss: 0.0080
 - val_f1: 0.9781
Epoch 171/200
 - 22s - loss: 0.0087 - val_loss: 0.0077
 - val_f1: 0.9837
Epoch 172/200
 - 22s - loss: 0.0087 - val_loss: 0.0075
 - val_f1: 0.9822
Epoch 173/200
 - 22s - loss: 0.0087 - val_loss: 0.0075
 - val_f1: 0.9781
Epoch 174/200
 - 22s - loss: 0.0087 - val_loss: 0.0086
 - val_f1: 0.9784
Epoch 175/200
 - 22s - loss: 0.0087 - val_loss: 0.0080
 - val_f1: 0.9796
Epoch 176/200
 - 22s - loss: 0.0087 - val_loss: 0.0080
 - val_f1: 0.9777
Epoch 177/200
 - 22s - loss: 0.0087 - val_loss: 0.0082
 - val_f1: 0.9775
Epoch 178/200
 - 22s - loss: 0.0087 - val_loss: 0.0077
 - val_f1: 0.9798
Epoch 179/200
 - 22s - loss: 0.0087 - val_loss: 0.0080
 - val_f1: 0.9788
Epoch 180/200
 - 22s - loss: 0.0087 - val_loss: 0.0081
 - val_f1: 0.9792
Epoch 181/200
 - 22s - loss: 0.0087 - val_loss: 0.0079
2019-12-24 16:00:31,406 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep4/ann_model_epoch_180.pickle
 - val_f1: 0.9820
Epoch 182/200
 - 22s - loss: 0.0087 - val_loss: 0.0080
 - val_f1: 0.9785
Epoch 183/200
 - 22s - loss: 0.0087 - val_loss: 0.0080
 - val_f1: 0.9778
Epoch 184/200
 - 22s - loss: 0.0087 - val_loss: 0.0078
 - val_f1: 0.9778
Epoch 185/200
 - 22s - loss: 0.0087 - val_loss: 0.0076
 - val_f1: 0.9826
Epoch 186/200
 - 22s - loss: 0.0087 - val_loss: 0.0082
 - val_f1: 0.9790
Epoch 187/200
 - 22s - loss: 0.0087 - val_loss: 0.0075
 - val_f1: 0.9838
Epoch 188/200
 - 22s - loss: 0.0086 - val_loss: 0.0075
 - val_f1: 0.9786
Epoch 189/200
 - 22s - loss: 0.0087 - val_loss: 0.0077
 - val_f1: 0.9789
Epoch 190/200
 - 22s - loss: 0.0087 - val_loss: 0.0077
 - val_f1: 0.9770
Epoch 191/200
 - 22s - loss: 0.0086 - val_loss: 0.0086
 - val_f1: 0.9786
Epoch 192/200
 - 22s - loss: 0.0087 - val_loss: 0.0077
 - val_f1: 0.9800
Epoch 193/200
 - 22s - loss: 0.0086 - val_loss: 0.0077
 - val_f1: 0.9798
Epoch 194/200
 - 22s - loss: 0.0087 - val_loss: 0.0087
 - val_f1: 0.9781
Epoch 195/200
 - 22s - loss: 0.0086 - val_loss: 0.0074
 - val_f1: 0.9831
Epoch 196/200
 - 22s - loss: 0.0087 - val_loss: 0.0077
 - val_f1: 0.9795
Epoch 197/200
 - 22s - loss: 0.0086 - val_loss: 0.0082
 - val_f1: 0.9786
Epoch 198/200
 - 22s - loss: 0.0086 - val_loss: 0.0076
 - val_f1: 0.9833
Epoch 199/200
 - 22s - loss: 0.0086 - val_loss: 0.0078
 - val_f1: 0.9774
Epoch 200/200
 - 22s - loss: 0.0087 - val_loss: 0.0085
2019-12-24 16:14:12,417 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-24 16:15:24,671 [INFO] Last epoch loss evaluation: train_loss = 0.007357, val_loss = 0.007448
2019-12-24 16:15:24,680 [INFO] Training complete. time_to_train = 13685.61 sec, 228.09 min
2019-12-24 16:15:24,707 [INFO] Model saved to results_selected_models/selected_ids17_ae_ann_deep_rep4/best_model.pickle
/home/sunanda/research/ids_experiments/utility.py:268: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
2019-12-24 16:15:24,902 [INFO] Plot saved to: results_selected_models/selected_ids17_ae_ann_deep_rep4/training_error_history.png
/home/sunanda/research/ids_experiments/utility.py:285: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
2019-12-24 16:15:25,098 [INFO] Plot saved to: results_selected_models/selected_ids17_ae_ann_deep_rep4/training_f1_history.png
2019-12-24 16:15:25,098 [INFO] Making predictions on training, validation, testing data
2019-12-24 16:19:04,718 [INFO] Evaluating predictions (results)
/home/sunanda/anaconda3/envs/ml_env_cpu/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2019-12-24 16:19:24,588 [INFO] Dataset: Testing. Classification report below
2019-12-24 16:19:24,589 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       0.99      0.99      0.99    454265
                   Bot       0.94      0.36      0.52       391
                  DDoS       1.00      0.99      0.99     25605
         DoS GoldenEye       0.98      0.95      0.96      2058
              DoS Hulk       0.98      0.96      0.97     46025
      DoS Slowhttptest       0.88      0.97      0.92      1100
         DoS slowloris       0.98      0.93      0.96      1159
           FTP-Patator       0.98      0.99      0.99      1587
              PortScan       0.90      0.94      0.92     31761
           SSH-Patator       0.94      0.98      0.96      1179
Web Attack Brute Force       0.00      0.00      0.00       302
        Web Attack XSS       0.00      0.00      0.00       130

              accuracy                           0.98    565562
             macro avg       0.80      0.76      0.76    565562
          weighted avg       0.98      0.98      0.98    565562

2019-12-24 16:19:24,589 [INFO] Overall accuracy (micro avg): 0.9829267171415336
/home/sunanda/research/ids_experiments/utility.py:307: RuntimeWarning: invalid value encountered in true_divide
  PPV = TP / (TP + FP)    # Precision or positive predictive value
/home/sunanda/research/ids_experiments/utility.py:311: RuntimeWarning: invalid value encountered in true_divide
  FDR = FP / (TP + FP)    # False discovery rate
2019-12-24 16:19:45,950 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9829         0.9829                       0.9829                0.0016                   0.0171  0.9829
1     Macro avg        0.9972         0.7959                       0.7555                0.0043                   0.2445  0.7646
2  Weighted avg        0.9855         0.9824                       0.9829                0.0351                   0.0171  0.9825
2019-12-24 16:20:05,976 [INFO] Dataset: Validation. Classification report below
2019-12-24 16:20:05,976 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       0.99      0.99      0.99    454264
                   Bot       0.88      0.31      0.46       391
                  DDoS       1.00      0.99      0.99     25605
         DoS GoldenEye       0.97      0.94      0.95      2059
              DoS Hulk       0.98      0.96      0.97     46025
      DoS Slowhttptest       0.89      0.96      0.92      1099
         DoS slowloris       0.98      0.93      0.95      1159
           FTP-Patator       0.98      0.99      0.99      1587
              PortScan       0.90      0.94      0.92     31761
           SSH-Patator       0.94      0.97      0.96      1180
Web Attack Brute Force       0.00      0.00      0.00       301
        Web Attack XSS       0.00      0.00      0.00       131

              accuracy                           0.98    565562
             macro avg       0.79      0.75      0.76    565562
          weighted avg       0.98      0.98      0.98    565562

2019-12-24 16:20:05,976 [INFO] Overall accuracy (micro avg): 0.9831388954703463
2019-12-24 16:20:27,548 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9831         0.9831                       0.9831                0.0015                   0.0169  0.9831
1     Macro avg        0.9972         0.7920                       0.7492                0.0043                   0.2508  0.7591
2  Weighted avg        0.9857         0.9826                       0.9831                0.0346                   0.0169  0.9827
2019-12-24 16:21:34,437 [INFO] Dataset: Training. Classification report below
2019-12-24 16:21:34,437 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       0.99      0.99      0.99   1362791
                   Bot       0.93      0.36      0.52      1174
                  DDoS       1.00      0.99      0.99     76815
         DoS GoldenEye       0.97      0.94      0.96      6176
              DoS Hulk       0.98      0.96      0.97    138074
      DoS Slowhttptest       0.90      0.97      0.93      3300
         DoS slowloris       0.98      0.95      0.96      3478
           FTP-Patator       0.98      0.99      0.99      4761
              PortScan       0.90      0.95      0.92     95282
           SSH-Patator       0.95      0.98      0.96      3538
Web Attack Brute Force       0.00      0.00      0.00       904
        Web Attack XSS       0.00      0.00      0.00       391

              accuracy                           0.98   1696684
             macro avg       0.80      0.76      0.77   1696684
          weighted avg       0.98      0.98      0.98   1696684

2019-12-24 16:21:34,437 [INFO] Overall accuracy (micro avg): 0.983329836316014
2019-12-24 16:22:46,538 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9833         0.9833                       0.9833                0.0015                   0.0167  0.9833
1     Macro avg        0.9972         0.7984                       0.7552                0.0043                   0.2448  0.7659
2  Weighted avg        0.9858         0.9828                       0.9833                0.0346                   0.0167  0.9829
2019-12-24 16:22:46,603 [INFO] Results saved to: results_selected_models/selected_ids17_ae_ann_deep_rep4/selected_ids17_ae_ann_deep_rep4_results.xlsx
2019-12-24 16:22:46,606 [INFO] ================= Finished running experiment no. 4 ================= 

2019-12-24 16:22:46,647 [INFO] Created directory: results_selected_models/selected_ids17_ae_ann_deep_rep5
2019-12-24 16:22:46,648 [INFO] Initialized logging. log_filename = results_selected_models/selected_ids17_ae_ann_deep_rep5/run_log.log
2019-12-24 16:22:46,648 [INFO] ================= Running experiment no. 5  ================= 

2019-12-24 16:22:46,648 [INFO] Experiment parameters given below
2019-12-24 16:22:46,648 [INFO] 
{'experiment_num': 5, 'results_dir': 'results_selected_models/selected_ids17_ae_ann_deep_rep5', 'model_type': 'classifier', 'model': 'ae_ann', 'normal_label': 'BENIGN', 'scaling_type': 'NA', 'unsupervised_ratio': 0.25, 'ae_encoder_units': [128, 64, 32], 'ae_encoder_activations': ['relu', 'relu', 'relu'], 'ae_encoder_dropout_rates': [0.2, 0.2, 0.2], 'ae_encoder_l1_param': -1, 'ae_decoder_units': [64, 128], 'ae_decoder_activations': ['relu', 'relu'], 'ae_decoder_dropout_rates': [0.2, 0.2], 'output_activation': 'sigmoid', 'loss_function': 'binary_crossentropy', 'ae_epochs': 200, 'ann_layer_units': [64], 'ann_layer_activations': ['relu'], 'ann_layer_dropout_rates': [0.2], 'ann_epochs': 200, 'early_stop_patience': 50, 'batch_size': 256, 'dataset_dir': '../Datasets/full_datasets/ids2017', 'description': 'selected_ids17_ae_ann_deep_rep5'}
2019-12-24 16:22:46,648 [INFO] Created tensorboard log directory: results_selected_models/selected_ids17_ae_ann_deep_rep5/tf_logs_run_2019_12_24-16_22_46
2019-12-24 16:22:46,648 [INFO] Loading datsets from: ../Datasets/full_datasets/ids2017
2019-12-24 16:22:46,648 [INFO] Reading X, y files
2019-12-24 16:22:46,648 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_train.h5
2019-12-24 16:22:50,447 [INFO] Reading complete. time_to_read=3.80 seconds
2019-12-24 16:22:50,447 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_val.h5
2019-12-24 16:22:51,734 [INFO] Reading complete. time_to_read=1.29 seconds
2019-12-24 16:22:51,734 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_test.h5
2019-12-24 16:22:53,027 [INFO] Reading complete. time_to_read=1.29 seconds
2019-12-24 16:22:53,027 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_train.h5
2019-12-24 16:22:53,263 [INFO] Reading complete. time_to_read=0.24 seconds
2019-12-24 16:22:53,263 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_val.h5
2019-12-24 16:22:53,347 [INFO] Reading complete. time_to_read=0.08 seconds
2019-12-24 16:22:53,347 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_test.h5
2019-12-24 16:22:53,429 [INFO] Reading complete. time_to_read=0.08 seconds
2019-12-24 16:22:56,351 [INFO] Initializing model
2019-12-24 16:22:56,933 [INFO] _________________________________________________________________
2019-12-24 16:22:56,936 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-24 16:22:56,936 [INFO] =================================================================
2019-12-24 16:22:56,936 [INFO] dense_93 (Dense)             (None, 128)               10112     
2019-12-24 16:22:56,936 [INFO] _________________________________________________________________
2019-12-24 16:22:56,936 [INFO] batch_normalization_65 (Batc (None, 128)               512       
2019-12-24 16:22:56,936 [INFO] _________________________________________________________________
2019-12-24 16:22:56,936 [INFO] dropout_65 (Dropout)         (None, 128)               0         
2019-12-24 16:22:56,936 [INFO] _________________________________________________________________
2019-12-24 16:22:56,936 [INFO] dense_94 (Dense)             (None, 64)                8256      
2019-12-24 16:22:56,937 [INFO] _________________________________________________________________
2019-12-24 16:22:56,937 [INFO] batch_normalization_66 (Batc (None, 64)                256       
2019-12-24 16:22:56,937 [INFO] _________________________________________________________________
2019-12-24 16:22:56,937 [INFO] dropout_66 (Dropout)         (None, 64)                0         
2019-12-24 16:22:56,937 [INFO] _________________________________________________________________
2019-12-24 16:22:56,937 [INFO] dense_95 (Dense)             (None, 32)                2080      
2019-12-24 16:22:56,937 [INFO] _________________________________________________________________
2019-12-24 16:22:56,937 [INFO] batch_normalization_67 (Batc (None, 32)                128       
2019-12-24 16:22:56,937 [INFO] _________________________________________________________________
2019-12-24 16:22:56,937 [INFO] dropout_67 (Dropout)         (None, 32)                0         
2019-12-24 16:22:56,937 [INFO] _________________________________________________________________
2019-12-24 16:22:56,937 [INFO] dense_96 (Dense)             (None, 64)                2112      
2019-12-24 16:22:56,938 [INFO] _________________________________________________________________
2019-12-24 16:22:56,938 [INFO] batch_normalization_68 (Batc (None, 64)                256       
2019-12-24 16:22:56,938 [INFO] _________________________________________________________________
2019-12-24 16:22:56,938 [INFO] dropout_68 (Dropout)         (None, 64)                0         
2019-12-24 16:22:56,938 [INFO] _________________________________________________________________
2019-12-24 16:22:56,938 [INFO] dense_97 (Dense)             (None, 128)               8320      
2019-12-24 16:22:56,938 [INFO] _________________________________________________________________
2019-12-24 16:22:56,938 [INFO] batch_normalization_69 (Batc (None, 128)               512       
2019-12-24 16:22:56,938 [INFO] _________________________________________________________________
2019-12-24 16:22:56,938 [INFO] dropout_69 (Dropout)         (None, 128)               0         
2019-12-24 16:22:56,938 [INFO] _________________________________________________________________
2019-12-24 16:22:56,938 [INFO] dense_98 (Dense)             (None, 78)                10062     
2019-12-24 16:22:56,939 [INFO] =================================================================
2019-12-24 16:22:56,939 [INFO] Total params: 42,606
2019-12-24 16:22:56,939 [INFO] Trainable params: 41,774
2019-12-24 16:22:56,939 [INFO] Non-trainable params: 832
2019-12-24 16:22:56,939 [INFO] _________________________________________________________________
2019-12-24 16:22:57,084 [INFO] _________________________________________________________________
2019-12-24 16:22:57,084 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-24 16:22:57,084 [INFO] =================================================================
2019-12-24 16:22:57,084 [INFO] dense_99 (Dense)             (None, 64)                2112      
2019-12-24 16:22:57,084 [INFO] _________________________________________________________________
2019-12-24 16:22:57,085 [INFO] batch_normalization_70 (Batc (None, 64)                256       
2019-12-24 16:22:57,085 [INFO] _________________________________________________________________
2019-12-24 16:22:57,085 [INFO] dropout_70 (Dropout)         (None, 64)                0         
2019-12-24 16:22:57,085 [INFO] _________________________________________________________________
2019-12-24 16:22:57,085 [INFO] dense_100 (Dense)            (None, 12)                780       
2019-12-24 16:22:57,085 [INFO] =================================================================
2019-12-24 16:22:57,085 [INFO] Total params: 3,148
2019-12-24 16:22:57,085 [INFO] Trainable params: 3,020
2019-12-24 16:22:57,085 [INFO] Non-trainable params: 128
2019-12-24 16:22:57,085 [INFO] _________________________________________________________________
2019-12-24 16:22:57,085 [INFO] Training model
2019-12-24 16:22:57,085 [INFO] Splitting train set into 2 sets (unsupervised, supervised), random_seed = None
2019-12-24 16:23:22,499 [INFO] Split sizes (instances). total = 1696684, unsupervised = 424171, supervised = 1272513, unsupervised dataset hash = 542cd24db2044fdf6d94f1718a918c9666255222
2019-12-24 16:23:22,499 [INFO] Training autoencoder
 - val_f1: 0.9765
Train on 424171 samples, validate on 565562 samples
Epoch 1/200
 - 28s - loss: -3.1863e+00 - val_loss: -3.9884e+00
Epoch 2/200
 - 25s - loss: -3.9086e+00 - val_loss: -4.0573e+00
Epoch 3/200
 - 25s - loss: -3.9664e+00 - val_loss: -4.0875e+00
Epoch 4/200
 - 25s - loss: -3.9961e+00 - val_loss: -4.0952e+00
Epoch 5/200
 - 25s - loss: -4.0151e+00 - val_loss: -4.1050e+00
Epoch 6/200
 - 25s - loss: -4.0293e+00 - val_loss: -4.1068e+00
Epoch 7/200
 - 25s - loss: -4.0381e+00 - val_loss: -4.1135e+00
Epoch 8/200
 - 25s - loss: -4.0473e+00 - val_loss: -4.1207e+00
Epoch 9/200
 - 25s - loss: -4.0543e+00 - val_loss: -4.1190e+00
Epoch 10/200
 - 25s - loss: -4.0558e+00 - val_loss: -4.1248e+00
Epoch 11/200
 - 25s - loss: -4.0635e+00 - val_loss: -4.1265e+00
Epoch 12/200
 - 25s - loss: -4.0660e+00 - val_loss: -4.1303e+00
Epoch 13/200
 - 25s - loss: -4.0692e+00 - val_loss: -4.1299e+00
Epoch 14/200
 - 25s - loss: -4.0739e+00 - val_loss: -4.1326e+00
Epoch 15/200
 - 25s - loss: -4.0767e+00 - val_loss: -4.1338e+00
Epoch 16/200
 - 25s - loss: -4.0785e+00 - val_loss: -4.1320e+00
Epoch 17/200
 - 25s - loss: -4.0807e+00 - val_loss: -4.1355e+00
Epoch 18/200
 - 25s - loss: -4.0822e+00 - val_loss: -4.1355e+00
Epoch 19/200
 - 25s - loss: -4.0851e+00 - val_loss: -4.1372e+00
Epoch 20/200
 - 25s - loss: -4.0864e+00 - val_loss: -4.1371e+00
Epoch 21/200
 - 25s - loss: -4.0882e+00 - val_loss: -4.1378e+00
2019-12-24 16:32:29,145 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep5/ae_model_epoch_20.pickle
Epoch 22/200
 - 25s - loss: -4.0901e+00 - val_loss: -4.1386e+00
Epoch 23/200
 - 25s - loss: -4.0904e+00 - val_loss: -4.1395e+00
Epoch 24/200
 - 25s - loss: -4.0913e+00 - val_loss: -4.1402e+00
Epoch 25/200
 - 25s - loss: -4.0933e+00 - val_loss: -4.1403e+00
Epoch 26/200
 - 25s - loss: -4.0926e+00 - val_loss: -4.1414e+00
Epoch 27/200
 - 25s - loss: -4.0945e+00 - val_loss: -4.1391e+00
Epoch 28/200
 - 25s - loss: -4.0954e+00 - val_loss: -4.1411e+00
Epoch 29/200
 - 25s - loss: -4.0956e+00 - val_loss: -4.1409e+00
Epoch 30/200
 - 25s - loss: -4.0971e+00 - val_loss: -4.1431e+00
Epoch 31/200
 - 25s - loss: -4.0979e+00 - val_loss: -4.1417e+00
Epoch 32/200
 - 25s - loss: -4.0967e+00 - val_loss: -4.1440e+00
Epoch 33/200
 - 25s - loss: -4.0992e+00 - val_loss: -4.1428e+00
Epoch 34/200
 - 25s - loss: -4.0970e+00 - val_loss: -4.1433e+00
Epoch 35/200
 - 25s - loss: -4.0982e+00 - val_loss: -4.1440e+00
Epoch 36/200
 - 25s - loss: -4.0998e+00 - val_loss: -4.1438e+00
Epoch 37/200
 - 25s - loss: -4.1005e+00 - val_loss: -4.1444e+00
Epoch 38/200
 - 25s - loss: -4.1014e+00 - val_loss: -4.1448e+00
Epoch 39/200
 - 25s - loss: -4.1016e+00 - val_loss: -4.1442e+00
Epoch 40/200
 - 25s - loss: -4.1014e+00 - val_loss: -4.1452e+00
Epoch 41/200
 - 25s - loss: -4.1011e+00 - val_loss: -4.1437e+00
2019-12-24 16:40:51,580 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep5/ae_model_epoch_40.pickle
Epoch 42/200
 - 25s - loss: -4.1018e+00 - val_loss: -4.1430e+00
Epoch 43/200
 - 25s - loss: -4.1030e+00 - val_loss: -4.1452e+00
Epoch 44/200
 - 25s - loss: -4.1027e+00 - val_loss: -4.1453e+00
Epoch 45/200
 - 25s - loss: -4.1016e+00 - val_loss: -4.1459e+00
Epoch 46/200
 - 25s - loss: -4.1036e+00 - val_loss: -4.1448e+00
Epoch 47/200
 - 25s - loss: -4.1028e+00 - val_loss: -4.1466e+00
Epoch 48/200
 - 25s - loss: -4.1046e+00 - val_loss: -4.1468e+00
Epoch 49/200
 - 25s - loss: -4.1051e+00 - val_loss: -4.1462e+00
Epoch 50/200
 - 25s - loss: -4.1059e+00 - val_loss: -4.1478e+00
Epoch 51/200
 - 25s - loss: -4.1058e+00 - val_loss: -4.1476e+00
Epoch 52/200
 - 25s - loss: -4.1049e+00 - val_loss: -4.1474e+00
Epoch 53/200
 - 25s - loss: -4.1056e+00 - val_loss: -4.1485e+00
Epoch 54/200
 - 25s - loss: -4.1059e+00 - val_loss: -4.1463e+00
Epoch 55/200
 - 25s - loss: -4.1079e+00 - val_loss: -4.1483e+00
Epoch 56/200
 - 25s - loss: -4.1058e+00 - val_loss: -4.1485e+00
Epoch 57/200
 - 25s - loss: -4.1071e+00 - val_loss: -4.1492e+00
Epoch 58/200
 - 25s - loss: -4.1082e+00 - val_loss: -4.1486e+00
Epoch 59/200
 - 25s - loss: -4.1080e+00 - val_loss: -4.1485e+00
Epoch 60/200
 - 25s - loss: -4.1086e+00 - val_loss: -4.1491e+00
Epoch 61/200
 - 25s - loss: -4.1088e+00 - val_loss: -4.1481e+00
2019-12-24 16:49:14,369 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep5/ae_model_epoch_60.pickle
Epoch 62/200
 - 25s - loss: -4.1090e+00 - val_loss: -4.1481e+00
Epoch 63/200
 - 25s - loss: -4.1057e+00 - val_loss: -4.1497e+00
Epoch 64/200
 - 25s - loss: -4.1096e+00 - val_loss: -4.1492e+00
Epoch 65/200
 - 25s - loss: -4.1097e+00 - val_loss: -4.1499e+00
Epoch 66/200
 - 25s - loss: -4.1085e+00 - val_loss: -4.1473e+00
Epoch 67/200
 - 25s - loss: -4.1099e+00 - val_loss: -4.1501e+00
Epoch 68/200
 - 25s - loss: -4.1098e+00 - val_loss: -4.1503e+00
Epoch 69/200
 - 25s - loss: -4.1105e+00 - val_loss: -4.1470e+00
Epoch 70/200
 - 25s - loss: -4.1086e+00 - val_loss: -4.1471e+00
Epoch 71/200
 - 25s - loss: -4.1103e+00 - val_loss: -4.1501e+00
Epoch 72/200
 - 25s - loss: -4.1108e+00 - val_loss: -4.1499e+00
Epoch 73/200
 - 25s - loss: -4.1113e+00 - val_loss: -4.1505e+00
Epoch 74/200
 - 25s - loss: -4.1112e+00 - val_loss: -4.1502e+00
Epoch 75/200
 - 25s - loss: -4.1096e+00 - val_loss: -4.1488e+00
Epoch 76/200
 - 25s - loss: -4.1116e+00 - val_loss: -4.1512e+00
Epoch 77/200
 - 25s - loss: -4.1105e+00 - val_loss: -4.1507e+00
Epoch 78/200
 - 25s - loss: -4.1110e+00 - val_loss: -4.1494e+00
Epoch 79/200
 - 25s - loss: -4.1124e+00 - val_loss: -4.1488e+00
Epoch 80/200
 - 25s - loss: -4.1118e+00 - val_loss: -4.1501e+00
Epoch 81/200
 - 25s - loss: -4.1112e+00 - val_loss: -4.1509e+00
2019-12-24 16:57:36,754 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep5/ae_model_epoch_80.pickle
Epoch 82/200
 - 25s - loss: -4.1121e+00 - val_loss: -4.1503e+00
Epoch 83/200
 - 25s - loss: -4.1120e+00 - val_loss: -4.1509e+00
Epoch 84/200
 - 25s - loss: -4.1128e+00 - val_loss: -4.1511e+00
Epoch 85/200
 - 25s - loss: -4.1130e+00 - val_loss: -4.1504e+00
Epoch 86/200
 - 25s - loss: -4.1109e+00 - val_loss: -4.1489e+00
Epoch 87/200
 - 25s - loss: -4.1126e+00 - val_loss: -4.1506e+00
Epoch 88/200
 - 25s - loss: -4.1108e+00 - val_loss: -4.1489e+00
Epoch 89/200
 - 25s - loss: -4.1125e+00 - val_loss: -4.1498e+00
Epoch 90/200
 - 25s - loss: -4.1133e+00 - val_loss: -4.1517e+00
Epoch 91/200
 - 25s - loss: -4.1133e+00 - val_loss: -4.1512e+00
Epoch 92/200
 - 25s - loss: -4.1130e+00 - val_loss: -4.1516e+00
Epoch 93/200
 - 25s - loss: -4.1130e+00 - val_loss: -4.1493e+00
Epoch 94/200
 - 25s - loss: -4.1142e+00 - val_loss: -4.1518e+00
Epoch 95/200
 - 25s - loss: -4.1131e+00 - val_loss: -4.1496e+00
Epoch 96/200
 - 25s - loss: -4.1140e+00 - val_loss: -4.1519e+00
Epoch 97/200
 - 25s - loss: -4.1127e+00 - val_loss: -4.1506e+00
Epoch 98/200
 - 25s - loss: -4.1111e+00 - val_loss: -4.1502e+00
Epoch 99/200
 - 25s - loss: -4.1123e+00 - val_loss: -4.1501e+00
Epoch 100/200
 - 25s - loss: -4.1143e+00 - val_loss: -4.1511e+00
Epoch 101/200
 - 25s - loss: -4.1145e+00 - val_loss: -4.1497e+00
2019-12-24 17:05:59,368 [INFO] epoch = 100. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep5/ae_model_epoch_100.pickle
Epoch 102/200
 - 25s - loss: -4.1150e+00 - val_loss: -4.1507e+00
Epoch 103/200
 - 25s - loss: -4.1138e+00 - val_loss: -4.1514e+00
Epoch 104/200
 - 25s - loss: -4.1139e+00 - val_loss: -4.1516e+00
Epoch 105/200
 - 25s - loss: -4.1144e+00 - val_loss: -4.1507e+00
Epoch 106/200
 - 25s - loss: -4.1142e+00 - val_loss: -4.1515e+00
Epoch 107/200
 - 25s - loss: -4.1146e+00 - val_loss: -4.1517e+00
Epoch 108/200
 - 25s - loss: -4.1161e+00 - val_loss: -4.1525e+00
Epoch 109/200
 - 25s - loss: -4.1149e+00 - val_loss: -4.1508e+00
Epoch 110/200
 - 25s - loss: -4.1132e+00 - val_loss: -4.1518e+00
Epoch 111/200
 - 25s - loss: -4.1156e+00 - val_loss: -4.1506e+00
Epoch 112/200
 - 25s - loss: -4.1141e+00 - val_loss: -4.1521e+00
Epoch 113/200
 - 25s - loss: -4.1155e+00 - val_loss: -4.1521e+00
Epoch 114/200
 - 25s - loss: -4.1158e+00 - val_loss: -4.1519e+00
Epoch 115/200
 - 25s - loss: -4.1153e+00 - val_loss: -4.1518e+00
Epoch 116/200
 - 25s - loss: -4.1149e+00 - val_loss: -4.1505e+00
Epoch 117/200
 - 25s - loss: -4.1161e+00 - val_loss: -4.1516e+00
Epoch 118/200
 - 25s - loss: -4.1151e+00 - val_loss: -4.1516e+00
Epoch 119/200
 - 25s - loss: -4.1160e+00 - val_loss: -4.1522e+00
Epoch 120/200
 - 25s - loss: -4.1146e+00 - val_loss: -4.1515e+00
Epoch 121/200
 - 25s - loss: -4.1159e+00 - val_loss: -4.1526e+00
2019-12-24 17:14:21,899 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep5/ae_model_epoch_120.pickle
Epoch 122/200
 - 25s - loss: -4.1148e+00 - val_loss: -4.1528e+00
Epoch 123/200
 - 25s - loss: -4.1150e+00 - val_loss: -4.1523e+00
Epoch 124/200
 - 25s - loss: -4.1153e+00 - val_loss: -4.1517e+00
Epoch 125/200
 - 25s - loss: -4.1157e+00 - val_loss: -4.1515e+00
Epoch 126/200
 - 25s - loss: -4.1160e+00 - val_loss: -4.1524e+00
Epoch 127/200
 - 25s - loss: -4.1156e+00 - val_loss: -4.1507e+00
Epoch 128/200
 - 25s - loss: -4.1161e+00 - val_loss: -4.1518e+00
Epoch 129/200
 - 25s - loss: -4.1168e+00 - val_loss: -4.1509e+00
Epoch 130/200
 - 25s - loss: -4.1161e+00 - val_loss: -4.1522e+00
Epoch 131/200
 - 25s - loss: -4.1174e+00 - val_loss: -4.1527e+00
Epoch 132/200
 - 25s - loss: -4.1165e+00 - val_loss: -4.1522e+00
Epoch 133/200
 - 25s - loss: -4.1168e+00 - val_loss: -4.1527e+00
Epoch 134/200
 - 25s - loss: -4.1166e+00 - val_loss: -4.1531e+00
Epoch 135/200
 - 25s - loss: -4.1172e+00 - val_loss: -4.1520e+00
Epoch 136/200
 - 25s - loss: -4.1167e+00 - val_loss: -4.1525e+00
Epoch 137/200
 - 25s - loss: -4.1161e+00 - val_loss: -4.1518e+00
Epoch 138/200
 - 25s - loss: -4.1165e+00 - val_loss: -4.1506e+00
Epoch 139/200
 - 25s - loss: -4.1149e+00 - val_loss: -4.1527e+00
Epoch 140/200
 - 25s - loss: -4.1160e+00 - val_loss: -4.1509e+00
Epoch 141/200
 - 25s - loss: -4.1166e+00 - val_loss: -4.1530e+00
2019-12-24 17:22:44,275 [INFO] epoch = 140. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep5/ae_model_epoch_140.pickle
Epoch 142/200
 - 25s - loss: -4.1179e+00 - val_loss: -4.1523e+00
Epoch 143/200
 - 25s - loss: -4.1153e+00 - val_loss: -4.1527e+00
Epoch 144/200
 - 25s - loss: -4.1165e+00 - val_loss: -4.1533e+00
Epoch 145/200
 - 25s - loss: -4.1166e+00 - val_loss: -4.1523e+00
Epoch 146/200
 - 25s - loss: -4.1175e+00 - val_loss: -4.1523e+00
Epoch 147/200
 - 25s - loss: -4.1182e+00 - val_loss: -4.1537e+00
Epoch 148/200
 - 25s - loss: -4.1176e+00 - val_loss: -4.1522e+00
Epoch 149/200
 - 25s - loss: -4.1174e+00 - val_loss: -4.1541e+00
Epoch 150/200
 - 25s - loss: -4.1170e+00 - val_loss: -4.1525e+00
Epoch 151/200
 - 25s - loss: -4.1177e+00 - val_loss: -4.1526e+00
Epoch 152/200
 - 25s - loss: -4.1172e+00 - val_loss: -4.1520e+00
Epoch 153/200
 - 25s - loss: -4.1179e+00 - val_loss: -4.1532e+00
Epoch 154/200
 - 25s - loss: -4.1174e+00 - val_loss: -4.1528e+00
Epoch 155/200
 - 25s - loss: -4.1168e+00 - val_loss: -4.1536e+00
Epoch 156/200
 - 25s - loss: -4.1165e+00 - val_loss: -4.1524e+00
Epoch 157/200
 - 25s - loss: -4.1176e+00 - val_loss: -4.1541e+00
Epoch 158/200
 - 25s - loss: -4.1178e+00 - val_loss: -4.1534e+00
Epoch 159/200
 - 25s - loss: -4.1177e+00 - val_loss: -4.1533e+00
Epoch 160/200
 - 25s - loss: -4.1177e+00 - val_loss: -4.1507e+00
Epoch 161/200
 - 25s - loss: -4.1174e+00 - val_loss: -4.1535e+00
2019-12-24 17:31:07,091 [INFO] epoch = 160. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep5/ae_model_epoch_160.pickle
Epoch 162/200
 - 25s - loss: -4.1178e+00 - val_loss: -4.1531e+00
Epoch 163/200
 - 25s - loss: -4.1179e+00 - val_loss: -4.1533e+00
Epoch 164/200
 - 25s - loss: -4.1185e+00 - val_loss: -4.1513e+00
Epoch 165/200
 - 25s - loss: -4.1185e+00 - val_loss: -4.1533e+00
Epoch 166/200
 - 25s - loss: -4.1181e+00 - val_loss: -4.1525e+00
Epoch 167/200
 - 25s - loss: -4.1183e+00 - val_loss: -4.1539e+00
Epoch 168/200
 - 25s - loss: -4.1174e+00 - val_loss: -4.1528e+00
Epoch 169/200
 - 25s - loss: -4.1170e+00 - val_loss: -4.1529e+00
Epoch 170/200
 - 25s - loss: -4.1178e+00 - val_loss: -4.1538e+00
Epoch 171/200
 - 25s - loss: -4.1180e+00 - val_loss: -4.1521e+00
Epoch 172/200
 - 25s - loss: -4.1183e+00 - val_loss: -4.1542e+00
Epoch 173/200
 - 25s - loss: -4.1181e+00 - val_loss: -4.1540e+00
Epoch 174/200
 - 25s - loss: -4.1188e+00 - val_loss: -4.1525e+00
Epoch 175/200
 - 25s - loss: -4.1181e+00 - val_loss: -4.1541e+00
Epoch 176/200
 - 25s - loss: -4.1190e+00 - val_loss: -4.1527e+00
Epoch 177/200
 - 25s - loss: -4.1187e+00 - val_loss: -4.1533e+00
Epoch 178/200
 - 25s - loss: -4.1183e+00 - val_loss: -4.1536e+00
Epoch 179/200
 - 25s - loss: -4.1183e+00 - val_loss: -4.1538e+00
Epoch 180/200
 - 25s - loss: -4.1175e+00 - val_loss: -4.1540e+00
Epoch 181/200
 - 25s - loss: -4.1194e+00 - val_loss: -4.1539e+00
2019-12-24 17:39:29,676 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep5/ae_model_epoch_180.pickle
Epoch 182/200
 - 25s - loss: -4.1189e+00 - val_loss: -4.1541e+00
Epoch 183/200
 - 25s - loss: -4.1178e+00 - val_loss: -4.1540e+00
Epoch 184/200
 - 25s - loss: -4.1196e+00 - val_loss: -4.1529e+00
Epoch 185/200
 - 25s - loss: -4.1193e+00 - val_loss: -4.1540e+00
Epoch 186/200
 - 25s - loss: -4.1191e+00 - val_loss: -4.1543e+00
Epoch 187/200
 - 25s - loss: -4.1175e+00 - val_loss: -4.1518e+00
Epoch 188/200
 - 25s - loss: -4.1196e+00 - val_loss: -4.1535e+00
Epoch 189/200
 - 25s - loss: -4.1180e+00 - val_loss: -4.1544e+00
Epoch 190/200
 - 25s - loss: -4.1169e+00 - val_loss: -4.1541e+00
Epoch 191/200
 - 25s - loss: -4.1195e+00 - val_loss: -4.1533e+00
Epoch 192/200
 - 25s - loss: -4.1200e+00 - val_loss: -4.1535e+00
Epoch 193/200
 - 25s - loss: -4.1186e+00 - val_loss: -4.1536e+00
Epoch 194/200
 - 25s - loss: -4.1179e+00 - val_loss: -4.1541e+00
Epoch 195/200
 - 25s - loss: -4.1192e+00 - val_loss: -4.1544e+00
Epoch 196/200
 - 25s - loss: -4.1185e+00 - val_loss: -4.1548e+00
Epoch 197/200
 - 25s - loss: -4.1186e+00 - val_loss: -4.1536e+00
Epoch 198/200
 - 25s - loss: -4.1201e+00 - val_loss: -4.1537e+00
Epoch 199/200
 - 25s - loss: -4.1189e+00 - val_loss: -4.1541e+00
Epoch 200/200
 - 25s - loss: -4.1186e+00 - val_loss: -4.1537e+00
2019-12-24 17:47:27,187 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-24 17:48:40,037 [INFO] Last epoch loss evaluation: train_loss = -4.160695, val_loss = -4.154768
2019-12-24 17:48:40,038 [INFO] Training autoencoder complete
2019-12-24 17:48:40,038 [INFO] Encoding data for supervised training
2019-12-24 17:50:03,949 [INFO] Encoding complete
2019-12-24 17:50:03,949 [INFO] Training neural network layers (after autoencoder)
Train on 1272513 samples, validate on 565562 samples
Epoch 1/200
 - 24s - loss: 0.0238 - val_loss: 0.0135
 - val_f1: 0.9675
Epoch 2/200
 - 22s - loss: 0.0139 - val_loss: 0.0117
 - val_f1: 0.9684
Epoch 3/200
 - 22s - loss: 0.0126 - val_loss: 0.0124
 - val_f1: 0.9663
Epoch 4/200
 - 22s - loss: 0.0120 - val_loss: 0.0104
 - val_f1: 0.9746
Epoch 5/200
 - 22s - loss: 0.0117 - val_loss: 0.0104
 - val_f1: 0.9738
Epoch 6/200
 - 22s - loss: 0.0114 - val_loss: 0.0100
 - val_f1: 0.9738
Epoch 7/200
 - 22s - loss: 0.0112 - val_loss: 0.0101
 - val_f1: 0.9715
Epoch 8/200
 - 22s - loss: 0.0110 - val_loss: 0.0095
 - val_f1: 0.9747
Epoch 9/200
 - 22s - loss: 0.0108 - val_loss: 0.0104
 - val_f1: 0.9687
Epoch 10/200
 - 22s - loss: 0.0109 - val_loss: 0.0094
 - val_f1: 0.9771
Epoch 11/200
 - 22s - loss: 0.0108 - val_loss: 0.0097
 - val_f1: 0.9743
Epoch 12/200
 - 22s - loss: 0.0107 - val_loss: 0.0095
 - val_f1: 0.9742
Epoch 13/200
 - 23s - loss: 0.0106 - val_loss: 0.0089
 - val_f1: 0.9774
Epoch 14/200
 - 22s - loss: 0.0106 - val_loss: 0.0099
 - val_f1: 0.9727
Epoch 15/200
 - 23s - loss: 0.0106 - val_loss: 0.0090
 - val_f1: 0.9782
Epoch 16/200
 - 23s - loss: 0.0105 - val_loss: 0.0090
 - val_f1: 0.9782
Epoch 17/200
 - 23s - loss: 0.0104 - val_loss: 0.0099
 - val_f1: 0.9742
Epoch 18/200
 - 23s - loss: 0.0104 - val_loss: 0.0098
 - val_f1: 0.9741
Epoch 19/200
 - 23s - loss: 0.0103 - val_loss: 0.0087
 - val_f1: 0.9771
Epoch 20/200
 - 23s - loss: 0.0103 - val_loss: 0.0095
 - val_f1: 0.9741
Epoch 21/200
 - 23s - loss: 0.0103 - val_loss: 0.0089
2019-12-24 18:05:21,209 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep5/ann_model_epoch_20.pickle
 - val_f1: 0.9809
Epoch 22/200
 - 23s - loss: 0.0103 - val_loss: 0.0089
 - val_f1: 0.9750
Epoch 23/200
 - 23s - loss: 0.0103 - val_loss: 0.0088
 - val_f1: 0.9782
Epoch 24/200
 - 23s - loss: 0.0103 - val_loss: 0.0089
 - val_f1: 0.9780
Epoch 25/200
 - 23s - loss: 0.0102 - val_loss: 0.0091
 - val_f1: 0.9747
Epoch 26/200
 - 23s - loss: 0.0102 - val_loss: 0.0087
 - val_f1: 0.9762
Epoch 27/200
 - 23s - loss: 0.0101 - val_loss: 0.0104
 - val_f1: 0.9748
Epoch 28/200
 - 23s - loss: 0.0101 - val_loss: 0.0088
 - val_f1: 0.9757
Epoch 29/200
 - 23s - loss: 0.0101 - val_loss: 0.0087
 - val_f1: 0.9754
Epoch 30/200
 - 23s - loss: 0.0100 - val_loss: 0.0096
 - val_f1: 0.9726
Epoch 31/200
 - 23s - loss: 0.0100 - val_loss: 0.0088
 - val_f1: 0.9780
Epoch 32/200
 - 23s - loss: 0.0100 - val_loss: 0.0091
 - val_f1: 0.9773
Epoch 33/200
 - 23s - loss: 0.0100 - val_loss: 0.0110
 - val_f1: 0.9748
Epoch 34/200
 - 23s - loss: 0.0100 - val_loss: 0.0084
 - val_f1: 0.9780
Epoch 35/200
 - 23s - loss: 0.0100 - val_loss: 0.0086
 - val_f1: 0.9744
Epoch 36/200
 - 23s - loss: 0.0099 - val_loss: 0.0086
 - val_f1: 0.9757
Epoch 37/200
 - 23s - loss: 0.0099 - val_loss: 0.0088
 - val_f1: 0.9778
Epoch 38/200
 - 23s - loss: 0.0099 - val_loss: 0.0088
 - val_f1: 0.9781
Epoch 39/200
 - 23s - loss: 0.0099 - val_loss: 0.0097
 - val_f1: 0.9712
Epoch 40/200
 - 23s - loss: 0.0098 - val_loss: 0.0089
 - val_f1: 0.9749
Epoch 41/200
 - 23s - loss: 0.0098 - val_loss: 0.0092
2019-12-24 18:20:06,077 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep5/ann_model_epoch_40.pickle
 - val_f1: 0.9760
Epoch 42/200
 - 23s - loss: 0.0098 - val_loss: 0.0089
 - val_f1: 0.9739
Epoch 43/200
 - 23s - loss: 0.0098 - val_loss: 0.0089
 - val_f1: 0.9757
Epoch 44/200
 - 23s - loss: 0.0097 - val_loss: 0.0090
 - val_f1: 0.9759
Epoch 45/200
 - 23s - loss: 0.0098 - val_loss: 0.0089
 - val_f1: 0.9773
Epoch 46/200
 - 23s - loss: 0.0097 - val_loss: 0.0096
 - val_f1: 0.9724
Epoch 47/200
 - 23s - loss: 0.0097 - val_loss: 0.0084
 - val_f1: 0.9764
Epoch 48/200
 - 23s - loss: 0.0097 - val_loss: 0.0096
 - val_f1: 0.9757
Epoch 49/200
 - 23s - loss: 0.0097 - val_loss: 0.0084
 - val_f1: 0.9775
Epoch 50/200
 - 23s - loss: 0.0097 - val_loss: 0.0090
 - val_f1: 0.9730
Epoch 51/200
 - 23s - loss: 0.0096 - val_loss: 0.0084
 - val_f1: 0.9768
Epoch 52/200
 - 23s - loss: 0.0097 - val_loss: 0.0106
 - val_f1: 0.9758
Epoch 53/200
 - 23s - loss: 0.0097 - val_loss: 0.0084
 - val_f1: 0.9795
Epoch 54/200
 - 23s - loss: 0.0097 - val_loss: 0.0085
 - val_f1: 0.9777
Epoch 55/200
 - 23s - loss: 0.0097 - val_loss: 0.0103
 - val_f1: 0.9749
Epoch 56/200
 - 23s - loss: 0.0096 - val_loss: 0.0084
 - val_f1: 0.9799
Epoch 57/200
 - 23s - loss: 0.0096 - val_loss: 0.0084
 - val_f1: 0.9778
Epoch 58/200
 - 23s - loss: 0.0096 - val_loss: 0.0085
 - val_f1: 0.9792
Epoch 59/200
 - 23s - loss: 0.0096 - val_loss: 0.0086
 - val_f1: 0.9779
Epoch 60/200
 - 23s - loss: 0.0096 - val_loss: 0.0089
 - val_f1: 0.9761
Epoch 61/200
 - 23s - loss: 0.0096 - val_loss: 0.0097
2019-12-24 18:34:51,512 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep5/ann_model_epoch_60.pickle
 - val_f1: 0.9758
Epoch 62/200
 - 23s - loss: 0.0096 - val_loss: 0.0094
 - val_f1: 0.9724
Epoch 63/200
 - 23s - loss: 0.0096 - val_loss: 0.0081
 - val_f1: 0.9802
Epoch 64/200
 - 23s - loss: 0.0096 - val_loss: 0.0083
 - val_f1: 0.9782
Epoch 65/200
 - 23s - loss: 0.0095 - val_loss: 0.0084
 - val_f1: 0.9763
Epoch 66/200
 - 23s - loss: 0.0096 - val_loss: 0.0080
 - val_f1: 0.9788
Epoch 67/200
 - 23s - loss: 0.0095 - val_loss: 0.0082
 - val_f1: 0.9778
Epoch 68/200
 - 23s - loss: 0.0095 - val_loss: 0.0084
 - val_f1: 0.9784
Epoch 69/200
 - 23s - loss: 0.0095 - val_loss: 0.0080
 - val_f1: 0.9805
Epoch 70/200
 - 23s - loss: 0.0095 - val_loss: 0.0092
 - val_f1: 0.9759
Epoch 71/200
 - 23s - loss: 0.0095 - val_loss: 0.0092
 - val_f1: 0.9734
Epoch 72/200
 - 23s - loss: 0.0095 - val_loss: 0.0083
 - val_f1: 0.9762
Epoch 73/200
 - 23s - loss: 0.0094 - val_loss: 0.0094
 - val_f1: 0.9722
Epoch 74/200
 - 23s - loss: 0.0095 - val_loss: 0.0083
 - val_f1: 0.9794
Epoch 75/200
 - 23s - loss: 0.0095 - val_loss: 0.0083
 - val_f1: 0.9768
Epoch 76/200
 - 23s - loss: 0.0094 - val_loss: 0.0081
 - val_f1: 0.9779
Epoch 77/200
 - 23s - loss: 0.0094 - val_loss: 0.0081
 - val_f1: 0.9789
Epoch 78/200
 - 23s - loss: 0.0094 - val_loss: 0.0091
 - val_f1: 0.9731
Epoch 79/200
 - 23s - loss: 0.0095 - val_loss: 0.0082
 - val_f1: 0.9799
Epoch 80/200
 - 23s - loss: 0.0094 - val_loss: 0.0093
 - val_f1: 0.9762
Epoch 81/200
 - 23s - loss: 0.0095 - val_loss: 0.0084
2019-12-24 18:49:36,332 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep5/ann_model_epoch_80.pickle
 - val_f1: 0.9792
Epoch 82/200
 - 23s - loss: 0.0094 - val_loss: 0.0089
 - val_f1: 0.9749
Epoch 83/200
 - 23s - loss: 0.0094 - val_loss: 0.0091
 - val_f1: 0.9766
Epoch 84/200
 - 23s - loss: 0.0093 - val_loss: 0.0086
 - val_f1: 0.9776
Epoch 85/200
 - 23s - loss: 0.0094 - val_loss: 0.0079
 - val_f1: 0.9791
Epoch 86/200
 - 23s - loss: 0.0094 - val_loss: 0.0089
 - val_f1: 0.9742
Epoch 87/200
 - 23s - loss: 0.0094 - val_loss: 0.0092
 - val_f1: 0.9736
Epoch 88/200
 - 23s - loss: 0.0094 - val_loss: 0.0081
 - val_f1: 0.9773
Epoch 89/200
 - 23s - loss: 0.0094 - val_loss: 0.0083
 - val_f1: 0.9776
Epoch 90/200
 - 23s - loss: 0.0093 - val_loss: 0.0079
 - val_f1: 0.9800
Epoch 91/200
 - 23s - loss: 0.0093 - val_loss: 0.0091
 - val_f1: 0.9762
Epoch 92/200
 - 23s - loss: 0.0093 - val_loss: 0.0081
 - val_f1: 0.9794
Epoch 93/200
 - 23s - loss: 0.0093 - val_loss: 0.0081
 - val_f1: 0.9793
Epoch 94/200
 - 23s - loss: 0.0093 - val_loss: 0.0081
 - val_f1: 0.9791
Epoch 95/200
 - 23s - loss: 0.0093 - val_loss: 0.0082
 - val_f1: 0.9779
Epoch 96/200
 - 23s - loss: 0.0093 - val_loss: 0.0084
 - val_f1: 0.9750
Epoch 97/200
 - 23s - loss: 0.0093 - val_loss: 0.0081
 - val_f1: 0.9800
Epoch 98/200
 - 23s - loss: 0.0093 - val_loss: 0.0087
 - val_f1: 0.9764
Epoch 99/200
 - 23s - loss: 0.0093 - val_loss: 0.0085
 - val_f1: 0.9780
Epoch 100/200
 - 23s - loss: 0.0093 - val_loss: 0.0083
 - val_f1: 0.9754
Epoch 101/200
 - 23s - loss: 0.0093 - val_loss: 0.0090
2019-12-24 19:04:21,252 [INFO] epoch = 100. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep5/ann_model_epoch_100.pickle
 - val_f1: 0.9775
Epoch 102/200
 - 23s - loss: 0.0093 - val_loss: 0.0087
 - val_f1: 0.9773
Epoch 103/200
 - 23s - loss: 0.0093 - val_loss: 0.0082
 - val_f1: 0.9778
Epoch 104/200
 - 23s - loss: 0.0093 - val_loss: 0.0092
 - val_f1: 0.9770
Epoch 105/200
 - 23s - loss: 0.0093 - val_loss: 0.0082
 - val_f1: 0.9778
Epoch 106/200
 - 23s - loss: 0.0093 - val_loss: 0.0087
 - val_f1: 0.9735
Epoch 107/200
 - 23s - loss: 0.0093 - val_loss: 0.0081
 - val_f1: 0.9779
Epoch 108/200
 - 23s - loss: 0.0093 - val_loss: 0.0080
 - val_f1: 0.9795
Epoch 109/200
 - 23s - loss: 0.0092 - val_loss: 0.0078
 - val_f1: 0.9778
Epoch 110/200
 - 23s - loss: 0.0092 - val_loss: 0.0080
 - val_f1: 0.9780
Epoch 111/200
 - 23s - loss: 0.0092 - val_loss: 0.0079
 - val_f1: 0.9807
Epoch 112/200
 - 23s - loss: 0.0093 - val_loss: 0.0082
 - val_f1: 0.9778
Epoch 113/200
 - 23s - loss: 0.0092 - val_loss: 0.0085
 - val_f1: 0.9756
Epoch 114/200
 - 23s - loss: 0.0092 - val_loss: 0.0089
 - val_f1: 0.9747
Epoch 115/200
 - 23s - loss: 0.0092 - val_loss: 0.0084
 - val_f1: 0.9760
Epoch 116/200
 - 23s - loss: 0.0092 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 117/200
 - 23s - loss: 0.0092 - val_loss: 0.0081
 - val_f1: 0.9803
Epoch 118/200
 - 23s - loss: 0.0092 - val_loss: 0.0079
 - val_f1: 0.9799
Epoch 119/200
 - 23s - loss: 0.0092 - val_loss: 0.0083
 - val_f1: 0.9794
Epoch 120/200
 - 23s - loss: 0.0092 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 121/200
 - 23s - loss: 0.0092 - val_loss: 0.0078
2019-12-24 19:19:06,181 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep5/ann_model_epoch_120.pickle
 - val_f1: 0.9806
Epoch 122/200
 - 23s - loss: 0.0092 - val_loss: 0.0082
 - val_f1: 0.9767
Epoch 123/200
 - 23s - loss: 0.0092 - val_loss: 0.0079
 - val_f1: 0.9804
Epoch 124/200
 - 23s - loss: 0.0092 - val_loss: 0.0081
 - val_f1: 0.9783
Epoch 125/200
 - 23s - loss: 0.0092 - val_loss: 0.0080
 - val_f1: 0.9814
Epoch 126/200
 - 23s - loss: 0.0092 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 127/200
 - 23s - loss: 0.0091 - val_loss: 0.0083
 - val_f1: 0.9779
Epoch 128/200
 - 23s - loss: 0.0091 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 129/200
 - 23s - loss: 0.0091 - val_loss: 0.0081
 - val_f1: 0.9787
Epoch 130/200
 - 23s - loss: 0.0091 - val_loss: 0.0086
 - val_f1: 0.9770
Epoch 131/200
 - 23s - loss: 0.0091 - val_loss: 0.0079
 - val_f1: 0.9818
Epoch 132/200
 - 23s - loss: 0.0092 - val_loss: 0.0079
 - val_f1: 0.9784
Epoch 133/200
 - 23s - loss: 0.0092 - val_loss: 0.0078
 - val_f1: 0.9822
Epoch 134/200
 - 23s - loss: 0.0092 - val_loss: 0.0081
 - val_f1: 0.9800
Epoch 135/200
 - 23s - loss: 0.0091 - val_loss: 0.0082
 - val_f1: 0.9787
Epoch 136/200
 - 23s - loss: 0.0091 - val_loss: 0.0080
 - val_f1: 0.9788
Epoch 137/200
 - 23s - loss: 0.0091 - val_loss: 0.0082
 - val_f1: 0.9789
Epoch 138/200
 - 23s - loss: 0.0091 - val_loss: 0.0080
 - val_f1: 0.9784
Epoch 139/200
 - 23s - loss: 0.0092 - val_loss: 0.0078
 - val_f1: 0.9807
Epoch 140/200
 - 23s - loss: 0.0091 - val_loss: 0.0079
 - val_f1: 0.9791
Epoch 141/200
 - 23s - loss: 0.0091 - val_loss: 0.0082
2019-12-24 19:33:50,888 [INFO] epoch = 140. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep5/ann_model_epoch_140.pickle
 - val_f1: 0.9767
Epoch 142/200
 - 23s - loss: 0.0092 - val_loss: 0.0083
 - val_f1: 0.9797
Epoch 143/200
 - 23s - loss: 0.0091 - val_loss: 0.0077
 - val_f1: 0.9792
Epoch 144/200
 - 23s - loss: 0.0091 - val_loss: 0.0080
 - val_f1: 0.9782
Epoch 145/200
 - 23s - loss: 0.0091 - val_loss: 0.0086
 - val_f1: 0.9775
Epoch 146/200
 - 23s - loss: 0.0091 - val_loss: 0.0080
 - val_f1: 0.9790
Epoch 147/200
 - 23s - loss: 0.0091 - val_loss: 0.0078
 - val_f1: 0.9796
Epoch 148/200
 - 23s - loss: 0.0091 - val_loss: 0.0081
 - val_f1: 0.9823
Epoch 149/200
 - 23s - loss: 0.0091 - val_loss: 0.0079
 - val_f1: 0.9810
Epoch 150/200
 - 23s - loss: 0.0090 - val_loss: 0.0078
 - val_f1: 0.9808
Epoch 151/200
 - 23s - loss: 0.0091 - val_loss: 0.0077
 - val_f1: 0.9783
Epoch 152/200
 - 23s - loss: 0.0091 - val_loss: 0.0077
 - val_f1: 0.9796
Epoch 153/200
 - 23s - loss: 0.0091 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 154/200
 - 23s - loss: 0.0091 - val_loss: 0.0081
 - val_f1: 0.9781
Epoch 155/200
 - 23s - loss: 0.0091 - val_loss: 0.0077
 - val_f1: 0.9790
Epoch 156/200
 - 23s - loss: 0.0091 - val_loss: 0.0077
 - val_f1: 0.9814
Epoch 157/200
 - 23s - loss: 0.0091 - val_loss: 0.0079
 - val_f1: 0.9771
Epoch 158/200
 - 23s - loss: 0.0091 - val_loss: 0.0087
 - val_f1: 0.9752
Epoch 159/200
 - 23s - loss: 0.0091 - val_loss: 0.0080
 - val_f1: 0.9807
Epoch 160/200
 - 23s - loss: 0.0091 - val_loss: 0.0080
 - val_f1: 0.9807
Epoch 161/200
 - 23s - loss: 0.0090 - val_loss: 0.0078
2019-12-24 19:48:36,875 [INFO] epoch = 160. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep5/ann_model_epoch_160.pickle
 - val_f1: 0.9794
Epoch 162/200
 - 23s - loss: 0.0090 - val_loss: 0.0086
 - val_f1: 0.9763
Epoch 163/200
 - 23s - loss: 0.0090 - val_loss: 0.0077
 - val_f1: 0.9814
Epoch 164/200
 - 23s - loss: 0.0091 - val_loss: 0.0081
 - val_f1: 0.9772
Epoch 165/200
 - 23s - loss: 0.0091 - val_loss: 0.0077
 - val_f1: 0.9813
Epoch 166/200
 - 23s - loss: 0.0091 - val_loss: 0.0078
 - val_f1: 0.9789
Epoch 167/200
 - 23s - loss: 0.0091 - val_loss: 0.0082
 - val_f1: 0.9772
Epoch 168/200
 - 23s - loss: 0.0090 - val_loss: 0.0076
 - val_f1: 0.9825
Epoch 169/200
 - 23s - loss: 0.0090 - val_loss: 0.0080
 - val_f1: 0.9807
Epoch 170/200
 - 23s - loss: 0.0090 - val_loss: 0.0076
 - val_f1: 0.9817
Epoch 171/200
 - 23s - loss: 0.0090 - val_loss: 0.0080
 - val_f1: 0.9768
Epoch 172/200
 - 23s - loss: 0.0090 - val_loss: 0.0080
 - val_f1: 0.9808
Epoch 173/200
 - 23s - loss: 0.0091 - val_loss: 0.0092
 - val_f1: 0.9758
Epoch 174/200
 - 23s - loss: 0.0090 - val_loss: 0.0083
 - val_f1: 0.9783
Epoch 175/200
 - 23s - loss: 0.0090 - val_loss: 0.0083
 - val_f1: 0.9764
Epoch 176/200
 - 23s - loss: 0.0090 - val_loss: 0.0077
 - val_f1: 0.9827
Epoch 177/200
 - 23s - loss: 0.0090 - val_loss: 0.0079
 - val_f1: 0.9802
Epoch 178/200
 - 23s - loss: 0.0091 - val_loss: 0.0082
 - val_f1: 0.9800
Epoch 179/200
 - 23s - loss: 0.0090 - val_loss: 0.0078
 - val_f1: 0.9810
Epoch 180/200
 - 23s - loss: 0.0090 - val_loss: 0.0080
 - val_f1: 0.9814
Epoch 181/200
 - 23s - loss: 0.0090 - val_loss: 0.0084
2019-12-24 20:03:22,395 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_ids17_ae_ann_deep_rep5/ann_model_epoch_180.pickle
 - val_f1: 0.9775
Epoch 182/200
 - 22s - loss: 0.0090 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 183/200
 - 22s - loss: 0.0090 - val_loss: 0.0083
 - val_f1: 0.9770
Epoch 184/200
 - 23s - loss: 0.0090 - val_loss: 0.0077
 - val_f1: 0.9788
Epoch 185/200
 - 23s - loss: 0.0090 - val_loss: 0.0079
 - val_f1: 0.9786
Epoch 186/200
 - 23s - loss: 0.0090 - val_loss: 0.0081
 - val_f1: 0.9770
Epoch 187/200
 - 22s - loss: 0.0090 - val_loss: 0.0086
 - val_f1: 0.9758
Epoch 188/200
 - 23s - loss: 0.0090 - val_loss: 0.0077
 - val_f1: 0.9808
Epoch 189/200
 - 22s - loss: 0.0090 - val_loss: 0.0079
 - val_f1: 0.9787
Epoch 190/200
 - 22s - loss: 0.0090 - val_loss: 0.0079
 - val_f1: 0.9778
Epoch 191/200
 - 23s - loss: 0.0090 - val_loss: 0.0078
 - val_f1: 0.9787
Epoch 192/200
 - 22s - loss: 0.0090 - val_loss: 0.0081
 - val_f1: 0.9784
Epoch 193/200
 - 22s - loss: 0.0090 - val_loss: 0.0078
 - val_f1: 0.9787
Epoch 194/200
 - 22s - loss: 0.0090 - val_loss: 0.0078
 - val_f1: 0.9795
Epoch 195/200
 - 23s - loss: 0.0090 - val_loss: 0.0080
 - val_f1: 0.9813
Epoch 196/200
 - 22s - loss: 0.0089 - val_loss: 0.0092
 - val_f1: 0.9780
Epoch 197/200
 - 22s - loss: 0.0090 - val_loss: 0.0076
 - val_f1: 0.9789
Epoch 198/200
 - 22s - loss: 0.0090 - val_loss: 0.0075
 - val_f1: 0.9819
Epoch 199/200
 - 22s - loss: 0.0089 - val_loss: 0.0082
 - val_f1: 0.9797
Epoch 200/200
 - 23s - loss: 0.0090 - val_loss: 0.0082
2019-12-24 20:17:38,417 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-24 20:18:53,015 [INFO] Last epoch loss evaluation: train_loss = 0.007449, val_loss = 0.007475
2019-12-24 20:18:53,023 [INFO] Training complete. time_to_train = 14155.94 sec, 235.93 min
2019-12-24 20:18:53,051 [INFO] Model saved to results_selected_models/selected_ids17_ae_ann_deep_rep5/best_model.pickle
/home/sunanda/research/ids_experiments/utility.py:268: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
2019-12-24 20:18:53,253 [INFO] Plot saved to: results_selected_models/selected_ids17_ae_ann_deep_rep5/training_error_history.png
/home/sunanda/research/ids_experiments/utility.py:285: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
2019-12-24 20:18:53,444 [INFO] Plot saved to: results_selected_models/selected_ids17_ae_ann_deep_rep5/training_f1_history.png
2019-12-24 20:18:53,444 [INFO] Making predictions on training, validation, testing data
2019-12-24 20:22:42,326 [INFO] Evaluating predictions (results)
/home/sunanda/anaconda3/envs/ml_env_cpu/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2019-12-24 20:23:02,181 [INFO] Dataset: Testing. Classification report below
2019-12-24 20:23:02,181 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       0.99      0.99      0.99    454265
                   Bot       0.97      0.36      0.52       391
                  DDoS       1.00      0.98      0.99     25605
         DoS GoldenEye       0.97      0.95      0.96      2058
              DoS Hulk       0.96      0.97      0.96     46025
      DoS Slowhttptest       0.86      0.96      0.91      1100
         DoS slowloris       0.98      0.90      0.94      1159
           FTP-Patator       1.00      0.98      0.99      1587
              PortScan       0.89      0.94      0.92     31761
           SSH-Patator       0.96      0.95      0.95      1179
Web Attack Brute Force       0.90      0.06      0.11       302
        Web Attack XSS       0.00      0.00      0.00       130

              accuracy                           0.98    565562
             macro avg       0.87      0.75      0.77    565562
          weighted avg       0.98      0.98      0.98    565562

2019-12-24 20:23:02,181 [INFO] Overall accuracy (micro avg): 0.9815935299754934
/home/sunanda/research/ids_experiments/utility.py:307: RuntimeWarning: invalid value encountered in true_divide
  PPV = TP / (TP + FP)    # Precision or positive predictive value
/home/sunanda/research/ids_experiments/utility.py:311: RuntimeWarning: invalid value encountered in true_divide
  FDR = FP / (TP + FP)    # False discovery rate
2019-12-24 20:23:23,547 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9816         0.9816                       0.9816                0.0017                   0.0184  0.9816
1     Macro avg        0.9969         0.8730                       0.7523                0.0044                   0.2477  0.7697
2  Weighted avg        0.9844         0.9817                       0.9816                0.0343                   0.0184  0.9813
2019-12-24 20:23:43,584 [INFO] Dataset: Validation. Classification report below
2019-12-24 20:23:43,584 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       0.99      0.99      0.99    454264
                   Bot       0.98      0.31      0.48       391
                  DDoS       1.00      0.98      0.99     25605
         DoS GoldenEye       0.97      0.93      0.95      2059
              DoS Hulk       0.96      0.97      0.97     46025
      DoS Slowhttptest       0.86      0.96      0.91      1099
         DoS slowloris       0.97      0.89      0.93      1159
           FTP-Patator       1.00      0.97      0.98      1587
              PortScan       0.89      0.95      0.92     31761
           SSH-Patator       0.96      0.95      0.96      1180
Web Attack Brute Force       1.00      0.04      0.07       301
        Web Attack XSS       0.00      0.00      0.00       131

              accuracy                           0.98    565562
             macro avg       0.88      0.75      0.76    565562
          weighted avg       0.98      0.98      0.98    565562

2019-12-24 20:23:43,584 [INFO] Overall accuracy (micro avg): 0.9819082611632324
2019-12-24 20:24:05,175 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9819         0.9819                       0.9819                0.0016                   0.0181  0.9819
1     Macro avg        0.9970         0.8827                       0.7455                0.0043                   0.2545  0.7619
2  Weighted avg        0.9847         0.9821                       0.9819                0.0335                   0.0181  0.9816
2019-12-24 20:25:12,145 [INFO] Dataset: Training. Classification report below
2019-12-24 20:25:12,145 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       0.99      0.99      0.99   1362791
                   Bot       0.98      0.35      0.52      1174
                  DDoS       1.00      0.98      0.99     76815
         DoS GoldenEye       0.97      0.94      0.96      6176
              DoS Hulk       0.96      0.97      0.97    138074
      DoS Slowhttptest       0.88      0.96      0.92      3300
         DoS slowloris       0.98      0.91      0.94      3478
           FTP-Patator       1.00      0.98      0.99      4761
              PortScan       0.89      0.95      0.92     95282
           SSH-Patator       0.97      0.95      0.96      3538
Web Attack Brute Force       1.00      0.05      0.09       904
        Web Attack XSS       0.00      0.00      0.00       391

              accuracy                           0.98   1696684
             macro avg       0.89      0.75      0.77   1696684
          weighted avg       0.98      0.98      0.98   1696684

2019-12-24 20:25:12,145 [INFO] Overall accuracy (micro avg): 0.9820962536335581
2019-12-24 20:26:24,324 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9821         0.9821                       0.9821                0.0016                   0.0179  0.9821
1     Macro avg        0.9970         0.8852                       0.7519                0.0043                   0.2481  0.7695
2  Weighted avg        0.9848         0.9822                       0.9821                0.0333                   0.0179  0.9818
2019-12-24 20:26:24,389 [INFO] Results saved to: results_selected_models/selected_ids17_ae_ann_deep_rep5/selected_ids17_ae_ann_deep_rep5_results.xlsx
2019-12-24 20:26:24,393 [INFO] ================= Finished running experiment no. 5 ================= 

2019-12-24 20:26:24,434 [INFO] Created directory: results_selected_models/selected_kdd99_ae_ann_deep_rep1
2019-12-24 20:26:24,435 [INFO] Initialized logging. log_filename = results_selected_models/selected_kdd99_ae_ann_deep_rep1/run_log.log
2019-12-24 20:26:24,435 [INFO] ================= Running experiment no. 1  ================= 

2019-12-24 20:26:24,435 [INFO] Experiment parameters given below
2019-12-24 20:26:24,435 [INFO] 
{'experiment_num': 1, 'results_dir': 'results_selected_models/selected_kdd99_ae_ann_deep_rep1', 'model_type': 'classifier', 'model': 'ae_ann', 'normal_label': 'normal', 'scaling_type': 'NA', 'unsupervised_ratio': 0.25, 'ae_encoder_units': [128, 64, 32], 'ae_encoder_activations': ['relu', 'relu', 'relu'], 'ae_encoder_dropout_rates': [0.2, 0.2, 0.2], 'ae_encoder_l1_param': -1, 'ae_decoder_units': [64, 128], 'ae_decoder_activations': ['relu', 'relu'], 'ae_decoder_dropout_rates': [0.2, 0.2], 'output_activation': 'sigmoid', 'loss_function': 'binary_crossentropy', 'ae_epochs': 200, 'ann_layer_units': [64], 'ann_layer_activations': ['relu'], 'ann_layer_dropout_rates': [0.2], 'ann_epochs': 200, 'early_stop_patience': 50, 'batch_size': 256, 'dataset_dir': '../Datasets/full_datasets/kdd99_five_classes', 'description': 'selected_kdd99_ae_ann_deep_rep1'}
2019-12-24 20:26:24,435 [INFO] Created tensorboard log directory: results_selected_models/selected_kdd99_ae_ann_deep_rep1/tf_logs_run_2019_12_24-20_26_24
2019-12-24 20:26:24,435 [INFO] Loading datsets from: ../Datasets/full_datasets/kdd99_five_classes
2019-12-24 20:26:24,435 [INFO] Reading X, y files
2019-12-24 20:26:24,435 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/X_train.h5
2019-12-24 20:26:31,119 [INFO] Reading complete. time_to_read=6.68 seconds
2019-12-24 20:26:31,119 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/X_val.h5
2019-12-24 20:26:32,795 [INFO] Reading complete. time_to_read=1.68 seconds
2019-12-24 20:26:32,795 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/X_test.h5
2019-12-24 20:26:33,267 [INFO] Reading complete. time_to_read=0.47 seconds
2019-12-24 20:26:33,267 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/y_train.h5
2019-12-24 20:26:33,466 [INFO] Reading complete. time_to_read=0.20 seconds
2019-12-24 20:26:33,467 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/y_val.h5
2019-12-24 20:26:33,522 [INFO] Reading complete. time_to_read=0.05 seconds
2019-12-24 20:26:33,522 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/y_test.h5
2019-12-24 20:26:33,542 [INFO] Reading complete. time_to_read=0.02 seconds
2019-12-24 20:26:40,680 [INFO] Initializing model
2019-12-24 20:26:41,270 [INFO] _________________________________________________________________
2019-12-24 20:26:41,270 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-24 20:26:41,270 [INFO] =================================================================
2019-12-24 20:26:41,270 [INFO] dense_101 (Dense)            (None, 128)               15872     
2019-12-24 20:26:41,270 [INFO] _________________________________________________________________
2019-12-24 20:26:41,270 [INFO] batch_normalization_71 (Batc (None, 128)               512       
2019-12-24 20:26:41,270 [INFO] _________________________________________________________________
2019-12-24 20:26:41,270 [INFO] dropout_71 (Dropout)         (None, 128)               0         
2019-12-24 20:26:41,270 [INFO] _________________________________________________________________
2019-12-24 20:26:41,271 [INFO] dense_102 (Dense)            (None, 64)                8256      
2019-12-24 20:26:41,271 [INFO] _________________________________________________________________
2019-12-24 20:26:41,271 [INFO] batch_normalization_72 (Batc (None, 64)                256       
2019-12-24 20:26:41,271 [INFO] _________________________________________________________________
2019-12-24 20:26:41,271 [INFO] dropout_72 (Dropout)         (None, 64)                0         
2019-12-24 20:26:41,271 [INFO] _________________________________________________________________
2019-12-24 20:26:41,271 [INFO] dense_103 (Dense)            (None, 32)                2080      
2019-12-24 20:26:41,271 [INFO] _________________________________________________________________
2019-12-24 20:26:41,271 [INFO] batch_normalization_73 (Batc (None, 32)                128       
2019-12-24 20:26:41,271 [INFO] _________________________________________________________________
2019-12-24 20:26:41,271 [INFO] dropout_73 (Dropout)         (None, 32)                0         
2019-12-24 20:26:41,271 [INFO] _________________________________________________________________
2019-12-24 20:26:41,272 [INFO] dense_104 (Dense)            (None, 64)                2112      
2019-12-24 20:26:41,272 [INFO] _________________________________________________________________
2019-12-24 20:26:41,272 [INFO] batch_normalization_74 (Batc (None, 64)                256       
2019-12-24 20:26:41,272 [INFO] _________________________________________________________________
2019-12-24 20:26:41,272 [INFO] dropout_74 (Dropout)         (None, 64)                0         
2019-12-24 20:26:41,272 [INFO] _________________________________________________________________
2019-12-24 20:26:41,272 [INFO] dense_105 (Dense)            (None, 128)               8320      
2019-12-24 20:26:41,272 [INFO] _________________________________________________________________
2019-12-24 20:26:41,272 [INFO] batch_normalization_75 (Batc (None, 128)               512       
2019-12-24 20:26:41,272 [INFO] _________________________________________________________________
2019-12-24 20:26:41,272 [INFO] dropout_75 (Dropout)         (None, 128)               0         
2019-12-24 20:26:41,272 [INFO] _________________________________________________________________
2019-12-24 20:26:41,273 [INFO] dense_106 (Dense)            (None, 123)               15867     
2019-12-24 20:26:41,273 [INFO] =================================================================
2019-12-24 20:26:41,273 [INFO] Total params: 54,171
2019-12-24 20:26:41,273 [INFO] Trainable params: 53,339
2019-12-24 20:26:41,273 [INFO] Non-trainable params: 832
2019-12-24 20:26:41,273 [INFO] _________________________________________________________________
2019-12-24 20:26:41,419 [INFO] _________________________________________________________________
2019-12-24 20:26:41,419 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-24 20:26:41,419 [INFO] =================================================================
2019-12-24 20:26:41,419 [INFO] dense_107 (Dense)            (None, 64)                2112      
2019-12-24 20:26:41,419 [INFO] _________________________________________________________________
2019-12-24 20:26:41,420 [INFO] batch_normalization_76 (Batc (None, 64)                256       
2019-12-24 20:26:41,420 [INFO] _________________________________________________________________
2019-12-24 20:26:41,420 [INFO] dropout_76 (Dropout)         (None, 64)                0         
2019-12-24 20:26:41,420 [INFO] _________________________________________________________________
2019-12-24 20:26:41,420 [INFO] dense_108 (Dense)            (None, 5)                 325       
2019-12-24 20:26:41,420 [INFO] =================================================================
2019-12-24 20:26:41,420 [INFO] Total params: 2,693
2019-12-24 20:26:41,420 [INFO] Trainable params: 2,565
2019-12-24 20:26:41,420 [INFO] Non-trainable params: 128
2019-12-24 20:26:41,420 [INFO] _________________________________________________________________
2019-12-24 20:26:41,420 [INFO] Training model
2019-12-24 20:26:41,420 [INFO] Splitting train set into 2 sets (unsupervised, supervised), random_seed = None
2019-12-24 20:27:26,503 [INFO] Split sizes (instances). total = 3918744, unsupervised = 979686, supervised = 2939058, unsupervised dataset hash = 81cabc76acd5724feec1fe6b4db5378503c27a49
2019-12-24 20:27:26,503 [INFO] Training autoencoder
 - val_f1: 0.9779
Train on 979686 samples, validate on 979687 samples
Epoch 1/200
 - 65s - loss: -1.8396e+00 - val_loss: -2.0737e+00
Epoch 2/200
 - 61s - loss: -2.0559e+00 - val_loss: -2.0909e+00
Epoch 3/200
 - 61s - loss: -2.0710e+00 - val_loss: -2.0993e+00
Epoch 4/200
 - 61s - loss: -2.0803e+00 - val_loss: -2.1014e+00
Epoch 5/200
 - 61s - loss: -2.0856e+00 - val_loss: -2.1051e+00
Epoch 6/200
 - 61s - loss: -2.0891e+00 - val_loss: -2.1056e+00
Epoch 7/200
 - 61s - loss: -2.0915e+00 - val_loss: -2.1077e+00
Epoch 8/200
 - 61s - loss: -2.0933e+00 - val_loss: -2.1077e+00
Epoch 9/200
 - 61s - loss: -2.0952e+00 - val_loss: -2.1099e+00
Epoch 10/200
 - 61s - loss: -2.0960e+00 - val_loss: -2.1085e+00
Epoch 11/200
 - 61s - loss: -2.0963e+00 - val_loss: -2.1091e+00
Epoch 12/200
 - 61s - loss: -2.0972e+00 - val_loss: -2.1106e+00
Epoch 13/200
 - 61s - loss: -2.0979e+00 - val_loss: -2.1109e+00
Epoch 14/200
 - 61s - loss: -2.0992e+00 - val_loss: -2.1114e+00
Epoch 15/200
 - 61s - loss: -2.0993e+00 - val_loss: -2.1124e+00
Epoch 16/200
 - 61s - loss: -2.0995e+00 - val_loss: -2.1122e+00
Epoch 17/200
 - 61s - loss: -2.0997e+00 - val_loss: -2.1120e+00
Epoch 18/200
 - 61s - loss: -2.1004e+00 - val_loss: -2.1123e+00
Epoch 19/200
 - 61s - loss: -2.1006e+00 - val_loss: -2.1111e+00
Epoch 20/200
 - 61s - loss: -2.1010e+00 - val_loss: -2.1111e+00
Epoch 21/200
 - 61s - loss: -2.1011e+00 - val_loss: -2.1126e+00
2019-12-24 20:49:14,471 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep1/ae_model_epoch_20.pickle
Epoch 22/200
 - 61s - loss: -2.1020e+00 - val_loss: -2.1120e+00
Epoch 23/200
 - 61s - loss: -2.1018e+00 - val_loss: -2.1125e+00
Epoch 24/200
 - 61s - loss: -2.1020e+00 - val_loss: -2.1126e+00
Epoch 25/200
 - 61s - loss: -2.1019e+00 - val_loss: -2.1110e+00
Epoch 26/200
 - 61s - loss: -2.1021e+00 - val_loss: -2.1123e+00
Epoch 27/200
 - 61s - loss: -2.1024e+00 - val_loss: -2.1110e+00
Epoch 28/200
 - 61s - loss: -2.1022e+00 - val_loss: -2.1128e+00
Epoch 29/200
 - 61s - loss: -2.1023e+00 - val_loss: -2.1103e+00
Epoch 30/200
 - 61s - loss: -2.1026e+00 - val_loss: -2.1095e+00
Epoch 31/200
 - 61s - loss: -2.1025e+00 - val_loss: -2.1113e+00
Epoch 32/200
 - 61s - loss: -2.1031e+00 - val_loss: -2.1108e+00
Epoch 33/200
 - 61s - loss: -2.1031e+00 - val_loss: -2.1114e+00
Epoch 34/200
 - 61s - loss: -2.1030e+00 - val_loss: -2.1111e+00
Epoch 35/200
 - 61s - loss: -2.1030e+00 - val_loss: -2.1115e+00
Epoch 36/200
 - 61s - loss: -2.1031e+00 - val_loss: -2.1117e+00
Epoch 37/200
 - 61s - loss: -2.1030e+00 - val_loss: -2.1125e+00
Epoch 38/200
 - 61s - loss: -2.1034e+00 - val_loss: -2.1125e+00
Epoch 39/200
 - 61s - loss: -2.1034e+00 - val_loss: -2.1121e+00
Epoch 40/200
 - 61s - loss: -2.1034e+00 - val_loss: -2.1114e+00
Epoch 41/200
 - 61s - loss: -2.1038e+00 - val_loss: -2.1118e+00
2019-12-24 21:09:36,106 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep1/ae_model_epoch_40.pickle
Epoch 42/200
 - 61s - loss: -2.1033e+00 - val_loss: -2.1122e+00
Epoch 43/200
 - 61s - loss: -2.1037e+00 - val_loss: -2.1114e+00
Epoch 44/200
 - 61s - loss: -2.1040e+00 - val_loss: -2.1125e+00
Epoch 45/200
 - 61s - loss: -2.1043e+00 - val_loss: -2.1127e+00
Epoch 46/200
 - 61s - loss: -2.1037e+00 - val_loss: -2.1127e+00
Epoch 47/200
 - 61s - loss: -2.1043e+00 - val_loss: -2.1126e+00
Epoch 48/200
 - 61s - loss: -2.1039e+00 - val_loss: -2.1121e+00
Epoch 49/200
 - 61s - loss: -2.1044e+00 - val_loss: -2.1125e+00
Epoch 50/200
 - 61s - loss: -2.1047e+00 - val_loss: -2.1122e+00
Epoch 51/200
 - 61s - loss: -2.1046e+00 - val_loss: -2.1126e+00
Epoch 52/200
 - 61s - loss: -2.1044e+00 - val_loss: -2.1128e+00
Epoch 53/200
 - 61s - loss: -2.1051e+00 - val_loss: -2.1124e+00
Epoch 54/200
 - 61s - loss: -2.1050e+00 - val_loss: -2.1125e+00
Epoch 55/200
 - 61s - loss: -2.1043e+00 - val_loss: -2.1123e+00
Epoch 56/200
 - 61s - loss: -2.1049e+00 - val_loss: -2.1123e+00
Epoch 57/200
 - 61s - loss: -2.1045e+00 - val_loss: -2.1129e+00
Epoch 58/200
 - 61s - loss: -2.1051e+00 - val_loss: -2.1127e+00
Epoch 59/200
 - 61s - loss: -2.1050e+00 - val_loss: -2.1128e+00
Epoch 60/200
 - 61s - loss: -2.1054e+00 - val_loss: -2.1124e+00
Epoch 61/200
 - 61s - loss: -2.1046e+00 - val_loss: -2.1126e+00
2019-12-24 21:29:58,070 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep1/ae_model_epoch_60.pickle
Epoch 62/200
 - 61s - loss: -2.1044e+00 - val_loss: -2.1119e+00
Epoch 63/200
 - 61s - loss: -2.1053e+00 - val_loss: -2.1121e+00
Epoch 64/200
 - 61s - loss: -2.1047e+00 - val_loss: -2.1119e+00
Epoch 65/200
 - 61s - loss: -2.1051e+00 - val_loss: -2.1123e+00
Epoch 66/200
 - 61s - loss: -2.1053e+00 - val_loss: -2.1127e+00
Epoch 67/200
 - 61s - loss: -2.1051e+00 - val_loss: -2.1124e+00
Epoch 68/200
 - 61s - loss: -2.1042e+00 - val_loss: -2.1124e+00
Epoch 69/200
 - 61s - loss: -2.1052e+00 - val_loss: -2.1126e+00
Epoch 70/200
 - 61s - loss: -2.1052e+00 - val_loss: -2.1121e+00
Epoch 71/200
 - 61s - loss: -2.1054e+00 - val_loss: -2.1128e+00
Epoch 72/200
 - 61s - loss: -2.1050e+00 - val_loss: -2.1129e+00
Epoch 73/200
 - 61s - loss: -2.1055e+00 - val_loss: -2.1128e+00
Epoch 74/200
 - 61s - loss: -2.1056e+00 - val_loss: -2.1129e+00
Epoch 75/200
 - 61s - loss: -2.1059e+00 - val_loss: -2.1115e+00
Epoch 76/200
 - 61s - loss: -2.1054e+00 - val_loss: -2.1133e+00
Epoch 77/200
 - 61s - loss: -2.1058e+00 - val_loss: -2.1138e+00
Epoch 78/200
 - 61s - loss: -2.1059e+00 - val_loss: -2.1131e+00
Epoch 79/200
 - 61s - loss: -2.1059e+00 - val_loss: -2.1128e+00
Epoch 80/200
 - 61s - loss: -2.1056e+00 - val_loss: -2.1130e+00
Epoch 81/200
 - 61s - loss: -2.1056e+00 - val_loss: -2.1135e+00
2019-12-24 21:50:20,332 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep1/ae_model_epoch_80.pickle
Epoch 82/200
 - 61s - loss: -2.1056e+00 - val_loss: -2.1139e+00
Epoch 83/200
 - 61s - loss: -2.1057e+00 - val_loss: -2.1138e+00
Epoch 84/200
 - 61s - loss: -2.1060e+00 - val_loss: -2.1129e+00
Epoch 85/200
 - 61s - loss: -2.1058e+00 - val_loss: -2.1135e+00
Epoch 86/200
 - 61s - loss: -2.1060e+00 - val_loss: -2.1140e+00
Epoch 87/200
 - 61s - loss: -2.1062e+00 - val_loss: -2.1136e+00
Epoch 88/200
 - 61s - loss: -2.1062e+00 - val_loss: -2.1137e+00
Epoch 89/200
 - 61s - loss: -2.1056e+00 - val_loss: -2.1137e+00
Epoch 90/200
 - 61s - loss: -2.1060e+00 - val_loss: -2.1115e+00
Epoch 91/200
 - 61s - loss: -2.1054e+00 - val_loss: -2.1132e+00
Epoch 92/200
 - 61s - loss: -2.1061e+00 - val_loss: -2.1139e+00
Epoch 93/200
 - 61s - loss: -2.1061e+00 - val_loss: -2.1136e+00
Epoch 94/200
 - 61s - loss: -2.1061e+00 - val_loss: -2.1134e+00
Epoch 95/200
 - 61s - loss: -2.1063e+00 - val_loss: -2.1139e+00
Epoch 96/200
 - 61s - loss: -2.1066e+00 - val_loss: -2.1137e+00
Epoch 97/200
 - 61s - loss: -2.1062e+00 - val_loss: -2.1143e+00
Epoch 98/200
 - 61s - loss: -2.1068e+00 - val_loss: -2.1135e+00
Epoch 99/200
 - 61s - loss: -2.1066e+00 - val_loss: -2.1132e+00
Epoch 100/200
 - 61s - loss: -2.1068e+00 - val_loss: -2.1129e+00
Epoch 101/200
 - 61s - loss: -2.1067e+00 - val_loss: -2.1142e+00
2019-12-24 22:10:42,301 [INFO] epoch = 100. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep1/ae_model_epoch_100.pickle
Epoch 102/200
 - 61s - loss: -2.1071e+00 - val_loss: -2.1128e+00
Epoch 103/200
 - 61s - loss: -2.1066e+00 - val_loss: -2.1131e+00
Epoch 104/200
 - 61s - loss: -2.1066e+00 - val_loss: -2.1129e+00
Epoch 105/200
 - 61s - loss: -2.1066e+00 - val_loss: -2.1135e+00
Epoch 106/200
 - 61s - loss: -2.1067e+00 - val_loss: -2.1144e+00
Epoch 107/200
 - 61s - loss: -2.1070e+00 - val_loss: -2.1140e+00
Epoch 108/200
 - 61s - loss: -2.1075e+00 - val_loss: -2.1149e+00
Epoch 109/200
 - 61s - loss: -2.1074e+00 - val_loss: -2.1143e+00
Epoch 110/200
 - 61s - loss: -2.1076e+00 - val_loss: -2.1134e+00
Epoch 111/200
 - 61s - loss: -2.1072e+00 - val_loss: -2.1143e+00
Epoch 112/200
 - 61s - loss: -2.1071e+00 - val_loss: -2.1140e+00
Epoch 113/200
 - 61s - loss: -2.1069e+00 - val_loss: -2.1137e+00
Epoch 114/200
 - 61s - loss: -2.1073e+00 - val_loss: -2.1149e+00
Epoch 115/200
 - 61s - loss: -2.1070e+00 - val_loss: -2.1142e+00
Epoch 116/200
 - 61s - loss: -2.1070e+00 - val_loss: -2.1130e+00
Epoch 117/200
 - 61s - loss: -2.1068e+00 - val_loss: -2.1152e+00
Epoch 118/200
 - 61s - loss: -2.1074e+00 - val_loss: -2.1146e+00
Epoch 119/200
 - 61s - loss: -2.1078e+00 - val_loss: -2.1154e+00
Epoch 120/200
 - 61s - loss: -2.1070e+00 - val_loss: -2.1158e+00
Epoch 121/200
 - 61s - loss: -2.1074e+00 - val_loss: -2.1163e+00
2019-12-24 22:31:04,133 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep1/ae_model_epoch_120.pickle
Epoch 122/200
 - 61s - loss: -2.1073e+00 - val_loss: -2.1160e+00
Epoch 123/200
 - 61s - loss: -2.1072e+00 - val_loss: -2.1152e+00
Epoch 124/200
 - 61s - loss: -2.1077e+00 - val_loss: -2.1159e+00
Epoch 125/200
 - 61s - loss: -2.1079e+00 - val_loss: -2.1165e+00
Epoch 126/200
 - 61s - loss: -2.1080e+00 - val_loss: -2.1154e+00
Epoch 127/200
 - 61s - loss: -2.1072e+00 - val_loss: -2.1162e+00
Epoch 128/200
 - 61s - loss: -2.1080e+00 - val_loss: -2.1148e+00
Epoch 129/200
 - 61s - loss: -2.1078e+00 - val_loss: -2.1146e+00
Epoch 130/200
 - 61s - loss: -2.1076e+00 - val_loss: -2.1145e+00
Epoch 131/200
 - 61s - loss: -2.1077e+00 - val_loss: -2.1163e+00
Epoch 132/200
 - 61s - loss: -2.1079e+00 - val_loss: -2.1147e+00
Epoch 133/200
 - 61s - loss: -2.1077e+00 - val_loss: -2.1151e+00
Epoch 134/200
 - 61s - loss: -2.1080e+00 - val_loss: -2.1143e+00
Epoch 135/200
 - 61s - loss: -2.1069e+00 - val_loss: -2.1160e+00
Epoch 136/200
 - 61s - loss: -2.1082e+00 - val_loss: -2.1148e+00
Epoch 137/200
 - 61s - loss: -2.1083e+00 - val_loss: -2.1145e+00
Epoch 138/200
 - 61s - loss: -2.1077e+00 - val_loss: -2.1149e+00
Epoch 139/200
 - 61s - loss: -2.1083e+00 - val_loss: -2.1153e+00
Epoch 140/200
 - 61s - loss: -2.1085e+00 - val_loss: -2.1148e+00
Epoch 141/200
 - 61s - loss: -2.1080e+00 - val_loss: -2.1149e+00
2019-12-24 22:51:25,521 [INFO] epoch = 140. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep1/ae_model_epoch_140.pickle
Epoch 142/200
 - 61s - loss: -2.1073e+00 - val_loss: -2.1150e+00
Epoch 143/200
 - 61s - loss: -2.1083e+00 - val_loss: -2.1149e+00
Epoch 144/200
 - 61s - loss: -2.1076e+00 - val_loss: -2.1147e+00
Epoch 145/200
 - 61s - loss: -2.1076e+00 - val_loss: -2.1144e+00
Epoch 146/200
 - 61s - loss: -2.1084e+00 - val_loss: -2.1135e+00
Epoch 147/200
 - 61s - loss: -2.1085e+00 - val_loss: -2.1146e+00
Epoch 148/200
 - 61s - loss: -2.1077e+00 - val_loss: -2.1137e+00
Epoch 149/200
 - 61s - loss: -2.1083e+00 - val_loss: -2.1156e+00
Epoch 150/200
 - 61s - loss: -2.1081e+00 - val_loss: -2.1154e+00
Epoch 151/200
 - 61s - loss: -2.1081e+00 - val_loss: -2.1142e+00
Epoch 152/200
 - 61s - loss: -2.1080e+00 - val_loss: -2.1155e+00
Epoch 153/200
 - 61s - loss: -2.1083e+00 - val_loss: -2.1135e+00
Epoch 154/200
 - 61s - loss: -2.1078e+00 - val_loss: -2.1143e+00
Epoch 155/200
 - 61s - loss: -2.1080e+00 - val_loss: -2.1133e+00
Epoch 156/200
 - 61s - loss: -2.1080e+00 - val_loss: -2.1148e+00
Epoch 157/200
 - 61s - loss: -2.1083e+00 - val_loss: -2.1154e+00
Epoch 158/200
 - 61s - loss: -2.1087e+00 - val_loss: -2.1140e+00
Epoch 159/200
 - 61s - loss: -2.1082e+00 - val_loss: -2.1147e+00
Epoch 160/200
 - 61s - loss: -2.1085e+00 - val_loss: -2.1140e+00
Epoch 161/200
 - 61s - loss: -2.1086e+00 - val_loss: -2.1133e+00
2019-12-24 23:11:47,979 [INFO] epoch = 160. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep1/ae_model_epoch_160.pickle
Epoch 162/200
 - 61s - loss: -2.1086e+00 - val_loss: -2.1165e+00
Epoch 163/200
 - 61s - loss: -2.1087e+00 - val_loss: -2.1168e+00
Epoch 164/200
 - 61s - loss: -2.1087e+00 - val_loss: -2.1158e+00
Epoch 165/200
 - 61s - loss: -2.1083e+00 - val_loss: -2.1150e+00
Epoch 166/200
 - 61s - loss: -2.1084e+00 - val_loss: -2.1160e+00
Epoch 167/200
 - 61s - loss: -2.1081e+00 - val_loss: -2.1159e+00
Epoch 168/200
 - 61s - loss: -2.1085e+00 - val_loss: -2.1157e+00
Epoch 169/200
 - 61s - loss: -2.1083e+00 - val_loss: -2.1151e+00
Epoch 170/200
 - 61s - loss: -2.1078e+00 - val_loss: -2.1142e+00
Epoch 171/200
 - 61s - loss: -2.1082e+00 - val_loss: -2.1145e+00
Epoch 172/200
 - 61s - loss: -2.1085e+00 - val_loss: -2.1158e+00
Epoch 173/200
 - 61s - loss: -2.1082e+00 - val_loss: -2.1156e+00
Epoch 174/200
 - 61s - loss: -2.1086e+00 - val_loss: -2.1162e+00
Epoch 175/200
 - 61s - loss: -2.1082e+00 - val_loss: -2.1153e+00
Epoch 176/200
 - 61s - loss: -2.1086e+00 - val_loss: -2.1146e+00
Epoch 177/200
 - 61s - loss: -2.1084e+00 - val_loss: -2.1144e+00
Epoch 178/200
 - 61s - loss: -2.1085e+00 - val_loss: -2.1150e+00
Epoch 179/200
 - 61s - loss: -2.1084e+00 - val_loss: -2.1163e+00
Epoch 180/200
 - 61s - loss: -2.1086e+00 - val_loss: -2.1151e+00
Epoch 181/200
 - 61s - loss: -2.1081e+00 - val_loss: -2.1164e+00
2019-12-24 23:32:10,850 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep1/ae_model_epoch_180.pickle
Epoch 182/200
 - 61s - loss: -2.1079e+00 - val_loss: -2.1158e+00
Epoch 183/200
 - 61s - loss: -2.1081e+00 - val_loss: -2.1151e+00
Epoch 184/200
 - 61s - loss: -2.1085e+00 - val_loss: -2.1165e+00
Epoch 185/200
 - 61s - loss: -2.1086e+00 - val_loss: -2.1148e+00
Epoch 186/200
 - 61s - loss: -2.1088e+00 - val_loss: -2.1153e+00
Epoch 187/200
 - 61s - loss: -2.1081e+00 - val_loss: -2.1161e+00
Epoch 188/200
 - 61s - loss: -2.1082e+00 - val_loss: -2.1156e+00
Epoch 189/200
 - 61s - loss: -2.1080e+00 - val_loss: -2.1153e+00
Epoch 190/200
 - 61s - loss: -2.1083e+00 - val_loss: -2.1154e+00
Epoch 191/200
 - 61s - loss: -2.1089e+00 - val_loss: -2.1153e+00
Epoch 192/200
 - 61s - loss: -2.1086e+00 - val_loss: -2.1151e+00
Epoch 193/200
 - 61s - loss: -2.1085e+00 - val_loss: -2.1155e+00
Epoch 194/200
 - 61s - loss: -2.1089e+00 - val_loss: -2.1152e+00
Epoch 195/200
 - 61s - loss: -2.1083e+00 - val_loss: -2.1157e+00
Epoch 196/200
 - 61s - loss: -2.1089e+00 - val_loss: -2.1150e+00
Epoch 197/200
 - 61s - loss: -2.1082e+00 - val_loss: -2.1156e+00
Epoch 198/200
 - 61s - loss: -2.1086e+00 - val_loss: -2.1154e+00
Epoch 199/200
 - 61s - loss: -2.1087e+00 - val_loss: -2.1148e+00
Epoch 200/200
 - 61s - loss: -2.1084e+00 - val_loss: -2.1152e+00
2019-12-24 23:51:31,953 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-24 23:54:03,125 [INFO] Last epoch loss evaluation: train_loss = -2.119535, val_loss = -2.116753
2019-12-24 23:54:03,125 [INFO] Training autoencoder complete
2019-12-24 23:54:03,125 [INFO] Encoding data for supervised training
2019-12-24 23:57:08,186 [INFO] Encoding complete
2019-12-24 23:57:08,187 [INFO] Training neural network layers (after autoencoder)
Train on 2939058 samples, validate on 979687 samples
Epoch 1/200
 - 53s - loss: 0.0065 - val_loss: 0.0013
 - val_f1: 0.9992
Epoch 2/200
 - 51s - loss: 0.0014 - val_loss: 0.0010
 - val_f1: 0.9994
Epoch 3/200
 - 51s - loss: 0.0012 - val_loss: 8.8478e-04
 - val_f1: 0.9994
Epoch 4/200
 - 51s - loss: 0.0011 - val_loss: 8.8705e-04
 - val_f1: 0.9995
Epoch 5/200
 - 51s - loss: 0.0010 - val_loss: 8.2113e-04
 - val_f1: 0.9995
Epoch 6/200
 - 51s - loss: 9.8535e-04 - val_loss: 8.0023e-04
 - val_f1: 0.9995
Epoch 7/200
 - 51s - loss: 9.2609e-04 - val_loss: 7.4257e-04
 - val_f1: 0.9995
Epoch 8/200
 - 51s - loss: 9.0073e-04 - val_loss: 7.9554e-04
 - val_f1: 0.9996
Epoch 9/200
 - 51s - loss: 8.9655e-04 - val_loss: 7.3434e-04
 - val_f1: 0.9995
Epoch 10/200
 - 51s - loss: 8.5713e-04 - val_loss: 7.1562e-04
 - val_f1: 0.9996
Epoch 11/200
 - 51s - loss: 8.2852e-04 - val_loss: 7.0582e-04
 - val_f1: 0.9996
Epoch 12/200
 - 51s - loss: 8.0823e-04 - val_loss: 6.9228e-04
 - val_f1: 0.9996
Epoch 13/200
 - 51s - loss: 7.8200e-04 - val_loss: 7.0485e-04
 - val_f1: 0.9996
Epoch 14/200
 - 51s - loss: 7.8872e-04 - val_loss: 6.7037e-04
 - val_f1: 0.9996
Epoch 15/200
 - 51s - loss: 7.8519e-04 - val_loss: 6.9148e-04
 - val_f1: 0.9996
Epoch 16/200
 - 51s - loss: 7.6152e-04 - val_loss: 6.9952e-04
 - val_f1: 0.9996
Epoch 17/200
 - 51s - loss: 7.4955e-04 - val_loss: 6.5181e-04
 - val_f1: 0.9996
Epoch 18/200
 - 51s - loss: 7.5291e-04 - val_loss: 6.1703e-04
 - val_f1: 0.9996
Epoch 19/200
 - 51s - loss: 7.5237e-04 - val_loss: 5.6388e-04
 - val_f1: 0.9996
Epoch 20/200
 - 51s - loss: 7.3595e-04 - val_loss: 6.0737e-04
 - val_f1: 0.9997
Epoch 21/200
 - 51s - loss: 7.2111e-04 - val_loss: 6.2031e-04
2019-12-25 00:27:53,594 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep1/ann_model_epoch_20.pickle
 - val_f1: 0.9996
Epoch 22/200
 - 51s - loss: 7.2188e-04 - val_loss: 7.9223e-04
 - val_f1: 0.9995
Epoch 23/200
 - 51s - loss: 7.0945e-04 - val_loss: 6.6827e-04
 - val_f1: 0.9996
Epoch 24/200
 - 51s - loss: 6.9978e-04 - val_loss: 5.9201e-04
 - val_f1: 0.9996
Epoch 25/200
 - 51s - loss: 6.9219e-04 - val_loss: 5.8484e-04
 - val_f1: 0.9996
Epoch 26/200
 - 51s - loss: 6.9398e-04 - val_loss: 9.3286e-04
 - val_f1: 0.9994
Epoch 27/200
 - 51s - loss: 6.7677e-04 - val_loss: 6.6748e-04
 - val_f1: 0.9996
Epoch 28/200
 - 51s - loss: 6.8255e-04 - val_loss: 7.2475e-04
 - val_f1: 0.9995
Epoch 29/200
 - 51s - loss: 6.8268e-04 - val_loss: 6.8069e-04
 - val_f1: 0.9996
Epoch 30/200
 - 51s - loss: 6.8218e-04 - val_loss: 5.9815e-04
 - val_f1: 0.9996
Epoch 31/200
 - 51s - loss: 6.7843e-04 - val_loss: 5.9453e-04
 - val_f1: 0.9996
Epoch 32/200
 - 51s - loss: 6.6947e-04 - val_loss: 5.8486e-04
 - val_f1: 0.9997
Epoch 33/200
 - 51s - loss: 6.7089e-04 - val_loss: 6.1541e-04
 - val_f1: 0.9997
Epoch 34/200
 - 51s - loss: 6.6388e-04 - val_loss: 6.5668e-04
 - val_f1: 0.9996
Epoch 35/200
 - 51s - loss: 6.5677e-04 - val_loss: 5.9522e-04
 - val_f1: 0.9996
Epoch 36/200
 - 51s - loss: 6.4569e-04 - val_loss: 5.7091e-04
 - val_f1: 0.9996
Epoch 37/200
 - 51s - loss: 6.5257e-04 - val_loss: 5.6184e-04
 - val_f1: 0.9997
Epoch 38/200
 - 51s - loss: 6.3261e-04 - val_loss: 6.2258e-04
 - val_f1: 0.9996
Epoch 39/200
 - 51s - loss: 6.3908e-04 - val_loss: 5.5005e-04
 - val_f1: 0.9996
Epoch 40/200
 - 51s - loss: 6.4284e-04 - val_loss: 6.3831e-04
 - val_f1: 0.9996
Epoch 41/200
 - 51s - loss: 6.3377e-04 - val_loss: 5.7980e-04
2019-12-25 00:57:32,795 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep1/ann_model_epoch_40.pickle
 - val_f1: 0.9996
Epoch 42/200
 - 51s - loss: 6.4539e-04 - val_loss: 6.0448e-04
 - val_f1: 0.9996
Epoch 43/200
 - 51s - loss: 6.2952e-04 - val_loss: 6.6080e-04
 - val_f1: 0.9996
Epoch 44/200
 - 51s - loss: 6.3930e-04 - val_loss: 6.2180e-04
 - val_f1: 0.9996
Epoch 45/200
 - 51s - loss: 6.2170e-04 - val_loss: 7.9980e-04
 - val_f1: 0.9996
Epoch 46/200
 - 51s - loss: 6.2676e-04 - val_loss: 6.3681e-04
 - val_f1: 0.9996
Epoch 47/200
 - 51s - loss: 6.3177e-04 - val_loss: 5.5629e-04
 - val_f1: 0.9997
Epoch 48/200
 - 51s - loss: 6.3711e-04 - val_loss: 5.2984e-04
 - val_f1: 0.9996
Epoch 49/200
 - 51s - loss: 6.2890e-04 - val_loss: 5.6315e-04
 - val_f1: 0.9996
Epoch 50/200
 - 51s - loss: 6.3410e-04 - val_loss: 6.5043e-04
 - val_f1: 0.9996
Epoch 51/200
 - 51s - loss: 6.1614e-04 - val_loss: 8.0368e-04
 - val_f1: 0.9996
Epoch 52/200
 - 51s - loss: 6.1088e-04 - val_loss: 6.7475e-04
 - val_f1: 0.9997
Epoch 53/200
 - 51s - loss: 6.0047e-04 - val_loss: 6.3810e-04
 - val_f1: 0.9997
Epoch 54/200
 - 51s - loss: 5.9762e-04 - val_loss: 6.6935e-04
 - val_f1: 0.9996
Epoch 55/200
 - 51s - loss: 6.0946e-04 - val_loss: 6.3744e-04
 - val_f1: 0.9996
Epoch 56/200
 - 51s - loss: 6.0441e-04 - val_loss: 6.2990e-04
 - val_f1: 0.9996
Epoch 57/200
 - 51s - loss: 5.9898e-04 - val_loss: 7.0569e-04
 - val_f1: 0.9996
Epoch 58/200
 - 51s - loss: 6.1218e-04 - val_loss: 5.4762e-04
 - val_f1: 0.9997
Epoch 59/200
 - 51s - loss: 5.9293e-04 - val_loss: 6.5914e-04
 - val_f1: 0.9997
Epoch 60/200
 - 51s - loss: 6.0468e-04 - val_loss: 6.8664e-04
 - val_f1: 0.9996
Epoch 61/200
 - 51s - loss: 5.9396e-04 - val_loss: 6.5980e-04
2019-12-25 01:27:10,297 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep1/ann_model_epoch_60.pickle
 - val_f1: 0.9997
Epoch 62/200
 - 51s - loss: 5.9106e-04 - val_loss: 6.6892e-04
 - val_f1: 0.9997
Epoch 63/200
 - 51s - loss: 6.1464e-04 - val_loss: 7.0308e-04
 - val_f1: 0.9996
Epoch 64/200
 - 51s - loss: 6.0417e-04 - val_loss: 6.2423e-04
 - val_f1: 0.9996
Epoch 65/200
 - 51s - loss: 6.0549e-04 - val_loss: 6.5621e-04
 - val_f1: 0.9996
Epoch 66/200
 - 51s - loss: 6.0629e-04 - val_loss: 5.7981e-04
 - val_f1: 0.9997
Epoch 67/200
 - 51s - loss: 6.1360e-04 - val_loss: 6.9530e-04
 - val_f1: 0.9997
Epoch 68/200
 - 51s - loss: 6.0610e-04 - val_loss: 6.5621e-04
 - val_f1: 0.9997
Epoch 69/200
 - 51s - loss: 6.0627e-04 - val_loss: 6.1617e-04
 - val_f1: 0.9997
Epoch 70/200
 - 51s - loss: 5.9380e-04 - val_loss: 5.7389e-04
 - val_f1: 0.9997
Epoch 71/200
 - 51s - loss: 5.9446e-04 - val_loss: 6.2312e-04
 - val_f1: 0.9996
Epoch 72/200
 - 51s - loss: 5.9858e-04 - val_loss: 5.4749e-04
 - val_f1: 0.9997
Epoch 73/200
 - 51s - loss: 5.9402e-04 - val_loss: 6.6867e-04
 - val_f1: 0.9996
Epoch 74/200
 - 51s - loss: 5.8115e-04 - val_loss: 5.9647e-04
 - val_f1: 0.9997
Epoch 75/200
 - 51s - loss: 5.9780e-04 - val_loss: 5.8913e-04
 - val_f1: 0.9997
Epoch 76/200
 - 51s - loss: 5.9040e-04 - val_loss: 5.2620e-04
 - val_f1: 0.9997
Epoch 77/200
 - 51s - loss: 5.9523e-04 - val_loss: 5.3948e-04
 - val_f1: 0.9997
Epoch 78/200
 - 51s - loss: 5.9559e-04 - val_loss: 5.5376e-04
 - val_f1: 0.9997
Epoch 79/200
 - 51s - loss: 6.0188e-04 - val_loss: 6.8304e-04
 - val_f1: 0.9997
Epoch 80/200
 - 51s - loss: 5.8206e-04 - val_loss: 5.7935e-04
 - val_f1: 0.9997
Epoch 81/200
 - 51s - loss: 5.8023e-04 - val_loss: 5.7094e-04
2019-12-25 01:56:48,530 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep1/ann_model_epoch_80.pickle
 - val_f1: 0.9997
Epoch 82/200
 - 51s - loss: 5.7865e-04 - val_loss: 5.8752e-04
 - val_f1: 0.9996
Epoch 83/200
 - 51s - loss: 5.8446e-04 - val_loss: 6.9597e-04
 - val_f1: 0.9996
Epoch 84/200
 - 51s - loss: 5.9340e-04 - val_loss: 6.1324e-04
 - val_f1: 0.9997
Epoch 85/200
 - 51s - loss: 5.8218e-04 - val_loss: 5.6594e-04
 - val_f1: 0.9997
Epoch 86/200
 - 51s - loss: 5.7030e-04 - val_loss: 5.4677e-04
 - val_f1: 0.9997
Epoch 87/200
 - 51s - loss: 5.6986e-04 - val_loss: 5.2663e-04
 - val_f1: 0.9997
Epoch 88/200
 - 51s - loss: 5.6992e-04 - val_loss: 7.2157e-04
 - val_f1: 0.9994
Epoch 89/200
 - 51s - loss: 5.7225e-04 - val_loss: 6.0727e-04
 - val_f1: 0.9997
Epoch 90/200
 - 51s - loss: 5.6536e-04 - val_loss: 6.2963e-04
 - val_f1: 0.9997
Epoch 91/200
 - 51s - loss: 5.8242e-04 - val_loss: 5.3082e-04
 - val_f1: 0.9997
Epoch 92/200
 - 51s - loss: 5.7095e-04 - val_loss: 5.7759e-04
 - val_f1: 0.9997
Epoch 93/200
 - 51s - loss: 5.5009e-04 - val_loss: 5.9552e-04
 - val_f1: 0.9997
Epoch 94/200
 - 51s - loss: 5.7496e-04 - val_loss: 6.0829e-04
 - val_f1: 0.9997
Epoch 95/200
 - 51s - loss: 5.8301e-04 - val_loss: 6.6379e-04
 - val_f1: 0.9997
Epoch 96/200
 - 51s - loss: 5.8058e-04 - val_loss: 6.9161e-04
 - val_f1: 0.9997
Epoch 97/200
 - 51s - loss: 5.7037e-04 - val_loss: 5.6366e-04
 - val_f1: 0.9997
Epoch 98/200
 - 51s - loss: 5.6116e-04 - val_loss: 5.6502e-04
 - val_f1: 0.9997
Epoch 99/200
 - 51s - loss: 5.6296e-04 - val_loss: 5.4263e-04
 - val_f1: 0.9997
Epoch 100/200
 - 51s - loss: 5.7212e-04 - val_loss: 6.0022e-04
 - val_f1: 0.9997
Epoch 101/200
 - 51s - loss: 5.4990e-04 - val_loss: 5.8527e-04
2019-12-25 02:26:26,483 [INFO] epoch = 100. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep1/ann_model_epoch_100.pickle
 - val_f1: 0.9997
Epoch 102/200
 - 51s - loss: 5.6172e-04 - val_loss: 6.1427e-04
 - val_f1: 0.9997
Epoch 103/200
 - 51s - loss: 5.5128e-04 - val_loss: 5.5588e-04
 - val_f1: 0.9997
Epoch 104/200
 - 51s - loss: 5.4693e-04 - val_loss: 6.1339e-04
 - val_f1: 0.9997
Epoch 105/200
 - 51s - loss: 5.4121e-04 - val_loss: 5.7281e-04
 - val_f1: 0.9997
Epoch 106/200
 - 51s - loss: 5.3852e-04 - val_loss: 5.8082e-04
 - val_f1: 0.9997
Epoch 107/200
 - 51s - loss: 5.4562e-04 - val_loss: 6.2540e-04
 - val_f1: 0.9997
Epoch 108/200
 - 51s - loss: 5.5464e-04 - val_loss: 6.7061e-04
 - val_f1: 0.9997
Epoch 109/200
 - 51s - loss: 5.5370e-04 - val_loss: 5.8770e-04
 - val_f1: 0.9997
Epoch 110/200
 - 51s - loss: 5.4763e-04 - val_loss: 5.8710e-04
 - val_f1: 0.9997
Epoch 111/200
 - 51s - loss: 5.6000e-04 - val_loss: 5.5847e-04
 - val_f1: 0.9997
Epoch 112/200
 - 51s - loss: 5.4779e-04 - val_loss: 5.8903e-04
 - val_f1: 0.9997
Epoch 113/200
 - 51s - loss: 5.4163e-04 - val_loss: 6.2676e-04
 - val_f1: 0.9997
Epoch 114/200
 - 51s - loss: 5.6121e-04 - val_loss: 6.5185e-04
 - val_f1: 0.9997
Epoch 115/200
 - 51s - loss: 5.5448e-04 - val_loss: 5.4797e-04
 - val_f1: 0.9997
Epoch 116/200
 - 51s - loss: 5.3964e-04 - val_loss: 5.4244e-04
 - val_f1: 0.9997
Epoch 117/200
 - 51s - loss: 5.6040e-04 - val_loss: 5.3686e-04
 - val_f1: 0.9997
Epoch 118/200
 - 51s - loss: 5.4783e-04 - val_loss: 5.7684e-04
 - val_f1: 0.9997
Epoch 119/200
 - 51s - loss: 5.5225e-04 - val_loss: 6.5462e-04
 - val_f1: 0.9996
Epoch 120/200
 - 51s - loss: 5.5123e-04 - val_loss: 5.6947e-04
 - val_f1: 0.9997
Epoch 121/200
 - 51s - loss: 5.4540e-04 - val_loss: 5.5393e-04
2019-12-25 02:56:03,536 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep1/ann_model_epoch_120.pickle
 - val_f1: 0.9997
Epoch 122/200
 - 51s - loss: 5.4133e-04 - val_loss: 5.4218e-04
 - val_f1: 0.9997
Epoch 123/200
 - 51s - loss: 5.3436e-04 - val_loss: 5.4747e-04
 - val_f1: 0.9997
Epoch 124/200
 - 51s - loss: 5.5940e-04 - val_loss: 5.2908e-04
 - val_f1: 0.9997
Epoch 125/200
 - 51s - loss: 5.5380e-04 - val_loss: 5.7437e-04
 - val_f1: 0.9997
Epoch 126/200
 - 51s - loss: 5.4401e-04 - val_loss: 5.8286e-04
2019-12-25 03:04:05,850 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-25 03:06:46,750 [INFO] Last epoch loss evaluation: train_loss = 0.000454, val_loss = 0.000526
2019-12-25 03:06:46,773 [INFO] Training complete. time_to_train = 24005.35 sec, 400.09 min
2019-12-25 03:06:46,802 [INFO] Model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep1/best_model.pickle
/home/sunanda/research/ids_experiments/utility.py:268: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
2019-12-25 03:06:46,982 [INFO] Plot saved to: results_selected_models/selected_kdd99_ae_ann_deep_rep1/training_error_history.png
/home/sunanda/research/ids_experiments/utility.py:285: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
2019-12-25 03:06:47,153 [INFO] Plot saved to: results_selected_models/selected_kdd99_ae_ann_deep_rep1/training_f1_history.png
2019-12-25 03:06:47,153 [INFO] Making predictions on training, validation, testing data
2019-12-25 03:14:08,956 [INFO] Evaluating predictions (results)
2019-12-25 03:14:17,629 [INFO] Dataset: Testing. Classification report below
2019-12-25 03:14:17,629 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      0.97      0.99    229853
     normal.       0.73      0.99      0.84     60593
       probe       0.75      0.77      0.76      4166
         r2l       0.99      0.06      0.12     13781
         u2r       0.32      0.00      0.00      2636

    accuracy                           0.92    311029
   macro avg       0.76      0.56      0.54    311029
weighted avg       0.94      0.92      0.91    311029

2019-12-25 03:14:17,629 [INFO] Overall accuracy (micro avg): 0.92426429689836
2019-12-25 03:14:26,941 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9243         0.9243                       0.9243                0.0189                   0.0757  0.9243
1     Macro avg        0.9697         0.7577                       0.5589                0.0194                   0.4411  0.5423
2  Weighted avg        0.9679         0.9365                       0.9243                0.0213                   0.0757  0.9072
2019-12-25 03:14:57,169 [INFO] Dataset: Validation. Classification report below
2019-12-25 03:14:57,169 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00    776675
     normal.       1.00      1.00      1.00    194556
       probe       0.99      0.99      0.99      8221
         r2l       0.96      0.67      0.79       225
         u2r       0.00      0.00      0.00        10

    accuracy                           1.00    979687
   macro avg       0.79      0.73      0.76    979687
weighted avg       1.00      1.00      1.00    979687

2019-12-25 03:14:57,170 [INFO] Overall accuracy (micro avg): 0.9997090907606205
/home/sunanda/research/ids_experiments/utility.py:313: RuntimeWarning: invalid value encountered in true_divide
  F1 = (2 * PPV * TPR) / (PPV + TPR)  # F1 score
2019-12-25 03:15:29,792 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9997         0.9997                       0.9997                0.0001                   0.0003  0.9997
1     Macro avg        0.9999         0.7911                       0.7305                0.0001                   0.2695  0.7554
2  Weighted avg        0.9999         0.9997                       0.9997                0.0002                   0.0003  0.9997
2019-12-25 03:17:43,433 [INFO] Dataset: Training. Classification report below
2019-12-25 03:17:43,434 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00   3106695
     normal.       1.00      1.00      1.00    778225
       probe       1.00      0.99      0.99     32881
         r2l       0.98      0.68      0.80       901
         u2r       0.12      0.12      0.12        42

    accuracy                           1.00   3918744
   macro avg       0.82      0.76      0.78   3918744
weighted avg       1.00      1.00      1.00   3918744

2019-12-25 03:17:43,434 [INFO] Overall accuracy (micro avg): 0.9997136837721474
2019-12-25 03:20:07,593 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9997         0.9997                       0.9997                0.0001                   0.0003  0.9997
1     Macro avg        0.9999         0.8191                       0.7566                0.0001                   0.2434  0.7825
2  Weighted avg        0.9999         0.9997                       0.9997                0.0003                   0.0003  0.9997
2019-12-25 03:20:07,640 [INFO] Results saved to: results_selected_models/selected_kdd99_ae_ann_deep_rep1/selected_kdd99_ae_ann_deep_rep1_results.xlsx
2019-12-25 03:20:07,647 [INFO] ================= Finished running experiment no. 1 ================= 

2019-12-25 03:20:07,670 [INFO] Created directory: results_selected_models/selected_kdd99_ae_ann_deep_rep2
2019-12-25 03:20:07,670 [INFO] Initialized logging. log_filename = results_selected_models/selected_kdd99_ae_ann_deep_rep2/run_log.log
2019-12-25 03:20:07,670 [INFO] ================= Running experiment no. 2  ================= 

2019-12-25 03:20:07,670 [INFO] Experiment parameters given below
2019-12-25 03:20:07,671 [INFO] 
{'experiment_num': 2, 'results_dir': 'results_selected_models/selected_kdd99_ae_ann_deep_rep2', 'model_type': 'classifier', 'model': 'ae_ann', 'normal_label': 'normal', 'scaling_type': 'NA', 'unsupervised_ratio': 0.25, 'ae_encoder_units': [128, 64, 32], 'ae_encoder_activations': ['relu', 'relu', 'relu'], 'ae_encoder_dropout_rates': [0.2, 0.2, 0.2], 'ae_encoder_l1_param': -1, 'ae_decoder_units': [64, 128], 'ae_decoder_activations': ['relu', 'relu'], 'ae_decoder_dropout_rates': [0.2, 0.2], 'output_activation': 'sigmoid', 'loss_function': 'binary_crossentropy', 'ae_epochs': 200, 'ann_layer_units': [64], 'ann_layer_activations': ['relu'], 'ann_layer_dropout_rates': [0.2], 'ann_epochs': 200, 'early_stop_patience': 50, 'batch_size': 256, 'dataset_dir': '../Datasets/full_datasets/kdd99_five_classes', 'description': 'selected_kdd99_ae_ann_deep_rep2'}
2019-12-25 03:20:07,671 [INFO] Created tensorboard log directory: results_selected_models/selected_kdd99_ae_ann_deep_rep2/tf_logs_run_2019_12_25-03_20_07
2019-12-25 03:20:07,671 [INFO] Loading datsets from: ../Datasets/full_datasets/kdd99_five_classes
2019-12-25 03:20:07,671 [INFO] Reading X, y files
2019-12-25 03:20:07,671 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/X_train.h5
2019-12-25 03:20:14,339 [INFO] Reading complete. time_to_read=6.67 seconds
2019-12-25 03:20:14,340 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/X_val.h5
2019-12-25 03:20:16,007 [INFO] Reading complete. time_to_read=1.67 seconds
2019-12-25 03:20:16,007 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/X_test.h5
2019-12-25 03:20:16,474 [INFO] Reading complete. time_to_read=0.47 seconds
2019-12-25 03:20:16,474 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/y_train.h5
2019-12-25 03:20:16,678 [INFO] Reading complete. time_to_read=0.20 seconds
2019-12-25 03:20:16,678 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/y_val.h5
2019-12-25 03:20:16,733 [INFO] Reading complete. time_to_read=0.05 seconds
2019-12-25 03:20:16,733 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/y_test.h5
2019-12-25 03:20:16,753 [INFO] Reading complete. time_to_read=0.02 seconds
2019-12-25 03:20:23,917 [INFO] Initializing model
2019-12-25 03:20:24,522 [INFO] _________________________________________________________________
2019-12-25 03:20:24,523 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-25 03:20:24,523 [INFO] =================================================================
2019-12-25 03:20:24,523 [INFO] dense_109 (Dense)            (None, 128)               15872     
2019-12-25 03:20:24,523 [INFO] _________________________________________________________________
2019-12-25 03:20:24,523 [INFO] batch_normalization_77 (Batc (None, 128)               512       
2019-12-25 03:20:24,523 [INFO] _________________________________________________________________
2019-12-25 03:20:24,523 [INFO] dropout_77 (Dropout)         (None, 128)               0         
2019-12-25 03:20:24,523 [INFO] _________________________________________________________________
2019-12-25 03:20:24,523 [INFO] dense_110 (Dense)            (None, 64)                8256      
2019-12-25 03:20:24,523 [INFO] _________________________________________________________________
2019-12-25 03:20:24,523 [INFO] batch_normalization_78 (Batc (None, 64)                256       
2019-12-25 03:20:24,524 [INFO] _________________________________________________________________
2019-12-25 03:20:24,524 [INFO] dropout_78 (Dropout)         (None, 64)                0         
2019-12-25 03:20:24,524 [INFO] _________________________________________________________________
2019-12-25 03:20:24,524 [INFO] dense_111 (Dense)            (None, 32)                2080      
2019-12-25 03:20:24,524 [INFO] _________________________________________________________________
2019-12-25 03:20:24,524 [INFO] batch_normalization_79 (Batc (None, 32)                128       
2019-12-25 03:20:24,524 [INFO] _________________________________________________________________
2019-12-25 03:20:24,524 [INFO] dropout_79 (Dropout)         (None, 32)                0         
2019-12-25 03:20:24,524 [INFO] _________________________________________________________________
2019-12-25 03:20:24,524 [INFO] dense_112 (Dense)            (None, 64)                2112      
2019-12-25 03:20:24,524 [INFO] _________________________________________________________________
2019-12-25 03:20:24,524 [INFO] batch_normalization_80 (Batc (None, 64)                256       
2019-12-25 03:20:24,525 [INFO] _________________________________________________________________
2019-12-25 03:20:24,525 [INFO] dropout_80 (Dropout)         (None, 64)                0         
2019-12-25 03:20:24,525 [INFO] _________________________________________________________________
2019-12-25 03:20:24,525 [INFO] dense_113 (Dense)            (None, 128)               8320      
2019-12-25 03:20:24,525 [INFO] _________________________________________________________________
2019-12-25 03:20:24,525 [INFO] batch_normalization_81 (Batc (None, 128)               512       
2019-12-25 03:20:24,525 [INFO] _________________________________________________________________
2019-12-25 03:20:24,525 [INFO] dropout_81 (Dropout)         (None, 128)               0         
2019-12-25 03:20:24,525 [INFO] _________________________________________________________________
2019-12-25 03:20:24,525 [INFO] dense_114 (Dense)            (None, 123)               15867     
2019-12-25 03:20:24,525 [INFO] =================================================================
2019-12-25 03:20:24,526 [INFO] Total params: 54,171
2019-12-25 03:20:24,526 [INFO] Trainable params: 53,339
2019-12-25 03:20:24,526 [INFO] Non-trainable params: 832
2019-12-25 03:20:24,526 [INFO] _________________________________________________________________
2019-12-25 03:20:24,676 [INFO] _________________________________________________________________
2019-12-25 03:20:24,676 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-25 03:20:24,676 [INFO] =================================================================
2019-12-25 03:20:24,676 [INFO] dense_115 (Dense)            (None, 64)                2112      
2019-12-25 03:20:24,676 [INFO] _________________________________________________________________
2019-12-25 03:20:24,676 [INFO] batch_normalization_82 (Batc (None, 64)                256       
2019-12-25 03:20:24,676 [INFO] _________________________________________________________________
2019-12-25 03:20:24,676 [INFO] dropout_82 (Dropout)         (None, 64)                0         
2019-12-25 03:20:24,676 [INFO] _________________________________________________________________
2019-12-25 03:20:24,677 [INFO] dense_116 (Dense)            (None, 5)                 325       
2019-12-25 03:20:24,677 [INFO] =================================================================
2019-12-25 03:20:24,677 [INFO] Total params: 2,693
2019-12-25 03:20:24,677 [INFO] Trainable params: 2,565
2019-12-25 03:20:24,677 [INFO] Non-trainable params: 128
2019-12-25 03:20:24,677 [INFO] _________________________________________________________________
2019-12-25 03:20:24,677 [INFO] Training model
2019-12-25 03:20:24,677 [INFO] Splitting train set into 2 sets (unsupervised, supervised), random_seed = None
2019-12-25 03:21:07,665 [INFO] Split sizes (instances). total = 3918744, unsupervised = 979686, supervised = 2939058, unsupervised dataset hash = a0326032d9da5908a602c0fb85e68c28cf83099c
2019-12-25 03:21:07,665 [INFO] Training autoencoder
 - val_f1: 0.9997
Epoch 00126: early stopping
Train on 979686 samples, validate on 979687 samples
Epoch 1/200
 - 66s - loss: -1.8487e+00 - val_loss: -2.0754e+00
Epoch 2/200
 - 63s - loss: -2.0584e+00 - val_loss: -2.0914e+00
Epoch 3/200
 - 63s - loss: -2.0747e+00 - val_loss: -2.0973e+00
Epoch 4/200
 - 63s - loss: -2.0828e+00 - val_loss: -2.1022e+00
Epoch 5/200
 - 63s - loss: -2.0877e+00 - val_loss: -2.1049e+00
Epoch 6/200
 - 63s - loss: -2.0912e+00 - val_loss: -2.1052e+00
Epoch 7/200
 - 63s - loss: -2.0931e+00 - val_loss: -2.1069e+00
Epoch 8/200
 - 63s - loss: -2.0956e+00 - val_loss: -2.1079e+00
Epoch 9/200
 - 63s - loss: -2.0960e+00 - val_loss: -2.1094e+00
Epoch 10/200
 - 63s - loss: -2.0975e+00 - val_loss: -2.1099e+00
Epoch 11/200
 - 63s - loss: -2.0973e+00 - val_loss: -2.1097e+00
Epoch 12/200
 - 63s - loss: -2.0987e+00 - val_loss: -2.1098e+00
Epoch 13/200
 - 63s - loss: -2.0983e+00 - val_loss: -2.1099e+00
Epoch 14/200
 - 63s - loss: -2.0994e+00 - val_loss: -2.1109e+00
Epoch 15/200
 - 63s - loss: -2.0996e+00 - val_loss: -2.1097e+00
Epoch 16/200
 - 63s - loss: -2.1003e+00 - val_loss: -2.1104e+00
Epoch 17/200
 - 63s - loss: -2.1006e+00 - val_loss: -2.1098e+00
Epoch 18/200
 - 63s - loss: -2.1003e+00 - val_loss: -2.1110e+00
Epoch 19/200
 - 63s - loss: -2.1007e+00 - val_loss: -2.1106e+00
Epoch 20/200
 - 63s - loss: -2.1009e+00 - val_loss: -2.1111e+00
Epoch 21/200
 - 63s - loss: -2.1014e+00 - val_loss: -2.1106e+00
2019-12-25 03:43:26,649 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep2/ae_model_epoch_20.pickle
Epoch 22/200
 - 62s - loss: -2.1013e+00 - val_loss: -2.1103e+00
Epoch 23/200
 - 62s - loss: -2.1020e+00 - val_loss: -2.1119e+00
Epoch 24/200
 - 62s - loss: -2.1022e+00 - val_loss: -2.1113e+00
Epoch 25/200
 - 63s - loss: -2.1021e+00 - val_loss: -2.1105e+00
Epoch 26/200
 - 62s - loss: -2.1021e+00 - val_loss: -2.1117e+00
Epoch 27/200
 - 62s - loss: -2.1027e+00 - val_loss: -2.1114e+00
Epoch 28/200
 - 62s - loss: -2.1024e+00 - val_loss: -2.1118e+00
Epoch 29/200
 - 62s - loss: -2.1025e+00 - val_loss: -2.1124e+00
Epoch 30/200
 - 63s - loss: -2.1028e+00 - val_loss: -2.1116e+00
Epoch 31/200
 - 63s - loss: -2.1031e+00 - val_loss: -2.1114e+00
Epoch 32/200
 - 63s - loss: -2.1024e+00 - val_loss: -2.1116e+00
Epoch 33/200
 - 63s - loss: -2.1031e+00 - val_loss: -2.1115e+00
Epoch 34/200
 - 63s - loss: -2.1033e+00 - val_loss: -2.1116e+00
Epoch 35/200
 - 63s - loss: -2.1028e+00 - val_loss: -2.1121e+00
Epoch 36/200
 - 63s - loss: -2.1031e+00 - val_loss: -2.1121e+00
Epoch 37/200
 - 63s - loss: -2.1035e+00 - val_loss: -2.1122e+00
Epoch 38/200
 - 63s - loss: -2.1029e+00 - val_loss: -2.1118e+00
Epoch 39/200
 - 63s - loss: -2.1031e+00 - val_loss: -2.1117e+00
Epoch 40/200
 - 63s - loss: -2.1038e+00 - val_loss: -2.1117e+00
Epoch 41/200
 - 63s - loss: -2.1045e+00 - val_loss: -2.1122e+00
2019-12-25 04:04:17,429 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep2/ae_model_epoch_40.pickle
Epoch 42/200
 - 63s - loss: -2.1035e+00 - val_loss: -2.1129e+00
Epoch 43/200
 - 63s - loss: -2.1039e+00 - val_loss: -2.1119e+00
Epoch 44/200
 - 63s - loss: -2.1043e+00 - val_loss: -2.1127e+00
Epoch 45/200
 - 63s - loss: -2.1045e+00 - val_loss: -2.1111e+00
Epoch 46/200
 - 62s - loss: -2.1044e+00 - val_loss: -2.1110e+00
Epoch 47/200
 - 62s - loss: -2.1041e+00 - val_loss: -2.1132e+00
Epoch 48/200
 - 63s - loss: -2.1045e+00 - val_loss: -2.1133e+00
Epoch 49/200
 - 62s - loss: -2.1047e+00 - val_loss: -2.1127e+00
Epoch 50/200
 - 62s - loss: -2.1050e+00 - val_loss: -2.1126e+00
Epoch 51/200
 - 63s - loss: -2.1047e+00 - val_loss: -2.1140e+00
Epoch 52/200
 - 63s - loss: -2.1051e+00 - val_loss: -2.1128e+00
Epoch 53/200
 - 63s - loss: -2.1045e+00 - val_loss: -2.1126e+00
Epoch 54/200
 - 62s - loss: -2.1044e+00 - val_loss: -2.1131e+00
Epoch 55/200
 - 62s - loss: -2.1052e+00 - val_loss: -2.1130e+00
Epoch 56/200
 - 62s - loss: -2.1048e+00 - val_loss: -2.1142e+00
Epoch 57/200
 - 63s - loss: -2.1048e+00 - val_loss: -2.1136e+00
Epoch 58/200
 - 62s - loss: -2.1046e+00 - val_loss: -2.1142e+00
Epoch 59/200
 - 63s - loss: -2.1053e+00 - val_loss: -2.1129e+00
Epoch 60/200
 - 62s - loss: -2.1050e+00 - val_loss: -2.1117e+00
Epoch 61/200
 - 63s - loss: -2.1049e+00 - val_loss: -2.1141e+00
2019-12-25 04:25:07,262 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep2/ae_model_epoch_60.pickle
Epoch 62/200
 - 62s - loss: -2.1052e+00 - val_loss: -2.1133e+00
Epoch 63/200
 - 63s - loss: -2.1052e+00 - val_loss: -2.1136e+00
Epoch 64/200
 - 63s - loss: -2.1061e+00 - val_loss: -2.1127e+00
Epoch 65/200
 - 63s - loss: -2.1049e+00 - val_loss: -2.1141e+00
Epoch 66/200
 - 63s - loss: -2.1052e+00 - val_loss: -2.1137e+00
Epoch 67/200
 - 62s - loss: -2.1052e+00 - val_loss: -2.1138e+00
Epoch 68/200
 - 63s - loss: -2.1053e+00 - val_loss: -2.1125e+00
Epoch 69/200
 - 62s - loss: -2.1059e+00 - val_loss: -2.1135e+00
Epoch 70/200
 - 63s - loss: -2.1060e+00 - val_loss: -2.1130e+00
Epoch 71/200
 - 63s - loss: -2.1056e+00 - val_loss: -2.1144e+00
Epoch 72/200
 - 63s - loss: -2.1057e+00 - val_loss: -2.1139e+00
Epoch 73/200
 - 63s - loss: -2.1059e+00 - val_loss: -2.1129e+00
Epoch 74/200
 - 63s - loss: -2.1063e+00 - val_loss: -2.1131e+00
Epoch 75/200
 - 62s - loss: -2.1066e+00 - val_loss: -2.1142e+00
Epoch 76/200
 - 63s - loss: -2.1061e+00 - val_loss: -2.1139e+00
Epoch 77/200
 - 63s - loss: -2.1062e+00 - val_loss: -2.1141e+00
Epoch 78/200
 - 63s - loss: -2.1069e+00 - val_loss: -2.1147e+00
Epoch 79/200
 - 63s - loss: -2.1062e+00 - val_loss: -2.1133e+00
Epoch 80/200
 - 62s - loss: -2.1065e+00 - val_loss: -2.1143e+00
Epoch 81/200
 - 62s - loss: -2.1065e+00 - val_loss: -2.1132e+00
2019-12-25 04:45:57,996 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep2/ae_model_epoch_80.pickle
Epoch 82/200
 - 62s - loss: -2.1062e+00 - val_loss: -2.1146e+00
Epoch 83/200
 - 62s - loss: -2.1062e+00 - val_loss: -2.1148e+00
Epoch 84/200
 - 62s - loss: -2.1067e+00 - val_loss: -2.1147e+00
Epoch 85/200
 - 62s - loss: -2.1070e+00 - val_loss: -2.1146e+00
Epoch 86/200
 - 62s - loss: -2.1050e+00 - val_loss: -2.1147e+00
Epoch 87/200
 - 62s - loss: -2.1071e+00 - val_loss: -2.1152e+00
Epoch 88/200
 - 63s - loss: -2.1067e+00 - val_loss: -2.1145e+00
Epoch 89/200
 - 62s - loss: -2.1064e+00 - val_loss: -2.1138e+00
Epoch 90/200
 - 63s - loss: -2.1067e+00 - val_loss: -2.1141e+00
Epoch 91/200
 - 63s - loss: -2.1071e+00 - val_loss: -2.1142e+00
Epoch 92/200
 - 63s - loss: -2.1072e+00 - val_loss: -2.1149e+00
Epoch 93/200
 - 63s - loss: -2.1069e+00 - val_loss: -2.1145e+00
Epoch 94/200
 - 63s - loss: -2.1066e+00 - val_loss: -2.1145e+00
Epoch 95/200
 - 63s - loss: -2.1068e+00 - val_loss: -2.1146e+00
Epoch 96/200
 - 63s - loss: -2.1067e+00 - val_loss: -2.1142e+00
Epoch 97/200
 - 63s - loss: -2.1063e+00 - val_loss: -2.1144e+00
Epoch 98/200
 - 62s - loss: -2.1070e+00 - val_loss: -2.1154e+00
Epoch 99/200
 - 62s - loss: -2.1066e+00 - val_loss: -2.1151e+00
Epoch 100/200
 - 62s - loss: -2.1069e+00 - val_loss: -2.1147e+00
Epoch 101/200
 - 63s - loss: -2.1072e+00 - val_loss: -2.1150e+00
2019-12-25 05:06:48,177 [INFO] epoch = 100. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep2/ae_model_epoch_100.pickle
Epoch 102/200
 - 63s - loss: -2.1068e+00 - val_loss: -2.1159e+00
Epoch 103/200
 - 63s - loss: -2.1068e+00 - val_loss: -2.1154e+00
Epoch 104/200
 - 63s - loss: -2.1070e+00 - val_loss: -2.1148e+00
Epoch 105/200
 - 63s - loss: -2.1070e+00 - val_loss: -2.1150e+00
Epoch 106/200
 - 62s - loss: -2.1073e+00 - val_loss: -2.1147e+00
Epoch 107/200
 - 63s - loss: -2.1068e+00 - val_loss: -2.1153e+00
Epoch 108/200
 - 63s - loss: -2.1070e+00 - val_loss: -2.1152e+00
Epoch 109/200
 - 63s - loss: -2.1070e+00 - val_loss: -2.1143e+00
Epoch 110/200
 - 63s - loss: -2.1070e+00 - val_loss: -2.1155e+00
Epoch 111/200
 - 63s - loss: -2.1069e+00 - val_loss: -2.1146e+00
Epoch 112/200
 - 63s - loss: -2.1077e+00 - val_loss: -2.1145e+00
Epoch 113/200
 - 63s - loss: -2.1073e+00 - val_loss: -2.1154e+00
Epoch 114/200
 - 62s - loss: -2.1077e+00 - val_loss: -2.1142e+00
Epoch 115/200
 - 62s - loss: -2.1073e+00 - val_loss: -2.1144e+00
Epoch 116/200
 - 62s - loss: -2.1068e+00 - val_loss: -2.1144e+00
Epoch 117/200
 - 62s - loss: -2.1072e+00 - val_loss: -2.1151e+00
Epoch 118/200
 - 63s - loss: -2.1080e+00 - val_loss: -2.1159e+00
Epoch 119/200
 - 63s - loss: -2.1079e+00 - val_loss: -2.1154e+00
Epoch 120/200
 - 62s - loss: -2.1073e+00 - val_loss: -2.1142e+00
Epoch 121/200
 - 63s - loss: -2.1069e+00 - val_loss: -2.1142e+00
2019-12-25 05:27:39,222 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep2/ae_model_epoch_120.pickle
Epoch 122/200
 - 63s - loss: -2.1077e+00 - val_loss: -2.1150e+00
Epoch 123/200
 - 62s - loss: -2.1074e+00 - val_loss: -2.1123e+00
Epoch 124/200
 - 62s - loss: -2.1073e+00 - val_loss: -2.1149e+00
Epoch 125/200
 - 63s - loss: -2.1075e+00 - val_loss: -2.1154e+00
Epoch 126/200
 - 63s - loss: -2.1071e+00 - val_loss: -2.1156e+00
Epoch 127/200
 - 63s - loss: -2.1077e+00 - val_loss: -2.1155e+00
Epoch 128/200
 - 63s - loss: -2.1075e+00 - val_loss: -2.1154e+00
Epoch 129/200
 - 63s - loss: -2.1073e+00 - val_loss: -2.1157e+00
Epoch 130/200
 - 63s - loss: -2.1077e+00 - val_loss: -2.1158e+00
Epoch 131/200
 - 63s - loss: -2.1076e+00 - val_loss: -2.1152e+00
Epoch 132/200
 - 63s - loss: -2.1076e+00 - val_loss: -2.1154e+00
Epoch 133/200
 - 63s - loss: -2.1075e+00 - val_loss: -2.1152e+00
Epoch 134/200
 - 63s - loss: -2.1080e+00 - val_loss: -2.1147e+00
Epoch 135/200
 - 63s - loss: -2.1071e+00 - val_loss: -2.1151e+00
Epoch 136/200
 - 63s - loss: -2.1081e+00 - val_loss: -2.1151e+00
Epoch 137/200
 - 63s - loss: -2.1075e+00 - val_loss: -2.1157e+00
Epoch 138/200
 - 63s - loss: -2.1081e+00 - val_loss: -2.1154e+00
Epoch 139/200
 - 63s - loss: -2.1079e+00 - val_loss: -2.1158e+00
Epoch 140/200
 - 63s - loss: -2.1078e+00 - val_loss: -2.1157e+00
Epoch 141/200
 - 63s - loss: -2.1084e+00 - val_loss: -2.1156e+00
2019-12-25 05:48:30,774 [INFO] epoch = 140. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep2/ae_model_epoch_140.pickle
Epoch 142/200
 - 62s - loss: -2.1079e+00 - val_loss: -2.1152e+00
Epoch 143/200
 - 62s - loss: -2.1076e+00 - val_loss: -2.1156e+00
Epoch 144/200
 - 63s - loss: -2.1081e+00 - val_loss: -2.1153e+00
Epoch 145/200
 - 62s - loss: -2.1076e+00 - val_loss: -2.1155e+00
Epoch 146/200
 - 63s - loss: -2.1080e+00 - val_loss: -2.1154e+00
Epoch 147/200
 - 63s - loss: -2.1078e+00 - val_loss: -2.1159e+00
Epoch 148/200
 - 63s - loss: -2.1079e+00 - val_loss: -2.1154e+00
Epoch 149/200
 - 63s - loss: -2.1081e+00 - val_loss: -2.1154e+00
Epoch 150/200
 - 63s - loss: -2.1083e+00 - val_loss: -2.1157e+00
Epoch 151/200
 - 63s - loss: -2.1081e+00 - val_loss: -2.1148e+00
Epoch 152/200
 - 62s - loss: -2.1084e+00 - val_loss: -2.1155e+00
Epoch 153/200
 - 63s - loss: -2.1076e+00 - val_loss: -2.1159e+00
Epoch 154/200
 - 63s - loss: -2.1081e+00 - val_loss: -2.1152e+00
Epoch 155/200
 - 63s - loss: -2.1079e+00 - val_loss: -2.1157e+00
Epoch 156/200
 - 62s - loss: -2.1085e+00 - val_loss: -2.1154e+00
Epoch 157/200
 - 63s - loss: -2.1083e+00 - val_loss: -2.1161e+00
Epoch 158/200
 - 63s - loss: -2.1077e+00 - val_loss: -2.1149e+00
Epoch 159/200
 - 63s - loss: -2.1082e+00 - val_loss: -2.1154e+00
Epoch 160/200
 - 63s - loss: -2.1077e+00 - val_loss: -2.1158e+00
Epoch 161/200
 - 63s - loss: -2.1075e+00 - val_loss: -2.1158e+00
2019-12-25 06:09:22,222 [INFO] epoch = 160. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep2/ae_model_epoch_160.pickle
Epoch 162/200
 - 63s - loss: -2.1077e+00 - val_loss: -2.1155e+00
Epoch 163/200
 - 63s - loss: -2.1081e+00 - val_loss: -2.1156e+00
Epoch 164/200
 - 63s - loss: -2.1083e+00 - val_loss: -2.1158e+00
Epoch 165/200
 - 63s - loss: -2.1080e+00 - val_loss: -2.1160e+00
Epoch 166/200
 - 63s - loss: -2.1085e+00 - val_loss: -2.1159e+00
Epoch 167/200
 - 63s - loss: -2.1079e+00 - val_loss: -2.1155e+00
Epoch 168/200
 - 62s - loss: -2.1082e+00 - val_loss: -2.1152e+00
Epoch 169/200
 - 63s - loss: -2.1083e+00 - val_loss: -2.1154e+00
Epoch 170/200
 - 63s - loss: -2.1082e+00 - val_loss: -2.1155e+00
Epoch 171/200
 - 63s - loss: -2.1085e+00 - val_loss: -2.1157e+00
Epoch 172/200
 - 63s - loss: -2.1078e+00 - val_loss: -2.1156e+00
Epoch 173/200
 - 63s - loss: -2.1081e+00 - val_loss: -2.1155e+00
Epoch 174/200
 - 63s - loss: -2.1082e+00 - val_loss: -2.1163e+00
Epoch 175/200
 - 63s - loss: -2.1081e+00 - val_loss: -2.1161e+00
Epoch 176/200
 - 63s - loss: -2.1084e+00 - val_loss: -2.1159e+00
Epoch 177/200
 - 63s - loss: -2.1088e+00 - val_loss: -2.1160e+00
Epoch 178/200
 - 63s - loss: -2.1087e+00 - val_loss: -2.1159e+00
Epoch 179/200
 - 63s - loss: -2.1081e+00 - val_loss: -2.1161e+00
Epoch 180/200
 - 63s - loss: -2.1081e+00 - val_loss: -2.1159e+00
Epoch 181/200
 - 63s - loss: -2.1086e+00 - val_loss: -2.1159e+00
2019-12-25 06:30:13,480 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep2/ae_model_epoch_180.pickle
Epoch 182/200
 - 63s - loss: -2.1083e+00 - val_loss: -2.1162e+00
Epoch 183/200
 - 63s - loss: -2.1086e+00 - val_loss: -2.1158e+00
Epoch 184/200
 - 62s - loss: -2.1082e+00 - val_loss: -2.1160e+00
Epoch 185/200
 - 63s - loss: -2.1085e+00 - val_loss: -2.1147e+00
Epoch 186/200
 - 63s - loss: -2.1073e+00 - val_loss: -2.1167e+00
Epoch 187/200
 - 63s - loss: -2.1083e+00 - val_loss: -2.1162e+00
Epoch 188/200
 - 63s - loss: -2.1085e+00 - val_loss: -2.1162e+00
Epoch 189/200
 - 62s - loss: -2.1080e+00 - val_loss: -2.1162e+00
Epoch 190/200
 - 63s - loss: -2.1083e+00 - val_loss: -2.1162e+00
Epoch 191/200
 - 63s - loss: -2.1082e+00 - val_loss: -2.1163e+00
Epoch 192/200
 - 62s - loss: -2.1079e+00 - val_loss: -2.1158e+00
Epoch 193/200
 - 63s - loss: -2.1080e+00 - val_loss: -2.1160e+00
Epoch 194/200
 - 63s - loss: -2.1084e+00 - val_loss: -2.1160e+00
Epoch 195/200
 - 63s - loss: -2.1087e+00 - val_loss: -2.1156e+00
Epoch 196/200
 - 63s - loss: -2.1085e+00 - val_loss: -2.1162e+00
Epoch 197/200
 - 63s - loss: -2.1083e+00 - val_loss: -2.1158e+00
Epoch 198/200
 - 63s - loss: -2.1085e+00 - val_loss: -2.1162e+00
Epoch 199/200
 - 63s - loss: -2.1086e+00 - val_loss: -2.1164e+00
Epoch 200/200
 - 63s - loss: -2.1087e+00 - val_loss: -2.1163e+00
2019-12-25 06:50:02,172 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-25 06:52:35,282 [INFO] Last epoch loss evaluation: train_loss = -2.119195, val_loss = -2.116705
2019-12-25 06:52:35,282 [INFO] Training autoencoder complete
2019-12-25 06:52:35,282 [INFO] Encoding data for supervised training
2019-12-25 06:55:45,689 [INFO] Encoding complete
2019-12-25 06:55:45,689 [INFO] Training neural network layers (after autoencoder)
Train on 2939058 samples, validate on 979687 samples
Epoch 1/200
 - 54s - loss: 0.0065 - val_loss: 0.0022
 - val_f1: 0.9988
Epoch 2/200
 - 52s - loss: 0.0022 - val_loss: 0.0016
 - val_f1: 0.9990
Epoch 3/200
 - 52s - loss: 0.0018 - val_loss: 0.0022
 - val_f1: 0.9989
Epoch 4/200
 - 52s - loss: 0.0016 - val_loss: 0.0014
 - val_f1: 0.9989
Epoch 5/200
 - 52s - loss: 0.0015 - val_loss: 0.0013
 - val_f1: 0.9989
Epoch 6/200
 - 52s - loss: 0.0014 - val_loss: 0.0011
 - val_f1: 0.9992
Epoch 7/200
 - 52s - loss: 0.0015 - val_loss: 0.0010
 - val_f1: 0.9991
Epoch 8/200
 - 52s - loss: 0.0014 - val_loss: 0.0013
 - val_f1: 0.9991
Epoch 9/200
 - 52s - loss: 0.0015 - val_loss: 0.0011
 - val_f1: 0.9991
Epoch 10/200
 - 52s - loss: 0.0013 - val_loss: 0.0012
 - val_f1: 0.9994
Epoch 11/200
 - 52s - loss: 0.0013 - val_loss: 0.0010
 - val_f1: 0.9992
Epoch 12/200
 - 52s - loss: 0.0013 - val_loss: 8.7508e-04
 - val_f1: 0.9994
Epoch 13/200
 - 52s - loss: 0.0013 - val_loss: 0.0010
 - val_f1: 0.9993
Epoch 14/200
 - 52s - loss: 0.0012 - val_loss: 7.9030e-04
 - val_f1: 0.9995
Epoch 15/200
 - 52s - loss: 0.0011 - val_loss: 0.0011
 - val_f1: 0.9991
Epoch 16/200
 - 52s - loss: 0.0011 - val_loss: 8.4198e-04
 - val_f1: 0.9994
Epoch 17/200
 - 52s - loss: 0.0011 - val_loss: 0.0014
 - val_f1: 0.9994
Epoch 18/200
 - 52s - loss: 0.0011 - val_loss: 7.9100e-04
 - val_f1: 0.9995
Epoch 19/200
 - 52s - loss: 0.0010 - val_loss: 9.6663e-04
 - val_f1: 0.9994
Epoch 20/200
 - 52s - loss: 0.0010 - val_loss: 7.5759e-04
 - val_f1: 0.9995
Epoch 21/200
 - 52s - loss: 0.0010 - val_loss: 8.0224e-04
2019-12-25 07:27:34,759 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep2/ann_model_epoch_20.pickle
 - val_f1: 0.9993
Epoch 22/200
 - 52s - loss: 0.0010 - val_loss: 6.7680e-04
 - val_f1: 0.9996
Epoch 23/200
 - 52s - loss: 9.5617e-04 - val_loss: 7.2056e-04
 - val_f1: 0.9995
Epoch 24/200
 - 52s - loss: 9.5251e-04 - val_loss: 7.5950e-04
 - val_f1: 0.9995
Epoch 25/200
 - 52s - loss: 9.2644e-04 - val_loss: 6.8282e-04
 - val_f1: 0.9996
Epoch 26/200
 - 52s - loss: 9.2175e-04 - val_loss: 7.0941e-04
 - val_f1: 0.9995
Epoch 27/200
 - 52s - loss: 9.3926e-04 - val_loss: 6.9765e-04
 - val_f1: 0.9995
Epoch 28/200
 - 52s - loss: 9.4250e-04 - val_loss: 7.0612e-04
 - val_f1: 0.9995
Epoch 29/200
 - 52s - loss: 9.3253e-04 - val_loss: 7.0528e-04
 - val_f1: 0.9995
Epoch 30/200
 - 52s - loss: 9.1791e-04 - val_loss: 6.3404e-04
 - val_f1: 0.9996
Epoch 31/200
 - 52s - loss: 9.0643e-04 - val_loss: 6.0774e-04
 - val_f1: 0.9996
Epoch 32/200
 - 52s - loss: 8.9599e-04 - val_loss: 7.3167e-04
 - val_f1: 0.9995
Epoch 33/200
 - 52s - loss: 8.8439e-04 - val_loss: 6.5887e-04
 - val_f1: 0.9996
Epoch 34/200
 - 52s - loss: 8.7752e-04 - val_loss: 7.1780e-04
 - val_f1: 0.9995
Epoch 35/200
 - 52s - loss: 8.4856e-04 - val_loss: 5.7991e-04
 - val_f1: 0.9996
Epoch 36/200
 - 52s - loss: 8.5280e-04 - val_loss: 7.4401e-04
 - val_f1: 0.9995
Epoch 37/200
 - 52s - loss: 8.4214e-04 - val_loss: 9.4362e-04
 - val_f1: 0.9994
Epoch 38/200
 - 52s - loss: 8.3037e-04 - val_loss: 6.1088e-04
 - val_f1: 0.9996
Epoch 39/200
 - 52s - loss: 8.0727e-04 - val_loss: 5.9969e-04
 - val_f1: 0.9996
Epoch 40/200
 - 52s - loss: 8.3770e-04 - val_loss: 5.4740e-04
 - val_f1: 0.9997
Epoch 41/200
 - 52s - loss: 8.1703e-04 - val_loss: 5.9498e-04
2019-12-25 07:58:14,582 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep2/ann_model_epoch_40.pickle
 - val_f1: 0.9996
Epoch 42/200
 - 52s - loss: 8.0439e-04 - val_loss: 6.1684e-04
 - val_f1: 0.9996
Epoch 43/200
 - 52s - loss: 8.0698e-04 - val_loss: 6.3823e-04
 - val_f1: 0.9996
Epoch 44/200
 - 52s - loss: 8.2355e-04 - val_loss: 6.2268e-04
 - val_f1: 0.9995
Epoch 45/200
 - 52s - loss: 8.2094e-04 - val_loss: 5.7370e-04
 - val_f1: 0.9996
Epoch 46/200
 - 52s - loss: 8.1564e-04 - val_loss: 5.6337e-04
 - val_f1: 0.9996
Epoch 47/200
 - 52s - loss: 8.1388e-04 - val_loss: 8.1126e-04
 - val_f1: 0.9993
Epoch 48/200
 - 52s - loss: 8.2218e-04 - val_loss: 5.5558e-04
 - val_f1: 0.9996
Epoch 49/200
 - 52s - loss: 7.8349e-04 - val_loss: 4.9635e-04
 - val_f1: 0.9997
Epoch 50/200
 - 52s - loss: 7.8848e-04 - val_loss: 5.4413e-04
 - val_f1: 0.9997
Epoch 51/200
 - 52s - loss: 7.9313e-04 - val_loss: 6.9001e-04
 - val_f1: 0.9995
Epoch 52/200
 - 52s - loss: 7.9636e-04 - val_loss: 7.7330e-04
 - val_f1: 0.9995
Epoch 53/200
 - 52s - loss: 8.0455e-04 - val_loss: 5.6125e-04
 - val_f1: 0.9996
Epoch 54/200
 - 52s - loss: 8.1101e-04 - val_loss: 8.8250e-04
 - val_f1: 0.9996
Epoch 55/200
 - 52s - loss: 8.5898e-04 - val_loss: 5.7844e-04
 - val_f1: 0.9996
Epoch 56/200
 - 52s - loss: 8.7171e-04 - val_loss: 7.7907e-04
 - val_f1: 0.9993
Epoch 57/200
 - 52s - loss: 8.3372e-04 - val_loss: 5.6758e-04
 - val_f1: 0.9996
Epoch 58/200
 - 52s - loss: 8.3097e-04 - val_loss: 5.8374e-04
 - val_f1: 0.9996
Epoch 59/200
 - 52s - loss: 8.2704e-04 - val_loss: 6.0930e-04
 - val_f1: 0.9996
Epoch 60/200
 - 52s - loss: 7.7725e-04 - val_loss: 6.2406e-04
 - val_f1: 0.9996
Epoch 61/200
 - 52s - loss: 7.9940e-04 - val_loss: 6.0943e-04
2019-12-25 08:28:54,399 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep2/ann_model_epoch_60.pickle
 - val_f1: 0.9996
Epoch 62/200
 - 52s - loss: 7.8830e-04 - val_loss: 9.0479e-04
 - val_f1: 0.9996
Epoch 63/200
 - 52s - loss: 8.0747e-04 - val_loss: 8.0936e-04
 - val_f1: 0.9996
Epoch 64/200
 - 52s - loss: 8.5124e-04 - val_loss: 5.8149e-04
 - val_f1: 0.9997
Epoch 65/200
 - 52s - loss: 8.1191e-04 - val_loss: 7.9201e-04
 - val_f1: 0.9996
Epoch 66/200
 - 52s - loss: 8.4565e-04 - val_loss: 5.6714e-04
 - val_f1: 0.9996
Epoch 67/200
 - 52s - loss: 8.2929e-04 - val_loss: 6.1369e-04
 - val_f1: 0.9996
Epoch 68/200
 - 52s - loss: 8.0897e-04 - val_loss: 5.8677e-04
 - val_f1: 0.9997
Epoch 69/200
 - 52s - loss: 8.0228e-04 - val_loss: 5.6430e-04
 - val_f1: 0.9997
Epoch 70/200
 - 52s - loss: 8.0845e-04 - val_loss: 5.6172e-04
 - val_f1: 0.9997
Epoch 71/200
 - 52s - loss: 8.1249e-04 - val_loss: 5.3853e-04
 - val_f1: 0.9997
Epoch 72/200
 - 52s - loss: 7.9904e-04 - val_loss: 5.8370e-04
 - val_f1: 0.9996
Epoch 73/200
 - 52s - loss: 8.0153e-04 - val_loss: 6.5705e-04
 - val_f1: 0.9995
Epoch 74/200
 - 52s - loss: 7.8886e-04 - val_loss: 7.0827e-04
 - val_f1: 0.9995
Epoch 75/200
 - 52s - loss: 7.9625e-04 - val_loss: 6.3041e-04
 - val_f1: 0.9996
Epoch 76/200
 - 52s - loss: 7.9389e-04 - val_loss: 6.1387e-04
 - val_f1: 0.9996
Epoch 77/200
 - 52s - loss: 7.6627e-04 - val_loss: 7.2503e-04
 - val_f1: 0.9996
Epoch 78/200
 - 52s - loss: 7.6685e-04 - val_loss: 5.7618e-04
 - val_f1: 0.9996
Epoch 79/200
 - 52s - loss: 7.7199e-04 - val_loss: 5.9566e-04
 - val_f1: 0.9997
Epoch 80/200
 - 52s - loss: 7.8220e-04 - val_loss: 7.5605e-04
 - val_f1: 0.9996
Epoch 81/200
 - 52s - loss: 7.6312e-04 - val_loss: 5.5415e-04
2019-12-25 08:59:34,503 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep2/ann_model_epoch_80.pickle
 - val_f1: 0.9997
Epoch 82/200
 - 52s - loss: 7.8527e-04 - val_loss: 5.8136e-04
 - val_f1: 0.9996
Epoch 83/200
 - 52s - loss: 7.6542e-04 - val_loss: 5.5161e-04
 - val_f1: 0.9996
Epoch 84/200
 - 52s - loss: 7.5314e-04 - val_loss: 5.7335e-04
 - val_f1: 0.9996
Epoch 85/200
 - 52s - loss: 8.1065e-04 - val_loss: 7.9000e-04
 - val_f1: 0.9993
Epoch 86/200
 - 52s - loss: 7.7907e-04 - val_loss: 7.4453e-04
 - val_f1: 0.9994
Epoch 87/200
 - 52s - loss: 7.7099e-04 - val_loss: 5.8679e-04
 - val_f1: 0.9996
Epoch 88/200
 - 52s - loss: 7.6195e-04 - val_loss: 5.6366e-04
 - val_f1: 0.9997
Epoch 89/200
 - 52s - loss: 7.4778e-04 - val_loss: 5.8015e-04
 - val_f1: 0.9996
Epoch 90/200
 - 52s - loss: 7.6410e-04 - val_loss: 7.4231e-04
 - val_f1: 0.9996
Epoch 91/200
 - 52s - loss: 7.5714e-04 - val_loss: 5.1210e-04
 - val_f1: 0.9997
Epoch 92/200
 - 52s - loss: 7.5805e-04 - val_loss: 5.4849e-04
 - val_f1: 0.9997
Epoch 93/200
 - 52s - loss: 7.3756e-04 - val_loss: 6.0338e-04
 - val_f1: 0.9996
Epoch 94/200
 - 52s - loss: 7.5802e-04 - val_loss: 5.5903e-04
 - val_f1: 0.9997
Epoch 95/200
 - 52s - loss: 7.3823e-04 - val_loss: 5.5973e-04
 - val_f1: 0.9997
Epoch 96/200
 - 52s - loss: 7.3780e-04 - val_loss: 5.2135e-04
 - val_f1: 0.9997
Epoch 97/200
 - 52s - loss: 7.2622e-04 - val_loss: 5.3435e-04
 - val_f1: 0.9997
Epoch 98/200
 - 52s - loss: 7.1402e-04 - val_loss: 5.7352e-04
 - val_f1: 0.9996
Epoch 99/200
 - 52s - loss: 7.2267e-04 - val_loss: 5.9424e-04
2019-12-25 09:27:52,699 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-25 09:30:38,409 [INFO] Last epoch loss evaluation: train_loss = 0.000486, val_loss = 0.000496
2019-12-25 09:30:38,432 [INFO] Training complete. time_to_train = 22213.75 sec, 370.23 min
2019-12-25 09:30:38,462 [INFO] Model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep2/best_model.pickle
/home/sunanda/research/ids_experiments/utility.py:268: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
2019-12-25 09:30:38,639 [INFO] Plot saved to: results_selected_models/selected_kdd99_ae_ann_deep_rep2/training_error_history.png
/home/sunanda/research/ids_experiments/utility.py:285: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
2019-12-25 09:30:38,801 [INFO] Plot saved to: results_selected_models/selected_kdd99_ae_ann_deep_rep2/training_f1_history.png
2019-12-25 09:30:38,802 [INFO] Making predictions on training, validation, testing data
2019-12-25 09:38:18,701 [INFO] Evaluating predictions (results)
2019-12-25 09:38:27,372 [INFO] Dataset: Testing. Classification report below
2019-12-25 09:38:27,373 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      0.97      0.98    229853
     normal.       0.73      1.00      0.84     60593
       probe       0.86      0.75      0.80      4166
         r2l       0.90      0.03      0.06     13781
         u2r       0.16      0.01      0.01      2636

    accuracy                           0.92    311029
   macro avg       0.73      0.55      0.54    311029
weighted avg       0.93      0.92      0.90    311029

2019-12-25 09:38:27,373 [INFO] Overall accuracy (micro avg): 0.9229878885891669
2019-12-25 09:38:36,682 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9230         0.9230                       0.9230                0.0193                   0.0770  0.9230
1     Macro avg        0.9692         0.7309                       0.5495                0.0199                   0.4505  0.5381
2  Weighted avg        0.9665         0.9316                       0.9230                0.0226                   0.0770  0.9042
2019-12-25 09:39:06,929 [INFO] Dataset: Validation. Classification report below
2019-12-25 09:39:06,930 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00    776675
     normal.       1.00      1.00      1.00    194556
       probe       0.99      0.98      0.99      8221
         r2l       0.90      0.66      0.76       225
         u2r       0.43      0.30      0.35        10

    accuracy                           1.00    979687
   macro avg       0.86      0.79      0.82    979687
weighted avg       1.00      1.00      1.00    979687

2019-12-25 09:39:06,930 [INFO] Overall accuracy (micro avg): 0.9996692821278633
2019-12-25 09:39:39,582 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9997         0.9997                       0.9997                0.0001                   0.0003  0.9997
1     Macro avg        0.9999         0.8637                       0.7884                0.0001                   0.2116  0.8201
2  Weighted avg        0.9999         0.9997                       0.9997                0.0002                   0.0003  0.9997
2019-12-25 09:41:53,457 [INFO] Dataset: Training. Classification report below
2019-12-25 09:41:53,458 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00   3106695
     normal.       1.00      1.00      1.00    778225
       probe       0.99      0.98      0.99     32881
         r2l       0.91      0.66      0.77       901
         u2r       0.53      0.50      0.51        42

    accuracy                           1.00   3918744
   macro avg       0.89      0.83      0.85   3918744
weighted avg       1.00      1.00      1.00   3918744

2019-12-25 09:41:53,458 [INFO] Overall accuracy (micro avg): 0.9996718336283258
2019-12-25 09:44:17,945 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9997         0.9997                       0.9997                0.0001                   0.0003  0.9997
1     Macro avg        0.9999         0.8866                       0.8291                0.0001                   0.1709  0.8537
2  Weighted avg        0.9999         0.9997                       0.9997                0.0002                   0.0003  0.9997
2019-12-25 09:44:17,992 [INFO] Results saved to: results_selected_models/selected_kdd99_ae_ann_deep_rep2/selected_kdd99_ae_ann_deep_rep2_results.xlsx
2019-12-25 09:44:17,999 [INFO] ================= Finished running experiment no. 2 ================= 

2019-12-25 09:44:18,022 [INFO] Created directory: results_selected_models/selected_kdd99_ae_ann_deep_rep3
2019-12-25 09:44:18,023 [INFO] Initialized logging. log_filename = results_selected_models/selected_kdd99_ae_ann_deep_rep3/run_log.log
2019-12-25 09:44:18,023 [INFO] ================= Running experiment no. 3  ================= 

2019-12-25 09:44:18,023 [INFO] Experiment parameters given below
2019-12-25 09:44:18,023 [INFO] 
{'experiment_num': 3, 'results_dir': 'results_selected_models/selected_kdd99_ae_ann_deep_rep3', 'model_type': 'classifier', 'model': 'ae_ann', 'normal_label': 'normal', 'scaling_type': 'NA', 'unsupervised_ratio': 0.25, 'ae_encoder_units': [128, 64, 32], 'ae_encoder_activations': ['relu', 'relu', 'relu'], 'ae_encoder_dropout_rates': [0.2, 0.2, 0.2], 'ae_encoder_l1_param': -1, 'ae_decoder_units': [64, 128], 'ae_decoder_activations': ['relu', 'relu'], 'ae_decoder_dropout_rates': [0.2, 0.2], 'output_activation': 'sigmoid', 'loss_function': 'binary_crossentropy', 'ae_epochs': 200, 'ann_layer_units': [64], 'ann_layer_activations': ['relu'], 'ann_layer_dropout_rates': [0.2], 'ann_epochs': 200, 'early_stop_patience': 50, 'batch_size': 256, 'dataset_dir': '../Datasets/full_datasets/kdd99_five_classes', 'description': 'selected_kdd99_ae_ann_deep_rep3'}
2019-12-25 09:44:18,023 [INFO] Created tensorboard log directory: results_selected_models/selected_kdd99_ae_ann_deep_rep3/tf_logs_run_2019_12_25-09_44_18
2019-12-25 09:44:18,023 [INFO] Loading datsets from: ../Datasets/full_datasets/kdd99_five_classes
2019-12-25 09:44:18,023 [INFO] Reading X, y files
2019-12-25 09:44:18,023 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/X_train.h5
2019-12-25 09:44:24,692 [INFO] Reading complete. time_to_read=6.67 seconds
2019-12-25 09:44:24,693 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/X_val.h5
2019-12-25 09:44:26,365 [INFO] Reading complete. time_to_read=1.67 seconds
2019-12-25 09:44:26,365 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/X_test.h5
2019-12-25 09:44:26,839 [INFO] Reading complete. time_to_read=0.47 seconds
2019-12-25 09:44:26,839 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/y_train.h5
2019-12-25 09:44:27,041 [INFO] Reading complete. time_to_read=0.20 seconds
2019-12-25 09:44:27,042 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/y_val.h5
2019-12-25 09:44:27,096 [INFO] Reading complete. time_to_read=0.05 seconds
2019-12-25 09:44:27,097 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/y_test.h5
2019-12-25 09:44:27,117 [INFO] Reading complete. time_to_read=0.02 seconds
2019-12-25 09:44:34,213 [INFO] Initializing model
2019-12-25 09:44:34,828 [INFO] _________________________________________________________________
2019-12-25 09:44:34,828 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-25 09:44:34,828 [INFO] =================================================================
2019-12-25 09:44:34,829 [INFO] dense_117 (Dense)            (None, 128)               15872     
2019-12-25 09:44:34,829 [INFO] _________________________________________________________________
2019-12-25 09:44:34,829 [INFO] batch_normalization_83 (Batc (None, 128)               512       
2019-12-25 09:44:34,829 [INFO] _________________________________________________________________
2019-12-25 09:44:34,829 [INFO] dropout_83 (Dropout)         (None, 128)               0         
2019-12-25 09:44:34,829 [INFO] _________________________________________________________________
2019-12-25 09:44:34,829 [INFO] dense_118 (Dense)            (None, 64)                8256      
2019-12-25 09:44:34,829 [INFO] _________________________________________________________________
2019-12-25 09:44:34,829 [INFO] batch_normalization_84 (Batc (None, 64)                256       
2019-12-25 09:44:34,829 [INFO] _________________________________________________________________
2019-12-25 09:44:34,829 [INFO] dropout_84 (Dropout)         (None, 64)                0         
2019-12-25 09:44:34,829 [INFO] _________________________________________________________________
2019-12-25 09:44:34,830 [INFO] dense_119 (Dense)            (None, 32)                2080      
2019-12-25 09:44:34,830 [INFO] _________________________________________________________________
2019-12-25 09:44:34,830 [INFO] batch_normalization_85 (Batc (None, 32)                128       
2019-12-25 09:44:34,830 [INFO] _________________________________________________________________
2019-12-25 09:44:34,830 [INFO] dropout_85 (Dropout)         (None, 32)                0         
2019-12-25 09:44:34,830 [INFO] _________________________________________________________________
2019-12-25 09:44:34,830 [INFO] dense_120 (Dense)            (None, 64)                2112      
2019-12-25 09:44:34,830 [INFO] _________________________________________________________________
2019-12-25 09:44:34,830 [INFO] batch_normalization_86 (Batc (None, 64)                256       
2019-12-25 09:44:34,830 [INFO] _________________________________________________________________
2019-12-25 09:44:34,830 [INFO] dropout_86 (Dropout)         (None, 64)                0         
2019-12-25 09:44:34,830 [INFO] _________________________________________________________________
2019-12-25 09:44:34,831 [INFO] dense_121 (Dense)            (None, 128)               8320      
2019-12-25 09:44:34,831 [INFO] _________________________________________________________________
2019-12-25 09:44:34,831 [INFO] batch_normalization_87 (Batc (None, 128)               512       
2019-12-25 09:44:34,831 [INFO] _________________________________________________________________
2019-12-25 09:44:34,831 [INFO] dropout_87 (Dropout)         (None, 128)               0         
2019-12-25 09:44:34,831 [INFO] _________________________________________________________________
2019-12-25 09:44:34,831 [INFO] dense_122 (Dense)            (None, 123)               15867     
2019-12-25 09:44:34,831 [INFO] =================================================================
2019-12-25 09:44:34,832 [INFO] Total params: 54,171
2019-12-25 09:44:34,832 [INFO] Trainable params: 53,339
2019-12-25 09:44:34,832 [INFO] Non-trainable params: 832
2019-12-25 09:44:34,832 [INFO] _________________________________________________________________
2019-12-25 09:44:34,984 [INFO] _________________________________________________________________
2019-12-25 09:44:34,984 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-25 09:44:34,984 [INFO] =================================================================
2019-12-25 09:44:34,984 [INFO] dense_123 (Dense)            (None, 64)                2112      
2019-12-25 09:44:34,984 [INFO] _________________________________________________________________
2019-12-25 09:44:34,984 [INFO] batch_normalization_88 (Batc (None, 64)                256       
2019-12-25 09:44:34,984 [INFO] _________________________________________________________________
2019-12-25 09:44:34,984 [INFO] dropout_88 (Dropout)         (None, 64)                0         
2019-12-25 09:44:34,984 [INFO] _________________________________________________________________
2019-12-25 09:44:34,985 [INFO] dense_124 (Dense)            (None, 5)                 325       
2019-12-25 09:44:34,985 [INFO] =================================================================
2019-12-25 09:44:34,985 [INFO] Total params: 2,693
2019-12-25 09:44:34,985 [INFO] Trainable params: 2,565
2019-12-25 09:44:34,985 [INFO] Non-trainable params: 128
2019-12-25 09:44:34,985 [INFO] _________________________________________________________________
2019-12-25 09:44:34,985 [INFO] Training model
2019-12-25 09:44:34,985 [INFO] Splitting train set into 2 sets (unsupervised, supervised), random_seed = None
2019-12-25 09:45:16,941 [INFO] Split sizes (instances). total = 3918744, unsupervised = 979686, supervised = 2939058, unsupervised dataset hash = 4ebf23f6b1b47cbe9d429befec4fb052eadbcd8f
2019-12-25 09:45:16,941 [INFO] Training autoencoder
 - val_f1: 0.9994
Epoch 00099: early stopping
Train on 979686 samples, validate on 979687 samples
Epoch 1/200
 - 67s - loss: -1.8408e+00 - val_loss: -2.0692e+00
Epoch 2/200
 - 64s - loss: -2.0522e+00 - val_loss: -2.0899e+00
Epoch 3/200
 - 64s - loss: -2.0702e+00 - val_loss: -2.0967e+00
Epoch 4/200
 - 64s - loss: -2.0793e+00 - val_loss: -2.1015e+00
Epoch 5/200
 - 64s - loss: -2.0844e+00 - val_loss: -2.1041e+00
Epoch 6/200
 - 64s - loss: -2.0869e+00 - val_loss: -2.1051e+00
Epoch 7/200
 - 64s - loss: -2.0894e+00 - val_loss: -2.1071e+00
Epoch 8/200
 - 64s - loss: -2.0920e+00 - val_loss: -2.1086e+00
Epoch 9/200
 - 64s - loss: -2.0934e+00 - val_loss: -2.1096e+00
Epoch 10/200
 - 64s - loss: -2.0941e+00 - val_loss: -2.1091e+00
Epoch 11/200
 - 64s - loss: -2.0954e+00 - val_loss: -2.1103e+00
Epoch 12/200
 - 64s - loss: -2.0959e+00 - val_loss: -2.1105e+00
Epoch 13/200
 - 64s - loss: -2.0972e+00 - val_loss: -2.1113e+00
Epoch 14/200
 - 64s - loss: -2.0972e+00 - val_loss: -2.1101e+00
Epoch 15/200
 - 64s - loss: -2.0975e+00 - val_loss: -2.1114e+00
Epoch 16/200
 - 64s - loss: -2.0973e+00 - val_loss: -2.1106e+00
Epoch 17/200
 - 64s - loss: -2.0986e+00 - val_loss: -2.1118e+00
Epoch 18/200
 - 64s - loss: -2.0990e+00 - val_loss: -2.1117e+00
Epoch 19/200
 - 64s - loss: -2.0986e+00 - val_loss: -2.1127e+00
Epoch 20/200
 - 64s - loss: -2.1000e+00 - val_loss: -2.1131e+00
Epoch 21/200
 - 64s - loss: -2.0997e+00 - val_loss: -2.1105e+00
2019-12-25 10:08:00,272 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep3/ae_model_epoch_20.pickle
Epoch 22/200
 - 64s - loss: -2.0999e+00 - val_loss: -2.1111e+00
Epoch 23/200
 - 64s - loss: -2.0995e+00 - val_loss: -2.1113e+00
Epoch 24/200
 - 64s - loss: -2.1001e+00 - val_loss: -2.1103e+00
Epoch 25/200
 - 64s - loss: -2.1001e+00 - val_loss: -2.1109e+00
Epoch 26/200
 - 64s - loss: -2.1000e+00 - val_loss: -2.1103e+00
Epoch 27/200
 - 64s - loss: -2.1000e+00 - val_loss: -2.1118e+00
Epoch 28/200
 - 64s - loss: -2.1008e+00 - val_loss: -2.1112e+00
Epoch 29/200
 - 64s - loss: -2.1010e+00 - val_loss: -2.1119e+00
Epoch 30/200
 - 64s - loss: -2.1006e+00 - val_loss: -2.1120e+00
Epoch 31/200
 - 64s - loss: -2.1009e+00 - val_loss: -2.1116e+00
Epoch 32/200
 - 64s - loss: -2.1008e+00 - val_loss: -2.1120e+00
Epoch 33/200
 - 64s - loss: -2.1013e+00 - val_loss: -2.1114e+00
Epoch 34/200
 - 64s - loss: -2.1010e+00 - val_loss: -2.1108e+00
Epoch 35/200
 - 64s - loss: -2.1017e+00 - val_loss: -2.1091e+00
Epoch 36/200
 - 64s - loss: -2.1006e+00 - val_loss: -2.1104e+00
Epoch 37/200
 - 64s - loss: -2.1012e+00 - val_loss: -2.1120e+00
Epoch 38/200
 - 64s - loss: -2.1005e+00 - val_loss: -2.1112e+00
Epoch 39/200
 - 64s - loss: -2.1012e+00 - val_loss: -2.1117e+00
Epoch 40/200
 - 64s - loss: -2.1014e+00 - val_loss: -2.1083e+00
Epoch 41/200
 - 64s - loss: -2.1007e+00 - val_loss: -2.1117e+00
2019-12-25 10:29:19,410 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep3/ae_model_epoch_40.pickle
Epoch 42/200
 - 64s - loss: -2.1007e+00 - val_loss: -2.1119e+00
Epoch 43/200
 - 64s - loss: -2.1015e+00 - val_loss: -2.1113e+00
Epoch 44/200
 - 64s - loss: -2.1010e+00 - val_loss: -2.1115e+00
Epoch 45/200
 - 64s - loss: -2.1016e+00 - val_loss: -2.1122e+00
Epoch 46/200
 - 64s - loss: -2.1015e+00 - val_loss: -2.1117e+00
Epoch 47/200
 - 64s - loss: -2.1017e+00 - val_loss: -2.1124e+00
Epoch 48/200
 - 64s - loss: -2.1012e+00 - val_loss: -2.1127e+00
Epoch 49/200
 - 64s - loss: -2.1021e+00 - val_loss: -2.1116e+00
Epoch 50/200
 - 64s - loss: -2.1019e+00 - val_loss: -2.1113e+00
Epoch 51/200
 - 64s - loss: -2.1019e+00 - val_loss: -2.1127e+00
Epoch 52/200
 - 64s - loss: -2.1025e+00 - val_loss: -2.1115e+00
Epoch 53/200
 - 64s - loss: -2.1026e+00 - val_loss: -2.1115e+00
Epoch 54/200
 - 64s - loss: -2.1024e+00 - val_loss: -2.1127e+00
Epoch 55/200
 - 64s - loss: -2.1022e+00 - val_loss: -2.1128e+00
Epoch 56/200
 - 64s - loss: -2.1022e+00 - val_loss: -2.1120e+00
Epoch 57/200
 - 64s - loss: -2.1027e+00 - val_loss: -2.1120e+00
Epoch 58/200
 - 64s - loss: -2.1023e+00 - val_loss: -2.1121e+00
Epoch 59/200
 - 64s - loss: -2.1024e+00 - val_loss: -2.1130e+00
Epoch 60/200
 - 64s - loss: -2.1024e+00 - val_loss: -2.1112e+00
Epoch 61/200
 - 64s - loss: -2.1025e+00 - val_loss: -2.1127e+00
2019-12-25 10:50:37,704 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep3/ae_model_epoch_60.pickle
Epoch 62/200
 - 64s - loss: -2.1026e+00 - val_loss: -2.1111e+00
Epoch 63/200
 - 64s - loss: -2.1025e+00 - val_loss: -2.1111e+00
Epoch 64/200
 - 64s - loss: -2.1024e+00 - val_loss: -2.1123e+00
Epoch 65/200
 - 64s - loss: -2.1025e+00 - val_loss: -2.1116e+00
Epoch 66/200
 - 64s - loss: -2.1026e+00 - val_loss: -2.1127e+00
Epoch 67/200
 - 64s - loss: -2.1029e+00 - val_loss: -2.1134e+00
Epoch 68/200
 - 64s - loss: -2.1029e+00 - val_loss: -2.1116e+00
Epoch 69/200
 - 64s - loss: -2.1029e+00 - val_loss: -2.1115e+00
Epoch 70/200
 - 64s - loss: -2.1031e+00 - val_loss: -2.1111e+00
Epoch 71/200
 - 64s - loss: -2.1038e+00 - val_loss: -2.1107e+00
Epoch 72/200
 - 64s - loss: -2.1028e+00 - val_loss: -2.1125e+00
Epoch 73/200
 - 64s - loss: -2.1038e+00 - val_loss: -2.1126e+00
Epoch 74/200
 - 64s - loss: -2.1035e+00 - val_loss: -2.1117e+00
Epoch 75/200
 - 64s - loss: -2.1033e+00 - val_loss: -2.1124e+00
Epoch 76/200
 - 64s - loss: -2.1039e+00 - val_loss: -2.1124e+00
Epoch 77/200
 - 64s - loss: -2.1035e+00 - val_loss: -2.1107e+00
Epoch 78/200
 - 64s - loss: -2.1039e+00 - val_loss: -2.1126e+00
Epoch 79/200
 - 64s - loss: -2.1037e+00 - val_loss: -2.1127e+00
Epoch 80/200
 - 64s - loss: -2.1040e+00 - val_loss: -2.1117e+00
Epoch 81/200
 - 64s - loss: -2.1033e+00 - val_loss: -2.1126e+00
2019-12-25 11:11:56,490 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep3/ae_model_epoch_80.pickle
Epoch 82/200
 - 64s - loss: -2.1031e+00 - val_loss: -2.1117e+00
Epoch 83/200
 - 64s - loss: -2.1038e+00 - val_loss: -2.1126e+00
Epoch 84/200
 - 64s - loss: -2.1042e+00 - val_loss: -2.1130e+00
Epoch 85/200
 - 64s - loss: -2.1034e+00 - val_loss: -2.1121e+00
Epoch 86/200
 - 64s - loss: -2.1039e+00 - val_loss: -2.1136e+00
Epoch 87/200
 - 64s - loss: -2.1036e+00 - val_loss: -2.1119e+00
Epoch 88/200
 - 64s - loss: -2.1042e+00 - val_loss: -2.1124e+00
Epoch 89/200
 - 64s - loss: -2.1039e+00 - val_loss: -2.1129e+00
Epoch 90/200
 - 64s - loss: -2.1041e+00 - val_loss: -2.1129e+00
Epoch 91/200
 - 64s - loss: -2.1039e+00 - val_loss: -2.1125e+00
Epoch 92/200
 - 64s - loss: -2.1045e+00 - val_loss: -2.1143e+00
Epoch 93/200
 - 64s - loss: -2.1044e+00 - val_loss: -2.1122e+00
Epoch 94/200
 - 64s - loss: -2.1046e+00 - val_loss: -2.1136e+00
Epoch 95/200
 - 64s - loss: -2.1047e+00 - val_loss: -2.1119e+00
Epoch 96/200
 - 64s - loss: -2.1041e+00 - val_loss: -2.1130e+00
Epoch 97/200
 - 64s - loss: -2.1040e+00 - val_loss: -2.1133e+00
Epoch 98/200
 - 64s - loss: -2.1047e+00 - val_loss: -2.1147e+00
Epoch 99/200
 - 64s - loss: -2.1045e+00 - val_loss: -2.1133e+00
Epoch 100/200
 - 64s - loss: -2.1044e+00 - val_loss: -2.1140e+00
Epoch 101/200
 - 64s - loss: -2.1047e+00 - val_loss: -2.1149e+00
2019-12-25 11:33:15,637 [INFO] epoch = 100. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep3/ae_model_epoch_100.pickle
Epoch 102/200
 - 64s - loss: -2.1043e+00 - val_loss: -2.1136e+00
Epoch 103/200
 - 64s - loss: -2.1047e+00 - val_loss: -2.1125e+00
Epoch 104/200
 - 64s - loss: -2.1048e+00 - val_loss: -2.1134e+00
Epoch 105/200
 - 64s - loss: -2.1044e+00 - val_loss: -2.1134e+00
Epoch 106/200
 - 64s - loss: -2.1044e+00 - val_loss: -2.1129e+00
Epoch 107/200
 - 64s - loss: -2.1045e+00 - val_loss: -2.1144e+00
Epoch 108/200
 - 64s - loss: -2.1047e+00 - val_loss: -2.1136e+00
Epoch 109/200
 - 64s - loss: -2.1053e+00 - val_loss: -2.1122e+00
Epoch 110/200
 - 64s - loss: -2.1050e+00 - val_loss: -2.1146e+00
Epoch 111/200
 - 65s - loss: -2.1045e+00 - val_loss: -2.1129e+00
Epoch 112/200
 - 64s - loss: -2.1051e+00 - val_loss: -2.1138e+00
Epoch 113/200
 - 65s - loss: -2.1049e+00 - val_loss: -2.1135e+00
Epoch 114/200
 - 64s - loss: -2.1050e+00 - val_loss: -2.1127e+00
Epoch 115/200
 - 65s - loss: -2.1042e+00 - val_loss: -2.1138e+00
Epoch 116/200
 - 64s - loss: -2.1052e+00 - val_loss: -2.1136e+00
Epoch 117/200
 - 64s - loss: -2.1049e+00 - val_loss: -2.1126e+00
Epoch 118/200
 - 64s - loss: -2.1048e+00 - val_loss: -2.1135e+00
Epoch 119/200
 - 64s - loss: -2.1050e+00 - val_loss: -2.1134e+00
Epoch 120/200
 - 64s - loss: -2.1047e+00 - val_loss: -2.1134e+00
Epoch 121/200
 - 64s - loss: -2.1050e+00 - val_loss: -2.1129e+00
2019-12-25 11:54:40,660 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep3/ae_model_epoch_120.pickle
Epoch 122/200
 - 64s - loss: -2.1046e+00 - val_loss: -2.1137e+00
Epoch 123/200
 - 64s - loss: -2.1053e+00 - val_loss: -2.1144e+00
Epoch 124/200
 - 65s - loss: -2.1049e+00 - val_loss: -2.1146e+00
Epoch 125/200
 - 64s - loss: -2.1045e+00 - val_loss: -2.1134e+00
Epoch 126/200
 - 64s - loss: -2.1048e+00 - val_loss: -2.1133e+00
Epoch 127/200
 - 64s - loss: -2.1048e+00 - val_loss: -2.1146e+00
Epoch 128/200
 - 64s - loss: -2.1055e+00 - val_loss: -2.1143e+00
Epoch 129/200
 - 64s - loss: -2.1045e+00 - val_loss: -2.1150e+00
Epoch 130/200
 - 64s - loss: -2.1056e+00 - val_loss: -2.1133e+00
Epoch 131/200
 - 64s - loss: -2.1047e+00 - val_loss: -2.1147e+00
Epoch 132/200
 - 64s - loss: -2.1053e+00 - val_loss: -2.1143e+00
Epoch 133/200
 - 64s - loss: -2.1058e+00 - val_loss: -2.1140e+00
Epoch 134/200
 - 64s - loss: -2.1052e+00 - val_loss: -2.1146e+00
Epoch 135/200
 - 64s - loss: -2.1048e+00 - val_loss: -2.1147e+00
Epoch 136/200
 - 64s - loss: -2.1052e+00 - val_loss: -2.1148e+00
Epoch 137/200
 - 64s - loss: -2.1048e+00 - val_loss: -2.1149e+00
Epoch 138/200
 - 64s - loss: -2.1058e+00 - val_loss: -2.1136e+00
Epoch 139/200
 - 64s - loss: -2.1049e+00 - val_loss: -2.1143e+00
Epoch 140/200
 - 64s - loss: -2.1053e+00 - val_loss: -2.1149e+00
Epoch 141/200
 - 64s - loss: -2.1051e+00 - val_loss: -2.1146e+00
2019-12-25 12:16:09,074 [INFO] epoch = 140. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep3/ae_model_epoch_140.pickle
Epoch 142/200
 - 65s - loss: -2.1058e+00 - val_loss: -2.1148e+00
Epoch 143/200
 - 64s - loss: -2.1057e+00 - val_loss: -2.1148e+00
Epoch 144/200
 - 64s - loss: -2.1056e+00 - val_loss: -2.1139e+00
Epoch 145/200
 - 64s - loss: -2.1054e+00 - val_loss: -2.1141e+00
Epoch 146/200
 - 64s - loss: -2.1057e+00 - val_loss: -2.1146e+00
Epoch 147/200
 - 64s - loss: -2.1057e+00 - val_loss: -2.1153e+00
Epoch 148/200
 - 64s - loss: -2.1056e+00 - val_loss: -2.1153e+00
Epoch 149/200
 - 64s - loss: -2.1061e+00 - val_loss: -2.1143e+00
Epoch 150/200
 - 64s - loss: -2.1056e+00 - val_loss: -2.1139e+00
Epoch 151/200
 - 64s - loss: -2.1057e+00 - val_loss: -2.1149e+00
Epoch 152/200
 - 64s - loss: -2.1047e+00 - val_loss: -2.1154e+00
Epoch 153/200
 - 64s - loss: -2.1058e+00 - val_loss: -2.1144e+00
Epoch 154/200
 - 64s - loss: -2.1056e+00 - val_loss: -2.1151e+00
Epoch 155/200
 - 64s - loss: -2.1054e+00 - val_loss: -2.1139e+00
Epoch 156/200
 - 64s - loss: -2.1061e+00 - val_loss: -2.1147e+00
Epoch 157/200
 - 64s - loss: -2.1058e+00 - val_loss: -2.1154e+00
Epoch 158/200
 - 64s - loss: -2.1054e+00 - val_loss: -2.1150e+00
Epoch 159/200
 - 64s - loss: -2.1054e+00 - val_loss: -2.1148e+00
Epoch 160/200
 - 64s - loss: -2.1062e+00 - val_loss: -2.1142e+00
Epoch 161/200
 - 64s - loss: -2.1057e+00 - val_loss: -2.1152e+00
2019-12-25 12:37:30,052 [INFO] epoch = 160. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep3/ae_model_epoch_160.pickle
Epoch 162/200
 - 64s - loss: -2.1058e+00 - val_loss: -2.1150e+00
Epoch 163/200
 - 64s - loss: -2.1062e+00 - val_loss: -2.1142e+00
Epoch 164/200
 - 64s - loss: -2.1062e+00 - val_loss: -2.1153e+00
Epoch 165/200
 - 64s - loss: -2.1056e+00 - val_loss: -2.1147e+00
Epoch 166/200
 - 64s - loss: -2.1057e+00 - val_loss: -2.1150e+00
Epoch 167/200
 - 64s - loss: -2.1064e+00 - val_loss: -2.1140e+00
Epoch 168/200
 - 64s - loss: -2.1056e+00 - val_loss: -2.1135e+00
Epoch 169/200
 - 64s - loss: -2.1060e+00 - val_loss: -2.1151e+00
Epoch 170/200
 - 64s - loss: -2.1056e+00 - val_loss: -2.1139e+00
Epoch 171/200
 - 64s - loss: -2.1059e+00 - val_loss: -2.1144e+00
Epoch 172/200
 - 64s - loss: -2.1057e+00 - val_loss: -2.1141e+00
Epoch 173/200
 - 64s - loss: -2.1056e+00 - val_loss: -2.1151e+00
Epoch 174/200
 - 64s - loss: -2.1063e+00 - val_loss: -2.1152e+00
Epoch 175/200
 - 64s - loss: -2.1060e+00 - val_loss: -2.1140e+00
Epoch 176/200
 - 64s - loss: -2.1061e+00 - val_loss: -2.1153e+00
Epoch 177/200
 - 64s - loss: -2.1066e+00 - val_loss: -2.1153e+00
Epoch 178/200
 - 64s - loss: -2.1061e+00 - val_loss: -2.1155e+00
Epoch 179/200
 - 64s - loss: -2.1061e+00 - val_loss: -2.1156e+00
Epoch 180/200
 - 64s - loss: -2.1060e+00 - val_loss: -2.1152e+00
Epoch 181/200
 - 64s - loss: -2.1061e+00 - val_loss: -2.1151e+00
2019-12-25 12:58:51,275 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep3/ae_model_epoch_180.pickle
Epoch 182/200
 - 64s - loss: -2.1066e+00 - val_loss: -2.1152e+00
Epoch 183/200
 - 64s - loss: -2.1060e+00 - val_loss: -2.1144e+00
Epoch 184/200
 - 64s - loss: -2.1056e+00 - val_loss: -2.1153e+00
Epoch 185/200
 - 64s - loss: -2.1061e+00 - val_loss: -2.1154e+00
Epoch 186/200
 - 64s - loss: -2.1063e+00 - val_loss: -2.1135e+00
Epoch 187/200
 - 64s - loss: -2.1061e+00 - val_loss: -2.1146e+00
Epoch 188/200
 - 64s - loss: -2.1061e+00 - val_loss: -2.1157e+00
Epoch 189/200
 - 64s - loss: -2.1063e+00 - val_loss: -2.1160e+00
Epoch 190/200
 - 64s - loss: -2.1062e+00 - val_loss: -2.1143e+00
Epoch 191/200
 - 64s - loss: -2.1068e+00 - val_loss: -2.1148e+00
Epoch 192/200
 - 64s - loss: -2.1057e+00 - val_loss: -2.1160e+00
Epoch 193/200
 - 64s - loss: -2.1065e+00 - val_loss: -2.1151e+00
Epoch 194/200
 - 64s - loss: -2.1065e+00 - val_loss: -2.1141e+00
Epoch 195/200
 - 64s - loss: -2.1066e+00 - val_loss: -2.1139e+00
Epoch 196/200
 - 64s - loss: -2.1058e+00 - val_loss: -2.1149e+00
Epoch 197/200
 - 64s - loss: -2.1062e+00 - val_loss: -2.1156e+00
Epoch 198/200
 - 64s - loss: -2.1069e+00 - val_loss: -2.1147e+00
Epoch 199/200
 - 64s - loss: -2.1066e+00 - val_loss: -2.1161e+00
Epoch 200/200
 - 64s - loss: -2.1072e+00 - val_loss: -2.1148e+00
2019-12-25 13:19:07,631 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-25 13:21:43,286 [INFO] Last epoch loss evaluation: train_loss = -2.116797, val_loss = -2.116113
2019-12-25 13:21:43,286 [INFO] Training autoencoder complete
2019-12-25 13:21:43,286 [INFO] Encoding data for supervised training
2019-12-25 13:24:58,824 [INFO] Encoding complete
2019-12-25 13:24:58,824 [INFO] Training neural network layers (after autoencoder)
Train on 2939058 samples, validate on 979687 samples
Epoch 1/200
 - 54s - loss: 0.0048 - val_loss: 0.0013
 - val_f1: 0.9991
Epoch 2/200
 - 52s - loss: 0.0013 - val_loss: 0.0011
 - val_f1: 0.9993
Epoch 3/200
 - 52s - loss: 0.0011 - val_loss: 9.2764e-04
 - val_f1: 0.9994
Epoch 4/200
 - 52s - loss: 0.0010 - val_loss: 8.4874e-04
 - val_f1: 0.9995
Epoch 5/200
 - 52s - loss: 9.6128e-04 - val_loss: 8.0875e-04
 - val_f1: 0.9995
Epoch 6/200
 - 52s - loss: 9.4201e-04 - val_loss: 8.2928e-04
 - val_f1: 0.9994
Epoch 7/200
 - 52s - loss: 9.0992e-04 - val_loss: 8.3418e-04
 - val_f1: 0.9995
Epoch 8/200
 - 52s - loss: 8.7621e-04 - val_loss: 8.0966e-04
 - val_f1: 0.9995
Epoch 9/200
 - 52s - loss: 8.6139e-04 - val_loss: 7.0118e-04
 - val_f1: 0.9995
Epoch 10/200
 - 52s - loss: 8.3691e-04 - val_loss: 7.3720e-04
 - val_f1: 0.9995
Epoch 11/200
 - 52s - loss: 8.2011e-04 - val_loss: 7.1230e-04
 - val_f1: 0.9995
Epoch 12/200
 - 52s - loss: 7.8560e-04 - val_loss: 6.7404e-04
 - val_f1: 0.9996
Epoch 13/200
 - 52s - loss: 7.9019e-04 - val_loss: 6.4049e-04
 - val_f1: 0.9996
Epoch 14/200
 - 52s - loss: 7.9412e-04 - val_loss: 6.7626e-04
 - val_f1: 0.9994
Epoch 15/200
 - 52s - loss: 7.6679e-04 - val_loss: 6.7878e-04
 - val_f1: 0.9996
Epoch 16/200
 - 52s - loss: 7.4545e-04 - val_loss: 6.7133e-04
 - val_f1: 0.9996
Epoch 17/200
 - 52s - loss: 7.4506e-04 - val_loss: 6.2947e-04
 - val_f1: 0.9996
Epoch 18/200
 - 52s - loss: 7.5259e-04 - val_loss: 6.6659e-04
 - val_f1: 0.9995
Epoch 19/200
 - 52s - loss: 7.2911e-04 - val_loss: 6.5318e-04
 - val_f1: 0.9996
Epoch 20/200
 - 52s - loss: 7.1249e-04 - val_loss: 6.0146e-04
 - val_f1: 0.9996
Epoch 21/200
 - 52s - loss: 7.1568e-04 - val_loss: 6.8728e-04
2019-12-25 13:57:29,482 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep3/ann_model_epoch_20.pickle
 - val_f1: 0.9995
Epoch 22/200
 - 52s - loss: 7.0801e-04 - val_loss: 6.0271e-04
 - val_f1: 0.9996
Epoch 23/200
 - 52s - loss: 7.1454e-04 - val_loss: 6.2062e-04
 - val_f1: 0.9996
Epoch 24/200
 - 52s - loss: 7.0488e-04 - val_loss: 6.2063e-04
 - val_f1: 0.9996
Epoch 25/200
 - 52s - loss: 6.9625e-04 - val_loss: 6.2912e-04
 - val_f1: 0.9996
Epoch 26/200
 - 52s - loss: 6.9730e-04 - val_loss: 5.6404e-04
 - val_f1: 0.9996
Epoch 27/200
 - 52s - loss: 6.9300e-04 - val_loss: 5.7527e-04
 - val_f1: 0.9996
Epoch 28/200
 - 52s - loss: 6.8478e-04 - val_loss: 5.6862e-04
 - val_f1: 0.9996
Epoch 29/200
 - 52s - loss: 6.8434e-04 - val_loss: 6.2471e-04
 - val_f1: 0.9996
Epoch 30/200
 - 52s - loss: 6.6843e-04 - val_loss: 5.9967e-04
 - val_f1: 0.9996
Epoch 31/200
 - 52s - loss: 6.7537e-04 - val_loss: 5.7454e-04
 - val_f1: 0.9996
Epoch 32/200
 - 52s - loss: 6.6138e-04 - val_loss: 6.2860e-04
 - val_f1: 0.9996
Epoch 33/200
 - 52s - loss: 6.5252e-04 - val_loss: 5.5666e-04
 - val_f1: 0.9996
Epoch 34/200
 - 52s - loss: 6.4831e-04 - val_loss: 5.7891e-04
 - val_f1: 0.9996
Epoch 35/200
 - 52s - loss: 6.6010e-04 - val_loss: 5.4303e-04
 - val_f1: 0.9996
Epoch 36/200
 - 52s - loss: 6.4643e-04 - val_loss: 5.6170e-04
 - val_f1: 0.9996
Epoch 37/200
 - 52s - loss: 6.5337e-04 - val_loss: 5.6051e-04
 - val_f1: 0.9996
Epoch 38/200
 - 52s - loss: 6.4664e-04 - val_loss: 5.5176e-04
 - val_f1: 0.9996
Epoch 39/200
 - 52s - loss: 6.4160e-04 - val_loss: 6.2883e-04
 - val_f1: 0.9996
Epoch 40/200
 - 52s - loss: 6.3859e-04 - val_loss: 5.9747e-04
 - val_f1: 0.9996
Epoch 41/200
 - 52s - loss: 6.3742e-04 - val_loss: 5.7490e-04
2019-12-25 14:28:48,807 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep3/ann_model_epoch_40.pickle
 - val_f1: 0.9997
Epoch 42/200
 - 52s - loss: 6.3068e-04 - val_loss: 5.6865e-04
 - val_f1: 0.9996
Epoch 43/200
 - 52s - loss: 6.3605e-04 - val_loss: 6.2488e-04
 - val_f1: 0.9996
Epoch 44/200
 - 52s - loss: 6.2340e-04 - val_loss: 6.2265e-04
 - val_f1: 0.9996
Epoch 45/200
 - 52s - loss: 6.2974e-04 - val_loss: 5.4331e-04
 - val_f1: 0.9997
Epoch 46/200
 - 52s - loss: 6.3079e-04 - val_loss: 5.5733e-04
 - val_f1: 0.9996
Epoch 47/200
 - 52s - loss: 6.2717e-04 - val_loss: 5.1534e-04
 - val_f1: 0.9997
Epoch 48/200
 - 52s - loss: 6.1087e-04 - val_loss: 5.6798e-04
 - val_f1: 0.9996
Epoch 49/200
 - 52s - loss: 6.1277e-04 - val_loss: 5.5513e-04
 - val_f1: 0.9996
Epoch 50/200
 - 52s - loss: 6.0854e-04 - val_loss: 5.5447e-04
 - val_f1: 0.9996
Epoch 51/200
 - 52s - loss: 5.9468e-04 - val_loss: 5.8603e-04
 - val_f1: 0.9996
Epoch 52/200
 - 52s - loss: 6.1610e-04 - val_loss: 5.4953e-04
 - val_f1: 0.9996
Epoch 53/200
 - 52s - loss: 6.1305e-04 - val_loss: 5.5780e-04
 - val_f1: 0.9997
Epoch 54/200
 - 52s - loss: 6.1535e-04 - val_loss: 5.6336e-04
 - val_f1: 0.9996
Epoch 55/200
 - 52s - loss: 6.0823e-04 - val_loss: 5.4677e-04
 - val_f1: 0.9997
Epoch 56/200
 - 52s - loss: 6.1683e-04 - val_loss: 5.2302e-04
 - val_f1: 0.9997
Epoch 57/200
 - 52s - loss: 6.0444e-04 - val_loss: 6.9407e-04
 - val_f1: 0.9996
Epoch 58/200
 - 52s - loss: 6.0050e-04 - val_loss: 5.5422e-04
 - val_f1: 0.9996
Epoch 59/200
 - 52s - loss: 5.9850e-04 - val_loss: 5.6522e-04
 - val_f1: 0.9997
Epoch 60/200
 - 52s - loss: 5.9175e-04 - val_loss: 5.2848e-04
 - val_f1: 0.9997
Epoch 61/200
 - 52s - loss: 6.1079e-04 - val_loss: 5.2963e-04
2019-12-25 15:00:08,273 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep3/ann_model_epoch_60.pickle
 - val_f1: 0.9997
Epoch 62/200
 - 52s - loss: 5.9880e-04 - val_loss: 5.8394e-04
 - val_f1: 0.9996
Epoch 63/200
 - 52s - loss: 6.0171e-04 - val_loss: 8.0874e-04
 - val_f1: 0.9996
Epoch 64/200
 - 52s - loss: 5.8576e-04 - val_loss: 5.6494e-04
 - val_f1: 0.9996
Epoch 65/200
 - 52s - loss: 5.8652e-04 - val_loss: 5.5460e-04
 - val_f1: 0.9996
Epoch 66/200
 - 52s - loss: 5.8061e-04 - val_loss: 7.4607e-04
 - val_f1: 0.9996
Epoch 67/200
 - 52s - loss: 5.7337e-04 - val_loss: 5.7666e-04
 - val_f1: 0.9996
Epoch 68/200
 - 52s - loss: 5.9202e-04 - val_loss: 5.4367e-04
 - val_f1: 0.9997
Epoch 69/200
 - 52s - loss: 5.7596e-04 - val_loss: 5.7943e-04
 - val_f1: 0.9996
Epoch 70/200
 - 52s - loss: 5.9024e-04 - val_loss: 7.9977e-04
 - val_f1: 0.9996
Epoch 71/200
 - 52s - loss: 5.9659e-04 - val_loss: 5.3352e-04
 - val_f1: 0.9996
Epoch 72/200
 - 52s - loss: 5.8263e-04 - val_loss: 5.5770e-04
 - val_f1: 0.9996
Epoch 73/200
 - 52s - loss: 5.8051e-04 - val_loss: 5.6672e-04
 - val_f1: 0.9996
Epoch 74/200
 - 52s - loss: 5.7196e-04 - val_loss: 5.6815e-04
 - val_f1: 0.9996
Epoch 75/200
 - 52s - loss: 5.6786e-04 - val_loss: 5.7042e-04
 - val_f1: 0.9997
Epoch 76/200
 - 52s - loss: 5.7900e-04 - val_loss: 5.5589e-04
 - val_f1: 0.9997
Epoch 77/200
 - 52s - loss: 5.7001e-04 - val_loss: 5.0434e-04
 - val_f1: 0.9996
Epoch 78/200
 - 52s - loss: 5.5496e-04 - val_loss: 7.4184e-04
 - val_f1: 0.9996
Epoch 79/200
 - 52s - loss: 5.7665e-04 - val_loss: 5.5938e-04
 - val_f1: 0.9996
Epoch 80/200
 - 52s - loss: 5.8408e-04 - val_loss: 5.5201e-04
 - val_f1: 0.9996
Epoch 81/200
 - 52s - loss: 5.5906e-04 - val_loss: 5.6762e-04
2019-12-25 15:31:27,753 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep3/ann_model_epoch_80.pickle
 - val_f1: 0.9997
Epoch 82/200
 - 52s - loss: 5.7268e-04 - val_loss: 5.3972e-04
 - val_f1: 0.9996
Epoch 83/200
 - 52s - loss: 5.5521e-04 - val_loss: 7.7562e-04
 - val_f1: 0.9996
Epoch 84/200
 - 52s - loss: 5.6601e-04 - val_loss: 5.6646e-04
 - val_f1: 0.9996
Epoch 85/200
 - 52s - loss: 5.6994e-04 - val_loss: 5.4378e-04
 - val_f1: 0.9996
Epoch 86/200
 - 52s - loss: 5.7956e-04 - val_loss: 5.1348e-04
 - val_f1: 0.9997
Epoch 87/200
 - 52s - loss: 5.5982e-04 - val_loss: 7.5404e-04
 - val_f1: 0.9996
Epoch 88/200
 - 52s - loss: 5.6303e-04 - val_loss: 5.3575e-04
 - val_f1: 0.9997
Epoch 89/200
 - 52s - loss: 5.7334e-04 - val_loss: 5.7490e-04
 - val_f1: 0.9997
Epoch 90/200
 - 52s - loss: 5.5882e-04 - val_loss: 5.3139e-04
 - val_f1: 0.9997
Epoch 91/200
 - 52s - loss: 5.5878e-04 - val_loss: 5.5268e-04
 - val_f1: 0.9997
Epoch 92/200
 - 52s - loss: 5.6546e-04 - val_loss: 5.7162e-04
 - val_f1: 0.9995
Epoch 93/200
 - 52s - loss: 5.6284e-04 - val_loss: 5.7781e-04
 - val_f1: 0.9996
Epoch 94/200
 - 52s - loss: 5.5721e-04 - val_loss: 5.1232e-04
 - val_f1: 0.9997
Epoch 95/200
 - 52s - loss: 5.5328e-04 - val_loss: 5.7956e-04
 - val_f1: 0.9997
Epoch 96/200
 - 52s - loss: 5.6608e-04 - val_loss: 5.3249e-04
 - val_f1: 0.9997
Epoch 97/200
 - 52s - loss: 5.4808e-04 - val_loss: 6.2527e-04
 - val_f1: 0.9996
Epoch 98/200
 - 52s - loss: 5.5350e-04 - val_loss: 5.8052e-04
 - val_f1: 0.9997
Epoch 99/200
 - 52s - loss: 5.4733e-04 - val_loss: 5.6603e-04
 - val_f1: 0.9996
Epoch 100/200
 - 52s - loss: 5.4184e-04 - val_loss: 5.4052e-04
 - val_f1: 0.9997
Epoch 101/200
 - 52s - loss: 5.4512e-04 - val_loss: 5.2608e-04
2019-12-25 16:02:47,277 [INFO] epoch = 100. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep3/ann_model_epoch_100.pickle
 - val_f1: 0.9997
Epoch 102/200
 - 52s - loss: 5.4534e-04 - val_loss: 5.4019e-04
 - val_f1: 0.9997
Epoch 103/200
 - 52s - loss: 5.4960e-04 - val_loss: 4.9838e-04
 - val_f1: 0.9997
Epoch 104/200
 - 52s - loss: 5.5472e-04 - val_loss: 5.6344e-04
 - val_f1: 0.9996
Epoch 105/200
 - 52s - loss: 5.5177e-04 - val_loss: 5.3927e-04
 - val_f1: 0.9997
Epoch 106/200
 - 52s - loss: 5.5891e-04 - val_loss: 5.2360e-04
 - val_f1: 0.9996
Epoch 107/200
 - 52s - loss: 5.3574e-04 - val_loss: 5.2485e-04
 - val_f1: 0.9997
Epoch 108/200
 - 52s - loss: 5.4502e-04 - val_loss: 5.3472e-04
 - val_f1: 0.9997
Epoch 109/200
 - 52s - loss: 5.4745e-04 - val_loss: 5.4414e-04
 - val_f1: 0.9997
Epoch 110/200
 - 52s - loss: 5.3586e-04 - val_loss: 5.5672e-04
 - val_f1: 0.9997
Epoch 111/200
 - 52s - loss: 5.3797e-04 - val_loss: 5.6741e-04
 - val_f1: 0.9997
Epoch 112/200
 - 52s - loss: 5.4451e-04 - val_loss: 5.0810e-04
 - val_f1: 0.9997
Epoch 113/200
 - 52s - loss: 5.4953e-04 - val_loss: 5.3769e-04
 - val_f1: 0.9997
Epoch 114/200
 - 52s - loss: 5.4876e-04 - val_loss: 5.2387e-04
 - val_f1: 0.9996
Epoch 115/200
 - 52s - loss: 5.4020e-04 - val_loss: 5.4159e-04
 - val_f1: 0.9997
Epoch 116/200
 - 52s - loss: 5.5663e-04 - val_loss: 5.8496e-04
 - val_f1: 0.9996
Epoch 117/200
 - 52s - loss: 5.4335e-04 - val_loss: 6.2023e-04
 - val_f1: 0.9996
Epoch 118/200
 - 52s - loss: 5.3254e-04 - val_loss: 5.1208e-04
 - val_f1: 0.9997
Epoch 119/200
 - 52s - loss: 5.2816e-04 - val_loss: 5.9085e-04
 - val_f1: 0.9996
Epoch 120/200
 - 52s - loss: 5.5046e-04 - val_loss: 5.2820e-04
 - val_f1: 0.9997
Epoch 121/200
 - 52s - loss: 5.3799e-04 - val_loss: 5.5305e-04
2019-12-25 16:34:07,017 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep3/ann_model_epoch_120.pickle
 - val_f1: 0.9997
Epoch 122/200
 - 52s - loss: 5.3756e-04 - val_loss: 6.7006e-04
 - val_f1: 0.9996
Epoch 123/200
 - 52s - loss: 5.3456e-04 - val_loss: 5.2863e-04
 - val_f1: 0.9997
Epoch 124/200
 - 52s - loss: 5.2817e-04 - val_loss: 4.9310e-04
 - val_f1: 0.9997
Epoch 125/200
 - 52s - loss: 5.2131e-04 - val_loss: 5.7236e-04
 - val_f1: 0.9996
Epoch 126/200
 - 52s - loss: 5.2308e-04 - val_loss: 5.6243e-04
 - val_f1: 0.9996
Epoch 127/200
 - 52s - loss: 5.3613e-04 - val_loss: 5.8001e-04
 - val_f1: 0.9996
Epoch 128/200
 - 52s - loss: 5.2424e-04 - val_loss: 5.2835e-04
 - val_f1: 0.9997
Epoch 129/200
 - 52s - loss: 5.4150e-04 - val_loss: 5.6098e-04
 - val_f1: 0.9997
Epoch 130/200
 - 52s - loss: 5.3549e-04 - val_loss: 5.0087e-04
 - val_f1: 0.9997
Epoch 131/200
 - 52s - loss: 5.3622e-04 - val_loss: 5.6378e-04
 - val_f1: 0.9997
Epoch 132/200
 - 52s - loss: 5.2529e-04 - val_loss: 6.0687e-04
 - val_f1: 0.9995
Epoch 133/200
 - 52s - loss: 5.4004e-04 - val_loss: 6.0515e-04
 - val_f1: 0.9996
Epoch 134/200
 - 52s - loss: 5.3247e-04 - val_loss: 5.6291e-04
 - val_f1: 0.9996
Epoch 135/200
 - 52s - loss: 5.3070e-04 - val_loss: 5.3641e-04
 - val_f1: 0.9996
Epoch 136/200
 - 52s - loss: 5.3463e-04 - val_loss: 5.3061e-04
 - val_f1: 0.9997
Epoch 137/200
 - 52s - loss: 5.2267e-04 - val_loss: 5.2632e-04
 - val_f1: 0.9997
Epoch 138/200
 - 52s - loss: 5.3791e-04 - val_loss: 5.1737e-04
 - val_f1: 0.9997
Epoch 139/200
 - 52s - loss: 5.2963e-04 - val_loss: 5.1077e-04
 - val_f1: 0.9997
Epoch 140/200
 - 52s - loss: 5.2029e-04 - val_loss: 9.3247e-04
 - val_f1: 0.9996
Epoch 141/200
 - 52s - loss: 5.2767e-04 - val_loss: 5.3033e-04
2019-12-25 17:05:26,545 [INFO] epoch = 140. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep3/ann_model_epoch_140.pickle
 - val_f1: 0.9997
Epoch 142/200
 - 52s - loss: 5.2900e-04 - val_loss: 5.2629e-04
 - val_f1: 0.9997
Epoch 143/200
 - 52s - loss: 5.2843e-04 - val_loss: 5.6184e-04
 - val_f1: 0.9997
Epoch 144/200
 - 52s - loss: 5.2252e-04 - val_loss: 5.4817e-04
 - val_f1: 0.9996
Epoch 145/200
 - 52s - loss: 5.2450e-04 - val_loss: 5.3033e-04
 - val_f1: 0.9997
Epoch 146/200
 - 52s - loss: 5.3269e-04 - val_loss: 5.4072e-04
 - val_f1: 0.9997
Epoch 147/200
 - 52s - loss: 5.0566e-04 - val_loss: 5.2846e-04
 - val_f1: 0.9997
Epoch 148/200
 - 52s - loss: 5.2331e-04 - val_loss: 5.3363e-04
 - val_f1: 0.9997
Epoch 149/200
 - 52s - loss: 5.3758e-04 - val_loss: 5.2478e-04
 - val_f1: 0.9997
Epoch 150/200
 - 52s - loss: 5.2952e-04 - val_loss: 5.0941e-04
 - val_f1: 0.9997
Epoch 151/200
 - 52s - loss: 5.2784e-04 - val_loss: 5.1776e-04
 - val_f1: 0.9997
Epoch 152/200
 - 52s - loss: 5.0324e-04 - val_loss: 4.8731e-04
 - val_f1: 0.9997
Epoch 153/200
 - 52s - loss: 5.2605e-04 - val_loss: 5.9207e-04
 - val_f1: 0.9997
Epoch 154/200
 - 52s - loss: 5.1917e-04 - val_loss: 5.2508e-04
 - val_f1: 0.9997
Epoch 155/200
 - 52s - loss: 5.2648e-04 - val_loss: 5.3567e-04
 - val_f1: 0.9997
Epoch 156/200
 - 52s - loss: 5.1513e-04 - val_loss: 4.9837e-04
 - val_f1: 0.9997
Epoch 157/200
 - 52s - loss: 5.2210e-04 - val_loss: 5.3048e-04
 - val_f1: 0.9997
Epoch 158/200
 - 52s - loss: 5.1421e-04 - val_loss: 5.5632e-04
 - val_f1: 0.9997
Epoch 159/200
 - 52s - loss: 5.2750e-04 - val_loss: 5.6292e-04
 - val_f1: 0.9996
Epoch 160/200
 - 52s - loss: 5.1050e-04 - val_loss: 5.1377e-04
 - val_f1: 0.9997
Epoch 161/200
 - 52s - loss: 5.1735e-04 - val_loss: 5.2334e-04
2019-12-25 17:36:46,885 [INFO] epoch = 160. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep3/ann_model_epoch_160.pickle
 - val_f1: 0.9997
Epoch 162/200
 - 52s - loss: 5.1363e-04 - val_loss: 5.5523e-04
 - val_f1: 0.9997
Epoch 163/200
 - 52s - loss: 5.2884e-04 - val_loss: 5.2009e-04
 - val_f1: 0.9997
Epoch 164/200
 - 52s - loss: 5.1625e-04 - val_loss: 4.9983e-04
 - val_f1: 0.9997
Epoch 165/200
 - 52s - loss: 5.0770e-04 - val_loss: 5.3565e-04
 - val_f1: 0.9997
Epoch 166/200
 - 52s - loss: 5.1049e-04 - val_loss: 5.9476e-04
 - val_f1: 0.9997
Epoch 167/200
 - 52s - loss: 5.1154e-04 - val_loss: 5.4951e-04
 - val_f1: 0.9997
Epoch 168/200
 - 52s - loss: 5.1156e-04 - val_loss: 5.3130e-04
 - val_f1: 0.9997
Epoch 169/200
 - 52s - loss: 5.1738e-04 - val_loss: 5.4800e-04
 - val_f1: 0.9997
Epoch 170/200
 - 52s - loss: 5.1393e-04 - val_loss: 5.2376e-04
 - val_f1: 0.9997
Epoch 171/200
 - 52s - loss: 5.1214e-04 - val_loss: 5.5113e-04
 - val_f1: 0.9997
Epoch 172/200
 - 52s - loss: 5.0427e-04 - val_loss: 8.5706e-04
 - val_f1: 0.9997
Epoch 173/200
 - 52s - loss: 5.1057e-04 - val_loss: 6.1913e-04
 - val_f1: 0.9997
Epoch 174/200
 - 52s - loss: 5.0837e-04 - val_loss: 5.1254e-04
 - val_f1: 0.9996
Epoch 175/200
 - 52s - loss: 5.1860e-04 - val_loss: 5.3567e-04
 - val_f1: 0.9996
Epoch 176/200
 - 52s - loss: 5.0325e-04 - val_loss: 5.9469e-04
 - val_f1: 0.9997
Epoch 177/200
 - 52s - loss: 5.1117e-04 - val_loss: 5.5360e-04
 - val_f1: 0.9997
Epoch 178/200
 - 52s - loss: 5.0272e-04 - val_loss: 5.9392e-04
 - val_f1: 0.9997
Epoch 179/200
 - 52s - loss: 5.1602e-04 - val_loss: 5.1784e-04
 - val_f1: 0.9996
Epoch 180/200
 - 52s - loss: 5.0948e-04 - val_loss: 5.2355e-04
 - val_f1: 0.9997
Epoch 181/200
 - 52s - loss: 5.0405e-04 - val_loss: 5.1541e-04
2019-12-25 18:08:05,958 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep3/ann_model_epoch_180.pickle
 - val_f1: 0.9997
Epoch 182/200
 - 52s - loss: 4.9459e-04 - val_loss: 5.6656e-04
 - val_f1: 0.9997
Epoch 183/200
 - 52s - loss: 5.0762e-04 - val_loss: 5.4753e-04
 - val_f1: 0.9997
Epoch 184/200
 - 52s - loss: 5.0260e-04 - val_loss: 5.9835e-04
 - val_f1: 0.9997
Epoch 185/200
 - 52s - loss: 5.0887e-04 - val_loss: 6.7498e-04
 - val_f1: 0.9996
Epoch 186/200
 - 52s - loss: 5.0434e-04 - val_loss: 5.0270e-04
 - val_f1: 0.9997
Epoch 187/200
 - 52s - loss: 4.9855e-04 - val_loss: 5.6207e-04
 - val_f1: 0.9997
Epoch 188/200
 - 52s - loss: 5.0725e-04 - val_loss: 6.0386e-04
 - val_f1: 0.9997
Epoch 189/200
 - 52s - loss: 5.0499e-04 - val_loss: 5.0040e-04
 - val_f1: 0.9997
Epoch 190/200
 - 52s - loss: 5.0711e-04 - val_loss: 5.5762e-04
 - val_f1: 0.9997
Epoch 191/200
 - 52s - loss: 5.0663e-04 - val_loss: 5.7184e-04
 - val_f1: 0.9997
Epoch 192/200
 - 52s - loss: 4.9901e-04 - val_loss: 5.2849e-04
 - val_f1: 0.9997
Epoch 193/200
 - 52s - loss: 5.0388e-04 - val_loss: 5.0207e-04
 - val_f1: 0.9997
Epoch 194/200
 - 52s - loss: 5.1053e-04 - val_loss: 5.0097e-04
 - val_f1: 0.9997
Epoch 195/200
 - 52s - loss: 4.9944e-04 - val_loss: 4.9948e-04
 - val_f1: 0.9997
Epoch 196/200
 - 52s - loss: 5.0258e-04 - val_loss: 5.8312e-04
 - val_f1: 0.9997
Epoch 197/200
 - 52s - loss: 4.9372e-04 - val_loss: 5.2130e-04
 - val_f1: 0.9996
Epoch 198/200
 - 52s - loss: 4.8450e-04 - val_loss: 5.0095e-04
 - val_f1: 0.9997
Epoch 199/200
 - 52s - loss: 5.0277e-04 - val_loss: 5.2693e-04
 - val_f1: 0.9997
Epoch 200/200
 - 52s - loss: 4.9853e-04 - val_loss: 5.3958e-04
2019-12-25 18:38:32,998 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-25 18:41:30,968 [INFO] Last epoch loss evaluation: train_loss = 0.000401, val_loss = 0.000487
2019-12-25 18:41:30,990 [INFO] Training complete. time_to_train = 32216.01 sec, 536.93 min
2019-12-25 18:41:31,022 [INFO] Model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep3/best_model.pickle
/home/sunanda/research/ids_experiments/utility.py:268: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
2019-12-25 18:41:31,207 [INFO] Plot saved to: results_selected_models/selected_kdd99_ae_ann_deep_rep3/training_error_history.png
/home/sunanda/research/ids_experiments/utility.py:285: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
2019-12-25 18:41:31,388 [INFO] Plot saved to: results_selected_models/selected_kdd99_ae_ann_deep_rep3/training_f1_history.png
2019-12-25 18:41:31,388 [INFO] Making predictions on training, validation, testing data
2019-12-25 18:49:25,669 [INFO] Evaluating predictions (results)
2019-12-25 18:49:34,334 [INFO] Dataset: Testing. Classification report below
2019-12-25 18:49:34,334 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      0.97      0.99    229853
     normal.       0.72      0.98      0.83     60593
       probe       0.77      0.82      0.79      4166
         r2l       0.98      0.01      0.01     13781
         u2r       0.67      0.01      0.01      2636

    accuracy                           0.92    311029
   macro avg       0.83      0.56      0.53    311029
weighted avg       0.94      0.92      0.90    311029

2019-12-25 18:49:34,334 [INFO] Overall accuracy (micro avg): 0.9219686910223805
2019-12-25 18:49:43,641 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9220         0.9220                       0.9220                0.0195                   0.0780  0.9220
1     Macro avg        0.9688         0.8269                       0.5588                0.0197                   0.4412  0.5278
2  Weighted avg        0.9672         0.9381                       0.9220                0.0207                   0.0780  0.9020
2019-12-25 18:50:13,915 [INFO] Dataset: Validation. Classification report below
2019-12-25 18:50:13,915 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00    776675
     normal.       1.00      1.00      1.00    194556
       probe       0.99      0.99      0.99      8221
         r2l       0.97      0.51      0.67       225
         u2r       0.29      0.20      0.24        10

    accuracy                           1.00    979687
   macro avg       0.85      0.74      0.78    979687
weighted avg       1.00      1.00      1.00    979687

2019-12-25 18:50:13,915 [INFO] Overall accuracy (micro avg): 0.9996845931404622
2019-12-25 18:50:46,568 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9997         0.9997                       0.9997                0.0001                   0.0003  0.9997
1     Macro avg        0.9999         0.8504                       0.7400                0.0001                   0.2600  0.7792
2  Weighted avg        0.9999         0.9997                       0.9997                0.0002                   0.0003  0.9997
2019-12-25 18:53:00,296 [INFO] Dataset: Training. Classification report below
2019-12-25 18:53:00,296 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00   3106695
     normal.       1.00      1.00      1.00    778225
       probe       0.99      0.99      0.99     32881
         r2l       0.95      0.53      0.69       901
         u2r       0.76      0.45      0.57        42

    accuracy                           1.00   3918744
   macro avg       0.94      0.80      0.85   3918744
weighted avg       1.00      1.00      1.00   3918744

2019-12-25 18:53:00,296 [INFO] Overall accuracy (micro avg): 0.9997055178904262
2019-12-25 18:55:24,681 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9997         0.9997                       0.9997                0.0001                   0.0003  0.9997
1     Macro avg        0.9999         0.9415                       0.7953                0.0001                   0.2047  0.8488
2  Weighted avg        0.9999         0.9997                       0.9997                0.0002                   0.0003  0.9997
2019-12-25 18:55:24,728 [INFO] Results saved to: results_selected_models/selected_kdd99_ae_ann_deep_rep3/selected_kdd99_ae_ann_deep_rep3_results.xlsx
2019-12-25 18:55:24,735 [INFO] ================= Finished running experiment no. 3 ================= 

2019-12-25 18:55:24,758 [INFO] Created directory: results_selected_models/selected_kdd99_ae_ann_deep_rep4
2019-12-25 18:55:24,758 [INFO] Initialized logging. log_filename = results_selected_models/selected_kdd99_ae_ann_deep_rep4/run_log.log
2019-12-25 18:55:24,758 [INFO] ================= Running experiment no. 4  ================= 

2019-12-25 18:55:24,758 [INFO] Experiment parameters given below
2019-12-25 18:55:24,758 [INFO] 
{'experiment_num': 4, 'results_dir': 'results_selected_models/selected_kdd99_ae_ann_deep_rep4', 'model_type': 'classifier', 'model': 'ae_ann', 'normal_label': 'normal', 'scaling_type': 'NA', 'unsupervised_ratio': 0.25, 'ae_encoder_units': [128, 64, 32], 'ae_encoder_activations': ['relu', 'relu', 'relu'], 'ae_encoder_dropout_rates': [0.2, 0.2, 0.2], 'ae_encoder_l1_param': -1, 'ae_decoder_units': [64, 128], 'ae_decoder_activations': ['relu', 'relu'], 'ae_decoder_dropout_rates': [0.2, 0.2], 'output_activation': 'sigmoid', 'loss_function': 'binary_crossentropy', 'ae_epochs': 200, 'ann_layer_units': [64], 'ann_layer_activations': ['relu'], 'ann_layer_dropout_rates': [0.2], 'ann_epochs': 200, 'early_stop_patience': 50, 'batch_size': 256, 'dataset_dir': '../Datasets/full_datasets/kdd99_five_classes', 'description': 'selected_kdd99_ae_ann_deep_rep4'}
2019-12-25 18:55:24,758 [INFO] Created tensorboard log directory: results_selected_models/selected_kdd99_ae_ann_deep_rep4/tf_logs_run_2019_12_25-18_55_24
2019-12-25 18:55:24,759 [INFO] Loading datsets from: ../Datasets/full_datasets/kdd99_five_classes
2019-12-25 18:55:24,759 [INFO] Reading X, y files
2019-12-25 18:55:24,759 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/X_train.h5
2019-12-25 18:55:31,461 [INFO] Reading complete. time_to_read=6.70 seconds
2019-12-25 18:55:31,462 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/X_val.h5
2019-12-25 18:55:33,138 [INFO] Reading complete. time_to_read=1.68 seconds
2019-12-25 18:55:33,138 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/X_test.h5
2019-12-25 18:55:33,611 [INFO] Reading complete. time_to_read=0.47 seconds
2019-12-25 18:55:33,611 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/y_train.h5
2019-12-25 18:55:33,827 [INFO] Reading complete. time_to_read=0.22 seconds
2019-12-25 18:55:33,827 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/y_val.h5
2019-12-25 18:55:33,881 [INFO] Reading complete. time_to_read=0.05 seconds
2019-12-25 18:55:33,881 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/y_test.h5
2019-12-25 18:55:33,902 [INFO] Reading complete. time_to_read=0.02 seconds
2019-12-25 18:55:41,089 [INFO] Initializing model
2019-12-25 18:55:41,716 [INFO] _________________________________________________________________
2019-12-25 18:55:41,717 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-25 18:55:41,717 [INFO] =================================================================
2019-12-25 18:55:41,717 [INFO] dense_125 (Dense)            (None, 128)               15872     
2019-12-25 18:55:41,717 [INFO] _________________________________________________________________
2019-12-25 18:55:41,717 [INFO] batch_normalization_89 (Batc (None, 128)               512       
2019-12-25 18:55:41,717 [INFO] _________________________________________________________________
2019-12-25 18:55:41,717 [INFO] dropout_89 (Dropout)         (None, 128)               0         
2019-12-25 18:55:41,717 [INFO] _________________________________________________________________
2019-12-25 18:55:41,717 [INFO] dense_126 (Dense)            (None, 64)                8256      
2019-12-25 18:55:41,717 [INFO] _________________________________________________________________
2019-12-25 18:55:41,717 [INFO] batch_normalization_90 (Batc (None, 64)                256       
2019-12-25 18:55:41,718 [INFO] _________________________________________________________________
2019-12-25 18:55:41,718 [INFO] dropout_90 (Dropout)         (None, 64)                0         
2019-12-25 18:55:41,718 [INFO] _________________________________________________________________
2019-12-25 18:55:41,718 [INFO] dense_127 (Dense)            (None, 32)                2080      
2019-12-25 18:55:41,718 [INFO] _________________________________________________________________
2019-12-25 18:55:41,718 [INFO] batch_normalization_91 (Batc (None, 32)                128       
2019-12-25 18:55:41,718 [INFO] _________________________________________________________________
2019-12-25 18:55:41,718 [INFO] dropout_91 (Dropout)         (None, 32)                0         
2019-12-25 18:55:41,718 [INFO] _________________________________________________________________
2019-12-25 18:55:41,718 [INFO] dense_128 (Dense)            (None, 64)                2112      
2019-12-25 18:55:41,718 [INFO] _________________________________________________________________
2019-12-25 18:55:41,718 [INFO] batch_normalization_92 (Batc (None, 64)                256       
2019-12-25 18:55:41,719 [INFO] _________________________________________________________________
2019-12-25 18:55:41,719 [INFO] dropout_92 (Dropout)         (None, 64)                0         
2019-12-25 18:55:41,719 [INFO] _________________________________________________________________
2019-12-25 18:55:41,719 [INFO] dense_129 (Dense)            (None, 128)               8320      
2019-12-25 18:55:41,719 [INFO] _________________________________________________________________
2019-12-25 18:55:41,719 [INFO] batch_normalization_93 (Batc (None, 128)               512       
2019-12-25 18:55:41,719 [INFO] _________________________________________________________________
2019-12-25 18:55:41,719 [INFO] dropout_93 (Dropout)         (None, 128)               0         
2019-12-25 18:55:41,719 [INFO] _________________________________________________________________
2019-12-25 18:55:41,719 [INFO] dense_130 (Dense)            (None, 123)               15867     
2019-12-25 18:55:41,719 [INFO] =================================================================
2019-12-25 18:55:41,720 [INFO] Total params: 54,171
2019-12-25 18:55:41,720 [INFO] Trainable params: 53,339
2019-12-25 18:55:41,720 [INFO] Non-trainable params: 832
2019-12-25 18:55:41,720 [INFO] _________________________________________________________________
2019-12-25 18:55:41,887 [INFO] _________________________________________________________________
2019-12-25 18:55:41,887 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-25 18:55:41,887 [INFO] =================================================================
2019-12-25 18:55:41,887 [INFO] dense_131 (Dense)            (None, 64)                2112      
2019-12-25 18:55:41,887 [INFO] _________________________________________________________________
2019-12-25 18:55:41,887 [INFO] batch_normalization_94 (Batc (None, 64)                256       
2019-12-25 18:55:41,888 [INFO] _________________________________________________________________
2019-12-25 18:55:41,888 [INFO] dropout_94 (Dropout)         (None, 64)                0         
2019-12-25 18:55:41,888 [INFO] _________________________________________________________________
2019-12-25 18:55:41,888 [INFO] dense_132 (Dense)            (None, 5)                 325       
2019-12-25 18:55:41,888 [INFO] =================================================================
2019-12-25 18:55:41,888 [INFO] Total params: 2,693
2019-12-25 18:55:41,888 [INFO] Trainable params: 2,565
2019-12-25 18:55:41,888 [INFO] Non-trainable params: 128
2019-12-25 18:55:41,888 [INFO] _________________________________________________________________
2019-12-25 18:55:41,888 [INFO] Training model
2019-12-25 18:55:41,888 [INFO] Splitting train set into 2 sets (unsupervised, supervised), random_seed = None
2019-12-25 18:56:25,818 [INFO] Split sizes (instances). total = 3918744, unsupervised = 979686, supervised = 2939058, unsupervised dataset hash = cbec8ef65591cb5e59d08366bb1e5d38b54d7466
2019-12-25 18:56:25,818 [INFO] Training autoencoder
 - val_f1: 0.9997
Train on 979686 samples, validate on 979687 samples
Epoch 1/200
 - 68s - loss: -1.8461e+00 - val_loss: -2.0758e+00
Epoch 2/200
 - 65s - loss: -2.0543e+00 - val_loss: -2.0887e+00
Epoch 3/200
 - 65s - loss: -2.0700e+00 - val_loss: -2.0978e+00
Epoch 4/200
 - 65s - loss: -2.0790e+00 - val_loss: -2.1006e+00
Epoch 5/200
 - 65s - loss: -2.0848e+00 - val_loss: -2.1039e+00
Epoch 6/200
 - 65s - loss: -2.0883e+00 - val_loss: -2.1050e+00
Epoch 7/200
 - 65s - loss: -2.0906e+00 - val_loss: -2.1067e+00
Epoch 8/200
 - 65s - loss: -2.0928e+00 - val_loss: -2.1076e+00
Epoch 9/200
 - 65s - loss: -2.0944e+00 - val_loss: -2.1080e+00
Epoch 10/200
 - 65s - loss: -2.0951e+00 - val_loss: -2.1077e+00
Epoch 11/200
 - 65s - loss: -2.0966e+00 - val_loss: -2.1084e+00
Epoch 12/200
 - 65s - loss: -2.0974e+00 - val_loss: -2.1086e+00
Epoch 13/200
 - 65s - loss: -2.0978e+00 - val_loss: -2.1088e+00
Epoch 14/200
 - 65s - loss: -2.0982e+00 - val_loss: -2.1090e+00
Epoch 15/200
 - 65s - loss: -2.0984e+00 - val_loss: -2.1096e+00
Epoch 16/200
 - 65s - loss: -2.0992e+00 - val_loss: -2.1096e+00
Epoch 17/200
 - 65s - loss: -2.0992e+00 - val_loss: -2.1099e+00
Epoch 18/200
 - 65s - loss: -2.0992e+00 - val_loss: -2.1101e+00
Epoch 19/200
 - 65s - loss: -2.1005e+00 - val_loss: -2.1100e+00
Epoch 20/200
 - 65s - loss: -2.1006e+00 - val_loss: -2.1098e+00
Epoch 21/200
 - 65s - loss: -2.1010e+00 - val_loss: -2.1092e+00
2019-12-25 19:19:32,580 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep4/ae_model_epoch_20.pickle
Epoch 22/200
 - 65s - loss: -2.1007e+00 - val_loss: -2.1109e+00
Epoch 23/200
 - 65s - loss: -2.1004e+00 - val_loss: -2.1099e+00
Epoch 24/200
 - 65s - loss: -2.1019e+00 - val_loss: -2.1115e+00
Epoch 25/200
 - 65s - loss: -2.1010e+00 - val_loss: -2.1112e+00
Epoch 26/200
 - 65s - loss: -2.1015e+00 - val_loss: -2.1112e+00
Epoch 27/200
 - 65s - loss: -2.1018e+00 - val_loss: -2.1108e+00
Epoch 28/200
 - 65s - loss: -2.1021e+00 - val_loss: -2.1110e+00
Epoch 29/200
 - 65s - loss: -2.1018e+00 - val_loss: -2.1112e+00
Epoch 30/200
 - 65s - loss: -2.1028e+00 - val_loss: -2.1106e+00
Epoch 31/200
 - 65s - loss: -2.1024e+00 - val_loss: -2.1121e+00
Epoch 32/200
 - 65s - loss: -2.1022e+00 - val_loss: -2.1110e+00
Epoch 33/200
 - 65s - loss: -2.1024e+00 - val_loss: -2.1117e+00
Epoch 34/200
 - 65s - loss: -2.1026e+00 - val_loss: -2.1114e+00
Epoch 35/200
 - 65s - loss: -2.1027e+00 - val_loss: -2.1120e+00
Epoch 36/200
 - 65s - loss: -2.1025e+00 - val_loss: -2.1118e+00
Epoch 37/200
 - 65s - loss: -2.1030e+00 - val_loss: -2.1125e+00
Epoch 38/200
 - 65s - loss: -2.1033e+00 - val_loss: -2.1118e+00
Epoch 39/200
 - 65s - loss: -2.1030e+00 - val_loss: -2.1113e+00
Epoch 40/200
 - 65s - loss: -2.1030e+00 - val_loss: -2.1111e+00
Epoch 41/200
 - 65s - loss: -2.1029e+00 - val_loss: -2.1117e+00
2019-12-25 19:41:10,351 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep4/ae_model_epoch_40.pickle
Epoch 42/200
 - 65s - loss: -2.1035e+00 - val_loss: -2.1107e+00
Epoch 43/200
 - 65s - loss: -2.1026e+00 - val_loss: -2.1118e+00
Epoch 44/200
 - 65s - loss: -2.1026e+00 - val_loss: -2.1109e+00
Epoch 45/200
 - 65s - loss: -2.1032e+00 - val_loss: -2.1122e+00
Epoch 46/200
 - 65s - loss: -2.1035e+00 - val_loss: -2.1109e+00
Epoch 47/200
 - 65s - loss: -2.1035e+00 - val_loss: -2.1116e+00
Epoch 48/200
 - 65s - loss: -2.1036e+00 - val_loss: -2.1116e+00
Epoch 49/200
 - 65s - loss: -2.1036e+00 - val_loss: -2.1112e+00
Epoch 50/200
 - 65s - loss: -2.1043e+00 - val_loss: -2.1112e+00
Epoch 51/200
 - 65s - loss: -2.1036e+00 - val_loss: -2.1087e+00
Epoch 52/200
 - 65s - loss: -2.1040e+00 - val_loss: -2.1120e+00
Epoch 53/200
 - 65s - loss: -2.1045e+00 - val_loss: -2.1124e+00
Epoch 54/200
 - 65s - loss: -2.1038e+00 - val_loss: -2.1118e+00
Epoch 55/200
 - 65s - loss: -2.1041e+00 - val_loss: -2.1129e+00
Epoch 56/200
 - 65s - loss: -2.1042e+00 - val_loss: -2.1112e+00
Epoch 57/200
 - 65s - loss: -2.1044e+00 - val_loss: -2.1131e+00
Epoch 58/200
 - 65s - loss: -2.1042e+00 - val_loss: -2.1116e+00
Epoch 59/200
 - 65s - loss: -2.1041e+00 - val_loss: -2.1133e+00
Epoch 60/200
 - 65s - loss: -2.1043e+00 - val_loss: -2.1131e+00
Epoch 61/200
 - 65s - loss: -2.1042e+00 - val_loss: -2.1098e+00
2019-12-25 20:02:47,734 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep4/ae_model_epoch_60.pickle
Epoch 62/200
 - 65s - loss: -2.1041e+00 - val_loss: -2.1117e+00
Epoch 63/200
 - 65s - loss: -2.1045e+00 - val_loss: -2.1126e+00
Epoch 64/200
 - 65s - loss: -2.1038e+00 - val_loss: -2.1122e+00
Epoch 65/200
 - 65s - loss: -2.1039e+00 - val_loss: -2.1107e+00
Epoch 66/200
 - 65s - loss: -2.1049e+00 - val_loss: -2.1130e+00
Epoch 67/200
 - 65s - loss: -2.1044e+00 - val_loss: -2.1116e+00
Epoch 68/200
 - 65s - loss: -2.1048e+00 - val_loss: -2.1122e+00
Epoch 69/200
 - 65s - loss: -2.1045e+00 - val_loss: -2.1133e+00
Epoch 70/200
 - 65s - loss: -2.1048e+00 - val_loss: -2.1130e+00
Epoch 71/200
 - 65s - loss: -2.1047e+00 - val_loss: -2.1126e+00
Epoch 72/200
 - 65s - loss: -2.1050e+00 - val_loss: -2.1132e+00
Epoch 73/200
 - 65s - loss: -2.1046e+00 - val_loss: -2.1139e+00
Epoch 74/200
 - 65s - loss: -2.1051e+00 - val_loss: -2.1137e+00
Epoch 75/200
 - 65s - loss: -2.1050e+00 - val_loss: -2.1138e+00
Epoch 76/200
 - 65s - loss: -2.1045e+00 - val_loss: -2.1131e+00
Epoch 77/200
 - 65s - loss: -2.1051e+00 - val_loss: -2.1133e+00
Epoch 78/200
 - 65s - loss: -2.1046e+00 - val_loss: -2.1130e+00
Epoch 79/200
 - 65s - loss: -2.1049e+00 - val_loss: -2.1134e+00
Epoch 80/200
 - 65s - loss: -2.1045e+00 - val_loss: -2.1129e+00
Epoch 81/200
 - 65s - loss: -2.1050e+00 - val_loss: -2.1124e+00
2019-12-25 20:24:25,618 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep4/ae_model_epoch_80.pickle
Epoch 82/200
 - 65s - loss: -2.1053e+00 - val_loss: -2.1139e+00
Epoch 83/200
 - 65s - loss: -2.1057e+00 - val_loss: -2.1129e+00
Epoch 84/200
 - 65s - loss: -2.1055e+00 - val_loss: -2.1138e+00
Epoch 85/200
 - 65s - loss: -2.1051e+00 - val_loss: -2.1133e+00
Epoch 86/200
 - 65s - loss: -2.1060e+00 - val_loss: -2.1135e+00
Epoch 87/200
 - 65s - loss: -2.1051e+00 - val_loss: -2.1131e+00
Epoch 88/200
 - 65s - loss: -2.1062e+00 - val_loss: -2.1136e+00
Epoch 89/200
 - 65s - loss: -2.1057e+00 - val_loss: -2.1141e+00
Epoch 90/200
 - 65s - loss: -2.1064e+00 - val_loss: -2.1119e+00
Epoch 91/200
 - 65s - loss: -2.1066e+00 - val_loss: -2.1134e+00
Epoch 92/200
 - 65s - loss: -2.1063e+00 - val_loss: -2.1149e+00
Epoch 93/200
 - 65s - loss: -2.1065e+00 - val_loss: -2.1140e+00
Epoch 94/200
 - 65s - loss: -2.1066e+00 - val_loss: -2.1145e+00
Epoch 95/200
 - 65s - loss: -2.1061e+00 - val_loss: -2.1139e+00
Epoch 96/200
 - 65s - loss: -2.1064e+00 - val_loss: -2.1143e+00
Epoch 97/200
 - 65s - loss: -2.1062e+00 - val_loss: -2.1134e+00
Epoch 98/200
 - 65s - loss: -2.1066e+00 - val_loss: -2.1137e+00
Epoch 99/200
 - 65s - loss: -2.1063e+00 - val_loss: -2.1143e+00
Epoch 100/200
 - 65s - loss: -2.1063e+00 - val_loss: -2.1141e+00
Epoch 101/200
 - 65s - loss: -2.1062e+00 - val_loss: -2.1142e+00
2019-12-25 20:46:04,222 [INFO] epoch = 100. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep4/ae_model_epoch_100.pickle
Epoch 102/200
 - 65s - loss: -2.1072e+00 - val_loss: -2.1145e+00
Epoch 103/200
 - 65s - loss: -2.1067e+00 - val_loss: -2.1142e+00
Epoch 104/200
 - 65s - loss: -2.1059e+00 - val_loss: -2.1145e+00
Epoch 105/200
 - 65s - loss: -2.1059e+00 - val_loss: -2.1145e+00
Epoch 106/200
 - 65s - loss: -2.1068e+00 - val_loss: -2.1144e+00
Epoch 107/200
 - 65s - loss: -2.1064e+00 - val_loss: -2.1143e+00
Epoch 108/200
 - 65s - loss: -2.1068e+00 - val_loss: -2.1140e+00
Epoch 109/200
 - 65s - loss: -2.1068e+00 - val_loss: -2.1143e+00
Epoch 110/200
 - 65s - loss: -2.1070e+00 - val_loss: -2.1145e+00
Epoch 111/200
 - 65s - loss: -2.1065e+00 - val_loss: -2.1145e+00
Epoch 112/200
 - 65s - loss: -2.1069e+00 - val_loss: -2.1143e+00
Epoch 113/200
 - 65s - loss: -2.1059e+00 - val_loss: -2.1150e+00
Epoch 114/200
 - 65s - loss: -2.1067e+00 - val_loss: -2.1145e+00
Epoch 115/200
 - 65s - loss: -2.1066e+00 - val_loss: -2.1142e+00
Epoch 116/200
 - 65s - loss: -2.1065e+00 - val_loss: -2.1146e+00
Epoch 117/200
 - 65s - loss: -2.1071e+00 - val_loss: -2.1142e+00
Epoch 118/200
 - 65s - loss: -2.1070e+00 - val_loss: -2.1145e+00
Epoch 119/200
 - 65s - loss: -2.1066e+00 - val_loss: -2.1140e+00
Epoch 120/200
 - 65s - loss: -2.1069e+00 - val_loss: -2.1137e+00
Epoch 121/200
 - 65s - loss: -2.1069e+00 - val_loss: -2.1153e+00
2019-12-25 21:07:42,625 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep4/ae_model_epoch_120.pickle
Epoch 122/200
 - 65s - loss: -2.1074e+00 - val_loss: -2.1154e+00
Epoch 123/200
 - 65s - loss: -2.1070e+00 - val_loss: -2.1152e+00
Epoch 124/200
 - 65s - loss: -2.1072e+00 - val_loss: -2.1154e+00
Epoch 125/200
 - 65s - loss: -2.1071e+00 - val_loss: -2.1154e+00
Epoch 126/200
 - 65s - loss: -2.1072e+00 - val_loss: -2.1150e+00
Epoch 127/200
 - 65s - loss: -2.1069e+00 - val_loss: -2.1155e+00
Epoch 128/200
 - 65s - loss: -2.1065e+00 - val_loss: -2.1156e+00
Epoch 129/200
 - 65s - loss: -2.1072e+00 - val_loss: -2.1155e+00
Epoch 130/200
 - 65s - loss: -2.1073e+00 - val_loss: -2.1152e+00
Epoch 131/200
 - 65s - loss: -2.1073e+00 - val_loss: -2.1153e+00
Epoch 132/200
 - 65s - loss: -2.1068e+00 - val_loss: -2.1140e+00
Epoch 133/200
 - 65s - loss: -2.1075e+00 - val_loss: -2.1154e+00
Epoch 134/200
 - 65s - loss: -2.1073e+00 - val_loss: -2.1152e+00
Epoch 135/200
 - 65s - loss: -2.1070e+00 - val_loss: -2.1155e+00
Epoch 136/200
 - 65s - loss: -2.1072e+00 - val_loss: -2.1157e+00
Epoch 137/200
 - 65s - loss: -2.1070e+00 - val_loss: -2.1149e+00
Epoch 138/200
 - 65s - loss: -2.1083e+00 - val_loss: -2.1154e+00
Epoch 139/200
 - 65s - loss: -2.1074e+00 - val_loss: -2.1151e+00
Epoch 140/200
 - 65s - loss: -2.1072e+00 - val_loss: -2.1149e+00
Epoch 141/200
 - 65s - loss: -2.1075e+00 - val_loss: -2.1143e+00
2019-12-25 21:29:21,132 [INFO] epoch = 140. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep4/ae_model_epoch_140.pickle
Epoch 142/200
 - 65s - loss: -2.1074e+00 - val_loss: -2.1160e+00
Epoch 143/200
 - 65s - loss: -2.1069e+00 - val_loss: -2.1155e+00
Epoch 144/200
 - 65s - loss: -2.1078e+00 - val_loss: -2.1154e+00
Epoch 145/200
 - 65s - loss: -2.1078e+00 - val_loss: -2.1151e+00
Epoch 146/200
 - 65s - loss: -2.1075e+00 - val_loss: -2.1150e+00
Epoch 147/200
 - 65s - loss: -2.1079e+00 - val_loss: -2.1154e+00
Epoch 148/200
 - 65s - loss: -2.1072e+00 - val_loss: -2.1157e+00
Epoch 149/200
 - 65s - loss: -2.1075e+00 - val_loss: -2.1160e+00
Epoch 150/200
 - 65s - loss: -2.1070e+00 - val_loss: -2.1154e+00
Epoch 151/200
 - 65s - loss: -2.1074e+00 - val_loss: -2.1157e+00
Epoch 152/200
 - 65s - loss: -2.1078e+00 - val_loss: -2.1159e+00
Epoch 153/200
 - 65s - loss: -2.1078e+00 - val_loss: -2.1159e+00
Epoch 154/200
 - 65s - loss: -2.1074e+00 - val_loss: -2.1158e+00
Epoch 155/200
 - 65s - loss: -2.1079e+00 - val_loss: -2.1158e+00
Epoch 156/200
 - 65s - loss: -2.1075e+00 - val_loss: -2.1157e+00
Epoch 157/200
 - 65s - loss: -2.1075e+00 - val_loss: -2.1154e+00
Epoch 158/200
 - 65s - loss: -2.1066e+00 - val_loss: -2.1152e+00
Epoch 159/200
 - 65s - loss: -2.1073e+00 - val_loss: -2.1158e+00
Epoch 160/200
 - 65s - loss: -2.1076e+00 - val_loss: -2.1153e+00
Epoch 161/200
 - 65s - loss: -2.1075e+00 - val_loss: -2.1160e+00
2019-12-25 21:50:59,796 [INFO] epoch = 160. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep4/ae_model_epoch_160.pickle
Epoch 162/200
 - 65s - loss: -2.1075e+00 - val_loss: -2.1158e+00
Epoch 163/200
 - 65s - loss: -2.1073e+00 - val_loss: -2.1151e+00
Epoch 164/200
 - 65s - loss: -2.1069e+00 - val_loss: -2.1157e+00
Epoch 165/200
 - 65s - loss: -2.1074e+00 - val_loss: -2.1156e+00
Epoch 166/200
 - 65s - loss: -2.1085e+00 - val_loss: -2.1154e+00
Epoch 167/200
 - 65s - loss: -2.1074e+00 - val_loss: -2.1158e+00
Epoch 168/200
 - 65s - loss: -2.1079e+00 - val_loss: -2.1154e+00
Epoch 169/200
 - 65s - loss: -2.1080e+00 - val_loss: -2.1160e+00
Epoch 170/200
 - 65s - loss: -2.1078e+00 - val_loss: -2.1158e+00
Epoch 171/200
 - 65s - loss: -2.1075e+00 - val_loss: -2.1158e+00
Epoch 172/200
 - 65s - loss: -2.1080e+00 - val_loss: -2.1157e+00
Epoch 173/200
 - 65s - loss: -2.1079e+00 - val_loss: -2.1155e+00
Epoch 174/200
 - 65s - loss: -2.1078e+00 - val_loss: -2.1164e+00
Epoch 175/200
 - 65s - loss: -2.1081e+00 - val_loss: -2.1153e+00
Epoch 176/200
 - 65s - loss: -2.1086e+00 - val_loss: -2.1152e+00
Epoch 177/200
 - 65s - loss: -2.1077e+00 - val_loss: -2.1155e+00
Epoch 178/200
 - 65s - loss: -2.1079e+00 - val_loss: -2.1150e+00
Epoch 179/200
 - 65s - loss: -2.1075e+00 - val_loss: -2.1155e+00
Epoch 180/200
 - 65s - loss: -2.1078e+00 - val_loss: -2.1166e+00
Epoch 181/200
 - 65s - loss: -2.1083e+00 - val_loss: -2.1154e+00
2019-12-25 22:12:38,524 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep4/ae_model_epoch_180.pickle
Epoch 182/200
 - 65s - loss: -2.1085e+00 - val_loss: -2.1147e+00
Epoch 183/200
 - 65s - loss: -2.1076e+00 - val_loss: -2.1157e+00
Epoch 184/200
 - 65s - loss: -2.1079e+00 - val_loss: -2.1156e+00
Epoch 185/200
 - 65s - loss: -2.1075e+00 - val_loss: -2.1160e+00
Epoch 186/200
 - 65s - loss: -2.1084e+00 - val_loss: -2.1155e+00
Epoch 187/200
 - 65s - loss: -2.1083e+00 - val_loss: -2.1150e+00
Epoch 188/200
 - 65s - loss: -2.1077e+00 - val_loss: -2.1159e+00
Epoch 189/200
 - 65s - loss: -2.1077e+00 - val_loss: -2.1160e+00
Epoch 190/200
 - 65s - loss: -2.1080e+00 - val_loss: -2.1171e+00
Epoch 191/200
 - 65s - loss: -2.1078e+00 - val_loss: -2.1166e+00
Epoch 192/200
 - 65s - loss: -2.1078e+00 - val_loss: -2.1153e+00
Epoch 193/200
 - 65s - loss: -2.1087e+00 - val_loss: -2.1167e+00
Epoch 194/200
 - 65s - loss: -2.1080e+00 - val_loss: -2.1171e+00
Epoch 195/200
 - 65s - loss: -2.1084e+00 - val_loss: -2.1163e+00
Epoch 196/200
 - 65s - loss: -2.1084e+00 - val_loss: -2.1171e+00
Epoch 197/200
 - 65s - loss: -2.1083e+00 - val_loss: -2.1157e+00
Epoch 198/200
 - 65s - loss: -2.1088e+00 - val_loss: -2.1162e+00
Epoch 199/200
 - 65s - loss: -2.1084e+00 - val_loss: -2.1172e+00
Epoch 200/200
 - 65s - loss: -2.1084e+00 - val_loss: -2.1164e+00
2019-12-25 22:33:12,770 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-25 22:35:51,627 [INFO] Last epoch loss evaluation: train_loss = -2.119530, val_loss = -2.117216
2019-12-25 22:35:51,627 [INFO] Training autoencoder complete
2019-12-25 22:35:51,628 [INFO] Encoding data for supervised training
2019-12-25 22:39:20,708 [INFO] Encoding complete
2019-12-25 22:39:20,708 [INFO] Training neural network layers (after autoencoder)
Train on 2939058 samples, validate on 979687 samples
Epoch 1/200
 - 55s - loss: 0.0059 - val_loss: 0.0016
 - val_f1: 0.9991
Epoch 2/200
 - 53s - loss: 0.0016 - val_loss: 0.0013
 - val_f1: 0.9992
Epoch 3/200
 - 53s - loss: 0.0015 - val_loss: 0.0013
 - val_f1: 0.9991
Epoch 4/200
 - 53s - loss: 0.0013 - val_loss: 0.0012
 - val_f1: 0.9992
Epoch 5/200
 - 53s - loss: 0.0013 - val_loss: 0.0011
 - val_f1: 0.9993
Epoch 6/200
 - 53s - loss: 0.0012 - val_loss: 9.7801e-04
 - val_f1: 0.9993
Epoch 7/200
 - 53s - loss: 0.0011 - val_loss: 8.2476e-04
 - val_f1: 0.9994
Epoch 8/200
 - 53s - loss: 0.0011 - val_loss: 0.0012
 - val_f1: 0.9992
Epoch 9/200
 - 53s - loss: 0.0011 - val_loss: 9.4753e-04
 - val_f1: 0.9994
Epoch 10/200
 - 53s - loss: 0.0011 - val_loss: 9.2386e-04
 - val_f1: 0.9994
Epoch 11/200
 - 53s - loss: 0.0011 - val_loss: 0.0011
 - val_f1: 0.9993
Epoch 12/200
 - 53s - loss: 0.0010 - val_loss: 7.8885e-04
 - val_f1: 0.9994
Epoch 13/200
 - 53s - loss: 9.9114e-04 - val_loss: 0.0012
 - val_f1: 0.9991
Epoch 14/200
 - 53s - loss: 9.5547e-04 - val_loss: 9.7304e-04
 - val_f1: 0.9993
Epoch 15/200
 - 53s - loss: 9.3337e-04 - val_loss: 9.9037e-04
 - val_f1: 0.9994
Epoch 16/200
 - 53s - loss: 9.3827e-04 - val_loss: 9.1230e-04
 - val_f1: 0.9994
Epoch 17/200
 - 53s - loss: 9.1065e-04 - val_loss: 0.0011
 - val_f1: 0.9993
Epoch 18/200
 - 53s - loss: 9.0109e-04 - val_loss: 8.1170e-04
 - val_f1: 0.9994
Epoch 19/200
 - 53s - loss: 9.1843e-04 - val_loss: 0.0010
 - val_f1: 0.9994
Epoch 20/200
 - 53s - loss: 8.9982e-04 - val_loss: 8.6698e-04
 - val_f1: 0.9995
Epoch 21/200
 - 53s - loss: 8.7531e-04 - val_loss: 9.2983e-04
2019-12-25 23:12:46,259 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep4/ann_model_epoch_20.pickle
 - val_f1: 0.9994
Epoch 22/200
 - 53s - loss: 8.7816e-04 - val_loss: 9.1273e-04
 - val_f1: 0.9995
Epoch 23/200
 - 53s - loss: 8.3626e-04 - val_loss: 0.0011
 - val_f1: 0.9995
Epoch 24/200
 - 53s - loss: 8.4658e-04 - val_loss: 9.4261e-04
 - val_f1: 0.9995
Epoch 25/200
 - 53s - loss: 8.4291e-04 - val_loss: 7.5044e-04
 - val_f1: 0.9996
Epoch 26/200
 - 53s - loss: 8.2098e-04 - val_loss: 0.0010
 - val_f1: 0.9995
Epoch 27/200
 - 53s - loss: 8.2316e-04 - val_loss: 8.0280e-04
 - val_f1: 0.9995
Epoch 28/200
 - 53s - loss: 8.1626e-04 - val_loss: 7.0415e-04
 - val_f1: 0.9995
Epoch 29/200
 - 53s - loss: 8.0809e-04 - val_loss: 7.0684e-04
 - val_f1: 0.9995
Epoch 30/200
 - 53s - loss: 8.0376e-04 - val_loss: 6.9541e-04
 - val_f1: 0.9996
Epoch 31/200
 - 53s - loss: 7.9784e-04 - val_loss: 0.0011
 - val_f1: 0.9994
Epoch 32/200
 - 53s - loss: 8.1580e-04 - val_loss: 0.0012
 - val_f1: 0.9994
Epoch 33/200
 - 53s - loss: 7.8960e-04 - val_loss: 0.0011
 - val_f1: 0.9994
Epoch 34/200
 - 53s - loss: 7.7625e-04 - val_loss: 8.9725e-04
 - val_f1: 0.9995
Epoch 35/200
 - 53s - loss: 7.6929e-04 - val_loss: 9.1581e-04
 - val_f1: 0.9995
Epoch 36/200
 - 53s - loss: 7.7351e-04 - val_loss: 0.0011
 - val_f1: 0.9995
Epoch 37/200
 - 53s - loss: 7.5466e-04 - val_loss: 8.6466e-04
 - val_f1: 0.9995
Epoch 38/200
 - 53s - loss: 7.8283e-04 - val_loss: 0.0013
 - val_f1: 0.9994
Epoch 39/200
 - 53s - loss: 7.7017e-04 - val_loss: 9.1792e-04
 - val_f1: 0.9995
Epoch 40/200
 - 53s - loss: 7.6785e-04 - val_loss: 0.0011
 - val_f1: 0.9995
Epoch 41/200
 - 53s - loss: 7.4778e-04 - val_loss: 9.1472e-04
2019-12-25 23:44:56,478 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep4/ann_model_epoch_40.pickle
 - val_f1: 0.9995
Epoch 42/200
 - 53s - loss: 7.5918e-04 - val_loss: 9.0463e-04
 - val_f1: 0.9995
Epoch 43/200
 - 53s - loss: 7.6626e-04 - val_loss: 7.3942e-04
 - val_f1: 0.9995
Epoch 44/200
 - 53s - loss: 7.4608e-04 - val_loss: 9.4319e-04
 - val_f1: 0.9995
Epoch 45/200
 - 53s - loss: 7.3780e-04 - val_loss: 8.2411e-04
 - val_f1: 0.9996
Epoch 46/200
 - 53s - loss: 7.3594e-04 - val_loss: 6.1169e-04
 - val_f1: 0.9996
Epoch 47/200
 - 53s - loss: 7.3966e-04 - val_loss: 8.6078e-04
 - val_f1: 0.9996
Epoch 48/200
 - 53s - loss: 7.2708e-04 - val_loss: 8.9082e-04
 - val_f1: 0.9995
Epoch 49/200
 - 53s - loss: 7.2391e-04 - val_loss: 8.1508e-04
 - val_f1: 0.9995
Epoch 50/200
 - 53s - loss: 7.3274e-04 - val_loss: 8.9219e-04
 - val_f1: 0.9995
Epoch 51/200
 - 53s - loss: 7.1928e-04 - val_loss: 9.4575e-04
 - val_f1: 0.9995
Epoch 52/200
 - 53s - loss: 7.1010e-04 - val_loss: 8.5421e-04
 - val_f1: 0.9995
Epoch 53/200
 - 53s - loss: 7.1870e-04 - val_loss: 8.9242e-04
 - val_f1: 0.9996
Epoch 54/200
 - 53s - loss: 7.1622e-04 - val_loss: 7.9735e-04
 - val_f1: 0.9996
Epoch 55/200
 - 53s - loss: 7.1846e-04 - val_loss: 8.0469e-04
 - val_f1: 0.9996
Epoch 56/200
 - 53s - loss: 7.0202e-04 - val_loss: 6.3977e-04
 - val_f1: 0.9996
Epoch 57/200
 - 53s - loss: 7.2026e-04 - val_loss: 6.0595e-04
 - val_f1: 0.9996
Epoch 58/200
 - 53s - loss: 7.0732e-04 - val_loss: 6.8493e-04
 - val_f1: 0.9996
Epoch 59/200
 - 53s - loss: 7.1038e-04 - val_loss: 6.2764e-04
 - val_f1: 0.9996
Epoch 60/200
 - 53s - loss: 6.9376e-04 - val_loss: 5.6850e-04
 - val_f1: 0.9996
Epoch 61/200
 - 53s - loss: 6.9244e-04 - val_loss: 6.9710e-04
2019-12-26 00:17:06,239 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep4/ann_model_epoch_60.pickle
 - val_f1: 0.9996
Epoch 62/200
 - 53s - loss: 6.9172e-04 - val_loss: 7.0945e-04
 - val_f1: 0.9995
Epoch 63/200
 - 53s - loss: 6.8746e-04 - val_loss: 6.8647e-04
 - val_f1: 0.9995
Epoch 64/200
 - 53s - loss: 6.8283e-04 - val_loss: 6.2132e-04
 - val_f1: 0.9996
Epoch 65/200
 - 53s - loss: 6.8298e-04 - val_loss: 6.9188e-04
 - val_f1: 0.9995
Epoch 66/200
 - 53s - loss: 6.8972e-04 - val_loss: 5.6852e-04
 - val_f1: 0.9996
Epoch 67/200
 - 53s - loss: 6.9769e-04 - val_loss: 5.7282e-04
 - val_f1: 0.9997
Epoch 68/200
 - 53s - loss: 6.8773e-04 - val_loss: 6.4083e-04
 - val_f1: 0.9996
Epoch 69/200
 - 53s - loss: 6.9087e-04 - val_loss: 5.3820e-04
 - val_f1: 0.9997
Epoch 70/200
 - 53s - loss: 6.9964e-04 - val_loss: 6.0984e-04
 - val_f1: 0.9997
Epoch 71/200
 - 53s - loss: 6.8552e-04 - val_loss: 5.9892e-04
 - val_f1: 0.9996
Epoch 72/200
 - 53s - loss: 7.0746e-04 - val_loss: 6.1383e-04
 - val_f1: 0.9996
Epoch 73/200
 - 53s - loss: 7.0081e-04 - val_loss: 5.8201e-04
 - val_f1: 0.9996
Epoch 74/200
 - 53s - loss: 7.0001e-04 - val_loss: 6.4981e-04
 - val_f1: 0.9996
Epoch 75/200
 - 53s - loss: 6.6248e-04 - val_loss: 5.5807e-04
 - val_f1: 0.9997
Epoch 76/200
 - 53s - loss: 6.9220e-04 - val_loss: 5.9808e-04
 - val_f1: 0.9996
Epoch 77/200
 - 53s - loss: 6.8165e-04 - val_loss: 5.5638e-04
 - val_f1: 0.9996
Epoch 78/200
 - 53s - loss: 6.6453e-04 - val_loss: 5.8400e-04
 - val_f1: 0.9996
Epoch 79/200
 - 53s - loss: 6.7504e-04 - val_loss: 5.9964e-04
 - val_f1: 0.9996
Epoch 80/200
 - 53s - loss: 6.7474e-04 - val_loss: 6.5679e-04
 - val_f1: 0.9996
Epoch 81/200
 - 53s - loss: 6.7628e-04 - val_loss: 6.6585e-04
2019-12-26 00:49:15,914 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep4/ann_model_epoch_80.pickle
 - val_f1: 0.9996
Epoch 82/200
 - 53s - loss: 6.8861e-04 - val_loss: 5.6742e-04
 - val_f1: 0.9997
Epoch 83/200
 - 53s - loss: 6.8103e-04 - val_loss: 6.2527e-04
 - val_f1: 0.9996
Epoch 84/200
 - 53s - loss: 6.7808e-04 - val_loss: 6.3762e-04
 - val_f1: 0.9996
Epoch 85/200
 - 53s - loss: 6.8338e-04 - val_loss: 6.2458e-04
 - val_f1: 0.9996
Epoch 86/200
 - 53s - loss: 6.6465e-04 - val_loss: 6.1454e-04
 - val_f1: 0.9996
Epoch 87/200
 - 53s - loss: 6.6944e-04 - val_loss: 5.7885e-04
 - val_f1: 0.9996
Epoch 88/200
 - 53s - loss: 6.6045e-04 - val_loss: 7.1518e-04
 - val_f1: 0.9995
Epoch 89/200
 - 53s - loss: 6.6063e-04 - val_loss: 5.6553e-04
 - val_f1: 0.9996
Epoch 90/200
 - 53s - loss: 6.7114e-04 - val_loss: 7.6489e-04
 - val_f1: 0.9995
Epoch 91/200
 - 53s - loss: 6.6454e-04 - val_loss: 6.2862e-04
 - val_f1: 0.9995
Epoch 92/200
 - 53s - loss: 6.4278e-04 - val_loss: 5.4725e-04
 - val_f1: 0.9997
Epoch 93/200
 - 53s - loss: 6.5404e-04 - val_loss: 5.4084e-04
 - val_f1: 0.9997
Epoch 94/200
 - 53s - loss: 6.6057e-04 - val_loss: 5.9119e-04
 - val_f1: 0.9996
Epoch 95/200
 - 53s - loss: 6.5864e-04 - val_loss: 5.5075e-04
 - val_f1: 0.9996
Epoch 96/200
 - 53s - loss: 6.5416e-04 - val_loss: 5.4896e-04
 - val_f1: 0.9997
Epoch 97/200
 - 53s - loss: 6.4132e-04 - val_loss: 5.1980e-04
 - val_f1: 0.9997
Epoch 98/200
 - 53s - loss: 6.6317e-04 - val_loss: 5.6858e-04
 - val_f1: 0.9996
Epoch 99/200
 - 53s - loss: 6.4579e-04 - val_loss: 5.6552e-04
 - val_f1: 0.9997
Epoch 100/200
 - 53s - loss: 6.5302e-04 - val_loss: 7.4677e-04
 - val_f1: 0.9995
Epoch 101/200
 - 53s - loss: 6.4663e-04 - val_loss: 5.5114e-04
2019-12-26 01:21:24,323 [INFO] epoch = 100. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep4/ann_model_epoch_100.pickle
 - val_f1: 0.9996
Epoch 102/200
 - 53s - loss: 6.3841e-04 - val_loss: 6.3236e-04
 - val_f1: 0.9995
Epoch 103/200
 - 53s - loss: 6.5105e-04 - val_loss: 5.8356e-04
 - val_f1: 0.9996
Epoch 104/200
 - 53s - loss: 6.4018e-04 - val_loss: 5.4329e-04
 - val_f1: 0.9996
Epoch 105/200
 - 53s - loss: 6.3678e-04 - val_loss: 5.5328e-04
 - val_f1: 0.9996
Epoch 106/200
 - 53s - loss: 6.4762e-04 - val_loss: 5.4983e-04
 - val_f1: 0.9996
Epoch 107/200
 - 53s - loss: 6.2547e-04 - val_loss: 5.3268e-04
 - val_f1: 0.9997
Epoch 108/200
 - 53s - loss: 6.2141e-04 - val_loss: 5.3232e-04
 - val_f1: 0.9997
Epoch 109/200
 - 53s - loss: 6.4635e-04 - val_loss: 5.3083e-04
 - val_f1: 0.9997
Epoch 110/200
 - 53s - loss: 6.3135e-04 - val_loss: 6.8581e-04
 - val_f1: 0.9995
Epoch 111/200
 - 53s - loss: 6.2918e-04 - val_loss: 6.0621e-04
 - val_f1: 0.9996
Epoch 112/200
 - 53s - loss: 6.3707e-04 - val_loss: 6.1330e-04
 - val_f1: 0.9996
Epoch 113/200
 - 53s - loss: 6.4565e-04 - val_loss: 5.1743e-04
 - val_f1: 0.9997
Epoch 114/200
 - 53s - loss: 6.3984e-04 - val_loss: 5.8153e-04
 - val_f1: 0.9996
Epoch 115/200
 - 53s - loss: 6.4600e-04 - val_loss: 5.8730e-04
 - val_f1: 0.9997
Epoch 116/200
 - 53s - loss: 6.3567e-04 - val_loss: 5.3537e-04
 - val_f1: 0.9997
Epoch 117/200
 - 53s - loss: 6.3589e-04 - val_loss: 5.1034e-04
 - val_f1: 0.9997
Epoch 118/200
 - 53s - loss: 6.2350e-04 - val_loss: 5.5302e-04
 - val_f1: 0.9996
Epoch 119/200
 - 53s - loss: 6.2418e-04 - val_loss: 5.5008e-04
 - val_f1: 0.9997
Epoch 120/200
 - 53s - loss: 6.1511e-04 - val_loss: 5.0364e-04
 - val_f1: 0.9997
Epoch 121/200
 - 53s - loss: 6.2608e-04 - val_loss: 5.5293e-04
2019-12-26 01:53:33,141 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep4/ann_model_epoch_120.pickle
 - val_f1: 0.9996
Epoch 122/200
 - 53s - loss: 6.2934e-04 - val_loss: 6.0412e-04
 - val_f1: 0.9996
Epoch 123/200
 - 53s - loss: 6.1457e-04 - val_loss: 5.5461e-04
 - val_f1: 0.9996
Epoch 124/200
 - 53s - loss: 6.1795e-04 - val_loss: 5.1119e-04
 - val_f1: 0.9997
Epoch 125/200
 - 53s - loss: 6.2216e-04 - val_loss: 6.2193e-04
 - val_f1: 0.9995
Epoch 126/200
 - 53s - loss: 6.2165e-04 - val_loss: 5.2349e-04
 - val_f1: 0.9996
Epoch 127/200
 - 53s - loss: 6.3323e-04 - val_loss: 5.3069e-04
 - val_f1: 0.9997
Epoch 128/200
 - 53s - loss: 6.2304e-04 - val_loss: 5.9522e-04
 - val_f1: 0.9996
Epoch 129/200
 - 53s - loss: 6.3094e-04 - val_loss: 5.2889e-04
 - val_f1: 0.9996
Epoch 130/200
 - 53s - loss: 6.0538e-04 - val_loss: 5.9626e-04
 - val_f1: 0.9996
Epoch 131/200
 - 53s - loss: 6.2776e-04 - val_loss: 5.1685e-04
 - val_f1: 0.9997
Epoch 132/200
 - 53s - loss: 6.2687e-04 - val_loss: 5.4080e-04
 - val_f1: 0.9997
Epoch 133/200
 - 53s - loss: 6.1780e-04 - val_loss: 5.7175e-04
 - val_f1: 0.9996
Epoch 134/200
 - 53s - loss: 6.1856e-04 - val_loss: 5.2478e-04
 - val_f1: 0.9997
Epoch 135/200
 - 53s - loss: 6.1044e-04 - val_loss: 5.1409e-04
 - val_f1: 0.9997
Epoch 136/200
 - 53s - loss: 6.0353e-04 - val_loss: 5.4391e-04
 - val_f1: 0.9996
Epoch 137/200
 - 53s - loss: 6.1106e-04 - val_loss: 5.0224e-04
 - val_f1: 0.9997
Epoch 138/200
 - 53s - loss: 6.1314e-04 - val_loss: 5.3669e-04
 - val_f1: 0.9997
Epoch 139/200
 - 53s - loss: 6.2132e-04 - val_loss: 5.9055e-04
 - val_f1: 0.9996
Epoch 140/200
 - 53s - loss: 6.2949e-04 - val_loss: 5.1804e-04
 - val_f1: 0.9997
Epoch 141/200
 - 53s - loss: 6.1328e-04 - val_loss: 6.4148e-04
2019-12-26 02:25:44,485 [INFO] epoch = 140. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep4/ann_model_epoch_140.pickle
 - val_f1: 0.9996
Epoch 142/200
 - 53s - loss: 6.2744e-04 - val_loss: 5.5480e-04
 - val_f1: 0.9997
Epoch 143/200
 - 53s - loss: 6.1622e-04 - val_loss: 5.4750e-04
 - val_f1: 0.9997
Epoch 144/200
 - 53s - loss: 6.2353e-04 - val_loss: 4.8216e-04
 - val_f1: 0.9997
Epoch 145/200
 - 53s - loss: 5.9160e-04 - val_loss: 5.6280e-04
 - val_f1: 0.9996
Epoch 146/200
 - 53s - loss: 5.9903e-04 - val_loss: 6.2691e-04
 - val_f1: 0.9996
Epoch 147/200
 - 53s - loss: 6.0109e-04 - val_loss: 5.6457e-04
 - val_f1: 0.9996
Epoch 148/200
 - 53s - loss: 6.0143e-04 - val_loss: 5.3616e-04
 - val_f1: 0.9997
Epoch 149/200
 - 53s - loss: 6.1116e-04 - val_loss: 5.2869e-04
 - val_f1: 0.9997
Epoch 150/200
 - 53s - loss: 6.2068e-04 - val_loss: 5.1861e-04
 - val_f1: 0.9997
Epoch 151/200
 - 53s - loss: 6.1456e-04 - val_loss: 5.7319e-04
 - val_f1: 0.9996
Epoch 152/200
 - 53s - loss: 6.2329e-04 - val_loss: 5.2433e-04
 - val_f1: 0.9997
Epoch 153/200
 - 53s - loss: 6.0480e-04 - val_loss: 5.6627e-04
 - val_f1: 0.9996
Epoch 154/200
 - 53s - loss: 5.9676e-04 - val_loss: 5.1918e-04
 - val_f1: 0.9997
Epoch 155/200
 - 53s - loss: 5.9935e-04 - val_loss: 5.0489e-04
 - val_f1: 0.9997
Epoch 156/200
 - 53s - loss: 5.9333e-04 - val_loss: 5.1203e-04
 - val_f1: 0.9997
Epoch 157/200
 - 53s - loss: 5.9446e-04 - val_loss: 5.5626e-04
 - val_f1: 0.9996
Epoch 158/200
 - 53s - loss: 6.0287e-04 - val_loss: 5.6373e-04
 - val_f1: 0.9996
Epoch 159/200
 - 53s - loss: 5.7986e-04 - val_loss: 6.6528e-04
 - val_f1: 0.9996
Epoch 160/200
 - 53s - loss: 5.9496e-04 - val_loss: 5.2293e-04
 - val_f1: 0.9997
Epoch 161/200
 - 53s - loss: 5.9084e-04 - val_loss: 6.6009e-04
2019-12-26 02:57:54,740 [INFO] epoch = 160. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep4/ann_model_epoch_160.pickle
 - val_f1: 0.9996
Epoch 162/200
 - 53s - loss: 6.0533e-04 - val_loss: 5.5486e-04
 - val_f1: 0.9996
Epoch 163/200
 - 53s - loss: 5.9299e-04 - val_loss: 5.2161e-04
 - val_f1: 0.9997
Epoch 164/200
 - 53s - loss: 6.0065e-04 - val_loss: 5.2863e-04
 - val_f1: 0.9997
Epoch 165/200
 - 53s - loss: 6.0723e-04 - val_loss: 5.5275e-04
 - val_f1: 0.9997
Epoch 166/200
 - 53s - loss: 6.0780e-04 - val_loss: 5.4452e-04
 - val_f1: 0.9997
Epoch 167/200
 - 53s - loss: 5.9640e-04 - val_loss: 6.2303e-04
 - val_f1: 0.9996
Epoch 168/200
 - 53s - loss: 6.0370e-04 - val_loss: 5.4562e-04
 - val_f1: 0.9996
Epoch 169/200
 - 53s - loss: 6.0382e-04 - val_loss: 6.0500e-04
 - val_f1: 0.9996
Epoch 170/200
 - 53s - loss: 6.0228e-04 - val_loss: 5.0892e-04
 - val_f1: 0.9997
Epoch 171/200
 - 53s - loss: 6.0322e-04 - val_loss: 5.8092e-04
 - val_f1: 0.9996
Epoch 172/200
 - 53s - loss: 5.9103e-04 - val_loss: 5.6390e-04
 - val_f1: 0.9997
Epoch 173/200
 - 53s - loss: 5.8867e-04 - val_loss: 5.8863e-04
 - val_f1: 0.9996
Epoch 174/200
 - 53s - loss: 5.9899e-04 - val_loss: 5.1739e-04
 - val_f1: 0.9997
Epoch 175/200
 - 53s - loss: 6.0129e-04 - val_loss: 5.2597e-04
 - val_f1: 0.9997
Epoch 176/200
 - 53s - loss: 5.8317e-04 - val_loss: 5.1866e-04
 - val_f1: 0.9997
Epoch 177/200
 - 53s - loss: 5.8368e-04 - val_loss: 5.8657e-04
 - val_f1: 0.9996
Epoch 178/200
 - 53s - loss: 5.9059e-04 - val_loss: 5.2125e-04
 - val_f1: 0.9997
Epoch 179/200
 - 53s - loss: 5.9787e-04 - val_loss: 5.2806e-04
 - val_f1: 0.9997
Epoch 180/200
 - 53s - loss: 5.9059e-04 - val_loss: 5.7030e-04
 - val_f1: 0.9996
Epoch 181/200
 - 53s - loss: 5.8629e-04 - val_loss: 6.2245e-04
2019-12-26 03:30:04,616 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep4/ann_model_epoch_180.pickle
 - val_f1: 0.9996
Epoch 182/200
 - 53s - loss: 5.9797e-04 - val_loss: 5.2264e-04
 - val_f1: 0.9997
Epoch 183/200
 - 53s - loss: 5.8300e-04 - val_loss: 6.5144e-04
 - val_f1: 0.9995
Epoch 184/200
 - 53s - loss: 6.1131e-04 - val_loss: 6.0963e-04
 - val_f1: 0.9995
Epoch 185/200
 - 53s - loss: 5.7667e-04 - val_loss: 5.3819e-04
 - val_f1: 0.9997
Epoch 186/200
 - 53s - loss: 5.9136e-04 - val_loss: 5.3794e-04
 - val_f1: 0.9996
Epoch 187/200
 - 53s - loss: 5.8671e-04 - val_loss: 5.2428e-04
 - val_f1: 0.9997
Epoch 188/200
 - 53s - loss: 5.8310e-04 - val_loss: 5.4172e-04
 - val_f1: 0.9997
Epoch 189/200
 - 53s - loss: 5.8326e-04 - val_loss: 5.0207e-04
 - val_f1: 0.9997
Epoch 190/200
 - 53s - loss: 5.7531e-04 - val_loss: 5.3956e-04
 - val_f1: 0.9996
Epoch 191/200
 - 53s - loss: 5.7764e-04 - val_loss: 4.9759e-04
 - val_f1: 0.9997
Epoch 192/200
 - 53s - loss: 5.7691e-04 - val_loss: 5.3366e-04
 - val_f1: 0.9996
Epoch 193/200
 - 53s - loss: 5.6741e-04 - val_loss: 5.0571e-04
 - val_f1: 0.9996
Epoch 194/200
 - 53s - loss: 5.6433e-04 - val_loss: 4.8813e-04
2019-12-26 03:51:42,404 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-26 03:54:46,236 [INFO] Last epoch loss evaluation: train_loss = 0.000426, val_loss = 0.000482
2019-12-26 03:54:46,258 [INFO] Training complete. time_to_train = 32344.37 sec, 539.07 min
2019-12-26 03:54:46,290 [INFO] Model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep4/best_model.pickle
/home/sunanda/research/ids_experiments/utility.py:268: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
2019-12-26 03:54:46,479 [INFO] Plot saved to: results_selected_models/selected_kdd99_ae_ann_deep_rep4/training_error_history.png
/home/sunanda/research/ids_experiments/utility.py:285: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
2019-12-26 03:54:46,666 [INFO] Plot saved to: results_selected_models/selected_kdd99_ae_ann_deep_rep4/training_f1_history.png
2019-12-26 03:54:46,666 [INFO] Making predictions on training, validation, testing data
2019-12-26 04:03:02,039 [INFO] Evaluating predictions (results)
2019-12-26 04:03:10,710 [INFO] Dataset: Testing. Classification report below
2019-12-26 04:03:10,710 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      0.97      0.99    229853
     normal.       0.72      0.98      0.83     60593
       probe       0.74      0.80      0.77      4166
         r2l       0.53      0.00      0.01     13781
         u2r       0.41      0.00      0.01      2636

    accuracy                           0.92    311029
   macro avg       0.68      0.55      0.52    311029
weighted avg       0.92      0.92      0.90    311029

2019-12-26 04:03:10,710 [INFO] Overall accuracy (micro avg): 0.9215539387002498
2019-12-26 04:03:20,014 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9216         0.9216                       0.9216                0.0196                   0.0784  0.9216
1     Macro avg        0.9686         0.6790                       0.5537                0.0199                   0.4463  0.5212
2  Weighted avg        0.9674         0.9154                       0.9216                0.0209                   0.0784  0.9014
2019-12-26 04:03:50,331 [INFO] Dataset: Validation. Classification report below
2019-12-26 04:03:50,331 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00    776675
     normal.       1.00      1.00      1.00    194556
       probe       0.99      0.99      0.99      8221
         r2l       0.93      0.76      0.83       225
         u2r       0.33      0.20      0.25        10

    accuracy                           1.00    979687
   macro avg       0.85      0.79      0.81    979687
weighted avg       1.00      1.00      1.00    979687

2019-12-26 04:03:50,331 [INFO] Overall accuracy (micro avg): 0.9997264432415659
2019-12-26 04:04:23,035 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9997         0.9997                       0.9997                0.0001                   0.0003  0.9997
1     Macro avg        0.9999         0.8510                       0.7889                0.0001                   0.2111  0.8148
2  Weighted avg        0.9999         0.9997                       0.9997                0.0002                   0.0003  0.9997
2019-12-26 04:06:36,718 [INFO] Dataset: Training. Classification report below
2019-12-26 04:06:36,718 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00   3106695
     normal.       1.00      1.00      1.00    778225
       probe       0.99      0.99      0.99     32881
         r2l       0.91      0.77      0.84       901
         u2r       0.45      0.40      0.43        42

    accuracy                           1.00   3918744
   macro avg       0.87      0.83      0.85   3918744
weighted avg       1.00      1.00      1.00   3918744

2019-12-26 04:06:36,719 [INFO] Overall accuracy (micro avg): 0.9997440506447984
2019-12-26 04:09:01,030 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9997         0.9997                       0.9997                0.0001                   0.0003  0.9997
1     Macro avg        0.9999         0.8705                       0.8334                0.0001                   0.1666  0.8506
2  Weighted avg        0.9999         0.9997                       0.9997                0.0002                   0.0003  0.9997
2019-12-26 04:09:01,076 [INFO] Results saved to: results_selected_models/selected_kdd99_ae_ann_deep_rep4/selected_kdd99_ae_ann_deep_rep4_results.xlsx
2019-12-26 04:09:01,083 [INFO] ================= Finished running experiment no. 4 ================= 

2019-12-26 04:09:01,107 [INFO] Created directory: results_selected_models/selected_kdd99_ae_ann_deep_rep5
2019-12-26 04:09:01,107 [INFO] Initialized logging. log_filename = results_selected_models/selected_kdd99_ae_ann_deep_rep5/run_log.log
2019-12-26 04:09:01,107 [INFO] ================= Running experiment no. 5  ================= 

2019-12-26 04:09:01,108 [INFO] Experiment parameters given below
2019-12-26 04:09:01,108 [INFO] 
{'experiment_num': 5, 'results_dir': 'results_selected_models/selected_kdd99_ae_ann_deep_rep5', 'model_type': 'classifier', 'model': 'ae_ann', 'normal_label': 'normal', 'scaling_type': 'NA', 'unsupervised_ratio': 0.25, 'ae_encoder_units': [128, 64, 32], 'ae_encoder_activations': ['relu', 'relu', 'relu'], 'ae_encoder_dropout_rates': [0.2, 0.2, 0.2], 'ae_encoder_l1_param': -1, 'ae_decoder_units': [64, 128], 'ae_decoder_activations': ['relu', 'relu'], 'ae_decoder_dropout_rates': [0.2, 0.2], 'output_activation': 'sigmoid', 'loss_function': 'binary_crossentropy', 'ae_epochs': 200, 'ann_layer_units': [64], 'ann_layer_activations': ['relu'], 'ann_layer_dropout_rates': [0.2], 'ann_epochs': 200, 'early_stop_patience': 50, 'batch_size': 256, 'dataset_dir': '../Datasets/full_datasets/kdd99_five_classes', 'description': 'selected_kdd99_ae_ann_deep_rep5'}
2019-12-26 04:09:01,108 [INFO] Created tensorboard log directory: results_selected_models/selected_kdd99_ae_ann_deep_rep5/tf_logs_run_2019_12_26-04_09_01
2019-12-26 04:09:01,108 [INFO] Loading datsets from: ../Datasets/full_datasets/kdd99_five_classes
2019-12-26 04:09:01,108 [INFO] Reading X, y files
2019-12-26 04:09:01,108 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/X_train.h5
2019-12-26 04:09:07,787 [INFO] Reading complete. time_to_read=6.68 seconds
2019-12-26 04:09:07,787 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/X_val.h5
2019-12-26 04:09:09,444 [INFO] Reading complete. time_to_read=1.66 seconds
2019-12-26 04:09:09,445 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/X_test.h5
2019-12-26 04:09:09,913 [INFO] Reading complete. time_to_read=0.47 seconds
2019-12-26 04:09:09,913 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/y_train.h5
2019-12-26 04:09:10,126 [INFO] Reading complete. time_to_read=0.21 seconds
2019-12-26 04:09:10,126 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/y_val.h5
2019-12-26 04:09:10,180 [INFO] Reading complete. time_to_read=0.05 seconds
2019-12-26 04:09:10,180 [INFO] Reading HDF dataset ../Datasets/full_datasets/kdd99_five_classes/y_test.h5
2019-12-26 04:09:10,200 [INFO] Reading complete. time_to_read=0.02 seconds
2019-12-26 04:09:17,333 [INFO] Initializing model
2019-12-26 04:09:17,975 [INFO] _________________________________________________________________
2019-12-26 04:09:17,975 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-26 04:09:17,975 [INFO] =================================================================
2019-12-26 04:09:17,975 [INFO] dense_133 (Dense)            (None, 128)               15872     
2019-12-26 04:09:17,976 [INFO] _________________________________________________________________
2019-12-26 04:09:17,976 [INFO] batch_normalization_95 (Batc (None, 128)               512       
2019-12-26 04:09:17,976 [INFO] _________________________________________________________________
2019-12-26 04:09:17,976 [INFO] dropout_95 (Dropout)         (None, 128)               0         
2019-12-26 04:09:17,976 [INFO] _________________________________________________________________
2019-12-26 04:09:17,976 [INFO] dense_134 (Dense)            (None, 64)                8256      
2019-12-26 04:09:17,976 [INFO] _________________________________________________________________
2019-12-26 04:09:17,976 [INFO] batch_normalization_96 (Batc (None, 64)                256       
2019-12-26 04:09:17,976 [INFO] _________________________________________________________________
2019-12-26 04:09:17,976 [INFO] dropout_96 (Dropout)         (None, 64)                0         
2019-12-26 04:09:17,976 [INFO] _________________________________________________________________
2019-12-26 04:09:17,976 [INFO] dense_135 (Dense)            (None, 32)                2080      
2019-12-26 04:09:17,977 [INFO] _________________________________________________________________
2019-12-26 04:09:17,977 [INFO] batch_normalization_97 (Batc (None, 32)                128       
2019-12-26 04:09:17,977 [INFO] _________________________________________________________________
2019-12-26 04:09:17,977 [INFO] dropout_97 (Dropout)         (None, 32)                0         
2019-12-26 04:09:17,977 [INFO] _________________________________________________________________
2019-12-26 04:09:17,977 [INFO] dense_136 (Dense)            (None, 64)                2112      
2019-12-26 04:09:17,977 [INFO] _________________________________________________________________
2019-12-26 04:09:17,977 [INFO] batch_normalization_98 (Batc (None, 64)                256       
2019-12-26 04:09:17,977 [INFO] _________________________________________________________________
2019-12-26 04:09:17,977 [INFO] dropout_98 (Dropout)         (None, 64)                0         
2019-12-26 04:09:17,977 [INFO] _________________________________________________________________
2019-12-26 04:09:17,977 [INFO] dense_137 (Dense)            (None, 128)               8320      
2019-12-26 04:09:17,977 [INFO] _________________________________________________________________
2019-12-26 04:09:17,978 [INFO] batch_normalization_99 (Batc (None, 128)               512       
2019-12-26 04:09:17,978 [INFO] _________________________________________________________________
2019-12-26 04:09:17,978 [INFO] dropout_99 (Dropout)         (None, 128)               0         
2019-12-26 04:09:17,978 [INFO] _________________________________________________________________
2019-12-26 04:09:17,978 [INFO] dense_138 (Dense)            (None, 123)               15867     
2019-12-26 04:09:17,978 [INFO] =================================================================
2019-12-26 04:09:17,978 [INFO] Total params: 54,171
2019-12-26 04:09:17,979 [INFO] Trainable params: 53,339
2019-12-26 04:09:17,979 [INFO] Non-trainable params: 832
2019-12-26 04:09:17,979 [INFO] _________________________________________________________________
2019-12-26 04:09:18,136 [INFO] _________________________________________________________________
2019-12-26 04:09:18,136 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-26 04:09:18,136 [INFO] =================================================================
2019-12-26 04:09:18,136 [INFO] dense_139 (Dense)            (None, 64)                2112      
2019-12-26 04:09:18,137 [INFO] _________________________________________________________________
2019-12-26 04:09:18,137 [INFO] batch_normalization_100 (Bat (None, 64)                256       
2019-12-26 04:09:18,137 [INFO] _________________________________________________________________
2019-12-26 04:09:18,137 [INFO] dropout_100 (Dropout)        (None, 64)                0         
2019-12-26 04:09:18,137 [INFO] _________________________________________________________________
2019-12-26 04:09:18,137 [INFO] dense_140 (Dense)            (None, 5)                 325       
2019-12-26 04:09:18,137 [INFO] =================================================================
2019-12-26 04:09:18,137 [INFO] Total params: 2,693
2019-12-26 04:09:18,137 [INFO] Trainable params: 2,565
2019-12-26 04:09:18,137 [INFO] Non-trainable params: 128
2019-12-26 04:09:18,137 [INFO] _________________________________________________________________
2019-12-26 04:09:18,137 [INFO] Training model
2019-12-26 04:09:18,137 [INFO] Splitting train set into 2 sets (unsupervised, supervised), random_seed = None
2019-12-26 04:10:01,365 [INFO] Split sizes (instances). total = 3918744, unsupervised = 979686, supervised = 2939058, unsupervised dataset hash = 19998393593f31052b6f69c893d401c81be5f860
2019-12-26 04:10:01,365 [INFO] Training autoencoder
 - val_f1: 0.9997
Epoch 00194: early stopping
Train on 979686 samples, validate on 979687 samples
Epoch 1/200
 - 69s - loss: -1.8465e+00 - val_loss: -2.0730e+00
Epoch 2/200
 - 66s - loss: -2.0548e+00 - val_loss: -2.0925e+00
Epoch 3/200
 - 66s - loss: -2.0729e+00 - val_loss: -2.0977e+00
Epoch 4/200
 - 66s - loss: -2.0817e+00 - val_loss: -2.1023e+00
Epoch 5/200
 - 66s - loss: -2.0866e+00 - val_loss: -2.1043e+00
Epoch 6/200
 - 66s - loss: -2.0894e+00 - val_loss: -2.1063e+00
Epoch 7/200
 - 66s - loss: -2.0926e+00 - val_loss: -2.1068e+00
Epoch 8/200
 - 66s - loss: -2.0943e+00 - val_loss: -2.1085e+00
Epoch 9/200
 - 66s - loss: -2.0956e+00 - val_loss: -2.1091e+00
Epoch 10/200
 - 66s - loss: -2.0971e+00 - val_loss: -2.1094e+00
Epoch 11/200
 - 66s - loss: -2.0981e+00 - val_loss: -2.1103e+00
Epoch 12/200
 - 66s - loss: -2.0983e+00 - val_loss: -2.1105e+00
Epoch 13/200
 - 66s - loss: -2.0994e+00 - val_loss: -2.1114e+00
Epoch 14/200
 - 66s - loss: -2.0997e+00 - val_loss: -2.1112e+00
Epoch 15/200
 - 66s - loss: -2.1009e+00 - val_loss: -2.1121e+00
Epoch 16/200
 - 66s - loss: -2.1011e+00 - val_loss: -2.1120e+00
Epoch 17/200
 - 66s - loss: -2.1016e+00 - val_loss: -2.1116e+00
Epoch 18/200
 - 66s - loss: -2.1020e+00 - val_loss: -2.1130e+00
Epoch 19/200
 - 66s - loss: -2.1023e+00 - val_loss: -2.1125e+00
Epoch 20/200
 - 66s - loss: -2.1031e+00 - val_loss: -2.1136e+00
Epoch 21/200
 - 66s - loss: -2.1032e+00 - val_loss: -2.1137e+00
2019-12-26 04:33:29,855 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep5/ae_model_epoch_20.pickle
Epoch 22/200
 - 66s - loss: -2.1037e+00 - val_loss: -2.1138e+00
Epoch 23/200
 - 66s - loss: -2.1038e+00 - val_loss: -2.1135e+00
Epoch 24/200
 - 66s - loss: -2.1035e+00 - val_loss: -2.1132e+00
Epoch 25/200
 - 66s - loss: -2.1040e+00 - val_loss: -2.1142e+00
Epoch 26/200
 - 66s - loss: -2.1032e+00 - val_loss: -2.1130e+00
Epoch 27/200
 - 66s - loss: -2.1044e+00 - val_loss: -2.1139e+00
Epoch 28/200
 - 66s - loss: -2.1046e+00 - val_loss: -2.1125e+00
Epoch 29/200
 - 66s - loss: -2.1047e+00 - val_loss: -2.1138e+00
Epoch 30/200
 - 66s - loss: -2.1046e+00 - val_loss: -2.1138e+00
Epoch 31/200
 - 66s - loss: -2.1048e+00 - val_loss: -2.1136e+00
Epoch 32/200
 - 66s - loss: -2.1049e+00 - val_loss: -2.1139e+00
Epoch 33/200
 - 66s - loss: -2.1038e+00 - val_loss: -2.1137e+00
Epoch 34/200
 - 66s - loss: -2.1044e+00 - val_loss: -2.1145e+00
Epoch 35/200
 - 66s - loss: -2.1046e+00 - val_loss: -2.1131e+00
Epoch 36/200
 - 66s - loss: -2.1051e+00 - val_loss: -2.1126e+00
Epoch 37/200
 - 66s - loss: -2.1053e+00 - val_loss: -2.1126e+00
Epoch 38/200
 - 66s - loss: -2.1046e+00 - val_loss: -2.1127e+00
Epoch 39/200
 - 66s - loss: -2.1052e+00 - val_loss: -2.1122e+00
Epoch 40/200
 - 66s - loss: -2.1051e+00 - val_loss: -2.1135e+00
Epoch 41/200
 - 66s - loss: -2.1055e+00 - val_loss: -2.1130e+00
2019-12-26 04:55:27,729 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep5/ae_model_epoch_40.pickle
Epoch 42/200
 - 66s - loss: -2.1050e+00 - val_loss: -2.1132e+00
Epoch 43/200
 - 66s - loss: -2.1047e+00 - val_loss: -2.1127e+00
Epoch 44/200
 - 66s - loss: -2.1048e+00 - val_loss: -2.1131e+00
Epoch 45/200
 - 66s - loss: -2.1060e+00 - val_loss: -2.1129e+00
Epoch 46/200
 - 66s - loss: -2.1055e+00 - val_loss: -2.1127e+00
Epoch 47/200
 - 66s - loss: -2.1054e+00 - val_loss: -2.1132e+00
Epoch 48/200
 - 66s - loss: -2.1060e+00 - val_loss: -2.1125e+00
Epoch 49/200
 - 66s - loss: -2.1055e+00 - val_loss: -2.1151e+00
Epoch 50/200
 - 66s - loss: -2.1060e+00 - val_loss: -2.1148e+00
Epoch 51/200
 - 66s - loss: -2.1063e+00 - val_loss: -2.1140e+00
Epoch 52/200
 - 66s - loss: -2.1061e+00 - val_loss: -2.1156e+00
Epoch 53/200
 - 66s - loss: -2.1066e+00 - val_loss: -2.1155e+00
Epoch 54/200
 - 66s - loss: -2.1063e+00 - val_loss: -2.1154e+00
Epoch 55/200
 - 66s - loss: -2.1064e+00 - val_loss: -2.1143e+00
Epoch 56/200
 - 66s - loss: -2.1058e+00 - val_loss: -2.1141e+00
Epoch 57/200
 - 66s - loss: -2.1067e+00 - val_loss: -2.1135e+00
Epoch 58/200
 - 66s - loss: -2.1071e+00 - val_loss: -2.1144e+00
Epoch 59/200
 - 66s - loss: -2.1064e+00 - val_loss: -2.1153e+00
Epoch 60/200
 - 66s - loss: -2.1068e+00 - val_loss: -2.1148e+00
Epoch 61/200
 - 66s - loss: -2.1071e+00 - val_loss: -2.1144e+00
2019-12-26 05:17:25,712 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep5/ae_model_epoch_60.pickle
Epoch 62/200
 - 66s - loss: -2.1072e+00 - val_loss: -2.1142e+00
Epoch 63/200
 - 66s - loss: -2.1069e+00 - val_loss: -2.1145e+00
Epoch 64/200
 - 66s - loss: -2.1068e+00 - val_loss: -2.1157e+00
Epoch 65/200
 - 66s - loss: -2.1069e+00 - val_loss: -2.1156e+00
Epoch 66/200
 - 66s - loss: -2.1073e+00 - val_loss: -2.1159e+00
Epoch 67/200
 - 66s - loss: -2.1076e+00 - val_loss: -2.1151e+00
Epoch 68/200
 - 66s - loss: -2.1076e+00 - val_loss: -2.1150e+00
Epoch 69/200
 - 66s - loss: -2.1066e+00 - val_loss: -2.1153e+00
Epoch 70/200
 - 66s - loss: -2.1071e+00 - val_loss: -2.1149e+00
Epoch 71/200
 - 66s - loss: -2.1072e+00 - val_loss: -2.1159e+00
Epoch 72/200
 - 66s - loss: -2.1068e+00 - val_loss: -2.1134e+00
Epoch 73/200
 - 66s - loss: -2.1072e+00 - val_loss: -2.1154e+00
Epoch 74/200
 - 66s - loss: -2.1069e+00 - val_loss: -2.1152e+00
Epoch 75/200
 - 66s - loss: -2.1069e+00 - val_loss: -2.1140e+00
Epoch 76/200
 - 66s - loss: -2.1075e+00 - val_loss: -2.1141e+00
Epoch 77/200
 - 66s - loss: -2.1074e+00 - val_loss: -2.1140e+00
Epoch 78/200
 - 66s - loss: -2.1076e+00 - val_loss: -2.1155e+00
Epoch 79/200
 - 66s - loss: -2.1079e+00 - val_loss: -2.1147e+00
Epoch 80/200
 - 66s - loss: -2.1077e+00 - val_loss: -2.1146e+00
Epoch 81/200
 - 66s - loss: -2.1077e+00 - val_loss: -2.1146e+00
2019-12-26 05:39:23,530 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep5/ae_model_epoch_80.pickle
Epoch 82/200
 - 66s - loss: -2.1077e+00 - val_loss: -2.1144e+00
Epoch 83/200
 - 66s - loss: -2.1075e+00 - val_loss: -2.1152e+00
Epoch 84/200
 - 66s - loss: -2.1075e+00 - val_loss: -2.1149e+00
Epoch 85/200
 - 66s - loss: -2.1078e+00 - val_loss: -2.1148e+00
Epoch 86/200
 - 66s - loss: -2.1080e+00 - val_loss: -2.1143e+00
Epoch 87/200
 - 66s - loss: -2.1088e+00 - val_loss: -2.1149e+00
Epoch 88/200
 - 66s - loss: -2.1083e+00 - val_loss: -2.1147e+00
Epoch 89/200
 - 66s - loss: -2.1082e+00 - val_loss: -2.1155e+00
Epoch 90/200
 - 66s - loss: -2.1084e+00 - val_loss: -2.1148e+00
Epoch 91/200
 - 66s - loss: -2.1084e+00 - val_loss: -2.1149e+00
Epoch 92/200
 - 66s - loss: -2.1088e+00 - val_loss: -2.1147e+00
Epoch 93/200
 - 66s - loss: -2.1087e+00 - val_loss: -2.1146e+00
Epoch 94/200
 - 66s - loss: -2.1080e+00 - val_loss: -2.1134e+00
Epoch 95/200
 - 66s - loss: -2.1091e+00 - val_loss: -2.1144e+00
Epoch 96/200
 - 66s - loss: -2.1084e+00 - val_loss: -2.1154e+00
Epoch 97/200
 - 66s - loss: -2.1081e+00 - val_loss: -2.1141e+00
Epoch 98/200
 - 66s - loss: -2.1085e+00 - val_loss: -2.1145e+00
Epoch 99/200
 - 66s - loss: -2.1086e+00 - val_loss: -2.1143e+00
Epoch 100/200
 - 66s - loss: -2.1084e+00 - val_loss: -2.1126e+00
Epoch 101/200
 - 66s - loss: -2.1091e+00 - val_loss: -2.1142e+00
2019-12-26 06:01:19,736 [INFO] epoch = 100. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep5/ae_model_epoch_100.pickle
Epoch 102/200
 - 66s - loss: -2.1087e+00 - val_loss: -2.1141e+00
Epoch 103/200
 - 66s - loss: -2.1084e+00 - val_loss: -2.1150e+00
Epoch 104/200
 - 66s - loss: -2.1086e+00 - val_loss: -2.1144e+00
Epoch 105/200
 - 66s - loss: -2.1087e+00 - val_loss: -2.1145e+00
Epoch 106/200
 - 66s - loss: -2.1086e+00 - val_loss: -2.1155e+00
Epoch 107/200
 - 66s - loss: -2.1090e+00 - val_loss: -2.1163e+00
Epoch 108/200
 - 66s - loss: -2.1085e+00 - val_loss: -2.1150e+00
Epoch 109/200
 - 66s - loss: -2.1088e+00 - val_loss: -2.1149e+00
Epoch 110/200
 - 66s - loss: -2.1089e+00 - val_loss: -2.1147e+00
Epoch 111/200
 - 66s - loss: -2.1085e+00 - val_loss: -2.1155e+00
Epoch 112/200
 - 66s - loss: -2.1088e+00 - val_loss: -2.1158e+00
Epoch 113/200
 - 66s - loss: -2.1097e+00 - val_loss: -2.1146e+00
Epoch 114/200
 - 66s - loss: -2.1089e+00 - val_loss: -2.1147e+00
Epoch 115/200
 - 66s - loss: -2.1084e+00 - val_loss: -2.1149e+00
Epoch 116/200
 - 66s - loss: -2.1091e+00 - val_loss: -2.1144e+00
Epoch 117/200
 - 66s - loss: -2.1094e+00 - val_loss: -2.1152e+00
Epoch 118/200
 - 66s - loss: -2.1099e+00 - val_loss: -2.1151e+00
Epoch 119/200
 - 66s - loss: -2.1093e+00 - val_loss: -2.1149e+00
Epoch 120/200
 - 66s - loss: -2.1085e+00 - val_loss: -2.1152e+00
Epoch 121/200
 - 66s - loss: -2.1093e+00 - val_loss: -2.1169e+00
2019-12-26 06:23:18,114 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep5/ae_model_epoch_120.pickle
Epoch 122/200
 - 66s - loss: -2.1091e+00 - val_loss: -2.1161e+00
Epoch 123/200
 - 66s - loss: -2.1087e+00 - val_loss: -2.1162e+00
Epoch 124/200
 - 66s - loss: -2.1097e+00 - val_loss: -2.1144e+00
Epoch 125/200
 - 66s - loss: -2.1095e+00 - val_loss: -2.1144e+00
Epoch 126/200
 - 66s - loss: -2.1092e+00 - val_loss: -2.1146e+00
Epoch 127/200
 - 66s - loss: -2.1096e+00 - val_loss: -2.1150e+00
Epoch 128/200
 - 66s - loss: -2.1095e+00 - val_loss: -2.1154e+00
Epoch 129/200
 - 66s - loss: -2.1096e+00 - val_loss: -2.1161e+00
Epoch 130/200
 - 66s - loss: -2.1093e+00 - val_loss: -2.1160e+00
Epoch 131/200
 - 66s - loss: -2.1099e+00 - val_loss: -2.1149e+00
Epoch 132/200
 - 66s - loss: -2.1094e+00 - val_loss: -2.1157e+00
Epoch 133/200
 - 66s - loss: -2.1098e+00 - val_loss: -2.1154e+00
Epoch 134/200
 - 66s - loss: -2.1097e+00 - val_loss: -2.1157e+00
Epoch 135/200
 - 66s - loss: -2.1099e+00 - val_loss: -2.1160e+00
Epoch 136/200
 - 66s - loss: -2.1094e+00 - val_loss: -2.1153e+00
Epoch 137/200
 - 66s - loss: -2.1097e+00 - val_loss: -2.1146e+00
Epoch 138/200
 - 66s - loss: -2.1098e+00 - val_loss: -2.1150e+00
Epoch 139/200
 - 67s - loss: -2.1097e+00 - val_loss: -2.1149e+00
Epoch 140/200
 - 66s - loss: -2.1098e+00 - val_loss: -2.1151e+00
Epoch 141/200
 - 66s - loss: -2.1101e+00 - val_loss: -2.1160e+00
2019-12-26 06:45:14,147 [INFO] epoch = 140. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep5/ae_model_epoch_140.pickle
Epoch 142/200
 - 66s - loss: -2.1101e+00 - val_loss: -2.1164e+00
Epoch 143/200
 - 66s - loss: -2.1093e+00 - val_loss: -2.1160e+00
Epoch 144/200
 - 66s - loss: -2.1090e+00 - val_loss: -2.1162e+00
Epoch 145/200
 - 66s - loss: -2.1103e+00 - val_loss: -2.1152e+00
Epoch 146/200
 - 66s - loss: -2.1094e+00 - val_loss: -2.1169e+00
Epoch 147/200
 - 66s - loss: -2.1100e+00 - val_loss: -2.1150e+00
Epoch 148/200
 - 66s - loss: -2.1095e+00 - val_loss: -2.1157e+00
Epoch 149/200
 - 66s - loss: -2.1096e+00 - val_loss: -2.1147e+00
Epoch 150/200
 - 66s - loss: -2.1099e+00 - val_loss: -2.1151e+00
Epoch 151/200
 - 66s - loss: -2.1098e+00 - val_loss: -2.1165e+00
Epoch 152/200
 - 66s - loss: -2.1094e+00 - val_loss: -2.1158e+00
Epoch 153/200
 - 66s - loss: -2.1100e+00 - val_loss: -2.1159e+00
Epoch 154/200
 - 66s - loss: -2.1103e+00 - val_loss: -2.1165e+00
Epoch 155/200
 - 66s - loss: -2.1102e+00 - val_loss: -2.1157e+00
Epoch 156/200
 - 66s - loss: -2.1094e+00 - val_loss: -2.1167e+00
Epoch 157/200
 - 66s - loss: -2.1099e+00 - val_loss: -2.1153e+00
Epoch 158/200
 - 66s - loss: -2.1107e+00 - val_loss: -2.1150e+00
Epoch 159/200
 - 66s - loss: -2.1094e+00 - val_loss: -2.1146e+00
Epoch 160/200
 - 66s - loss: -2.1094e+00 - val_loss: -2.1145e+00
Epoch 161/200
 - 66s - loss: -2.1096e+00 - val_loss: -2.1152e+00
2019-12-26 07:07:11,477 [INFO] epoch = 160. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep5/ae_model_epoch_160.pickle
Epoch 162/200
 - 66s - loss: -2.1102e+00 - val_loss: -2.1155e+00
Epoch 163/200
 - 66s - loss: -2.1093e+00 - val_loss: -2.1156e+00
Epoch 164/200
 - 66s - loss: -2.1098e+00 - val_loss: -2.1161e+00
Epoch 165/200
 - 66s - loss: -2.1097e+00 - val_loss: -2.1142e+00
Epoch 166/200
 - 66s - loss: -2.1103e+00 - val_loss: -2.1144e+00
Epoch 167/200
 - 66s - loss: -2.1099e+00 - val_loss: -2.1160e+00
Epoch 168/200
 - 66s - loss: -2.1103e+00 - val_loss: -2.1170e+00
Epoch 169/200
 - 66s - loss: -2.1103e+00 - val_loss: -2.1149e+00
Epoch 170/200
 - 66s - loss: -2.1107e+00 - val_loss: -2.1161e+00
Epoch 171/200
 - 66s - loss: -2.1100e+00 - val_loss: -2.1166e+00
Epoch 172/200
 - 66s - loss: -2.1102e+00 - val_loss: -2.1164e+00
Epoch 173/200
 - 66s - loss: -2.1101e+00 - val_loss: -2.1151e+00
Epoch 174/200
 - 66s - loss: -2.1099e+00 - val_loss: -2.1165e+00
Epoch 175/200
 - 66s - loss: -2.1100e+00 - val_loss: -2.1163e+00
Epoch 176/200
 - 66s - loss: -2.1105e+00 - val_loss: -2.1166e+00
Epoch 177/200
 - 66s - loss: -2.1095e+00 - val_loss: -2.1167e+00
Epoch 178/200
 - 66s - loss: -2.1104e+00 - val_loss: -2.1166e+00
Epoch 179/200
 - 66s - loss: -2.1105e+00 - val_loss: -2.1161e+00
Epoch 180/200
 - 66s - loss: -2.1098e+00 - val_loss: -2.1160e+00
Epoch 181/200
 - 66s - loss: -2.1108e+00 - val_loss: -2.1173e+00
2019-12-26 07:29:07,214 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep5/ae_model_epoch_180.pickle
Epoch 182/200
 - 66s - loss: -2.1099e+00 - val_loss: -2.1172e+00
Epoch 183/200
 - 66s - loss: -2.1106e+00 - val_loss: -2.1163e+00
Epoch 184/200
 - 66s - loss: -2.1096e+00 - val_loss: -2.1170e+00
Epoch 185/200
 - 66s - loss: -2.1094e+00 - val_loss: -2.1160e+00
Epoch 186/200
 - 66s - loss: -2.1107e+00 - val_loss: -2.1160e+00
Epoch 187/200
 - 66s - loss: -2.1101e+00 - val_loss: -2.1166e+00
Epoch 188/200
 - 66s - loss: -2.1108e+00 - val_loss: -2.1164e+00
Epoch 189/200
 - 66s - loss: -2.1104e+00 - val_loss: -2.1155e+00
Epoch 190/200
 - 66s - loss: -2.1100e+00 - val_loss: -2.1163e+00
Epoch 191/200
 - 66s - loss: -2.1105e+00 - val_loss: -2.1165e+00
Epoch 192/200
 - 66s - loss: -2.1106e+00 - val_loss: -2.1164e+00
Epoch 193/200
 - 66s - loss: -2.1104e+00 - val_loss: -2.1168e+00
Epoch 194/200
 - 66s - loss: -2.1106e+00 - val_loss: -2.1169e+00
Epoch 195/200
 - 66s - loss: -2.1109e+00 - val_loss: -2.1172e+00
Epoch 196/200
 - 66s - loss: -2.1100e+00 - val_loss: -2.1159e+00
Epoch 197/200
 - 66s - loss: -2.1109e+00 - val_loss: -2.1169e+00
Epoch 198/200
 - 66s - loss: -2.1105e+00 - val_loss: -2.1163e+00
Epoch 199/200
 - 66s - loss: -2.1100e+00 - val_loss: -2.1156e+00
Epoch 200/200
 - 66s - loss: -2.1099e+00 - val_loss: -2.1168e+00
2019-12-26 07:49:59,430 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-26 07:52:40,233 [INFO] Last epoch loss evaluation: train_loss = -2.120721, val_loss = -2.117321
2019-12-26 07:52:40,233 [INFO] Training autoencoder complete
2019-12-26 07:52:40,233 [INFO] Encoding data for supervised training
2019-12-26 07:56:15,002 [INFO] Encoding complete
2019-12-26 07:56:15,002 [INFO] Training neural network layers (after autoencoder)
Train on 2939058 samples, validate on 979687 samples
Epoch 1/200
 - 55s - loss: 0.0050 - val_loss: 0.0012
 - val_f1: 0.9993
Epoch 2/200
 - 53s - loss: 0.0013 - val_loss: 0.0011
 - val_f1: 0.9994
Epoch 3/200
 - 53s - loss: 0.0011 - val_loss: 9.6819e-04
 - val_f1: 0.9994
Epoch 4/200
 - 53s - loss: 0.0011 - val_loss: 8.5143e-04
 - val_f1: 0.9995
Epoch 5/200
 - 53s - loss: 9.6993e-04 - val_loss: 0.0010
 - val_f1: 0.9991
Epoch 6/200
 - 53s - loss: 9.4119e-04 - val_loss: 8.3580e-04
 - val_f1: 0.9995
Epoch 7/200
 - 53s - loss: 9.0133e-04 - val_loss: 0.0012
 - val_f1: 0.9993
Epoch 8/200
 - 53s - loss: 8.9276e-04 - val_loss: 0.0012
 - val_f1: 0.9993
Epoch 9/200
 - 53s - loss: 8.6627e-04 - val_loss: 8.1201e-04
 - val_f1: 0.9994
Epoch 10/200
 - 53s - loss: 8.5724e-04 - val_loss: 8.4011e-04
 - val_f1: 0.9994
Epoch 11/200
 - 53s - loss: 8.3439e-04 - val_loss: 7.0772e-04
 - val_f1: 0.9996
Epoch 12/200
 - 53s - loss: 8.3114e-04 - val_loss: 8.3065e-04
 - val_f1: 0.9993
Epoch 13/200
 - 53s - loss: 8.0574e-04 - val_loss: 6.9370e-04
 - val_f1: 0.9996
Epoch 14/200
 - 53s - loss: 8.0120e-04 - val_loss: 7.1680e-04
 - val_f1: 0.9995
Epoch 15/200
 - 53s - loss: 7.8413e-04 - val_loss: 6.9021e-04
 - val_f1: 0.9995
Epoch 16/200
 - 53s - loss: 7.7886e-04 - val_loss: 7.7666e-04
 - val_f1: 0.9996
Epoch 17/200
 - 53s - loss: 7.7832e-04 - val_loss: 7.5631e-04
 - val_f1: 0.9996
Epoch 18/200
 - 53s - loss: 7.4970e-04 - val_loss: 7.0566e-04
 - val_f1: 0.9996
Epoch 19/200
 - 53s - loss: 7.6139e-04 - val_loss: 6.5121e-04
 - val_f1: 0.9996
Epoch 20/200
 - 53s - loss: 7.4672e-04 - val_loss: 6.7545e-04
 - val_f1: 0.9995
Epoch 21/200
 - 53s - loss: 7.5011e-04 - val_loss: 7.6527e-04
2019-12-26 08:30:16,071 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep5/ann_model_epoch_20.pickle
 - val_f1: 0.9995
Epoch 22/200
 - 53s - loss: 7.4614e-04 - val_loss: 6.9334e-04
 - val_f1: 0.9996
Epoch 23/200
 - 53s - loss: 7.3620e-04 - val_loss: 7.0649e-04
 - val_f1: 0.9996
Epoch 24/200
 - 53s - loss: 7.2913e-04 - val_loss: 7.0551e-04
 - val_f1: 0.9995
Epoch 25/200
 - 53s - loss: 7.3032e-04 - val_loss: 6.4983e-04
 - val_f1: 0.9996
Epoch 26/200
 - 53s - loss: 7.2492e-04 - val_loss: 7.8589e-04
 - val_f1: 0.9995
Epoch 27/200
 - 53s - loss: 7.1896e-04 - val_loss: 8.2158e-04
 - val_f1: 0.9996
Epoch 28/200
 - 53s - loss: 7.0866e-04 - val_loss: 6.8151e-04
 - val_f1: 0.9996
Epoch 29/200
 - 53s - loss: 6.9863e-04 - val_loss: 7.9618e-04
 - val_f1: 0.9995
Epoch 30/200
 - 53s - loss: 7.1392e-04 - val_loss: 9.5947e-04
 - val_f1: 0.9994
Epoch 31/200
 - 53s - loss: 6.9971e-04 - val_loss: 7.1977e-04
 - val_f1: 0.9996
Epoch 32/200
 - 53s - loss: 6.8725e-04 - val_loss: 8.2114e-04
 - val_f1: 0.9995
Epoch 33/200
 - 53s - loss: 7.0421e-04 - val_loss: 6.9085e-04
 - val_f1: 0.9995
Epoch 34/200
 - 53s - loss: 6.9816e-04 - val_loss: 7.3784e-04
 - val_f1: 0.9996
Epoch 35/200
 - 53s - loss: 7.0054e-04 - val_loss: 6.9609e-04
 - val_f1: 0.9996
Epoch 36/200
 - 53s - loss: 6.8337e-04 - val_loss: 7.5999e-04
 - val_f1: 0.9996
Epoch 37/200
 - 53s - loss: 6.8409e-04 - val_loss: 6.5964e-04
 - val_f1: 0.9996
Epoch 38/200
 - 53s - loss: 6.8522e-04 - val_loss: 6.7412e-04
 - val_f1: 0.9996
Epoch 39/200
 - 53s - loss: 6.7430e-04 - val_loss: 6.7981e-04
 - val_f1: 0.9996
Epoch 40/200
 - 53s - loss: 6.8382e-04 - val_loss: 7.6073e-04
 - val_f1: 0.9996
Epoch 41/200
 - 53s - loss: 6.8725e-04 - val_loss: 6.8383e-04
2019-12-26 09:03:02,513 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep5/ann_model_epoch_40.pickle
 - val_f1: 0.9996
Epoch 42/200
 - 53s - loss: 6.8701e-04 - val_loss: 6.7365e-04
 - val_f1: 0.9996
Epoch 43/200
 - 53s - loss: 6.8940e-04 - val_loss: 7.3209e-04
 - val_f1: 0.9996
Epoch 44/200
 - 53s - loss: 6.6030e-04 - val_loss: 6.7902e-04
 - val_f1: 0.9996
Epoch 45/200
 - 53s - loss: 6.6259e-04 - val_loss: 7.8767e-04
 - val_f1: 0.9994
Epoch 46/200
 - 53s - loss: 6.5227e-04 - val_loss: 6.6146e-04
 - val_f1: 0.9997
Epoch 47/200
 - 53s - loss: 6.5359e-04 - val_loss: 6.9114e-04
 - val_f1: 0.9996
Epoch 48/200
 - 53s - loss: 6.5985e-04 - val_loss: 6.3605e-04
 - val_f1: 0.9996
Epoch 49/200
 - 53s - loss: 6.6067e-04 - val_loss: 7.2994e-04
 - val_f1: 0.9996
Epoch 50/200
 - 53s - loss: 6.6029e-04 - val_loss: 6.5737e-04
 - val_f1: 0.9996
Epoch 51/200
 - 53s - loss: 6.6683e-04 - val_loss: 6.2069e-04
 - val_f1: 0.9997
Epoch 52/200
 - 53s - loss: 6.5906e-04 - val_loss: 6.7717e-04
 - val_f1: 0.9996
Epoch 53/200
 - 53s - loss: 6.4374e-04 - val_loss: 7.6583e-04
 - val_f1: 0.9996
Epoch 54/200
 - 53s - loss: 6.5211e-04 - val_loss: 6.3187e-04
 - val_f1: 0.9996
Epoch 55/200
 - 53s - loss: 6.4820e-04 - val_loss: 6.3442e-04
 - val_f1: 0.9996
Epoch 56/200
 - 53s - loss: 6.3218e-04 - val_loss: 6.4722e-04
 - val_f1: 0.9996
Epoch 57/200
 - 53s - loss: 6.3906e-04 - val_loss: 5.9020e-04
 - val_f1: 0.9997
Epoch 58/200
 - 53s - loss: 6.3663e-04 - val_loss: 6.2644e-04
 - val_f1: 0.9996
Epoch 59/200
 - 53s - loss: 6.3373e-04 - val_loss: 6.2895e-04
 - val_f1: 0.9996
Epoch 60/200
 - 53s - loss: 6.4247e-04 - val_loss: 6.3638e-04
 - val_f1: 0.9996
Epoch 61/200
 - 53s - loss: 6.3983e-04 - val_loss: 6.4461e-04
2019-12-26 09:35:47,471 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep5/ann_model_epoch_60.pickle
 - val_f1: 0.9996
Epoch 62/200
 - 53s - loss: 6.2964e-04 - val_loss: 6.1708e-04
 - val_f1: 0.9997
Epoch 63/200
 - 53s - loss: 6.2765e-04 - val_loss: 6.8737e-04
 - val_f1: 0.9996
Epoch 64/200
 - 53s - loss: 6.2455e-04 - val_loss: 7.1646e-04
 - val_f1: 0.9995
Epoch 65/200
 - 53s - loss: 6.2429e-04 - val_loss: 7.2808e-04
 - val_f1: 0.9995
Epoch 66/200
 - 53s - loss: 6.1791e-04 - val_loss: 6.6809e-04
 - val_f1: 0.9995
Epoch 67/200
 - 53s - loss: 6.1521e-04 - val_loss: 6.3660e-04
 - val_f1: 0.9996
Epoch 68/200
 - 53s - loss: 6.2000e-04 - val_loss: 6.1633e-04
 - val_f1: 0.9996
Epoch 69/200
 - 53s - loss: 6.0663e-04 - val_loss: 6.2351e-04
 - val_f1: 0.9996
Epoch 70/200
 - 53s - loss: 6.2109e-04 - val_loss: 5.8935e-04
 - val_f1: 0.9996
Epoch 71/200
 - 53s - loss: 6.1666e-04 - val_loss: 6.4839e-04
 - val_f1: 0.9996
Epoch 72/200
 - 53s - loss: 6.1790e-04 - val_loss: 6.5310e-04
 - val_f1: 0.9996
Epoch 73/200
 - 53s - loss: 6.0542e-04 - val_loss: 6.8972e-04
 - val_f1: 0.9997
Epoch 74/200
 - 53s - loss: 6.1909e-04 - val_loss: 6.0821e-04
 - val_f1: 0.9996
Epoch 75/200
 - 53s - loss: 6.0732e-04 - val_loss: 5.6745e-04
 - val_f1: 0.9996
Epoch 76/200
 - 53s - loss: 6.1105e-04 - val_loss: 6.1671e-04
 - val_f1: 0.9996
Epoch 77/200
 - 53s - loss: 5.9140e-04 - val_loss: 6.1428e-04
 - val_f1: 0.9996
Epoch 78/200
 - 53s - loss: 6.0115e-04 - val_loss: 6.5970e-04
 - val_f1: 0.9996
Epoch 79/200
 - 53s - loss: 6.0094e-04 - val_loss: 6.7383e-04
 - val_f1: 0.9996
Epoch 80/200
 - 53s - loss: 6.0105e-04 - val_loss: 6.6401e-04
 - val_f1: 0.9996
Epoch 81/200
 - 53s - loss: 5.9909e-04 - val_loss: 6.4790e-04
2019-12-26 10:08:31,618 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep5/ann_model_epoch_80.pickle
 - val_f1: 0.9996
Epoch 82/200
 - 53s - loss: 5.9731e-04 - val_loss: 6.9053e-04
 - val_f1: 0.9996
Epoch 83/200
 - 53s - loss: 6.0450e-04 - val_loss: 5.9238e-04
 - val_f1: 0.9997
Epoch 84/200
 - 53s - loss: 5.9767e-04 - val_loss: 6.6178e-04
 - val_f1: 0.9996
Epoch 85/200
 - 53s - loss: 5.9312e-04 - val_loss: 5.7710e-04
 - val_f1: 0.9997
Epoch 86/200
 - 53s - loss: 6.1073e-04 - val_loss: 6.4719e-04
 - val_f1: 0.9997
Epoch 87/200
 - 53s - loss: 5.8088e-04 - val_loss: 5.9159e-04
 - val_f1: 0.9997
Epoch 88/200
 - 53s - loss: 5.9653e-04 - val_loss: 5.9554e-04
 - val_f1: 0.9997
Epoch 89/200
 - 53s - loss: 5.9942e-04 - val_loss: 5.7771e-04
 - val_f1: 0.9997
Epoch 90/200
 - 53s - loss: 5.8842e-04 - val_loss: 6.1390e-04
 - val_f1: 0.9997
Epoch 91/200
 - 53s - loss: 6.0325e-04 - val_loss: 5.7207e-04
 - val_f1: 0.9997
Epoch 92/200
 - 53s - loss: 5.9354e-04 - val_loss: 6.6147e-04
 - val_f1: 0.9996
Epoch 93/200
 - 53s - loss: 6.0148e-04 - val_loss: 5.9577e-04
 - val_f1: 0.9997
Epoch 94/200
 - 53s - loss: 6.0236e-04 - val_loss: 5.8964e-04
 - val_f1: 0.9997
Epoch 95/200
 - 53s - loss: 5.9370e-04 - val_loss: 6.1327e-04
 - val_f1: 0.9996
Epoch 96/200
 - 53s - loss: 5.8043e-04 - val_loss: 6.3332e-04
 - val_f1: 0.9997
Epoch 97/200
 - 53s - loss: 5.8202e-04 - val_loss: 5.6309e-04
 - val_f1: 0.9997
Epoch 98/200
 - 53s - loss: 5.8422e-04 - val_loss: 5.8259e-04
 - val_f1: 0.9997
Epoch 99/200
 - 53s - loss: 5.8865e-04 - val_loss: 5.4227e-04
 - val_f1: 0.9997
Epoch 100/200
 - 53s - loss: 5.7626e-04 - val_loss: 6.5437e-04
 - val_f1: 0.9996
Epoch 101/200
 - 53s - loss: 5.7393e-04 - val_loss: 6.9831e-04
2019-12-26 10:41:18,623 [INFO] epoch = 100. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep5/ann_model_epoch_100.pickle
 - val_f1: 0.9996
Epoch 102/200
 - 53s - loss: 5.8703e-04 - val_loss: 5.9945e-04
 - val_f1: 0.9997
Epoch 103/200
 - 53s - loss: 5.7040e-04 - val_loss: 5.7999e-04
 - val_f1: 0.9996
Epoch 104/200
 - 53s - loss: 5.7363e-04 - val_loss: 5.6678e-04
 - val_f1: 0.9997
Epoch 105/200
 - 53s - loss: 5.7209e-04 - val_loss: 5.8344e-04
 - val_f1: 0.9997
Epoch 106/200
 - 53s - loss: 5.6687e-04 - val_loss: 5.7015e-04
 - val_f1: 0.9997
Epoch 107/200
 - 53s - loss: 5.6258e-04 - val_loss: 6.0252e-04
 - val_f1: 0.9996
Epoch 108/200
 - 53s - loss: 5.6874e-04 - val_loss: 5.9576e-04
 - val_f1: 0.9996
Epoch 109/200
 - 53s - loss: 5.7574e-04 - val_loss: 5.8817e-04
 - val_f1: 0.9997
Epoch 110/200
 - 53s - loss: 5.7519e-04 - val_loss: 5.9242e-04
 - val_f1: 0.9997
Epoch 111/200
 - 53s - loss: 5.7995e-04 - val_loss: 5.3207e-04
 - val_f1: 0.9997
Epoch 112/200
 - 53s - loss: 5.6748e-04 - val_loss: 5.7655e-04
 - val_f1: 0.9997
Epoch 113/200
 - 53s - loss: 5.5433e-04 - val_loss: 5.6399e-04
 - val_f1: 0.9997
Epoch 114/200
 - 53s - loss: 5.6917e-04 - val_loss: 5.8272e-04
 - val_f1: 0.9996
Epoch 115/200
 - 53s - loss: 5.6205e-04 - val_loss: 5.3264e-04
 - val_f1: 0.9997
Epoch 116/200
 - 53s - loss: 5.5571e-04 - val_loss: 5.5867e-04
 - val_f1: 0.9997
Epoch 117/200
 - 53s - loss: 5.5745e-04 - val_loss: 5.6848e-04
 - val_f1: 0.9997
Epoch 118/200
 - 53s - loss: 5.6955e-04 - val_loss: 5.8589e-04
 - val_f1: 0.9997
Epoch 119/200
 - 53s - loss: 5.6080e-04 - val_loss: 5.6360e-04
 - val_f1: 0.9997
Epoch 120/200
 - 53s - loss: 5.6672e-04 - val_loss: 5.4600e-04
 - val_f1: 0.9997
Epoch 121/200
 - 53s - loss: 5.4872e-04 - val_loss: 5.5819e-04
2019-12-26 11:14:05,578 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep5/ann_model_epoch_120.pickle
 - val_f1: 0.9997
Epoch 122/200
 - 53s - loss: 5.6740e-04 - val_loss: 6.2863e-04
 - val_f1: 0.9997
Epoch 123/200
 - 53s - loss: 5.4282e-04 - val_loss: 5.4585e-04
 - val_f1: 0.9997
Epoch 124/200
 - 53s - loss: 5.5350e-04 - val_loss: 5.4733e-04
 - val_f1: 0.9997
Epoch 125/200
 - 53s - loss: 5.5821e-04 - val_loss: 5.4995e-04
 - val_f1: 0.9997
Epoch 126/200
 - 53s - loss: 5.6529e-04 - val_loss: 5.3859e-04
 - val_f1: 0.9997
Epoch 127/200
 - 53s - loss: 5.5997e-04 - val_loss: 5.7206e-04
 - val_f1: 0.9997
Epoch 128/200
 - 53s - loss: 5.5380e-04 - val_loss: 5.2423e-04
 - val_f1: 0.9997
Epoch 129/200
 - 53s - loss: 5.4389e-04 - val_loss: 5.6667e-04
 - val_f1: 0.9997
Epoch 130/200
 - 53s - loss: 5.6561e-04 - val_loss: 5.8186e-04
 - val_f1: 0.9997
Epoch 131/200
 - 53s - loss: 5.5709e-04 - val_loss: 5.5040e-04
 - val_f1: 0.9997
Epoch 132/200
 - 53s - loss: 5.5771e-04 - val_loss: 5.8944e-04
 - val_f1: 0.9997
Epoch 133/200
 - 53s - loss: 5.5020e-04 - val_loss: 5.6611e-04
 - val_f1: 0.9997
Epoch 134/200
 - 53s - loss: 5.4085e-04 - val_loss: 5.1776e-04
 - val_f1: 0.9997
Epoch 135/200
 - 53s - loss: 5.4259e-04 - val_loss: 5.4316e-04
 - val_f1: 0.9997
Epoch 136/200
 - 53s - loss: 5.5391e-04 - val_loss: 5.0473e-04
 - val_f1: 0.9997
Epoch 137/200
 - 53s - loss: 5.3468e-04 - val_loss: 5.4213e-04
 - val_f1: 0.9997
Epoch 138/200
 - 53s - loss: 5.3387e-04 - val_loss: 5.1265e-04
 - val_f1: 0.9997
Epoch 139/200
 - 53s - loss: 5.3509e-04 - val_loss: 7.2939e-04
 - val_f1: 0.9995
Epoch 140/200
 - 53s - loss: 5.4053e-04 - val_loss: 5.8458e-04
 - val_f1: 0.9997
Epoch 141/200
 - 53s - loss: 5.4316e-04 - val_loss: 5.5421e-04
2019-12-26 11:46:56,831 [INFO] epoch = 140. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep5/ann_model_epoch_140.pickle
 - val_f1: 0.9997
Epoch 142/200
 - 53s - loss: 5.3041e-04 - val_loss: 5.4045e-04
 - val_f1: 0.9997
Epoch 143/200
 - 53s - loss: 5.4452e-04 - val_loss: 5.4486e-04
 - val_f1: 0.9997
Epoch 144/200
 - 53s - loss: 5.3112e-04 - val_loss: 5.2134e-04
 - val_f1: 0.9997
Epoch 145/200
 - 53s - loss: 5.5213e-04 - val_loss: 5.5256e-04
 - val_f1: 0.9997
Epoch 146/200
 - 53s - loss: 5.7483e-04 - val_loss: 5.3776e-04
 - val_f1: 0.9997
Epoch 147/200
 - 53s - loss: 5.5117e-04 - val_loss: 5.0002e-04
 - val_f1: 0.9997
Epoch 148/200
 - 53s - loss: 5.4200e-04 - val_loss: 5.3759e-04
 - val_f1: 0.9997
Epoch 149/200
 - 53s - loss: 5.4474e-04 - val_loss: 5.6319e-04
 - val_f1: 0.9997
Epoch 150/200
 - 53s - loss: 5.5636e-04 - val_loss: 5.3850e-04
 - val_f1: 0.9997
Epoch 151/200
 - 53s - loss: 5.3718e-04 - val_loss: 5.5062e-04
 - val_f1: 0.9997
Epoch 152/200
 - 53s - loss: 5.3598e-04 - val_loss: 5.7463e-04
 - val_f1: 0.9995
Epoch 153/200
 - 53s - loss: 5.4475e-04 - val_loss: 5.3761e-04
 - val_f1: 0.9997
Epoch 154/200
 - 53s - loss: 5.4096e-04 - val_loss: 5.5979e-04
 - val_f1: 0.9997
Epoch 155/200
 - 53s - loss: 5.4748e-04 - val_loss: 5.2094e-04
 - val_f1: 0.9997
Epoch 156/200
 - 53s - loss: 5.4014e-04 - val_loss: 5.4526e-04
 - val_f1: 0.9997
Epoch 157/200
 - 53s - loss: 5.3579e-04 - val_loss: 5.4019e-04
 - val_f1: 0.9997
Epoch 158/200
 - 53s - loss: 5.4293e-04 - val_loss: 5.4170e-04
 - val_f1: 0.9997
Epoch 159/200
 - 53s - loss: 5.4468e-04 - val_loss: 5.0490e-04
 - val_f1: 0.9997
Epoch 160/200
 - 51s - loss: 5.3016e-04 - val_loss: 5.5284e-04
 - val_f1: 0.9997
Epoch 161/200
 - 53s - loss: 5.3432e-04 - val_loss: 7.9587e-04
2019-12-26 12:19:39,618 [INFO] epoch = 160. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep5/ann_model_epoch_160.pickle
 - val_f1: 0.9997
Epoch 162/200
 - 53s - loss: 5.2680e-04 - val_loss: 5.0586e-04
 - val_f1: 0.9997
Epoch 163/200
 - 53s - loss: 5.3732e-04 - val_loss: 5.3072e-04
 - val_f1: 0.9997
Epoch 164/200
 - 53s - loss: 5.2608e-04 - val_loss: 5.3263e-04
 - val_f1: 0.9997
Epoch 165/200
 - 53s - loss: 5.2028e-04 - val_loss: 6.7464e-04
 - val_f1: 0.9995
Epoch 166/200
 - 53s - loss: 5.4880e-04 - val_loss: 5.0503e-04
 - val_f1: 0.9997
Epoch 167/200
 - 53s - loss: 5.2365e-04 - val_loss: 5.6004e-04
 - val_f1: 0.9997
Epoch 168/200
 - 53s - loss: 5.3167e-04 - val_loss: 4.8919e-04
 - val_f1: 0.9997
Epoch 169/200
 - 53s - loss: 5.3879e-04 - val_loss: 5.0924e-04
 - val_f1: 0.9997
Epoch 170/200
 - 53s - loss: 5.2918e-04 - val_loss: 5.6314e-04
 - val_f1: 0.9997
Epoch 171/200
 - 53s - loss: 5.3789e-04 - val_loss: 5.3354e-04
 - val_f1: 0.9997
Epoch 172/200
 - 53s - loss: 5.3281e-04 - val_loss: 5.4497e-04
 - val_f1: 0.9997
Epoch 173/200
 - 53s - loss: 5.4615e-04 - val_loss: 5.4633e-04
 - val_f1: 0.9997
Epoch 174/200
 - 53s - loss: 5.0997e-04 - val_loss: 5.1661e-04
 - val_f1: 0.9997
Epoch 175/200
 - 53s - loss: 5.1423e-04 - val_loss: 5.5416e-04
 - val_f1: 0.9997
Epoch 176/200
 - 53s - loss: 5.1317e-04 - val_loss: 5.4187e-04
 - val_f1: 0.9997
Epoch 177/200
 - 53s - loss: 5.3133e-04 - val_loss: 4.9238e-04
 - val_f1: 0.9997
Epoch 178/200
 - 53s - loss: 5.2597e-04 - val_loss: 5.5625e-04
 - val_f1: 0.9997
Epoch 179/200
 - 53s - loss: 5.2674e-04 - val_loss: 5.4608e-04
 - val_f1: 0.9997
Epoch 180/200
 - 53s - loss: 5.1829e-04 - val_loss: 5.3086e-04
 - val_f1: 0.9997
Epoch 181/200
 - 53s - loss: 5.3056e-04 - val_loss: 5.3519e-04
2019-12-26 12:52:34,033 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep5/ann_model_epoch_180.pickle
 - val_f1: 0.9997
Epoch 182/200
 - 53s - loss: 5.2353e-04 - val_loss: 5.6458e-04
 - val_f1: 0.9997
Epoch 183/200
 - 53s - loss: 5.2668e-04 - val_loss: 5.1313e-04
 - val_f1: 0.9997
Epoch 184/200
 - 53s - loss: 5.2556e-04 - val_loss: 5.4765e-04
 - val_f1: 0.9997
Epoch 185/200
 - 53s - loss: 5.2234e-04 - val_loss: 5.6122e-04
 - val_f1: 0.9997
Epoch 186/200
 - 53s - loss: 5.1596e-04 - val_loss: 5.6802e-04
 - val_f1: 0.9997
Epoch 187/200
 - 53s - loss: 5.1621e-04 - val_loss: 5.3200e-04
 - val_f1: 0.9997
Epoch 188/200
 - 53s - loss: 5.2427e-04 - val_loss: 5.1896e-04
 - val_f1: 0.9997
Epoch 189/200
 - 53s - loss: 5.1142e-04 - val_loss: 5.0367e-04
 - val_f1: 0.9997
Epoch 190/200
 - 53s - loss: 5.1652e-04 - val_loss: 5.2377e-04
 - val_f1: 0.9997
Epoch 191/200
 - 53s - loss: 5.1286e-04 - val_loss: 5.5832e-04
 - val_f1: 0.9997
Epoch 192/200
 - 53s - loss: 5.1386e-04 - val_loss: 5.8597e-04
 - val_f1: 0.9997
Epoch 193/200
 - 53s - loss: 5.1675e-04 - val_loss: 5.1704e-04
 - val_f1: 0.9997
Epoch 194/200
 - 53s - loss: 5.1238e-04 - val_loss: 4.8905e-04
 - val_f1: 0.9997
Epoch 195/200
 - 53s - loss: 5.0973e-04 - val_loss: 5.3861e-04
 - val_f1: 0.9997
Epoch 196/200
 - 53s - loss: 5.2547e-04 - val_loss: 5.1412e-04
 - val_f1: 0.9997
Epoch 197/200
 - 53s - loss: 5.2087e-04 - val_loss: 5.0255e-04
 - val_f1: 0.9997
Epoch 198/200
 - 53s - loss: 5.1453e-04 - val_loss: 5.6925e-04
 - val_f1: 0.9996
Epoch 199/200
 - 53s - loss: 5.2684e-04 - val_loss: 5.3841e-04
 - val_f1: 0.9997
Epoch 200/200
 - 53s - loss: 5.1808e-04 - val_loss: 5.2084e-04
2019-12-26 13:24:26,151 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-26 13:27:36,258 [INFO] Last epoch loss evaluation: train_loss = 0.000373, val_loss = 0.000489
2019-12-26 13:27:36,280 [INFO] Training complete. time_to_train = 33498.14 sec, 558.30 min
2019-12-26 13:27:36,313 [INFO] Model saved to results_selected_models/selected_kdd99_ae_ann_deep_rep5/best_model.pickle
/home/sunanda/research/ids_experiments/utility.py:268: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
2019-12-26 13:27:36,498 [INFO] Plot saved to: results_selected_models/selected_kdd99_ae_ann_deep_rep5/training_error_history.png
/home/sunanda/research/ids_experiments/utility.py:285: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
2019-12-26 13:27:37,270 [INFO] Plot saved to: results_selected_models/selected_kdd99_ae_ann_deep_rep5/training_f1_history.png
2019-12-26 13:27:37,270 [INFO] Making predictions on training, validation, testing data
2019-12-26 13:36:19,797 [INFO] Evaluating predictions (results)
2019-12-26 13:36:28,462 [INFO] Dataset: Testing. Classification report below
2019-12-26 13:36:28,462 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      0.97      0.98    229853
     normal.       0.73      0.98      0.84     60593
       probe       0.67      0.74      0.70      4166
         r2l       0.98      0.03      0.07     13781
         u2r       0.62      0.00      0.01      2636

    accuracy                           0.92    311029
   macro avg       0.80      0.55      0.52    311029
weighted avg       0.94      0.92      0.90    311029

2019-12-26 13:36:28,462 [INFO] Overall accuracy (micro avg): 0.9211488317809593
2019-12-26 13:36:37,767 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9211         0.9211                       0.9211                0.0197                   0.0789  0.9211
1     Macro avg        0.9685         0.7987                       0.5465                0.0203                   0.4535  0.5202
2  Weighted avg        0.9664         0.9363                       0.9211                0.0225                   0.0789  0.9028
2019-12-26 13:37:08,078 [INFO] Dataset: Validation. Classification report below
2019-12-26 13:37:08,079 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00    776675
     normal.       1.00      1.00      1.00    194556
       probe       0.99      0.99      0.99      8221
         r2l       0.87      0.77      0.82       225
         u2r       0.29      0.20      0.24        10

    accuracy                           1.00    979687
   macro avg       0.83      0.79      0.81    979687
weighted avg       1.00      1.00      1.00    979687

2019-12-26 13:37:08,079 [INFO] Overall accuracy (micro avg): 0.9997213395706996
2019-12-26 13:37:40,778 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9997         0.9997                       0.9997                0.0001                   0.0003  0.9997
1     Macro avg        0.9999         0.8282                       0.7929                0.0001                   0.2071  0.8086
2  Weighted avg        0.9999         0.9997                       0.9997                0.0002                   0.0003  0.9997
2019-12-26 13:39:54,387 [INFO] Dataset: Training. Classification report below
2019-12-26 13:39:54,387 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00   3106695
     normal.       1.00      1.00      1.00    778225
       probe       0.99      0.99      0.99     32881
         r2l       0.86      0.79      0.83       901
         u2r       0.67      0.43      0.52        42

    accuracy                           1.00   3918744
   macro avg       0.90      0.84      0.87   3918744
weighted avg       1.00      1.00      1.00   3918744

2019-12-26 13:39:54,387 [INFO] Overall accuracy (micro avg): 0.9997437954609947
2019-12-26 13:42:18,591 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9997         0.9997                       0.9997                0.0001                   0.0003  0.9997
1     Macro avg        0.9999         0.9044                       0.8428                0.0001                   0.1572  0.8681
2  Weighted avg        0.9999         0.9997                       0.9997                0.0002                   0.0003  0.9997
2019-12-26 13:42:18,638 [INFO] Results saved to: results_selected_models/selected_kdd99_ae_ann_deep_rep5/selected_kdd99_ae_ann_deep_rep5_results.xlsx
2019-12-26 13:42:18,645 [INFO] ================= Finished running experiment no. 5 ================= 

2019-12-26 13:42:18,669 [INFO] Created directory: results_selected_models/selected_ids18_subset_ae_ann_deep_rep1
2019-12-26 13:42:18,670 [INFO] Initialized logging. log_filename = results_selected_models/selected_ids18_subset_ae_ann_deep_rep1/run_log.log
2019-12-26 13:42:18,670 [INFO] ================= Running experiment no. 1  ================= 

2019-12-26 13:42:18,670 [INFO] Experiment parameters given below
2019-12-26 13:42:18,670 [INFO] 
{'experiment_num': 1, 'results_dir': 'results_selected_models/selected_ids18_subset_ae_ann_deep_rep1', 'model_type': 'classifier', 'model': 'ae_ann', 'normal_label': 'BENIGN', 'scaling_type': 'NA', 'unsupervised_ratio': 0.25, 'ae_encoder_units': [128, 64, 32], 'ae_encoder_activations': ['relu', 'relu', 'relu'], 'ae_encoder_dropout_rates': [0.2, 0.2, 0.2], 'ae_encoder_l1_param': -1, 'ae_decoder_units': [64, 128], 'ae_decoder_activations': ['relu', 'relu'], 'ae_decoder_dropout_rates': [0.2, 0.2], 'output_activation': 'sigmoid', 'loss_function': 'binary_crossentropy', 'ae_epochs': 200, 'ann_layer_units': [64], 'ann_layer_activations': ['relu'], 'ann_layer_dropout_rates': [0.2], 'ann_epochs': 200, 'early_stop_patience': 50, 'batch_size': 256, 'dataset_dir': '../Datasets/small_datasets/ids2018', 'description': 'selected_ids18_subset_ae_ann_deep_rep1'}
2019-12-26 13:42:18,670 [INFO] Created tensorboard log directory: results_selected_models/selected_ids18_subset_ae_ann_deep_rep1/tf_logs_run_2019_12_26-13_42_18
2019-12-26 13:42:18,670 [INFO] Loading datsets from: ../Datasets/small_datasets/ids2018
2019-12-26 13:42:18,670 [INFO] Reading X, y files
2019-12-26 13:42:18,670 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_train.h5
2019-12-26 13:42:22,722 [INFO] Reading complete. time_to_read=4.05 seconds
2019-12-26 13:42:22,722 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_val.h5
2019-12-26 13:42:24,114 [INFO] Reading complete. time_to_read=1.39 seconds
2019-12-26 13:42:24,117 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_test.h5
2019-12-26 13:42:25,510 [INFO] Reading complete. time_to_read=1.39 seconds
2019-12-26 13:42:25,510 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_train.h5
2019-12-26 13:42:25,782 [INFO] Reading complete. time_to_read=0.27 seconds
2019-12-26 13:42:25,782 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_val.h5
2019-12-26 13:42:25,880 [INFO] Reading complete. time_to_read=0.10 seconds
2019-12-26 13:42:25,880 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_test.h5
2019-12-26 13:42:25,977 [INFO] Reading complete. time_to_read=0.10 seconds
2019-12-26 13:42:29,289 [INFO] Initializing model
2019-12-26 13:42:29,957 [INFO] _________________________________________________________________
2019-12-26 13:42:29,957 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-26 13:42:29,957 [INFO] =================================================================
2019-12-26 13:42:29,957 [INFO] dense_141 (Dense)            (None, 128)               9984      
2019-12-26 13:42:29,957 [INFO] _________________________________________________________________
2019-12-26 13:42:29,958 [INFO] batch_normalization_101 (Bat (None, 128)               512       
2019-12-26 13:42:29,958 [INFO] _________________________________________________________________
2019-12-26 13:42:29,958 [INFO] dropout_101 (Dropout)        (None, 128)               0         
2019-12-26 13:42:29,958 [INFO] _________________________________________________________________
2019-12-26 13:42:29,958 [INFO] dense_142 (Dense)            (None, 64)                8256      
2019-12-26 13:42:29,958 [INFO] _________________________________________________________________
2019-12-26 13:42:29,958 [INFO] batch_normalization_102 (Bat (None, 64)                256       
2019-12-26 13:42:29,958 [INFO] _________________________________________________________________
2019-12-26 13:42:29,958 [INFO] dropout_102 (Dropout)        (None, 64)                0         
2019-12-26 13:42:29,958 [INFO] _________________________________________________________________
2019-12-26 13:42:29,958 [INFO] dense_143 (Dense)            (None, 32)                2080      
2019-12-26 13:42:29,959 [INFO] _________________________________________________________________
2019-12-26 13:42:29,959 [INFO] batch_normalization_103 (Bat (None, 32)                128       
2019-12-26 13:42:29,959 [INFO] _________________________________________________________________
2019-12-26 13:42:29,959 [INFO] dropout_103 (Dropout)        (None, 32)                0         
2019-12-26 13:42:29,959 [INFO] _________________________________________________________________
2019-12-26 13:42:29,959 [INFO] dense_144 (Dense)            (None, 64)                2112      
2019-12-26 13:42:29,959 [INFO] _________________________________________________________________
2019-12-26 13:42:29,959 [INFO] batch_normalization_104 (Bat (None, 64)                256       
2019-12-26 13:42:29,959 [INFO] _________________________________________________________________
2019-12-26 13:42:29,959 [INFO] dropout_104 (Dropout)        (None, 64)                0         
2019-12-26 13:42:29,959 [INFO] _________________________________________________________________
2019-12-26 13:42:29,959 [INFO] dense_145 (Dense)            (None, 128)               8320      
2019-12-26 13:42:29,959 [INFO] _________________________________________________________________
2019-12-26 13:42:29,960 [INFO] batch_normalization_105 (Bat (None, 128)               512       
2019-12-26 13:42:29,960 [INFO] _________________________________________________________________
2019-12-26 13:42:29,960 [INFO] dropout_105 (Dropout)        (None, 128)               0         
2019-12-26 13:42:29,960 [INFO] _________________________________________________________________
2019-12-26 13:42:29,960 [INFO] dense_146 (Dense)            (None, 77)                9933      
2019-12-26 13:42:29,960 [INFO] =================================================================
2019-12-26 13:42:29,960 [INFO] Total params: 42,349
2019-12-26 13:42:29,960 [INFO] Trainable params: 41,517
2019-12-26 13:42:29,961 [INFO] Non-trainable params: 832
2019-12-26 13:42:29,961 [INFO] _________________________________________________________________
2019-12-26 13:42:30,120 [INFO] _________________________________________________________________
2019-12-26 13:42:30,121 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-26 13:42:30,121 [INFO] =================================================================
2019-12-26 13:42:30,121 [INFO] dense_147 (Dense)            (None, 64)                2112      
2019-12-26 13:42:30,121 [INFO] _________________________________________________________________
2019-12-26 13:42:30,121 [INFO] batch_normalization_106 (Bat (None, 64)                256       
2019-12-26 13:42:30,121 [INFO] _________________________________________________________________
2019-12-26 13:42:30,121 [INFO] dropout_106 (Dropout)        (None, 64)                0         
2019-12-26 13:42:30,121 [INFO] _________________________________________________________________
2019-12-26 13:42:30,121 [INFO] dense_148 (Dense)            (None, 15)                975       
2019-12-26 13:42:30,121 [INFO] =================================================================
2019-12-26 13:42:30,122 [INFO] Total params: 3,343
2019-12-26 13:42:30,122 [INFO] Trainable params: 3,215
2019-12-26 13:42:30,122 [INFO] Non-trainable params: 128
2019-12-26 13:42:30,122 [INFO] _________________________________________________________________
2019-12-26 13:42:30,122 [INFO] Training model
2019-12-26 13:42:30,122 [INFO] Splitting train set into 2 sets (unsupervised, supervised), random_seed = None
2019-12-26 13:43:03,082 [INFO] Split sizes (instances). total = 1936462, unsupervised = 484115, supervised = 1452347, unsupervised dataset hash = 77da23bb09506e9524e8bf200ee1e17872cf2b4a
2019-12-26 13:43:03,082 [INFO] Training autoencoder
 - val_f1: 0.9997
Train on 484115 samples, validate on 645487 samples
Epoch 1/200
 - 36s - loss: -2.8956e+00 - val_loss: -3.5418e+00
Epoch 2/200
 - 32s - loss: -3.4760e+00 - val_loss: -3.5918e+00
Epoch 3/200
 - 32s - loss: -3.5227e+00 - val_loss: -3.6082e+00
Epoch 4/200
 - 32s - loss: -3.5432e+00 - val_loss: -3.6197e+00
Epoch 5/200
 - 32s - loss: -3.5566e+00 - val_loss: -3.6248e+00
Epoch 6/200
 - 32s - loss: -3.5660e+00 - val_loss: -3.6297e+00
Epoch 7/200
 - 32s - loss: -3.5729e+00 - val_loss: -3.6322e+00
Epoch 8/200
 - 32s - loss: -3.5786e+00 - val_loss: -3.6355e+00
Epoch 9/200
 - 32s - loss: -3.5822e+00 - val_loss: -3.6373e+00
Epoch 10/200
 - 32s - loss: -3.5874e+00 - val_loss: -3.6395e+00
Epoch 11/200
 - 32s - loss: -3.5894e+00 - val_loss: -3.6422e+00
Epoch 12/200
 - 32s - loss: -3.5924e+00 - val_loss: -3.6414e+00
Epoch 13/200
 - 32s - loss: -3.5944e+00 - val_loss: -3.6441e+00
Epoch 14/200
 - 32s - loss: -3.5959e+00 - val_loss: -3.6455e+00
Epoch 15/200
 - 32s - loss: -3.5985e+00 - val_loss: -3.6456e+00
Epoch 16/200
 - 32s - loss: -3.5995e+00 - val_loss: -3.6473e+00
Epoch 17/200
 - 32s - loss: -3.6008e+00 - val_loss: -3.6480e+00
Epoch 18/200
 - 32s - loss: -3.6022e+00 - val_loss: -3.6498e+00
Epoch 19/200
 - 32s - loss: -3.6039e+00 - val_loss: -3.6506e+00
Epoch 20/200
 - 32s - loss: -3.6044e+00 - val_loss: -3.6521e+00
Epoch 21/200
 - 32s - loss: -3.6050e+00 - val_loss: -3.6522e+00
2019-12-26 13:54:44,364 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep1/ae_model_epoch_20.pickle
Epoch 22/200
 - 32s - loss: -3.6070e+00 - val_loss: -3.6513e+00
Epoch 23/200
 - 32s - loss: -3.6068e+00 - val_loss: -3.6544e+00
Epoch 24/200
 - 32s - loss: -3.6075e+00 - val_loss: -3.6533e+00
Epoch 25/200
 - 32s - loss: -3.6093e+00 - val_loss: -3.6539e+00
Epoch 26/200
 - 32s - loss: -3.6102e+00 - val_loss: -3.6551e+00
Epoch 27/200
 - 32s - loss: -3.6102e+00 - val_loss: -3.6560e+00
Epoch 28/200
 - 32s - loss: -3.6103e+00 - val_loss: -3.6557e+00
Epoch 29/200
 - 32s - loss: -3.6105e+00 - val_loss: -3.6557e+00
Epoch 30/200
 - 32s - loss: -3.6119e+00 - val_loss: -3.6569e+00
Epoch 31/200
 - 32s - loss: -3.6130e+00 - val_loss: -3.6579e+00
Epoch 32/200
 - 32s - loss: -3.6123e+00 - val_loss: -3.6578e+00
Epoch 33/200
 - 32s - loss: -3.6120e+00 - val_loss: -3.6571e+00
Epoch 34/200
 - 32s - loss: -3.6138e+00 - val_loss: -3.6568e+00
Epoch 35/200
 - 32s - loss: -3.6149e+00 - val_loss: -3.6585e+00
Epoch 36/200
 - 32s - loss: -3.6143e+00 - val_loss: -3.6579e+00
Epoch 37/200
 - 32s - loss: -3.6158e+00 - val_loss: -3.6591e+00
Epoch 38/200
 - 32s - loss: -3.6158e+00 - val_loss: -3.6591e+00
Epoch 39/200
 - 32s - loss: -3.6161e+00 - val_loss: -3.6597e+00
Epoch 40/200
 - 32s - loss: -3.6166e+00 - val_loss: -3.6596e+00
Epoch 41/200
 - 32s - loss: -3.6171e+00 - val_loss: -3.6600e+00
2019-12-26 14:05:25,405 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep1/ae_model_epoch_40.pickle
Epoch 42/200
 - 32s - loss: -3.6172e+00 - val_loss: -3.6589e+00
Epoch 43/200
 - 32s - loss: -3.6174e+00 - val_loss: -3.6604e+00
Epoch 44/200
 - 32s - loss: -3.6174e+00 - val_loss: -3.6592e+00
Epoch 45/200
 - 32s - loss: -3.6177e+00 - val_loss: -3.6599e+00
Epoch 46/200
 - 32s - loss: -3.6186e+00 - val_loss: -3.6605e+00
Epoch 47/200
 - 32s - loss: -3.6183e+00 - val_loss: -3.6610e+00
Epoch 48/200
 - 32s - loss: -3.6184e+00 - val_loss: -3.6616e+00
Epoch 49/200
 - 32s - loss: -3.6191e+00 - val_loss: -3.6611e+00
Epoch 50/200
 - 32s - loss: -3.6195e+00 - val_loss: -3.6616e+00
Epoch 51/200
 - 32s - loss: -3.6194e+00 - val_loss: -3.6619e+00
Epoch 52/200
 - 32s - loss: -3.6203e+00 - val_loss: -3.6608e+00
Epoch 53/200
 - 32s - loss: -3.6199e+00 - val_loss: -3.6618e+00
Epoch 54/200
 - 32s - loss: -3.6203e+00 - val_loss: -3.6603e+00
Epoch 55/200
 - 32s - loss: -3.6210e+00 - val_loss: -3.6602e+00
Epoch 56/200
 - 32s - loss: -3.6203e+00 - val_loss: -3.6610e+00
Epoch 57/200
 - 32s - loss: -3.6210e+00 - val_loss: -3.6612e+00
Epoch 58/200
 - 32s - loss: -3.6209e+00 - val_loss: -3.6615e+00
Epoch 59/200
 - 32s - loss: -3.6213e+00 - val_loss: -3.6621e+00
Epoch 60/200
 - 32s - loss: -3.6201e+00 - val_loss: -3.6613e+00
Epoch 61/200
 - 32s - loss: -3.6215e+00 - val_loss: -3.6617e+00
2019-12-26 14:16:06,990 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep1/ae_model_epoch_60.pickle
Epoch 62/200
 - 32s - loss: -3.6212e+00 - val_loss: -3.6601e+00
Epoch 63/200
 - 32s - loss: -3.6220e+00 - val_loss: -3.6616e+00
Epoch 64/200
 - 32s - loss: -3.6212e+00 - val_loss: -3.6613e+00
Epoch 65/200
 - 32s - loss: -3.6219e+00 - val_loss: -3.6627e+00
Epoch 66/200
 - 32s - loss: -3.6220e+00 - val_loss: -3.6636e+00
Epoch 67/200
 - 32s - loss: -3.6226e+00 - val_loss: -3.6612e+00
Epoch 68/200
 - 32s - loss: -3.6226e+00 - val_loss: -3.6632e+00
Epoch 69/200
 - 32s - loss: -3.6222e+00 - val_loss: -3.6625e+00
Epoch 70/200
 - 32s - loss: -3.6227e+00 - val_loss: -3.6628e+00
Epoch 71/200
 - 32s - loss: -3.6224e+00 - val_loss: -3.6633e+00
Epoch 72/200
 - 32s - loss: -3.6233e+00 - val_loss: -3.6635e+00
Epoch 73/200
 - 32s - loss: -3.6224e+00 - val_loss: -3.6612e+00
Epoch 74/200
 - 32s - loss: -3.6226e+00 - val_loss: -3.6627e+00
Epoch 75/200
 - 32s - loss: -3.6237e+00 - val_loss: -3.6624e+00
Epoch 76/200
 - 32s - loss: -3.6237e+00 - val_loss: -3.6621e+00
Epoch 77/200
 - 32s - loss: -3.6231e+00 - val_loss: -3.6622e+00
Epoch 78/200
 - 32s - loss: -3.6234e+00 - val_loss: -3.6626e+00
Epoch 79/200
 - 32s - loss: -3.6225e+00 - val_loss: -3.6624e+00
Epoch 80/200
 - 32s - loss: -3.6237e+00 - val_loss: -3.6629e+00
Epoch 81/200
 - 32s - loss: -3.6235e+00 - val_loss: -3.6622e+00
2019-12-26 14:26:49,087 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep1/ae_model_epoch_80.pickle
Epoch 82/200
 - 32s - loss: -3.6243e+00 - val_loss: -3.6630e+00
Epoch 83/200
 - 32s - loss: -3.6241e+00 - val_loss: -3.6639e+00
Epoch 84/200
 - 32s - loss: -3.6242e+00 - val_loss: -3.6630e+00
Epoch 85/200
 - 32s - loss: -3.6241e+00 - val_loss: -3.6627e+00
Epoch 86/200
 - 32s - loss: -3.6252e+00 - val_loss: -3.6630e+00
Epoch 87/200
 - 32s - loss: -3.6236e+00 - val_loss: -3.6632e+00
Epoch 88/200
 - 32s - loss: -3.6241e+00 - val_loss: -3.6633e+00
Epoch 89/200
 - 32s - loss: -3.6233e+00 - val_loss: -3.6634e+00
Epoch 90/200
 - 32s - loss: -3.6250e+00 - val_loss: -3.6632e+00
Epoch 91/200
 - 32s - loss: -3.6256e+00 - val_loss: -3.6633e+00
Epoch 92/200
 - 32s - loss: -3.6247e+00 - val_loss: -3.6636e+00
Epoch 93/200
 - 32s - loss: -3.6255e+00 - val_loss: -3.6624e+00
Epoch 94/200
 - 32s - loss: -3.6253e+00 - val_loss: -3.6631e+00
Epoch 95/200
 - 32s - loss: -3.6255e+00 - val_loss: -3.6637e+00
Epoch 96/200
 - 32s - loss: -3.6249e+00 - val_loss: -3.6628e+00
Epoch 97/200
 - 32s - loss: -3.6252e+00 - val_loss: -3.6630e+00
Epoch 98/200
 - 32s - loss: -3.6257e+00 - val_loss: -3.6640e+00
Epoch 99/200
 - 32s - loss: -3.6259e+00 - val_loss: -3.6653e+00
Epoch 100/200
 - 32s - loss: -3.6247e+00 - val_loss: -3.6639e+00
Epoch 101/200
 - 32s - loss: -3.6238e+00 - val_loss: -3.6636e+00
2019-12-26 14:37:31,061 [INFO] epoch = 100. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep1/ae_model_epoch_100.pickle
Epoch 102/200
 - 32s - loss: -3.6252e+00 - val_loss: -3.6636e+00
Epoch 103/200
 - 32s - loss: -3.6256e+00 - val_loss: -3.6635e+00
Epoch 104/200
 - 32s - loss: -3.6258e+00 - val_loss: -3.6632e+00
Epoch 105/200
 - 32s - loss: -3.6262e+00 - val_loss: -3.6638e+00
Epoch 106/200
 - 32s - loss: -3.6260e+00 - val_loss: -3.6635e+00
Epoch 107/200
 - 32s - loss: -3.6261e+00 - val_loss: -3.6642e+00
Epoch 108/200
 - 32s - loss: -3.6262e+00 - val_loss: -3.6642e+00
Epoch 109/200
 - 32s - loss: -3.6258e+00 - val_loss: -3.6645e+00
Epoch 110/200
 - 32s - loss: -3.6259e+00 - val_loss: -3.6632e+00
Epoch 111/200
 - 32s - loss: -3.6264e+00 - val_loss: -3.6641e+00
Epoch 112/200
 - 32s - loss: -3.6264e+00 - val_loss: -3.6642e+00
Epoch 113/200
 - 32s - loss: -3.6250e+00 - val_loss: -3.6637e+00
Epoch 114/200
 - 32s - loss: -3.6262e+00 - val_loss: -3.6643e+00
Epoch 115/200
 - 32s - loss: -3.6266e+00 - val_loss: -3.6644e+00
Epoch 116/200
 - 32s - loss: -3.6260e+00 - val_loss: -3.6644e+00
Epoch 117/200
 - 32s - loss: -3.6258e+00 - val_loss: -3.6637e+00
Epoch 118/200
 - 32s - loss: -3.6265e+00 - val_loss: -3.6636e+00
Epoch 119/200
 - 32s - loss: -3.6268e+00 - val_loss: -3.6642e+00
Epoch 120/200
 - 32s - loss: -3.6270e+00 - val_loss: -3.6632e+00
Epoch 121/200
 - 32s - loss: -3.6269e+00 - val_loss: -3.6627e+00
2019-12-26 14:48:12,647 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep1/ae_model_epoch_120.pickle
Epoch 122/200
 - 32s - loss: -3.6275e+00 - val_loss: -3.6643e+00
Epoch 123/200
 - 32s - loss: -3.6273e+00 - val_loss: -3.6632e+00
Epoch 124/200
 - 32s - loss: -3.6274e+00 - val_loss: -3.6637e+00
Epoch 125/200
 - 32s - loss: -3.6273e+00 - val_loss: -3.6644e+00
Epoch 126/200
 - 32s - loss: -3.6278e+00 - val_loss: -3.6650e+00
Epoch 127/200
 - 32s - loss: -3.6258e+00 - val_loss: -3.6643e+00
Epoch 128/200
 - 32s - loss: -3.6274e+00 - val_loss: -3.6650e+00
Epoch 129/200
 - 32s - loss: -3.6274e+00 - val_loss: -3.6642e+00
Epoch 130/200
 - 32s - loss: -3.6272e+00 - val_loss: -3.6655e+00
Epoch 131/200
 - 32s - loss: -3.6268e+00 - val_loss: -3.6641e+00
Epoch 132/200
 - 32s - loss: -3.6272e+00 - val_loss: -3.6643e+00
Epoch 133/200
 - 32s - loss: -3.6279e+00 - val_loss: -3.6649e+00
Epoch 134/200
 - 32s - loss: -3.6273e+00 - val_loss: -3.6644e+00
Epoch 135/200
 - 32s - loss: -3.6277e+00 - val_loss: -3.6639e+00
Epoch 136/200
 - 32s - loss: -3.6278e+00 - val_loss: -3.6657e+00
Epoch 137/200
 - 32s - loss: -3.6277e+00 - val_loss: -3.6643e+00
Epoch 138/200
 - 32s - loss: -3.6285e+00 - val_loss: -3.6661e+00
Epoch 139/200
 - 32s - loss: -3.6281e+00 - val_loss: -3.6649e+00
Epoch 140/200
 - 32s - loss: -3.6286e+00 - val_loss: -3.6659e+00
Epoch 141/200
 - 32s - loss: -3.6276e+00 - val_loss: -3.6644e+00
2019-12-26 14:58:54,273 [INFO] epoch = 140. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep1/ae_model_epoch_140.pickle
Epoch 142/200
 - 32s - loss: -3.6282e+00 - val_loss: -3.6651e+00
Epoch 143/200
 - 32s - loss: -3.6285e+00 - val_loss: -3.6652e+00
Epoch 144/200
 - 32s - loss: -3.6275e+00 - val_loss: -3.6658e+00
Epoch 145/200
 - 32s - loss: -3.6285e+00 - val_loss: -3.6651e+00
Epoch 146/200
 - 32s - loss: -3.6284e+00 - val_loss: -3.6657e+00
Epoch 147/200
 - 32s - loss: -3.6282e+00 - val_loss: -3.6654e+00
Epoch 148/200
 - 32s - loss: -3.6280e+00 - val_loss: -3.6659e+00
Epoch 149/200
 - 32s - loss: -3.6283e+00 - val_loss: -3.6666e+00
Epoch 150/200
 - 32s - loss: -3.6288e+00 - val_loss: -3.6666e+00
Epoch 151/200
 - 32s - loss: -3.6284e+00 - val_loss: -3.6653e+00
Epoch 152/200
 - 32s - loss: -3.6280e+00 - val_loss: -3.6660e+00
Epoch 153/200
 - 32s - loss: -3.6286e+00 - val_loss: -3.6665e+00
Epoch 154/200
 - 32s - loss: -3.6288e+00 - val_loss: -3.6649e+00
Epoch 155/200
 - 32s - loss: -3.6294e+00 - val_loss: -3.6661e+00
Epoch 156/200
 - 32s - loss: -3.6285e+00 - val_loss: -3.6657e+00
Epoch 157/200
 - 32s - loss: -3.6293e+00 - val_loss: -3.6658e+00
Epoch 158/200
 - 32s - loss: -3.6290e+00 - val_loss: -3.6661e+00
Epoch 159/200
 - 32s - loss: -3.6286e+00 - val_loss: -3.6650e+00
Epoch 160/200
 - 32s - loss: -3.6300e+00 - val_loss: -3.6656e+00
Epoch 161/200
 - 32s - loss: -3.6284e+00 - val_loss: -3.6658e+00
2019-12-26 15:09:35,779 [INFO] epoch = 160. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep1/ae_model_epoch_160.pickle
Epoch 162/200
 - 32s - loss: -3.6296e+00 - val_loss: -3.6660e+00
Epoch 163/200
 - 32s - loss: -3.6294e+00 - val_loss: -3.6663e+00
Epoch 164/200
 - 32s - loss: -3.6293e+00 - val_loss: -3.6659e+00
Epoch 165/200
 - 32s - loss: -3.6298e+00 - val_loss: -3.6657e+00
Epoch 166/200
 - 32s - loss: -3.6291e+00 - val_loss: -3.6663e+00
Epoch 167/200
 - 32s - loss: -3.6290e+00 - val_loss: -3.6652e+00
Epoch 168/200
 - 32s - loss: -3.6292e+00 - val_loss: -3.6639e+00
Epoch 169/200
 - 32s - loss: -3.6289e+00 - val_loss: -3.6656e+00
Epoch 170/200
 - 32s - loss: -3.6295e+00 - val_loss: -3.6664e+00
Epoch 171/200
 - 32s - loss: -3.6292e+00 - val_loss: -3.6661e+00
Epoch 172/200
 - 32s - loss: -3.6298e+00 - val_loss: -3.6657e+00
Epoch 173/200
 - 32s - loss: -3.6293e+00 - val_loss: -3.6654e+00
Epoch 174/200
 - 32s - loss: -3.6293e+00 - val_loss: -3.6659e+00
Epoch 175/200
 - 32s - loss: -3.6290e+00 - val_loss: -3.6658e+00
Epoch 176/200
 - 32s - loss: -3.6297e+00 - val_loss: -3.6668e+00
Epoch 177/200
 - 32s - loss: -3.6295e+00 - val_loss: -3.6664e+00
Epoch 178/200
 - 32s - loss: -3.6292e+00 - val_loss: -3.6658e+00
Epoch 179/200
 - 32s - loss: -3.6301e+00 - val_loss: -3.6661e+00
Epoch 180/200
 - 32s - loss: -3.6304e+00 - val_loss: -3.6661e+00
Epoch 181/200
 - 32s - loss: -3.6302e+00 - val_loss: -3.6666e+00
2019-12-26 15:20:17,507 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep1/ae_model_epoch_180.pickle
Epoch 182/200
 - 32s - loss: -3.6297e+00 - val_loss: -3.6664e+00
Epoch 183/200
 - 32s - loss: -3.6300e+00 - val_loss: -3.6662e+00
Epoch 184/200
 - 32s - loss: -3.6303e+00 - val_loss: -3.6664e+00
Epoch 185/200
 - 32s - loss: -3.6294e+00 - val_loss: -3.6660e+00
Epoch 186/200
 - 32s - loss: -3.6299e+00 - val_loss: -3.6661e+00
Epoch 187/200
 - 32s - loss: -3.6294e+00 - val_loss: -3.6659e+00
Epoch 188/200
 - 32s - loss: -3.6296e+00 - val_loss: -3.6667e+00
Epoch 189/200
 - 32s - loss: -3.6290e+00 - val_loss: -3.6663e+00
Epoch 190/200
 - 32s - loss: -3.6303e+00 - val_loss: -3.6662e+00
Epoch 191/200
 - 32s - loss: -3.6305e+00 - val_loss: -3.6665e+00
Epoch 192/200
 - 32s - loss: -3.6303e+00 - val_loss: -3.6669e+00
Epoch 193/200
 - 32s - loss: -3.6302e+00 - val_loss: -3.6663e+00
Epoch 194/200
 - 32s - loss: -3.6299e+00 - val_loss: -3.6666e+00
Epoch 195/200
 - 32s - loss: -3.6300e+00 - val_loss: -3.6663e+00
Epoch 196/200
 - 32s - loss: -3.6297e+00 - val_loss: -3.6670e+00
Epoch 197/200
 - 32s - loss: -3.6296e+00 - val_loss: -3.6664e+00
Epoch 198/200
 - 32s - loss: -3.6304e+00 - val_loss: -3.6666e+00
Epoch 199/200
 - 32s - loss: -3.6301e+00 - val_loss: -3.6663e+00
Epoch 200/200
 - 32s - loss: -3.6298e+00 - val_loss: -3.6664e+00
2019-12-26 15:30:27,677 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-26 15:31:59,835 [INFO] Last epoch loss evaluation: train_loss = -3.661244, val_loss = -3.666996
2019-12-26 15:31:59,835 [INFO] Training autoencoder complete
2019-12-26 15:31:59,835 [INFO] Encoding data for supervised training
2019-12-26 15:33:57,344 [INFO] Encoding complete
2019-12-26 15:33:57,344 [INFO] Training neural network layers (after autoencoder)
Train on 1452347 samples, validate on 645487 samples
Epoch 1/200
 - 32s - loss: 0.0163 - val_loss: 0.0092
 - val_f1: 0.9811
Epoch 2/200
 - 29s - loss: 0.0095 - val_loss: 0.0183
 - val_f1: 0.9545
Epoch 3/200
 - 29s - loss: 0.0091 - val_loss: 0.0088
 - val_f1: 0.9820
Epoch 4/200
 - 29s - loss: 0.0090 - val_loss: 0.0085
 - val_f1: 0.9820
Epoch 5/200
 - 29s - loss: 0.0089 - val_loss: 0.0250
 - val_f1: 0.9356
Epoch 6/200
 - 29s - loss: 0.0088 - val_loss: 0.0083
 - val_f1: 0.9808
Epoch 7/200
 - 29s - loss: 0.0087 - val_loss: 0.0083
 - val_f1: 0.9825
Epoch 8/200
 - 29s - loss: 0.0087 - val_loss: 0.0084
 - val_f1: 0.9809
Epoch 9/200
 - 29s - loss: 0.0086 - val_loss: 0.0083
 - val_f1: 0.9825
Epoch 10/200
 - 29s - loss: 0.0086 - val_loss: 0.0084
 - val_f1: 0.9824
Epoch 11/200
 - 29s - loss: 0.0086 - val_loss: 0.0082
 - val_f1: 0.9828
Epoch 12/200
 - 29s - loss: 0.0086 - val_loss: 0.0083
 - val_f1: 0.9827
Epoch 13/200
 - 30s - loss: 0.0086 - val_loss: 0.0083
 - val_f1: 0.9827
Epoch 14/200
 - 29s - loss: 0.0085 - val_loss: 0.0083
 - val_f1: 0.9823
Epoch 15/200
 - 29s - loss: 0.0085 - val_loss: 0.0083
 - val_f1: 0.9812
Epoch 16/200
 - 29s - loss: 0.0085 - val_loss: 0.0083
 - val_f1: 0.9828
Epoch 17/200
 - 29s - loss: 0.0085 - val_loss: 0.0084
 - val_f1: 0.9824
Epoch 18/200
 - 29s - loss: 0.0085 - val_loss: 0.0084
 - val_f1: 0.9828
Epoch 19/200
 - 29s - loss: 0.0085 - val_loss: 0.0082
 - val_f1: 0.9830
Epoch 20/200
 - 29s - loss: 0.0084 - val_loss: 0.0082
 - val_f1: 0.9826
Epoch 21/200
 - 29s - loss: 0.0085 - val_loss: 0.0085
2019-12-26 15:55:18,447 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep1/ann_model_epoch_20.pickle
 - val_f1: 0.9821
Epoch 22/200
 - 29s - loss: 0.0084 - val_loss: 0.0084
 - val_f1: 0.9816
Epoch 23/200
 - 30s - loss: 0.0084 - val_loss: 0.0083
 - val_f1: 0.9829
Epoch 24/200
 - 29s - loss: 0.0084 - val_loss: 0.0081
 - val_f1: 0.9828
Epoch 25/200
 - 29s - loss: 0.0084 - val_loss: 0.0081
 - val_f1: 0.9828
Epoch 26/200
 - 29s - loss: 0.0084 - val_loss: 0.0081
 - val_f1: 0.9828
Epoch 27/200
 - 29s - loss: 0.0084 - val_loss: 0.0082
 - val_f1: 0.9827
Epoch 28/200
 - 29s - loss: 0.0084 - val_loss: 0.0082
 - val_f1: 0.9811
Epoch 29/200
 - 29s - loss: 0.0083 - val_loss: 0.0082
 - val_f1: 0.9828
Epoch 30/200
 - 29s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9829
Epoch 31/200
 - 29s - loss: 0.0084 - val_loss: 0.0083
 - val_f1: 0.9812
Epoch 32/200
 - 29s - loss: 0.0084 - val_loss: 0.0082
 - val_f1: 0.9810
Epoch 33/200
 - 29s - loss: 0.0083 - val_loss: 0.0085
 - val_f1: 0.9809
Epoch 34/200
 - 29s - loss: 0.0083 - val_loss: 0.0082
 - val_f1: 0.9828
Epoch 35/200
 - 29s - loss: 0.0083 - val_loss: 0.0084
 - val_f1: 0.9826
Epoch 36/200
 - 29s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9812
Epoch 37/200
 - 29s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9828
Epoch 38/200
 - 29s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9812
Epoch 39/200
 - 29s - loss: 0.0083 - val_loss: 0.0104
 - val_f1: 0.9730
Epoch 40/200
 - 29s - loss: 0.0083 - val_loss: 0.0114
 - val_f1: 0.9704
Epoch 41/200
 - 29s - loss: 0.0083 - val_loss: 0.0081
2019-12-26 16:15:46,697 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep1/ann_model_epoch_40.pickle
 - val_f1: 0.9829
Epoch 42/200
 - 29s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9827
Epoch 43/200
 - 29s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9814
Epoch 44/200
 - 29s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9827
Epoch 45/200
 - 29s - loss: 0.0083 - val_loss: 0.0082
 - val_f1: 0.9826
Epoch 46/200
 - 29s - loss: 0.0083 - val_loss: 0.0082
 - val_f1: 0.9827
Epoch 47/200
 - 29s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9831
Epoch 48/200
 - 29s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 49/200
 - 29s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9828
Epoch 50/200
 - 29s - loss: 0.0083 - val_loss: 0.0083
 - val_f1: 0.9825
Epoch 51/200
 - 29s - loss: 0.0083 - val_loss: 0.0082
 - val_f1: 0.9810
Epoch 52/200
 - 30s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9829
Epoch 53/200
 - 29s - loss: 0.0083 - val_loss: 0.0084
 - val_f1: 0.9826
Epoch 54/200
 - 29s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9829
Epoch 55/200
 - 29s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 56/200
 - 29s - loss: 0.0083 - val_loss: 0.0082
 - val_f1: 0.9827
Epoch 57/200
 - 29s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 58/200
 - 29s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9828
Epoch 59/200
 - 29s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9812
Epoch 60/200
 - 29s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9832
Epoch 61/200
 - 29s - loss: 0.0083 - val_loss: 0.0081
2019-12-26 16:36:14,889 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep1/ann_model_epoch_60.pickle
 - val_f1: 0.9831
Epoch 62/200
 - 29s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9833
Epoch 63/200
 - 29s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 64/200
 - 29s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9827
Epoch 65/200
 - 29s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 66/200
 - 29s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 67/200
 - 29s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9831
Epoch 68/200
 - 29s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 69/200
 - 29s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9816
Epoch 70/200
 - 29s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9828
Epoch 71/200
 - 29s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9814
Epoch 72/200
 - 29s - loss: 0.0083 - val_loss: 0.0084
 - val_f1: 0.9827
Epoch 73/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 74/200
 - 29s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 75/200
 - 29s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9831
Epoch 76/200
 - 29s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9827
Epoch 77/200
 - 29s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 78/200
 - 29s - loss: 0.0083 - val_loss: 0.0082
 - val_f1: 0.9831
Epoch 79/200
 - 29s - loss: 0.0082 - val_loss: 0.0083
 - val_f1: 0.9825
Epoch 80/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 81/200
 - 29s - loss: 0.0082 - val_loss: 0.0082
2019-12-26 16:56:43,386 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep1/ann_model_epoch_80.pickle
 - val_f1: 0.9829
Epoch 82/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 83/200
 - 29s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9831
Epoch 84/200
 - 29s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9830
Epoch 85/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9813
Epoch 86/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9833
Epoch 87/200
 - 29s - loss: 0.0082 - val_loss: 0.0085
 - val_f1: 0.9823
Epoch 88/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 89/200
 - 29s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9830
Epoch 90/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 91/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9812
Epoch 92/200
 - 29s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9829
Epoch 93/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 94/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 95/200
 - 29s - loss: 0.0082 - val_loss: 0.0094
 - val_f1: 0.9812
Epoch 96/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9833
Epoch 97/200
 - 29s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9829
Epoch 98/200
 - 29s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9831
Epoch 99/200
 - 29s - loss: 0.0082 - val_loss: 0.0084
 - val_f1: 0.9826
Epoch 100/200
 - 29s - loss: 0.0082 - val_loss: 0.0082
 - val_f1: 0.9831
Epoch 101/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
2019-12-26 17:17:12,449 [INFO] epoch = 100. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep1/ann_model_epoch_100.pickle
 - val_f1: 0.9832
Epoch 102/200
 - 29s - loss: 0.0082 - val_loss: 0.0082
 - val_f1: 0.9826
Epoch 103/200
 - 29s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9816
Epoch 104/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9814
Epoch 105/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 106/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9833
Epoch 107/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 108/200
 - 29s - loss: 0.0082 - val_loss: 0.0114
 - val_f1: 0.9735
Epoch 109/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9835
Epoch 110/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 111/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 112/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 113/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9834
Epoch 114/200
 - 29s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9833
Epoch 115/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 116/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 117/200
 - 29s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9828
Epoch 118/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 119/200
 - 29s - loss: 0.0082 - val_loss: 0.0141
 - val_f1: 0.9723
Epoch 120/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9813
Epoch 121/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
2019-12-26 17:37:41,391 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep1/ann_model_epoch_120.pickle
 - val_f1: 0.9834
Epoch 122/200
 - 29s - loss: 0.0082 - val_loss: 0.0082
 - val_f1: 0.9822
Epoch 123/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 124/200
 - 29s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9828
Epoch 125/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 126/200
 - 29s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9832
Epoch 127/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9833
Epoch 128/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 129/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9833
Epoch 130/200
 - 29s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9829
Epoch 131/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9814
Epoch 132/200
 - 29s - loss: 0.0082 - val_loss: 0.0083
 - val_f1: 0.9828
Epoch 133/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9833
Epoch 134/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 135/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 136/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9827
Epoch 137/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 138/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9833
Epoch 139/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9833
Epoch 140/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 141/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
2019-12-26 17:58:10,648 [INFO] epoch = 140. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep1/ann_model_epoch_140.pickle
 - val_f1: 0.9832
Epoch 142/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 143/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 144/200
 - 29s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9832
Epoch 145/200
 - 29s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9829
Epoch 146/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 147/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 148/200
 - 29s - loss: 0.0082 - val_loss: 0.0095
 - val_f1: 0.9736
Epoch 149/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 150/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 151/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 152/200
 - 29s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9831
Epoch 153/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9815
Epoch 154/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9834
Epoch 155/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 156/200
 - 29s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9816
Epoch 157/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9817
Epoch 158/200
 - 29s - loss: 0.0082 - val_loss: 0.0084
 - val_f1: 0.9820
Epoch 159/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 160/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 161/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
2019-12-26 18:18:40,407 [INFO] epoch = 160. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep1/ann_model_epoch_160.pickle
 - val_f1: 0.9831
Epoch 162/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 163/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 164/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 165/200
 - 29s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 166/200
 - 29s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 167/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9833
Epoch 168/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9834
Epoch 169/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9833
Epoch 170/200
 - 29s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9829
Epoch 171/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 172/200
 - 29s - loss: 0.0081 - val_loss: 0.0081
 - val_f1: 0.9813
Epoch 173/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9833
Epoch 174/200
 - 29s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 175/200
 - 29s - loss: 0.0082 - val_loss: 0.0090
 - val_f1: 0.9802
Epoch 176/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9833
Epoch 177/200
 - 29s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9819
Epoch 178/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 179/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 180/200
 - 29s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9833
Epoch 181/200
 - 29s - loss: 0.0081 - val_loss: 0.0081
2019-12-26 18:39:11,940 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep1/ann_model_epoch_180.pickle
 - val_f1: 0.9835
Epoch 182/200
 - 29s - loss: 0.0081 - val_loss: 0.0082
 - val_f1: 0.9826
Epoch 183/200
 - 29s - loss: 0.0081 - val_loss: 0.0081
 - val_f1: 0.9832
Epoch 184/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9833
Epoch 185/200
 - 29s - loss: 0.0081 - val_loss: 0.0081
 - val_f1: 0.9833
Epoch 186/200
 - 29s - loss: 0.0081 - val_loss: 0.0081
 - val_f1: 0.9816
Epoch 187/200
 - 29s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9835
Epoch 188/200
 - 29s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9835
Epoch 189/200
 - 30s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 190/200
 - 29s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 191/200
 - 29s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 192/200
 - 29s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 193/200
 - 29s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 194/200
 - 29s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9834
Epoch 195/200
 - 29s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 196/200
 - 29s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9835
Epoch 197/200
 - 29s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 198/200
 - 29s - loss: 0.0081 - val_loss: 0.0081
 - val_f1: 0.9817
Epoch 199/200
 - 29s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9830
Epoch 200/200
 - 29s - loss: 0.0081 - val_loss: 0.0080
2019-12-26 18:59:12,831 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-26 19:01:01,653 [INFO] Last epoch loss evaluation: train_loss = 0.007895, val_loss = 0.007917
2019-12-26 19:01:01,662 [INFO] Training complete. time_to_train = 19111.54 sec, 318.53 min
2019-12-26 19:01:01,696 [INFO] Model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep1/best_model.pickle
/home/sunanda/research/ids_experiments/utility.py:268: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
2019-12-26 19:01:01,893 [INFO] Plot saved to: results_selected_models/selected_ids18_subset_ae_ann_deep_rep1/training_error_history.png
/home/sunanda/research/ids_experiments/utility.py:285: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
2019-12-26 19:01:02,066 [INFO] Plot saved to: results_selected_models/selected_ids18_subset_ae_ann_deep_rep1/training_f1_history.png
2019-12-26 19:01:02,067 [INFO] Making predictions on training, validation, testing data
2019-12-26 19:06:35,131 [INFO] Evaluating predictions (results)
/home/sunanda/anaconda3/envs/ml_env_cpu/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2019-12-26 19:06:58,290 [INFO] Dataset: Testing. Classification report below
2019-12-26 19:06:58,290 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99    535650
                     Bot       1.00      1.00      1.00     11465
        Brute Force -Web       0.00      0.00      0.00        24
        Brute Force -XSS       0.00      0.00      0.00         9
        DDOS attack-HOIC       1.00      1.00      1.00     27447
    DDOS attack-LOIC-UDP       0.68      1.00      0.81        67
  DDoS attacks-LOIC-HTTP       0.99      1.00      0.99     23010
   DoS attacks-GoldenEye       0.99      1.00      0.99      1651
        DoS attacks-Hulk       1.00      1.00      1.00     18478
DoS attacks-SlowHTTPTest       0.74      0.45      0.56      5596
   DoS attacks-Slowloris       1.00      0.73      0.84       440
          FTP-BruteForce       0.69      0.88      0.78      7718
           Infilteration       0.44      0.00      0.01      6404
           SQL Injection       0.00      0.00      0.00         4
          SSH-Bruteforce       1.00      1.00      1.00      7525

                accuracy                           0.98    645488
               macro avg       0.70      0.67      0.66    645488
            weighted avg       0.98      0.98      0.98    645488

2019-12-26 19:06:58,290 [INFO] Overall accuracy (micro avg): 0.9829245470093945
/home/sunanda/research/ids_experiments/utility.py:307: RuntimeWarning: invalid value encountered in true_divide
  PPV = TP / (TP + FP)    # Precision or positive predictive value
/home/sunanda/research/ids_experiments/utility.py:311: RuntimeWarning: invalid value encountered in true_divide
  FDR = FP / (TP + FP)    # False discovery rate
/home/sunanda/research/ids_experiments/utility.py:313: RuntimeWarning: invalid value encountered in true_divide
  F1 = (2 * PPV * TPR) / (PPV + TPR)  # F1 score
2019-12-26 19:07:23,212 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9829         0.9829                       0.9829                0.0012                   0.0171  0.9829
1     Macro avg        0.9977         0.7007                       0.6705                0.0045                   0.3295  0.6649
2  Weighted avg        0.9907         0.9777                       0.9829                0.0500                   0.0171  0.9777
2019-12-26 19:07:46,368 [INFO] Dataset: Validation. Classification report below
2019-12-26 19:07:46,368 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99    535650
                     Bot       1.00      1.00      1.00     11465
        Brute Force -Web       0.00      0.00      0.00        25
        Brute Force -XSS       0.00      0.00      0.00         9
        DDOS attack-HOIC       1.00      1.00      1.00     27447
    DDOS attack-LOIC-UDP       0.74      0.99      0.84        68
  DDoS attacks-LOIC-HTTP       0.99      1.00      0.99     23009
   DoS attacks-GoldenEye       0.99      1.00      0.99      1651
        DoS attacks-Hulk       1.00      1.00      1.00     18478
DoS attacks-SlowHTTPTest       0.75      0.45      0.56      5596
   DoS attacks-Slowloris       1.00      0.78      0.87       439
          FTP-BruteForce       0.69      0.89      0.78      7718
           Infilteration       0.35      0.00      0.01      6403
           SQL Injection       0.00      0.00      0.00         4
          SSH-Bruteforce       1.00      1.00      1.00      7525

                accuracy                           0.98    645487
               macro avg       0.70      0.67      0.67    645487
            weighted avg       0.98      0.98      0.98    645487

2019-12-26 19:07:46,368 [INFO] Overall accuracy (micro avg): 0.9829957845781557
2019-12-26 19:08:11,315 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9830         0.9830                       0.9830                0.0012                   0.0170  0.9830
1     Macro avg        0.9977         0.6990                       0.6732                0.0045                   0.3268  0.6694
2  Weighted avg        0.9907         0.9769                       0.9830                0.0498                   0.0170  0.9777
2019-12-26 19:09:26,913 [INFO] Dataset: Training. Classification report below
2019-12-26 19:09:26,913 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99   1606949
                     Bot       1.00      1.00      1.00     34396
        Brute Force -Web       0.00      0.00      0.00        73
        Brute Force -XSS       0.00      0.00      0.00        26
        DDOS attack-HOIC       1.00      1.00      1.00     82341
    DDOS attack-LOIC-UDP       0.71      0.99      0.83       203
  DDoS attacks-LOIC-HTTP       0.99      1.00      0.99     69029
   DoS attacks-GoldenEye       0.99      1.00      0.99      4954
        DoS attacks-Hulk       1.00      1.00      1.00     55435
DoS attacks-SlowHTTPTest       0.74      0.45      0.56     16787
   DoS attacks-Slowloris       1.00      0.76      0.86      1318
          FTP-BruteForce       0.69      0.89      0.78     23153
           Infilteration       0.45      0.00      0.01     19210
           SQL Injection       0.00      0.00      0.00        12
          SSH-Bruteforce       1.00      1.00      1.00     22576

                accuracy                           0.98   1936462
               macro avg       0.70      0.67      0.67   1936462
            weighted avg       0.98      0.98      0.98   1936462

2019-12-26 19:09:26,913 [INFO] Overall accuracy (micro avg): 0.9829710058859921
2019-12-26 19:10:48,376 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9830         0.9830                       0.9830                0.0012                   0.0170  0.9830
1     Macro avg        0.9977         0.7040                       0.6722                0.0045                   0.3278  0.6676
2  Weighted avg        0.9908         0.9779                       0.9830                0.0499                   0.0170  0.9777
2019-12-26 19:10:48,449 [INFO] Results saved to: results_selected_models/selected_ids18_subset_ae_ann_deep_rep1/selected_ids18_subset_ae_ann_deep_rep1_results.xlsx
2019-12-26 19:10:48,454 [INFO] ================= Finished running experiment no. 1 ================= 

2019-12-26 19:10:48,504 [INFO] Created directory: results_selected_models/selected_ids18_subset_ae_ann_deep_rep2
2019-12-26 19:10:48,504 [INFO] Initialized logging. log_filename = results_selected_models/selected_ids18_subset_ae_ann_deep_rep2/run_log.log
2019-12-26 19:10:48,504 [INFO] ================= Running experiment no. 2  ================= 

2019-12-26 19:10:48,504 [INFO] Experiment parameters given below
2019-12-26 19:10:48,504 [INFO] 
{'experiment_num': 2, 'results_dir': 'results_selected_models/selected_ids18_subset_ae_ann_deep_rep2', 'model_type': 'classifier', 'model': 'ae_ann', 'normal_label': 'BENIGN', 'scaling_type': 'NA', 'unsupervised_ratio': 0.25, 'ae_encoder_units': [128, 64, 32], 'ae_encoder_activations': ['relu', 'relu', 'relu'], 'ae_encoder_dropout_rates': [0.2, 0.2, 0.2], 'ae_encoder_l1_param': -1, 'ae_decoder_units': [64, 128], 'ae_decoder_activations': ['relu', 'relu'], 'ae_decoder_dropout_rates': [0.2, 0.2], 'output_activation': 'sigmoid', 'loss_function': 'binary_crossentropy', 'ae_epochs': 200, 'ann_layer_units': [64], 'ann_layer_activations': ['relu'], 'ann_layer_dropout_rates': [0.2], 'ann_epochs': 200, 'early_stop_patience': 50, 'batch_size': 256, 'dataset_dir': '../Datasets/small_datasets/ids2018', 'description': 'selected_ids18_subset_ae_ann_deep_rep2'}
2019-12-26 19:10:48,504 [INFO] Created tensorboard log directory: results_selected_models/selected_ids18_subset_ae_ann_deep_rep2/tf_logs_run_2019_12_26-19_10_48
2019-12-26 19:10:48,504 [INFO] Loading datsets from: ../Datasets/small_datasets/ids2018
2019-12-26 19:10:48,505 [INFO] Reading X, y files
2019-12-26 19:10:48,505 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_train.h5
2019-12-26 19:10:52,591 [INFO] Reading complete. time_to_read=4.09 seconds
2019-12-26 19:10:52,591 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_val.h5
2019-12-26 19:10:53,973 [INFO] Reading complete. time_to_read=1.38 seconds
2019-12-26 19:10:53,973 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_test.h5
2019-12-26 19:10:55,354 [INFO] Reading complete. time_to_read=1.38 seconds
2019-12-26 19:10:55,354 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_train.h5
2019-12-26 19:10:55,633 [INFO] Reading complete. time_to_read=0.28 seconds
2019-12-26 19:10:55,633 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_val.h5
2019-12-26 19:10:55,731 [INFO] Reading complete. time_to_read=0.10 seconds
2019-12-26 19:10:55,731 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_test.h5
2019-12-26 19:10:55,829 [INFO] Reading complete. time_to_read=0.10 seconds
2019-12-26 19:10:59,179 [INFO] Initializing model
2019-12-26 19:10:59,852 [INFO] _________________________________________________________________
2019-12-26 19:10:59,852 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-26 19:10:59,852 [INFO] =================================================================
2019-12-26 19:10:59,852 [INFO] dense_149 (Dense)            (None, 128)               9984      
2019-12-26 19:10:59,852 [INFO] _________________________________________________________________
2019-12-26 19:10:59,852 [INFO] batch_normalization_107 (Bat (None, 128)               512       
2019-12-26 19:10:59,852 [INFO] _________________________________________________________________
2019-12-26 19:10:59,853 [INFO] dropout_107 (Dropout)        (None, 128)               0         
2019-12-26 19:10:59,853 [INFO] _________________________________________________________________
2019-12-26 19:10:59,853 [INFO] dense_150 (Dense)            (None, 64)                8256      
2019-12-26 19:10:59,853 [INFO] _________________________________________________________________
2019-12-26 19:10:59,853 [INFO] batch_normalization_108 (Bat (None, 64)                256       
2019-12-26 19:10:59,853 [INFO] _________________________________________________________________
2019-12-26 19:10:59,853 [INFO] dropout_108 (Dropout)        (None, 64)                0         
2019-12-26 19:10:59,853 [INFO] _________________________________________________________________
2019-12-26 19:10:59,853 [INFO] dense_151 (Dense)            (None, 32)                2080      
2019-12-26 19:10:59,853 [INFO] _________________________________________________________________
2019-12-26 19:10:59,853 [INFO] batch_normalization_109 (Bat (None, 32)                128       
2019-12-26 19:10:59,853 [INFO] _________________________________________________________________
2019-12-26 19:10:59,854 [INFO] dropout_109 (Dropout)        (None, 32)                0         
2019-12-26 19:10:59,854 [INFO] _________________________________________________________________
2019-12-26 19:10:59,854 [INFO] dense_152 (Dense)            (None, 64)                2112      
2019-12-26 19:10:59,854 [INFO] _________________________________________________________________
2019-12-26 19:10:59,854 [INFO] batch_normalization_110 (Bat (None, 64)                256       
2019-12-26 19:10:59,854 [INFO] _________________________________________________________________
2019-12-26 19:10:59,854 [INFO] dropout_110 (Dropout)        (None, 64)                0         
2019-12-26 19:10:59,854 [INFO] _________________________________________________________________
2019-12-26 19:10:59,854 [INFO] dense_153 (Dense)            (None, 128)               8320      
2019-12-26 19:10:59,854 [INFO] _________________________________________________________________
2019-12-26 19:10:59,854 [INFO] batch_normalization_111 (Bat (None, 128)               512       
2019-12-26 19:10:59,854 [INFO] _________________________________________________________________
2019-12-26 19:10:59,854 [INFO] dropout_111 (Dropout)        (None, 128)               0         
2019-12-26 19:10:59,855 [INFO] _________________________________________________________________
2019-12-26 19:10:59,855 [INFO] dense_154 (Dense)            (None, 77)                9933      
2019-12-26 19:10:59,855 [INFO] =================================================================
2019-12-26 19:10:59,855 [INFO] Total params: 42,349
2019-12-26 19:10:59,855 [INFO] Trainable params: 41,517
2019-12-26 19:10:59,855 [INFO] Non-trainable params: 832
2019-12-26 19:10:59,855 [INFO] _________________________________________________________________
2019-12-26 19:11:00,018 [INFO] _________________________________________________________________
2019-12-26 19:11:00,019 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-26 19:11:00,019 [INFO] =================================================================
2019-12-26 19:11:00,019 [INFO] dense_155 (Dense)            (None, 64)                2112      
2019-12-26 19:11:00,019 [INFO] _________________________________________________________________
2019-12-26 19:11:00,019 [INFO] batch_normalization_112 (Bat (None, 64)                256       
2019-12-26 19:11:00,019 [INFO] _________________________________________________________________
2019-12-26 19:11:00,019 [INFO] dropout_112 (Dropout)        (None, 64)                0         
2019-12-26 19:11:00,019 [INFO] _________________________________________________________________
2019-12-26 19:11:00,019 [INFO] dense_156 (Dense)            (None, 15)                975       
2019-12-26 19:11:00,019 [INFO] =================================================================
2019-12-26 19:11:00,020 [INFO] Total params: 3,343
2019-12-26 19:11:00,020 [INFO] Trainable params: 3,215
2019-12-26 19:11:00,020 [INFO] Non-trainable params: 128
2019-12-26 19:11:00,020 [INFO] _________________________________________________________________
2019-12-26 19:11:00,020 [INFO] Training model
2019-12-26 19:11:00,020 [INFO] Splitting train set into 2 sets (unsupervised, supervised), random_seed = None
2019-12-26 19:11:31,565 [INFO] Split sizes (instances). total = 1936462, unsupervised = 484115, supervised = 1452347, unsupervised dataset hash = 05e75c92b68fdc060c0b341abad6d2ae51117178
2019-12-26 19:11:31,565 [INFO] Training autoencoder
 - val_f1: 0.9832
Train on 484115 samples, validate on 645487 samples
Epoch 1/200
 - 36s - loss: -2.9154e+00 - val_loss: -3.5575e+00
Epoch 2/200
 - 32s - loss: -3.4835e+00 - val_loss: -3.5952e+00
Epoch 3/200
 - 32s - loss: -3.5247e+00 - val_loss: -3.6104e+00
Epoch 4/200
 - 32s - loss: -3.5437e+00 - val_loss: -3.6211e+00
Epoch 5/200
 - 32s - loss: -3.5563e+00 - val_loss: -3.6260e+00
Epoch 6/200
 - 32s - loss: -3.5653e+00 - val_loss: -3.6314e+00
Epoch 7/200
 - 33s - loss: -3.5712e+00 - val_loss: -3.6340e+00
Epoch 8/200
 - 32s - loss: -3.5761e+00 - val_loss: -3.6348e+00
Epoch 9/200
 - 32s - loss: -3.5812e+00 - val_loss: -3.6387e+00
Epoch 10/200
 - 33s - loss: -3.5857e+00 - val_loss: -3.6403e+00
Epoch 11/200
 - 32s - loss: -3.5879e+00 - val_loss: -3.6417e+00
Epoch 12/200
 - 32s - loss: -3.5910e+00 - val_loss: -3.6441e+00
Epoch 13/200
 - 32s - loss: -3.5934e+00 - val_loss: -3.6451e+00
Epoch 14/200
 - 32s - loss: -3.5954e+00 - val_loss: -3.6472e+00
Epoch 15/200
 - 32s - loss: -3.5964e+00 - val_loss: -3.6471e+00
Epoch 16/200
 - 32s - loss: -3.5979e+00 - val_loss: -3.6485e+00
Epoch 17/200
 - 32s - loss: -3.5989e+00 - val_loss: -3.6501e+00
Epoch 18/200
 - 32s - loss: -3.6009e+00 - val_loss: -3.6493e+00
Epoch 19/200
 - 32s - loss: -3.6026e+00 - val_loss: -3.6482e+00
Epoch 20/200
 - 32s - loss: -3.6029e+00 - val_loss: -3.6515e+00
Epoch 21/200
 - 32s - loss: -3.6051e+00 - val_loss: -3.6534e+00
2019-12-26 19:23:22,426 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep2/ae_model_epoch_20.pickle
Epoch 22/200
 - 32s - loss: -3.6048e+00 - val_loss: -3.6515e+00
Epoch 23/200
 - 32s - loss: -3.6064e+00 - val_loss: -3.6526e+00
Epoch 24/200
 - 32s - loss: -3.6071e+00 - val_loss: -3.6553e+00
Epoch 25/200
 - 32s - loss: -3.6077e+00 - val_loss: -3.6562e+00
Epoch 26/200
 - 33s - loss: -3.6089e+00 - val_loss: -3.6554e+00
Epoch 27/200
 - 32s - loss: -3.6091e+00 - val_loss: -3.6561e+00
Epoch 28/200
 - 33s - loss: -3.6099e+00 - val_loss: -3.6557e+00
Epoch 29/200
 - 32s - loss: -3.6111e+00 - val_loss: -3.6554e+00
Epoch 30/200
 - 32s - loss: -3.6109e+00 - val_loss: -3.6566e+00
Epoch 31/200
 - 32s - loss: -3.6121e+00 - val_loss: -3.6567e+00
Epoch 32/200
 - 32s - loss: -3.6122e+00 - val_loss: -3.6563e+00
Epoch 33/200
 - 32s - loss: -3.6128e+00 - val_loss: -3.6590e+00
Epoch 34/200
 - 32s - loss: -3.6130e+00 - val_loss: -3.6578e+00
Epoch 35/200
 - 32s - loss: -3.6143e+00 - val_loss: -3.6588e+00
Epoch 36/200
 - 32s - loss: -3.6144e+00 - val_loss: -3.6597e+00
Epoch 37/200
 - 32s - loss: -3.6147e+00 - val_loss: -3.6615e+00
Epoch 38/200
 - 32s - loss: -3.6155e+00 - val_loss: -3.6612e+00
Epoch 39/200
 - 32s - loss: -3.6149e+00 - val_loss: -3.6616e+00
Epoch 40/200
 - 32s - loss: -3.6161e+00 - val_loss: -3.6611e+00
Epoch 41/200
 - 32s - loss: -3.6169e+00 - val_loss: -3.6562e+00
2019-12-26 19:34:11,721 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep2/ae_model_epoch_40.pickle
Epoch 42/200
 - 32s - loss: -3.6161e+00 - val_loss: -3.6600e+00
Epoch 43/200
 - 32s - loss: -3.6163e+00 - val_loss: -3.6612e+00
Epoch 44/200
 - 32s - loss: -3.6164e+00 - val_loss: -3.6603e+00
Epoch 45/200
 - 32s - loss: -3.6169e+00 - val_loss: -3.6612e+00
Epoch 46/200
 - 32s - loss: -3.6174e+00 - val_loss: -3.6609e+00
Epoch 47/200
 - 33s - loss: -3.6183e+00 - val_loss: -3.6610e+00
Epoch 48/200
 - 32s - loss: -3.6189e+00 - val_loss: -3.6619e+00
Epoch 49/200
 - 32s - loss: -3.6191e+00 - val_loss: -3.6614e+00
Epoch 50/200
 - 32s - loss: -3.6182e+00 - val_loss: -3.6605e+00
Epoch 51/200
 - 33s - loss: -3.6186e+00 - val_loss: -3.6611e+00
Epoch 52/200
 - 33s - loss: -3.6193e+00 - val_loss: -3.6635e+00
Epoch 53/200
 - 33s - loss: -3.6184e+00 - val_loss: -3.6615e+00
Epoch 54/200
 - 32s - loss: -3.6192e+00 - val_loss: -3.6619e+00
Epoch 55/200
 - 32s - loss: -3.6191e+00 - val_loss: -3.6617e+00
Epoch 56/200
 - 32s - loss: -3.6194e+00 - val_loss: -3.6633e+00
Epoch 57/200
 - 32s - loss: -3.6205e+00 - val_loss: -3.6613e+00
Epoch 58/200
 - 33s - loss: -3.6196e+00 - val_loss: -3.6619e+00
Epoch 59/200
 - 32s - loss: -3.6192e+00 - val_loss: -3.6609e+00
Epoch 60/200
 - 32s - loss: -3.6206e+00 - val_loss: -3.6616e+00
Epoch 61/200
 - 32s - loss: -3.6202e+00 - val_loss: -3.6624e+00
2019-12-26 19:45:01,441 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep2/ae_model_epoch_60.pickle
Epoch 62/200
 - 33s - loss: -3.6202e+00 - val_loss: -3.6641e+00
Epoch 63/200
 - 32s - loss: -3.6198e+00 - val_loss: -3.6622e+00
Epoch 64/200
 - 32s - loss: -3.6213e+00 - val_loss: -3.6649e+00
Epoch 65/200
 - 33s - loss: -3.6206e+00 - val_loss: -3.6636e+00
Epoch 66/200
 - 32s - loss: -3.6213e+00 - val_loss: -3.6632e+00
Epoch 67/200
 - 32s - loss: -3.6219e+00 - val_loss: -3.6621e+00
Epoch 68/200
 - 32s - loss: -3.6202e+00 - val_loss: -3.6623e+00
Epoch 69/200
 - 32s - loss: -3.6214e+00 - val_loss: -3.6622e+00
Epoch 70/200
 - 32s - loss: -3.6219e+00 - val_loss: -3.6629e+00
Epoch 71/200
 - 32s - loss: -3.6216e+00 - val_loss: -3.6639e+00
Epoch 72/200
 - 32s - loss: -3.6214e+00 - val_loss: -3.6632e+00
Epoch 73/200
 - 33s - loss: -3.6221e+00 - val_loss: -3.6622e+00
Epoch 74/200
 - 32s - loss: -3.6223e+00 - val_loss: -3.6623e+00
Epoch 75/200
 - 32s - loss: -3.6210e+00 - val_loss: -3.6624e+00
Epoch 76/200
 - 32s - loss: -3.6216e+00 - val_loss: -3.6624e+00
Epoch 77/200
 - 33s - loss: -3.6221e+00 - val_loss: -3.6625e+00
Epoch 78/200
 - 32s - loss: -3.6226e+00 - val_loss: -3.6639e+00
Epoch 79/200
 - 32s - loss: -3.6229e+00 - val_loss: -3.6627e+00
Epoch 80/200
 - 33s - loss: -3.6233e+00 - val_loss: -3.6641e+00
Epoch 81/200
 - 33s - loss: -3.6229e+00 - val_loss: -3.6620e+00
2019-12-26 19:55:51,245 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep2/ae_model_epoch_80.pickle
Epoch 82/200
 - 33s - loss: -3.6227e+00 - val_loss: -3.6635e+00
Epoch 83/200
 - 33s - loss: -3.6223e+00 - val_loss: -3.6637e+00
Epoch 84/200
 - 32s - loss: -3.6222e+00 - val_loss: -3.6634e+00
Epoch 85/200
 - 33s - loss: -3.6230e+00 - val_loss: -3.6631e+00
Epoch 86/200
 - 33s - loss: -3.6233e+00 - val_loss: -3.6632e+00
Epoch 87/200
 - 33s - loss: -3.6235e+00 - val_loss: -3.6630e+00
Epoch 88/200
 - 32s - loss: -3.6234e+00 - val_loss: -3.6641e+00
Epoch 89/200
 - 33s - loss: -3.6224e+00 - val_loss: -3.6628e+00
Epoch 90/200
 - 33s - loss: -3.6230e+00 - val_loss: -3.6644e+00
Epoch 91/200
 - 32s - loss: -3.6236e+00 - val_loss: -3.6623e+00
Epoch 92/200
 - 32s - loss: -3.6243e+00 - val_loss: -3.6636e+00
Epoch 93/200
 - 32s - loss: -3.6242e+00 - val_loss: -3.6630e+00
Epoch 94/200
 - 32s - loss: -3.6240e+00 - val_loss: -3.6634e+00
Epoch 95/200
 - 32s - loss: -3.6242e+00 - val_loss: -3.6634e+00
Epoch 96/200
 - 32s - loss: -3.6242e+00 - val_loss: -3.6630e+00
Epoch 97/200
 - 33s - loss: -3.6247e+00 - val_loss: -3.6635e+00
Epoch 98/200
 - 32s - loss: -3.6239e+00 - val_loss: -3.6631e+00
Epoch 99/200
 - 32s - loss: -3.6238e+00 - val_loss: -3.6634e+00
Epoch 100/200
 - 33s - loss: -3.6243e+00 - val_loss: -3.6627e+00
Epoch 101/200
 - 32s - loss: -3.6242e+00 - val_loss: -3.6642e+00
2019-12-26 20:06:41,020 [INFO] epoch = 100. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep2/ae_model_epoch_100.pickle
Epoch 102/200
 - 33s - loss: -3.6243e+00 - val_loss: -3.6634e+00
Epoch 103/200
 - 32s - loss: -3.6245e+00 - val_loss: -3.6644e+00
Epoch 104/200
 - 33s - loss: -3.6246e+00 - val_loss: -3.6640e+00
Epoch 105/200
 - 32s - loss: -3.6248e+00 - val_loss: -3.6631e+00
Epoch 106/200
 - 32s - loss: -3.6248e+00 - val_loss: -3.6648e+00
Epoch 107/200
 - 32s - loss: -3.6242e+00 - val_loss: -3.6641e+00
Epoch 108/200
 - 32s - loss: -3.6247e+00 - val_loss: -3.6652e+00
Epoch 109/200
 - 32s - loss: -3.6245e+00 - val_loss: -3.6649e+00
Epoch 110/200
 - 32s - loss: -3.6252e+00 - val_loss: -3.6634e+00
Epoch 111/200
 - 33s - loss: -3.6249e+00 - val_loss: -3.6644e+00
Epoch 112/200
 - 33s - loss: -3.6247e+00 - val_loss: -3.6648e+00
Epoch 113/200
 - 33s - loss: -3.6238e+00 - val_loss: -3.6646e+00
Epoch 114/200
 - 32s - loss: -3.6256e+00 - val_loss: -3.6641e+00
Epoch 115/200
 - 33s - loss: -3.6251e+00 - val_loss: -3.6642e+00
Epoch 116/200
 - 32s - loss: -3.6252e+00 - val_loss: -3.6637e+00
Epoch 117/200
 - 33s - loss: -3.6254e+00 - val_loss: -3.6636e+00
Epoch 118/200
 - 32s - loss: -3.6263e+00 - val_loss: -3.6636e+00
Epoch 119/200
 - 33s - loss: -3.6263e+00 - val_loss: -3.6646e+00
Epoch 120/200
 - 33s - loss: -3.6258e+00 - val_loss: -3.6640e+00
Epoch 121/200
 - 32s - loss: -3.6257e+00 - val_loss: -3.6639e+00
2019-12-26 20:17:31,170 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep2/ae_model_epoch_120.pickle
Epoch 122/200
 - 32s - loss: -3.6257e+00 - val_loss: -3.6639e+00
Epoch 123/200
 - 32s - loss: -3.6251e+00 - val_loss: -3.6648e+00
Epoch 124/200
 - 32s - loss: -3.6258e+00 - val_loss: -3.6646e+00
Epoch 125/200
 - 32s - loss: -3.6255e+00 - val_loss: -3.6643e+00
Epoch 126/200
 - 32s - loss: -3.6259e+00 - val_loss: -3.6634e+00
Epoch 127/200
 - 32s - loss: -3.6262e+00 - val_loss: -3.6637e+00
Epoch 128/200
 - 32s - loss: -3.6256e+00 - val_loss: -3.6639e+00
Epoch 129/200
 - 32s - loss: -3.6262e+00 - val_loss: -3.6646e+00
Epoch 130/200
 - 32s - loss: -3.6261e+00 - val_loss: -3.6647e+00
Epoch 131/200
 - 32s - loss: -3.6249e+00 - val_loss: -3.6650e+00
Epoch 132/200
 - 32s - loss: -3.6264e+00 - val_loss: -3.6647e+00
Epoch 133/200
 - 32s - loss: -3.6265e+00 - val_loss: -3.6663e+00
Epoch 134/200
 - 32s - loss: -3.6259e+00 - val_loss: -3.6642e+00
Epoch 135/200
 - 33s - loss: -3.6256e+00 - val_loss: -3.6660e+00
Epoch 136/200
 - 32s - loss: -3.6265e+00 - val_loss: -3.6632e+00
Epoch 137/200
 - 32s - loss: -3.6255e+00 - val_loss: -3.6645e+00
Epoch 138/200
 - 32s - loss: -3.6264e+00 - val_loss: -3.6649e+00
Epoch 139/200
 - 33s - loss: -3.6269e+00 - val_loss: -3.6650e+00
Epoch 140/200
 - 32s - loss: -3.6266e+00 - val_loss: -3.6625e+00
Epoch 141/200
 - 32s - loss: -3.6268e+00 - val_loss: -3.6666e+00
2019-12-26 20:28:20,590 [INFO] epoch = 140. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep2/ae_model_epoch_140.pickle
Epoch 142/200
 - 33s - loss: -3.6259e+00 - val_loss: -3.6656e+00
Epoch 143/200
 - 33s - loss: -3.6274e+00 - val_loss: -3.6668e+00
Epoch 144/200
 - 32s - loss: -3.6267e+00 - val_loss: -3.6655e+00
Epoch 145/200
 - 33s - loss: -3.6264e+00 - val_loss: -3.6658e+00
Epoch 146/200
 - 32s - loss: -3.6258e+00 - val_loss: -3.6656e+00
Epoch 147/200
 - 32s - loss: -3.6266e+00 - val_loss: -3.6650e+00
Epoch 148/200
 - 32s - loss: -3.6267e+00 - val_loss: -3.6667e+00
Epoch 149/200
 - 32s - loss: -3.6266e+00 - val_loss: -3.6664e+00
Epoch 150/200
 - 32s - loss: -3.6271e+00 - val_loss: -3.6644e+00
Epoch 151/200
 - 32s - loss: -3.6262e+00 - val_loss: -3.6635e+00
Epoch 152/200
 - 33s - loss: -3.6275e+00 - val_loss: -3.6645e+00
Epoch 153/200
 - 32s - loss: -3.6274e+00 - val_loss: -3.6637e+00
Epoch 154/200
 - 32s - loss: -3.6264e+00 - val_loss: -3.6642e+00
Epoch 155/200
 - 33s - loss: -3.6279e+00 - val_loss: -3.6659e+00
Epoch 156/200
 - 33s - loss: -3.6270e+00 - val_loss: -3.6643e+00
Epoch 157/200
 - 33s - loss: -3.6272e+00 - val_loss: -3.6658e+00
Epoch 158/200
 - 33s - loss: -3.6267e+00 - val_loss: -3.6662e+00
Epoch 159/200
 - 32s - loss: -3.6275e+00 - val_loss: -3.6649e+00
Epoch 160/200
 - 33s - loss: -3.6279e+00 - val_loss: -3.6657e+00
Epoch 161/200
 - 32s - loss: -3.6271e+00 - val_loss: -3.6643e+00
2019-12-26 20:39:10,573 [INFO] epoch = 160. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep2/ae_model_epoch_160.pickle
Epoch 162/200
 - 32s - loss: -3.6271e+00 - val_loss: -3.6637e+00
Epoch 163/200
 - 33s - loss: -3.6275e+00 - val_loss: -3.6654e+00
Epoch 164/200
 - 32s - loss: -3.6279e+00 - val_loss: -3.6644e+00
Epoch 165/200
 - 33s - loss: -3.6275e+00 - val_loss: -3.6659e+00
Epoch 166/200
 - 32s - loss: -3.6272e+00 - val_loss: -3.6651e+00
Epoch 167/200
 - 33s - loss: -3.6283e+00 - val_loss: -3.6654e+00
Epoch 168/200
 - 32s - loss: -3.6277e+00 - val_loss: -3.6645e+00
Epoch 169/200
 - 33s - loss: -3.6279e+00 - val_loss: -3.6663e+00
Epoch 170/200
 - 33s - loss: -3.6276e+00 - val_loss: -3.6654e+00
Epoch 171/200
 - 32s - loss: -3.6279e+00 - val_loss: -3.6652e+00
Epoch 172/200
 - 32s - loss: -3.6281e+00 - val_loss: -3.6649e+00
Epoch 173/200
 - 33s - loss: -3.6279e+00 - val_loss: -3.6653e+00
Epoch 174/200
 - 32s - loss: -3.6283e+00 - val_loss: -3.6643e+00
Epoch 175/200
 - 33s - loss: -3.6282e+00 - val_loss: -3.6652e+00
Epoch 176/200
 - 33s - loss: -3.6281e+00 - val_loss: -3.6658e+00
Epoch 177/200
 - 33s - loss: -3.6273e+00 - val_loss: -3.6648e+00
Epoch 178/200
 - 32s - loss: -3.6274e+00 - val_loss: -3.6658e+00
Epoch 179/200
 - 32s - loss: -3.6281e+00 - val_loss: -3.6656e+00
Epoch 180/200
 - 32s - loss: -3.6286e+00 - val_loss: -3.6646e+00
Epoch 181/200
 - 32s - loss: -3.6280e+00 - val_loss: -3.6643e+00
2019-12-26 20:50:00,505 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep2/ae_model_epoch_180.pickle
Epoch 182/200
 - 32s - loss: -3.6278e+00 - val_loss: -3.6668e+00
Epoch 183/200
 - 32s - loss: -3.6271e+00 - val_loss: -3.6652e+00
Epoch 184/200
 - 33s - loss: -3.6280e+00 - val_loss: -3.6649e+00
Epoch 185/200
 - 32s - loss: -3.6278e+00 - val_loss: -3.6642e+00
Epoch 186/200
 - 32s - loss: -3.6282e+00 - val_loss: -3.6653e+00
Epoch 187/200
 - 32s - loss: -3.6282e+00 - val_loss: -3.6653e+00
Epoch 188/200
 - 32s - loss: -3.6280e+00 - val_loss: -3.6648e+00
Epoch 189/200
 - 32s - loss: -3.6272e+00 - val_loss: -3.6644e+00
Epoch 190/200
 - 33s - loss: -3.6288e+00 - val_loss: -3.6661e+00
Epoch 191/200
 - 32s - loss: -3.6283e+00 - val_loss: -3.6653e+00
Epoch 192/200
 - 32s - loss: -3.6287e+00 - val_loss: -3.6642e+00
Epoch 193/200
 - 32s - loss: -3.6283e+00 - val_loss: -3.6642e+00
2019-12-26 20:56:30,124 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-26 20:58:03,799 [INFO] Last epoch loss evaluation: train_loss = -3.657982, val_loss = -3.666823
2019-12-26 20:58:03,799 [INFO] Training autoencoder complete
2019-12-26 20:58:03,800 [INFO] Encoding data for supervised training
2019-12-26 21:00:03,854 [INFO] Encoding complete
2019-12-26 21:00:03,855 [INFO] Training neural network layers (after autoencoder)
Epoch 00193: early stopping
Train on 1452347 samples, validate on 645487 samples
Epoch 1/200
 - 32s - loss: 0.0161 - val_loss: 0.0089
 - val_f1: 0.9816
Epoch 2/200
 - 30s - loss: 0.0092 - val_loss: 0.0100
 - val_f1: 0.9804
Epoch 3/200
 - 30s - loss: 0.0089 - val_loss: 0.0084
 - val_f1: 0.9802
Epoch 4/200
 - 30s - loss: 0.0088 - val_loss: 0.0086
 - val_f1: 0.9821
Epoch 5/200
 - 30s - loss: 0.0086 - val_loss: 0.0085
 - val_f1: 0.9824
Epoch 6/200
 - 30s - loss: 0.0086 - val_loss: 0.0083
 - val_f1: 0.9807
Epoch 7/200
 - 30s - loss: 0.0085 - val_loss: 0.0089
 - val_f1: 0.9818
Epoch 8/200
 - 30s - loss: 0.0085 - val_loss: 0.0081
 - val_f1: 0.9834
Epoch 9/200
 - 30s - loss: 0.0085 - val_loss: 0.0083
 - val_f1: 0.9830
Epoch 10/200
 - 30s - loss: 0.0084 - val_loss: 0.0083
 - val_f1: 0.9827
Epoch 11/200
 - 30s - loss: 0.0085 - val_loss: 0.0084
 - val_f1: 0.9815
Epoch 12/200
 - 30s - loss: 0.0084 - val_loss: 0.0081
 - val_f1: 0.9829
Epoch 13/200
 - 30s - loss: 0.0084 - val_loss: 0.0081
 - val_f1: 0.9829
Epoch 14/200
 - 30s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9829
Epoch 15/200
 - 30s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9828
Epoch 16/200
 - 30s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9823
Epoch 17/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 18/200
 - 30s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9833
Epoch 19/200
 - 30s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9830
Epoch 20/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 21/200
 - 30s - loss: 0.0083 - val_loss: 0.0082
2019-12-26 21:22:01,620 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep2/ann_model_epoch_20.pickle
 - val_f1: 0.9830
Epoch 22/200
 - 30s - loss: 0.0083 - val_loss: 0.0084
 - val_f1: 0.9824
Epoch 23/200
 - 30s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9832
Epoch 24/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 25/200
 - 30s - loss: 0.0082 - val_loss: 0.0157
 - val_f1: 0.9613
Epoch 26/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 27/200
 - 30s - loss: 0.0082 - val_loss: 0.0084
 - val_f1: 0.9820
Epoch 28/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9827
Epoch 29/200
 - 30s - loss: 0.0082 - val_loss: 0.0085
 - val_f1: 0.9814
Epoch 30/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 31/200
 - 30s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9815
Epoch 32/200
 - 30s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9824
Epoch 33/200
 - 30s - loss: 0.0082 - val_loss: 0.0082
 - val_f1: 0.9827
Epoch 34/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9835
Epoch 35/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 36/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 37/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9828
Epoch 38/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9835
Epoch 39/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 40/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9834
Epoch 41/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
2019-12-26 21:43:05,427 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep2/ann_model_epoch_40.pickle
 - val_f1: 0.9827
Epoch 42/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 43/200
 - 30s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9829
Epoch 44/200
 - 30s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9815
Epoch 45/200
 - 30s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 46/200
 - 30s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 47/200
 - 30s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9813
Epoch 48/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 49/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9830
Epoch 50/200
 - 30s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 51/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9815
Epoch 52/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9834
Epoch 53/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9833
Epoch 54/200
 - 30s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 55/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 56/200
 - 30s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9835
Epoch 57/200
 - 30s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 58/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9830
Epoch 59/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9833
Epoch 60/200
 - 30s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 61/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
2019-12-26 22:04:08,986 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep2/ann_model_epoch_60.pickle
 - val_f1: 0.9832
Epoch 62/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9836
Epoch 63/200
 - 30s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9833
Epoch 64/200
 - 30s - loss: 0.0081 - val_loss: 0.0087
 - val_f1: 0.9800
Epoch 65/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9835
Epoch 66/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9833
Epoch 67/200
 - 30s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 68/200
 - 30s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9815
Epoch 69/200
 - 30s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9834
Epoch 70/200
 - 30s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 71/200
 - 30s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 72/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9833
Epoch 73/200
 - 30s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 74/200
 - 30s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 75/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 76/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9835
Epoch 77/200
 - 30s - loss: 0.0081 - val_loss: 0.0082
 - val_f1: 0.9834
Epoch 78/200
 - 30s - loss: 0.0081 - val_loss: 0.0083
 - val_f1: 0.9815
Epoch 79/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9825
Epoch 80/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9830
Epoch 81/200
 - 30s - loss: 0.0081 - val_loss: 0.0080
2019-12-26 22:25:13,192 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep2/ann_model_epoch_80.pickle
 - val_f1: 0.9833
Epoch 82/200
 - 30s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9834
Epoch 83/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 84/200
 - 30s - loss: 0.0081 - val_loss: 0.0084
 - val_f1: 0.9817
Epoch 85/200
 - 30s - loss: 0.0081 - val_loss: 0.0084
 - val_f1: 0.9811
Epoch 86/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9830
Epoch 87/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 88/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9834
Epoch 89/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9833
Epoch 90/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9835
Epoch 91/200
 - 30s - loss: 0.0081 - val_loss: 0.0082
 - val_f1: 0.9824
Epoch 92/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 93/200
 - 30s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9834
Epoch 94/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 95/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 96/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 97/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 98/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 99/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9819
Epoch 100/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9836
Epoch 101/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
2019-12-26 22:46:17,234 [INFO] epoch = 100. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep2/ann_model_epoch_100.pickle
 - val_f1: 0.9835
Epoch 102/200
 - 30s - loss: 0.0081 - val_loss: 0.0082
 - val_f1: 0.9833
Epoch 103/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9833
Epoch 104/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 105/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9819
Epoch 106/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9830
Epoch 107/200
 - 30s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 108/200
 - 30s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9834
Epoch 109/200
 - 30s - loss: 0.0080 - val_loss: 0.0078
 - val_f1: 0.9837
Epoch 110/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 111/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 112/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 113/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 114/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9834
Epoch 115/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9834
Epoch 116/200
 - 30s - loss: 0.0081 - val_loss: 0.0084
 - val_f1: 0.9827
Epoch 117/200
 - 30s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 118/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9818
Epoch 119/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9834
Epoch 120/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9834
Epoch 121/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
2019-12-26 23:07:21,457 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep2/ann_model_epoch_120.pickle
 - val_f1: 0.9835
Epoch 122/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9835
Epoch 123/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9833
Epoch 124/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9835
Epoch 125/200
 - 30s - loss: 0.0080 - val_loss: 0.0078
 - val_f1: 0.9837
Epoch 126/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 127/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9834
Epoch 128/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9817
Epoch 129/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9830
Epoch 130/200
 - 30s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9819
Epoch 131/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 132/200
 - 30s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9833
Epoch 133/200
 - 30s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 134/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 135/200
 - 30s - loss: 0.0080 - val_loss: 0.0084
 - val_f1: 0.9809
Epoch 136/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 137/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9836
Epoch 138/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9834
Epoch 139/200
 - 30s - loss: 0.0080 - val_loss: 0.0081
 - val_f1: 0.9833
Epoch 140/200
 - 30s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9826
Epoch 141/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
2019-12-26 23:28:25,944 [INFO] epoch = 140. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep2/ann_model_epoch_140.pickle
 - val_f1: 0.9832
Epoch 142/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 143/200
 - 30s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9828
Epoch 144/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9836
Epoch 145/200
 - 30s - loss: 0.0080 - val_loss: 0.0078
 - val_f1: 0.9834
Epoch 146/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9833
Epoch 147/200
 - 30s - loss: 0.0080 - val_loss: 0.0078
 - val_f1: 0.9836
Epoch 148/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 149/200
 - 30s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9822
Epoch 150/200
 - 30s - loss: 0.0080 - val_loss: 0.0078
 - val_f1: 0.9836
Epoch 151/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9818
Epoch 152/200
 - 30s - loss: 0.0080 - val_loss: 0.0078
 - val_f1: 0.9832
Epoch 153/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 154/200
 - 30s - loss: 0.0080 - val_loss: 0.0078
 - val_f1: 0.9836
Epoch 155/200
 - 30s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 156/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9835
Epoch 157/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9836
Epoch 158/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 159/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9833
Epoch 160/200
 - 30s - loss: 0.0080 - val_loss: 0.0078
 - val_f1: 0.9833
Epoch 161/200
 - 30s - loss: 0.0080 - val_loss: 0.0078
2019-12-26 23:49:34,661 [INFO] epoch = 160. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep2/ann_model_epoch_160.pickle
 - val_f1: 0.9833
Epoch 162/200
 - 30s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9823
Epoch 163/200
 - 30s - loss: 0.0080 - val_loss: 0.0083
 - val_f1: 0.9829
Epoch 164/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9835
Epoch 165/200
 - 30s - loss: 0.0080 - val_loss: 0.0078
 - val_f1: 0.9831
Epoch 166/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 167/200
 - 30s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 168/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9835
Epoch 169/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9836
Epoch 170/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 171/200
 - 30s - loss: 0.0080 - val_loss: 0.0078
 - val_f1: 0.9833
Epoch 172/200
 - 30s - loss: 0.0080 - val_loss: 0.0078
 - val_f1: 0.9837
Epoch 173/200
 - 30s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 174/200
 - 30s - loss: 0.0080 - val_loss: 0.0078
 - val_f1: 0.9835
Epoch 175/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9837
Epoch 176/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9836
Epoch 177/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9836
Epoch 178/200
 - 30s - loss: 0.0080 - val_loss: 0.0078
 - val_f1: 0.9837
Epoch 179/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 180/200
 - 30s - loss: 0.0080 - val_loss: 0.0078
 - val_f1: 0.9835
Epoch 181/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
2019-12-27 00:10:49,667 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep2/ann_model_epoch_180.pickle
 - val_f1: 0.9831
Epoch 182/200
 - 30s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 183/200
 - 30s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9825
Epoch 184/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9835
Epoch 185/200
 - 30s - loss: 0.0080 - val_loss: 0.0078
 - val_f1: 0.9837
Epoch 186/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9836
Epoch 187/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 188/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9836
Epoch 189/200
 - 30s - loss: 0.0080 - val_loss: 0.0078
 - val_f1: 0.9835
Epoch 190/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9836
Epoch 191/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9833
Epoch 192/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9835
Epoch 193/200
 - 30s - loss: 0.0080 - val_loss: 0.0078
 - val_f1: 0.9834
Epoch 194/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9836
Epoch 195/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 196/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 197/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 198/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9833
Epoch 199/200
 - 30s - loss: 0.0080 - val_loss: 0.0078
 - val_f1: 0.9831
Epoch 200/200
 - 30s - loss: 0.0080 - val_loss: 0.0079
2019-12-27 00:31:35,119 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-27 00:33:29,073 [INFO] Last epoch loss evaluation: train_loss = 0.007774, val_loss = 0.007819
2019-12-27 00:33:29,081 [INFO] Training complete. time_to_train = 19349.06 sec, 322.48 min
2019-12-27 00:33:29,116 [INFO] Model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep2/best_model.pickle
/home/sunanda/research/ids_experiments/utility.py:268: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
2019-12-27 00:33:29,302 [INFO] Plot saved to: results_selected_models/selected_ids18_subset_ae_ann_deep_rep2/training_error_history.png
/home/sunanda/research/ids_experiments/utility.py:285: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
2019-12-27 00:33:29,472 [INFO] Plot saved to: results_selected_models/selected_ids18_subset_ae_ann_deep_rep2/training_f1_history.png
2019-12-27 00:33:29,472 [INFO] Making predictions on training, validation, testing data
2019-12-27 00:39:11,041 [INFO] Evaluating predictions (results)
/home/sunanda/anaconda3/envs/ml_env_cpu/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2019-12-27 00:39:34,159 [INFO] Dataset: Testing. Classification report below
2019-12-27 00:39:34,159 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99    535650
                     Bot       1.00      1.00      1.00     11465
        Brute Force -Web       0.00      0.00      0.00        24
        Brute Force -XSS       1.00      0.33      0.50         9
        DDOS attack-HOIC       1.00      1.00      1.00     27447
    DDOS attack-LOIC-UDP       0.69      1.00      0.82        67
  DDoS attacks-LOIC-HTTP       0.99      0.99      0.99     23010
   DoS attacks-GoldenEye       0.99      0.99      0.99      1651
        DoS attacks-Hulk       1.00      1.00      1.00     18478
DoS attacks-SlowHTTPTest       0.75      0.49      0.59      5596
   DoS attacks-Slowloris       0.95      0.97      0.96       440
          FTP-BruteForce       0.70      0.88      0.78      7718
           Infilteration       0.49      0.02      0.04      6404
           SQL Injection       0.00      0.00      0.00         4
          SSH-Bruteforce       1.00      1.00      1.00      7525

                accuracy                           0.98    645488
               macro avg       0.77      0.71      0.71    645488
            weighted avg       0.98      0.98      0.98    645488

2019-12-27 00:39:34,159 [INFO] Overall accuracy (micro avg): 0.9835318394764891
/home/sunanda/research/ids_experiments/utility.py:307: RuntimeWarning: invalid value encountered in true_divide
  PPV = TP / (TP + FP)    # Precision or positive predictive value
/home/sunanda/research/ids_experiments/utility.py:311: RuntimeWarning: invalid value encountered in true_divide
  FDR = FP / (TP + FP)    # False discovery rate
2019-12-27 00:39:59,043 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9835         0.9835                       0.9835                0.0012                   0.0165  0.9835
1     Macro avg        0.9978         0.7698                       0.7115                0.0044                   0.2885  0.7107
2  Weighted avg        0.9910         0.9789                       0.9835                0.0488                   0.0165  0.9787
2019-12-27 00:40:22,159 [INFO] Dataset: Validation. Classification report below
2019-12-27 00:40:22,159 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99    535650
                     Bot       1.00      1.00      1.00     11465
        Brute Force -Web       0.00      0.00      0.00        25
        Brute Force -XSS       0.86      0.67      0.75         9
        DDOS attack-HOIC       1.00      1.00      1.00     27447
    DDOS attack-LOIC-UDP       0.74      0.99      0.84        68
  DDoS attacks-LOIC-HTTP       0.99      0.99      0.99     23009
   DoS attacks-GoldenEye       0.99      0.99      0.99      1651
        DoS attacks-Hulk       1.00      1.00      1.00     18478
DoS attacks-SlowHTTPTest       0.76      0.48      0.59      5596
   DoS attacks-Slowloris       0.94      0.97      0.96       439
          FTP-BruteForce       0.70      0.89      0.78      7718
           Infilteration       0.41      0.01      0.03      6403
           SQL Injection       0.00      0.00      0.00         4
          SSH-Bruteforce       1.00      1.00      1.00      7525

                accuracy                           0.98    645487
               macro avg       0.76      0.73      0.73    645487
            weighted avg       0.98      0.98      0.98    645487

2019-12-27 00:40:22,159 [INFO] Overall accuracy (micro avg): 0.98350857569556
/home/sunanda/research/ids_experiments/utility.py:313: RuntimeWarning: invalid value encountered in true_divide
  F1 = (2 * PPV * TPR) / (PPV + TPR)  # F1 score
2019-12-27 00:40:47,057 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9835         0.9835                       0.9835                0.0012                   0.0165  0.9835
1     Macro avg        0.9978         0.7580                       0.7328                0.0044                   0.2672  0.7283
2  Weighted avg        0.9910         0.9781                       0.9835                0.0489                   0.0165  0.9786
2019-12-27 00:42:02,995 [INFO] Dataset: Training. Classification report below
2019-12-27 00:42:02,996 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99   1606949
                     Bot       1.00      1.00      1.00     34396
        Brute Force -Web       0.00      0.00      0.00        73
        Brute Force -XSS       1.00      0.50      0.67        26
        DDOS attack-HOIC       1.00      1.00      1.00     82341
    DDOS attack-LOIC-UDP       0.71      0.98      0.82       203
  DDoS attacks-LOIC-HTTP       0.99      0.99      0.99     69029
   DoS attacks-GoldenEye       0.99      0.99      0.99      4954
        DoS attacks-Hulk       1.00      1.00      1.00     55435
DoS attacks-SlowHTTPTest       0.75      0.48      0.58     16787
   DoS attacks-Slowloris       0.95      0.99      0.97      1318
          FTP-BruteForce       0.70      0.89      0.78     23153
           Infilteration       0.49      0.02      0.04     19210
           SQL Injection       0.00      0.00      0.00        12
          SSH-Bruteforce       1.00      1.00      1.00     22576

                accuracy                           0.98   1936462
               macro avg       0.77      0.72      0.72   1936462
            weighted avg       0.98      0.98      0.98   1936462

2019-12-27 00:42:02,996 [INFO] Overall accuracy (micro avg): 0.9835199451370592
2019-12-27 00:43:24,825 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9835         0.9835                       0.9835                0.0012                   0.0165  0.9835
1     Macro avg        0.9978         0.7712                       0.7224                0.0043                   0.2776  0.7225
2  Weighted avg        0.9910         0.9789                       0.9835                0.0487                   0.0165  0.9786
2019-12-27 00:43:24,899 [INFO] Results saved to: results_selected_models/selected_ids18_subset_ae_ann_deep_rep2/selected_ids18_subset_ae_ann_deep_rep2_results.xlsx
2019-12-27 00:43:24,903 [INFO] ================= Finished running experiment no. 2 ================= 

2019-12-27 00:43:24,954 [INFO] Created directory: results_selected_models/selected_ids18_subset_ae_ann_deep_rep3
2019-12-27 00:43:24,954 [INFO] Initialized logging. log_filename = results_selected_models/selected_ids18_subset_ae_ann_deep_rep3/run_log.log
2019-12-27 00:43:24,954 [INFO] ================= Running experiment no. 3  ================= 

2019-12-27 00:43:24,954 [INFO] Experiment parameters given below
2019-12-27 00:43:24,954 [INFO] 
{'experiment_num': 3, 'results_dir': 'results_selected_models/selected_ids18_subset_ae_ann_deep_rep3', 'model_type': 'classifier', 'model': 'ae_ann', 'normal_label': 'BENIGN', 'scaling_type': 'NA', 'unsupervised_ratio': 0.25, 'ae_encoder_units': [128, 64, 32], 'ae_encoder_activations': ['relu', 'relu', 'relu'], 'ae_encoder_dropout_rates': [0.2, 0.2, 0.2], 'ae_encoder_l1_param': -1, 'ae_decoder_units': [64, 128], 'ae_decoder_activations': ['relu', 'relu'], 'ae_decoder_dropout_rates': [0.2, 0.2], 'output_activation': 'sigmoid', 'loss_function': 'binary_crossentropy', 'ae_epochs': 200, 'ann_layer_units': [64], 'ann_layer_activations': ['relu'], 'ann_layer_dropout_rates': [0.2], 'ann_epochs': 200, 'early_stop_patience': 50, 'batch_size': 256, 'dataset_dir': '../Datasets/small_datasets/ids2018', 'description': 'selected_ids18_subset_ae_ann_deep_rep3'}
2019-12-27 00:43:24,955 [INFO] Created tensorboard log directory: results_selected_models/selected_ids18_subset_ae_ann_deep_rep3/tf_logs_run_2019_12_27-00_43_24
2019-12-27 00:43:24,955 [INFO] Loading datsets from: ../Datasets/small_datasets/ids2018
2019-12-27 00:43:24,955 [INFO] Reading X, y files
2019-12-27 00:43:24,955 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_train.h5
2019-12-27 00:43:29,016 [INFO] Reading complete. time_to_read=4.06 seconds
2019-12-27 00:43:29,016 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_val.h5
2019-12-27 00:43:30,402 [INFO] Reading complete. time_to_read=1.39 seconds
2019-12-27 00:43:30,403 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_test.h5
2019-12-27 00:43:31,795 [INFO] Reading complete. time_to_read=1.39 seconds
2019-12-27 00:43:31,795 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_train.h5
2019-12-27 00:43:32,069 [INFO] Reading complete. time_to_read=0.27 seconds
2019-12-27 00:43:32,069 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_val.h5
2019-12-27 00:43:32,165 [INFO] Reading complete. time_to_read=0.10 seconds
2019-12-27 00:43:32,165 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_test.h5
2019-12-27 00:43:32,262 [INFO] Reading complete. time_to_read=0.10 seconds
2019-12-27 00:43:35,626 [INFO] Initializing model
2019-12-27 00:43:36,313 [INFO] _________________________________________________________________
2019-12-27 00:43:36,313 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-27 00:43:36,313 [INFO] =================================================================
2019-12-27 00:43:36,314 [INFO] dense_157 (Dense)            (None, 128)               9984      
2019-12-27 00:43:36,314 [INFO] _________________________________________________________________
2019-12-27 00:43:36,314 [INFO] batch_normalization_113 (Bat (None, 128)               512       
2019-12-27 00:43:36,314 [INFO] _________________________________________________________________
2019-12-27 00:43:36,314 [INFO] dropout_113 (Dropout)        (None, 128)               0         
2019-12-27 00:43:36,314 [INFO] _________________________________________________________________
2019-12-27 00:43:36,314 [INFO] dense_158 (Dense)            (None, 64)                8256      
2019-12-27 00:43:36,314 [INFO] _________________________________________________________________
2019-12-27 00:43:36,314 [INFO] batch_normalization_114 (Bat (None, 64)                256       
2019-12-27 00:43:36,314 [INFO] _________________________________________________________________
2019-12-27 00:43:36,314 [INFO] dropout_114 (Dropout)        (None, 64)                0         
2019-12-27 00:43:36,315 [INFO] _________________________________________________________________
2019-12-27 00:43:36,315 [INFO] dense_159 (Dense)            (None, 32)                2080      
2019-12-27 00:43:36,315 [INFO] _________________________________________________________________
2019-12-27 00:43:36,315 [INFO] batch_normalization_115 (Bat (None, 32)                128       
2019-12-27 00:43:36,315 [INFO] _________________________________________________________________
2019-12-27 00:43:36,315 [INFO] dropout_115 (Dropout)        (None, 32)                0         
2019-12-27 00:43:36,315 [INFO] _________________________________________________________________
2019-12-27 00:43:36,315 [INFO] dense_160 (Dense)            (None, 64)                2112      
2019-12-27 00:43:36,315 [INFO] _________________________________________________________________
2019-12-27 00:43:36,315 [INFO] batch_normalization_116 (Bat (None, 64)                256       
2019-12-27 00:43:36,315 [INFO] _________________________________________________________________
2019-12-27 00:43:36,315 [INFO] dropout_116 (Dropout)        (None, 64)                0         
2019-12-27 00:43:36,315 [INFO] _________________________________________________________________
2019-12-27 00:43:36,316 [INFO] dense_161 (Dense)            (None, 128)               8320      
2019-12-27 00:43:36,316 [INFO] _________________________________________________________________
2019-12-27 00:43:36,316 [INFO] batch_normalization_117 (Bat (None, 128)               512       
2019-12-27 00:43:36,316 [INFO] _________________________________________________________________
2019-12-27 00:43:36,316 [INFO] dropout_117 (Dropout)        (None, 128)               0         
2019-12-27 00:43:36,316 [INFO] _________________________________________________________________
2019-12-27 00:43:36,316 [INFO] dense_162 (Dense)            (None, 77)                9933      
2019-12-27 00:43:36,316 [INFO] =================================================================
2019-12-27 00:43:36,317 [INFO] Total params: 42,349
2019-12-27 00:43:36,317 [INFO] Trainable params: 41,517
2019-12-27 00:43:36,317 [INFO] Non-trainable params: 832
2019-12-27 00:43:36,317 [INFO] _________________________________________________________________
2019-12-27 00:43:36,482 [INFO] _________________________________________________________________
2019-12-27 00:43:36,482 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-27 00:43:36,482 [INFO] =================================================================
2019-12-27 00:43:36,482 [INFO] dense_163 (Dense)            (None, 64)                2112      
2019-12-27 00:43:36,482 [INFO] _________________________________________________________________
2019-12-27 00:43:36,482 [INFO] batch_normalization_118 (Bat (None, 64)                256       
2019-12-27 00:43:36,482 [INFO] _________________________________________________________________
2019-12-27 00:43:36,482 [INFO] dropout_118 (Dropout)        (None, 64)                0         
2019-12-27 00:43:36,482 [INFO] _________________________________________________________________
2019-12-27 00:43:36,483 [INFO] dense_164 (Dense)            (None, 15)                975       
2019-12-27 00:43:36,483 [INFO] =================================================================
2019-12-27 00:43:36,483 [INFO] Total params: 3,343
2019-12-27 00:43:36,483 [INFO] Trainable params: 3,215
2019-12-27 00:43:36,483 [INFO] Non-trainable params: 128
2019-12-27 00:43:36,483 [INFO] _________________________________________________________________
2019-12-27 00:43:36,483 [INFO] Training model
2019-12-27 00:43:36,483 [INFO] Splitting train set into 2 sets (unsupervised, supervised), random_seed = None
2019-12-27 00:44:08,788 [INFO] Split sizes (instances). total = 1936462, unsupervised = 484115, supervised = 1452347, unsupervised dataset hash = ce341f03d485fd4dfdad402c278451533af4cf3b
2019-12-27 00:44:08,788 [INFO] Training autoencoder
 - val_f1: 0.9831
Train on 484115 samples, validate on 645487 samples
Epoch 1/200
 - 37s - loss: -2.9087e+00 - val_loss: -3.5482e+00
Epoch 2/200
 - 33s - loss: -3.4815e+00 - val_loss: -3.5861e+00
Epoch 3/200
 - 33s - loss: -3.5199e+00 - val_loss: -3.6088e+00
Epoch 4/200
 - 33s - loss: -3.5424e+00 - val_loss: -3.6191e+00
Epoch 5/200
 - 33s - loss: -3.5582e+00 - val_loss: -3.6268e+00
Epoch 6/200
 - 33s - loss: -3.5633e+00 - val_loss: -3.6137e+00
Epoch 7/200
 - 33s - loss: -3.5734e+00 - val_loss: -3.6335e+00
Epoch 8/200
 - 33s - loss: -3.5783e+00 - val_loss: -3.6376e+00
Epoch 9/200
 - 33s - loss: -3.5817e+00 - val_loss: -3.6391e+00
Epoch 10/200
 - 33s - loss: -3.5851e+00 - val_loss: -3.6400e+00
Epoch 11/200
 - 33s - loss: -3.5876e+00 - val_loss: -3.6422e+00
Epoch 12/200
 - 33s - loss: -3.5916e+00 - val_loss: -3.6426e+00
Epoch 13/200
 - 33s - loss: -3.5947e+00 - val_loss: -3.6451e+00
Epoch 14/200
 - 33s - loss: -3.6002e+00 - val_loss: -3.6447e+00
Epoch 15/200
 - 33s - loss: -3.5999e+00 - val_loss: -3.6467e+00
Epoch 16/200
 - 33s - loss: -3.6034e+00 - val_loss: -3.6473e+00
Epoch 17/200
 - 33s - loss: -3.6025e+00 - val_loss: -3.6487e+00
Epoch 18/200
 - 33s - loss: -3.6065e+00 - val_loss: -3.6499e+00
Epoch 19/200
 - 33s - loss: -3.6083e+00 - val_loss: -3.6507e+00
Epoch 20/200
 - 33s - loss: -3.6065e+00 - val_loss: -3.6455e+00
Epoch 21/200
 - 33s - loss: -3.6097e+00 - val_loss: -3.6391e+00
2019-12-27 00:56:10,643 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep3/ae_model_epoch_20.pickle
Epoch 22/200
 - 33s - loss: -3.6092e+00 - val_loss: -3.6547e+00
Epoch 23/200
 - 33s - loss: -3.6121e+00 - val_loss: -3.6555e+00
Epoch 24/200
 - 33s - loss: -3.6137e+00 - val_loss: -3.6548e+00
Epoch 25/200
 - 33s - loss: -3.6146e+00 - val_loss: -3.6564e+00
Epoch 26/200
 - 33s - loss: -3.6123e+00 - val_loss: -3.6140e+00
Epoch 27/200
 - 33s - loss: -3.6083e+00 - val_loss: -3.6564e+00
Epoch 28/200
 - 33s - loss: -3.6167e+00 - val_loss: -3.6574e+00
Epoch 29/200
 - 33s - loss: -3.6161e+00 - val_loss: -3.6374e+00
Epoch 30/200
 - 33s - loss: -3.6173e+00 - val_loss: -3.6592e+00
Epoch 31/200
 - 33s - loss: -3.6134e+00 - val_loss: -3.6578e+00
Epoch 32/200
 - 33s - loss: -3.6158e+00 - val_loss: -3.6578e+00
Epoch 33/200
 - 33s - loss: -3.6140e+00 - val_loss: -3.6586e+00
Epoch 34/200
 - 33s - loss: -3.6191e+00 - val_loss: -3.6291e+00
Epoch 35/200
 - 33s - loss: -3.6190e+00 - val_loss: -3.6236e+00
Epoch 36/200
 - 33s - loss: -3.6197e+00 - val_loss: -3.6589e+00
Epoch 37/200
 - 33s - loss: -3.6193e+00 - val_loss: -3.6604e+00
Epoch 38/200
 - 33s - loss: -3.6187e+00 - val_loss: -3.6604e+00
Epoch 39/200
 - 33s - loss: -3.6217e+00 - val_loss: -3.6617e+00
Epoch 40/200
 - 33s - loss: -3.6231e+00 - val_loss: -3.6619e+00
Epoch 41/200
 - 33s - loss: -3.6221e+00 - val_loss: -3.6625e+00
2019-12-27 01:07:10,170 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep3/ae_model_epoch_40.pickle
Epoch 42/200
 - 33s - loss: -3.6239e+00 - val_loss: -3.6626e+00
Epoch 43/200
 - 33s - loss: -3.6235e+00 - val_loss: -3.6591e+00
Epoch 44/200
 - 33s - loss: -3.6246e+00 - val_loss: -3.6624e+00
Epoch 45/200
 - 33s - loss: -3.6159e+00 - val_loss: -3.6633e+00
Epoch 46/200
 - 33s - loss: -3.6161e+00 - val_loss: -3.6622e+00
Epoch 47/200
 - 33s - loss: -3.6195e+00 - val_loss: -3.6623e+00
Epoch 48/200
 - 33s - loss: -3.6242e+00 - val_loss: -3.6577e+00
Epoch 49/200
 - 33s - loss: -3.6266e+00 - val_loss: -3.6630e+00
Epoch 50/200
 - 33s - loss: -3.6249e+00 - val_loss: -3.6639e+00
Epoch 51/200
 - 33s - loss: -3.6271e+00 - val_loss: -3.6645e+00
Epoch 52/200
 - 33s - loss: -3.6220e+00 - val_loss: -3.6635e+00
Epoch 53/200
 - 33s - loss: -3.6200e+00 - val_loss: -3.6446e+00
Epoch 54/200
 - 33s - loss: -3.6271e+00 - val_loss: -3.6626e+00
Epoch 55/200
 - 33s - loss: -3.6253e+00 - val_loss: -3.6627e+00
Epoch 56/200
 - 33s - loss: -3.6258e+00 - val_loss: -3.6597e+00
Epoch 57/200
 - 33s - loss: -3.6260e+00 - val_loss: -3.6643e+00
Epoch 58/200
 - 33s - loss: -3.6196e+00 - val_loss: -3.6596e+00
Epoch 59/200
 - 33s - loss: -3.6257e+00 - val_loss: -3.6522e+00
Epoch 60/200
 - 33s - loss: -3.6280e+00 - val_loss: -3.6643e+00
Epoch 61/200
 - 33s - loss: -3.6250e+00 - val_loss: -3.6647e+00
2019-12-27 01:18:09,628 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep3/ae_model_epoch_60.pickle
Epoch 62/200
 - 33s - loss: -3.6282e+00 - val_loss: -3.6648e+00
Epoch 63/200
 - 33s - loss: -3.6293e+00 - val_loss: -3.6654e+00
Epoch 64/200
 - 33s - loss: -3.6252e+00 - val_loss: -3.6646e+00
Epoch 65/200
 - 33s - loss: -3.6239e+00 - val_loss: -3.6647e+00
Epoch 66/200
 - 33s - loss: -3.6258e+00 - val_loss: -3.6646e+00
Epoch 67/200
 - 33s - loss: -3.6224e+00 - val_loss: -3.6581e+00
Epoch 68/200
 - 33s - loss: -3.6287e+00 - val_loss: -3.6654e+00
Epoch 69/200
 - 33s - loss: -3.6237e+00 - val_loss: -3.6652e+00
Epoch 70/200
 - 33s - loss: -3.6263e+00 - val_loss: -3.6655e+00
Epoch 71/200
 - 33s - loss: -3.6297e+00 - val_loss: -3.6645e+00
Epoch 72/200
 - 33s - loss: -3.6261e+00 - val_loss: -3.6637e+00
Epoch 73/200
 - 33s - loss: -3.6292e+00 - val_loss: -3.6644e+00
Epoch 74/200
 - 33s - loss: -3.6226e+00 - val_loss: -3.6650e+00
Epoch 75/200
 - 33s - loss: -3.6278e+00 - val_loss: -3.6664e+00
Epoch 76/200
 - 33s - loss: -3.6307e+00 - val_loss: -3.6661e+00
Epoch 77/200
 - 33s - loss: -3.6274e+00 - val_loss: -3.6661e+00
Epoch 78/200
 - 33s - loss: -3.6314e+00 - val_loss: -3.6652e+00
Epoch 79/200
 - 33s - loss: -3.6312e+00 - val_loss: -3.6663e+00
Epoch 80/200
 - 33s - loss: -3.6239e+00 - val_loss: -3.6648e+00
Epoch 81/200
 - 33s - loss: -3.6317e+00 - val_loss: -3.6659e+00
2019-12-27 01:29:09,414 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep3/ae_model_epoch_80.pickle
Epoch 82/200
 - 33s - loss: -3.6291e+00 - val_loss: -3.6659e+00
Epoch 83/200
 - 33s - loss: -3.6310e+00 - val_loss: -3.6665e+00
Epoch 84/200
 - 33s - loss: -3.6323e+00 - val_loss: -3.6667e+00
Epoch 85/200
 - 33s - loss: -3.6305e+00 - val_loss: -3.6669e+00
Epoch 86/200
 - 33s - loss: -3.6280e+00 - val_loss: -3.6659e+00
Epoch 87/200
 - 33s - loss: -3.6276e+00 - val_loss: -3.6666e+00
Epoch 88/200
 - 33s - loss: -3.6312e+00 - val_loss: -3.6676e+00
Epoch 89/200
 - 33s - loss: -3.6283e+00 - val_loss: -3.6261e+00
Epoch 90/200
 - 33s - loss: -3.6326e+00 - val_loss: -3.6665e+00
Epoch 91/200
 - 33s - loss: -3.6310e+00 - val_loss: -3.6672e+00
Epoch 92/200
 - 33s - loss: -3.6313e+00 - val_loss: -3.6612e+00
Epoch 93/200
 - 33s - loss: -3.6291e+00 - val_loss: -3.6666e+00
Epoch 94/200
 - 33s - loss: -3.6289e+00 - val_loss: -3.6623e+00
Epoch 95/200
 - 33s - loss: -3.6315e+00 - val_loss: -3.6451e+00
Epoch 96/200
 - 33s - loss: -3.6341e+00 - val_loss: -3.6682e+00
Epoch 97/200
 - 33s - loss: -3.6324e+00 - val_loss: -3.6677e+00
Epoch 98/200
 - 33s - loss: -3.6330e+00 - val_loss: -3.6681e+00
Epoch 99/200
 - 33s - loss: -3.6332e+00 - val_loss: -3.6676e+00
Epoch 100/200
 - 33s - loss: -3.6339e+00 - val_loss: -3.6677e+00
Epoch 101/200
 - 33s - loss: -3.6267e+00 - val_loss: -3.6679e+00
2019-12-27 01:40:09,350 [INFO] epoch = 100. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep3/ae_model_epoch_100.pickle
Epoch 102/200
 - 33s - loss: -3.6288e+00 - val_loss: -3.6415e+00
Epoch 103/200
 - 33s - loss: -3.6342e+00 - val_loss: -3.6681e+00
Epoch 104/200
 - 33s - loss: -3.6336e+00 - val_loss: -3.6683e+00
Epoch 105/200
 - 33s - loss: -3.6332e+00 - val_loss: -3.6607e+00
Epoch 106/200
 - 33s - loss: -3.6317e+00 - val_loss: -3.6672e+00
Epoch 107/200
 - 33s - loss: -3.6354e+00 - val_loss: -3.6679e+00
Epoch 108/200
 - 33s - loss: -3.6305e+00 - val_loss: -3.6679e+00
Epoch 109/200
 - 33s - loss: -3.6347e+00 - val_loss: -3.6681e+00
Epoch 110/200
 - 33s - loss: -3.6316e+00 - val_loss: -3.6683e+00
Epoch 111/200
 - 33s - loss: -3.6325e+00 - val_loss: -3.6642e+00
Epoch 112/200
 - 33s - loss: -3.6320e+00 - val_loss: -3.6669e+00
Epoch 113/200
 - 33s - loss: -3.6265e+00 - val_loss: -3.6604e+00
Epoch 114/200
 - 33s - loss: -3.6356e+00 - val_loss: -3.6603e+00
Epoch 115/200
 - 33s - loss: -3.6269e+00 - val_loss: -3.6659e+00
Epoch 116/200
 - 33s - loss: -3.6283e+00 - val_loss: -3.6686e+00
Epoch 117/200
 - 33s - loss: -3.6339e+00 - val_loss: -3.6674e+00
Epoch 118/200
 - 33s - loss: -3.6354e+00 - val_loss: -3.6670e+00
Epoch 119/200
 - 33s - loss: -3.6342e+00 - val_loss: -3.6675e+00
Epoch 120/200
 - 33s - loss: -3.6304e+00 - val_loss: -3.6685e+00
Epoch 121/200
 - 33s - loss: -3.6354e+00 - val_loss: -3.6679e+00
2019-12-27 01:51:09,214 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep3/ae_model_epoch_120.pickle
Epoch 122/200
 - 33s - loss: -3.6363e+00 - val_loss: -3.6679e+00
Epoch 123/200
 - 33s - loss: -3.6350e+00 - val_loss: -3.6661e+00
Epoch 124/200
 - 33s - loss: -3.6365e+00 - val_loss: -3.6676e+00
Epoch 125/200
 - 33s - loss: -3.6366e+00 - val_loss: -3.6674e+00
Epoch 126/200
 - 33s - loss: -3.6337e+00 - val_loss: -3.6684e+00
Epoch 127/200
 - 33s - loss: -3.6350e+00 - val_loss: -3.6685e+00
Epoch 128/200
 - 33s - loss: -3.6279e+00 - val_loss: -3.6691e+00
Epoch 129/200
 - 33s - loss: -3.6334e+00 - val_loss: -3.6646e+00
Epoch 130/200
 - 33s - loss: -3.6345e+00 - val_loss: -3.6681e+00
Epoch 131/200
 - 33s - loss: -3.6365e+00 - val_loss: -3.6686e+00
Epoch 132/200
 - 33s - loss: -3.6373e+00 - val_loss: -3.6689e+00
Epoch 133/200
 - 33s - loss: -3.6342e+00 - val_loss: -3.6610e+00
Epoch 134/200
 - 33s - loss: -3.6332e+00 - val_loss: -3.6687e+00
Epoch 135/200
 - 33s - loss: -3.6336e+00 - val_loss: -3.6643e+00
Epoch 136/200
 - 33s - loss: -3.6372e+00 - val_loss: -3.6688e+00
Epoch 137/200
 - 33s - loss: -3.6367e+00 - val_loss: -3.6683e+00
Epoch 138/200
 - 33s - loss: -3.6335e+00 - val_loss: -3.6603e+00
Epoch 139/200
 - 33s - loss: -3.6353e+00 - val_loss: -3.6689e+00
Epoch 140/200
 - 33s - loss: -3.6333e+00 - val_loss: -3.6684e+00
Epoch 141/200
 - 33s - loss: -3.6327e+00 - val_loss: -3.6692e+00
2019-12-27 02:02:08,349 [INFO] epoch = 140. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep3/ae_model_epoch_140.pickle
Epoch 142/200
 - 33s - loss: -3.6312e+00 - val_loss: -3.6550e+00
Epoch 143/200
 - 33s - loss: -3.6371e+00 - val_loss: -3.6661e+00
Epoch 144/200
 - 33s - loss: -3.6349e+00 - val_loss: -3.6664e+00
Epoch 145/200
 - 33s - loss: -3.6313e+00 - val_loss: -3.6682e+00
Epoch 146/200
 - 33s - loss: -3.6364e+00 - val_loss: -3.6615e+00
Epoch 147/200
 - 33s - loss: -3.6306e+00 - val_loss: -3.6488e+00
Epoch 148/200
 - 33s - loss: -3.6351e+00 - val_loss: -3.6688e+00
Epoch 149/200
 - 33s - loss: -3.6365e+00 - val_loss: -3.6691e+00
Epoch 150/200
 - 33s - loss: -3.6366e+00 - val_loss: -3.6688e+00
Epoch 151/200
 - 33s - loss: -3.6350e+00 - val_loss: -3.6692e+00
Epoch 152/200
 - 33s - loss: -3.6372e+00 - val_loss: -3.6694e+00
Epoch 153/200
 - 33s - loss: -3.6368e+00 - val_loss: -3.6695e+00
Epoch 154/200
 - 33s - loss: -3.6367e+00 - val_loss: -3.6687e+00
Epoch 155/200
 - 33s - loss: -3.6337e+00 - val_loss: -3.6681e+00
Epoch 156/200
 - 33s - loss: -3.6327e+00 - val_loss: -3.6690e+00
Epoch 157/200
 - 33s - loss: -3.6365e+00 - val_loss: -3.6696e+00
Epoch 158/200
 - 33s - loss: -3.6384e+00 - val_loss: -3.6564e+00
Epoch 159/200
 - 33s - loss: -3.6355e+00 - val_loss: -3.6683e+00
Epoch 160/200
 - 33s - loss: -3.6347e+00 - val_loss: -3.6692e+00
Epoch 161/200
 - 33s - loss: -3.6389e+00 - val_loss: -3.6696e+00
2019-12-27 02:13:07,476 [INFO] epoch = 160. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep3/ae_model_epoch_160.pickle
Epoch 162/200
 - 33s - loss: -3.6364e+00 - val_loss: -3.6691e+00
Epoch 163/200
 - 33s - loss: -3.6363e+00 - val_loss: -3.6654e+00
Epoch 164/200
 - 33s - loss: -3.6379e+00 - val_loss: -3.6701e+00
Epoch 165/200
 - 33s - loss: -3.6314e+00 - val_loss: -3.6677e+00
Epoch 166/200
 - 33s - loss: -3.6366e+00 - val_loss: -3.6694e+00
Epoch 167/200
 - 33s - loss: -3.6382e+00 - val_loss: -3.6698e+00
Epoch 168/200
 - 33s - loss: -3.6365e+00 - val_loss: -3.6692e+00
Epoch 169/200
 - 33s - loss: -3.6367e+00 - val_loss: -3.6695e+00
Epoch 170/200
 - 33s - loss: -3.6353e+00 - val_loss: -3.6685e+00
Epoch 171/200
 - 33s - loss: -3.6376e+00 - val_loss: -3.6690e+00
Epoch 172/200
 - 33s - loss: -3.6377e+00 - val_loss: -3.6701e+00
Epoch 173/200
 - 33s - loss: -3.6325e+00 - val_loss: -3.6695e+00
Epoch 174/200
 - 33s - loss: -3.6394e+00 - val_loss: -3.6695e+00
Epoch 175/200
 - 33s - loss: -3.6377e+00 - val_loss: -3.6656e+00
Epoch 176/200
 - 33s - loss: -3.6385e+00 - val_loss: -3.6702e+00
Epoch 177/200
 - 33s - loss: -3.6364e+00 - val_loss: -3.6696e+00
Epoch 178/200
 - 33s - loss: -3.6349e+00 - val_loss: -3.6352e+00
Epoch 179/200
 - 33s - loss: -3.6384e+00 - val_loss: -3.6702e+00
Epoch 180/200
 - 33s - loss: -3.6325e+00 - val_loss: -3.6692e+00
Epoch 181/200
 - 33s - loss: -3.6383e+00 - val_loss: -3.6699e+00
2019-12-27 02:24:06,901 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep3/ae_model_epoch_180.pickle
Epoch 182/200
 - 33s - loss: -3.6383e+00 - val_loss: -3.6667e+00
Epoch 183/200
 - 33s - loss: -3.6374e+00 - val_loss: -3.6697e+00
Epoch 184/200
 - 33s - loss: -3.6381e+00 - val_loss: -3.6698e+00
Epoch 185/200
 - 33s - loss: -3.6346e+00 - val_loss: -3.6704e+00
Epoch 186/200
 - 33s - loss: -3.6371e+00 - val_loss: -3.6700e+00
Epoch 187/200
 - 33s - loss: -3.6353e+00 - val_loss: -3.6698e+00
Epoch 188/200
 - 33s - loss: -3.6376e+00 - val_loss: -3.6698e+00
Epoch 189/200
 - 33s - loss: -3.6385e+00 - val_loss: -3.6701e+00
Epoch 190/200
 - 33s - loss: -3.6389e+00 - val_loss: -3.6700e+00
Epoch 191/200
 - 33s - loss: -3.6396e+00 - val_loss: -3.6698e+00
Epoch 192/200
 - 33s - loss: -3.6388e+00 - val_loss: -3.6704e+00
Epoch 193/200
 - 33s - loss: -3.6386e+00 - val_loss: -3.6703e+00
Epoch 194/200
 - 33s - loss: -3.6394e+00 - val_loss: -3.6702e+00
Epoch 195/200
 - 33s - loss: -3.6389e+00 - val_loss: -3.6431e+00
Epoch 196/200
 - 33s - loss: -3.6388e+00 - val_loss: -3.6693e+00
Epoch 197/200
 - 33s - loss: -3.6395e+00 - val_loss: -3.6176e+00
Epoch 198/200
 - 33s - loss: -3.6337e+00 - val_loss: -3.6702e+00
Epoch 199/200
 - 33s - loss: -3.6398e+00 - val_loss: -3.6708e+00
Epoch 200/200
 - 33s - loss: -3.6396e+00 - val_loss: -3.6656e+00
2019-12-27 02:34:33,437 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-27 02:36:08,468 [INFO] Last epoch loss evaluation: train_loss = -3.671183, val_loss = -3.670812
2019-12-27 02:36:08,469 [INFO] Training autoencoder complete
2019-12-27 02:36:08,469 [INFO] Encoding data for supervised training
2019-12-27 02:38:13,639 [INFO] Encoding complete
2019-12-27 02:38:13,639 [INFO] Training neural network layers (after autoencoder)
Train on 1452347 samples, validate on 645487 samples
Epoch 1/200
 - 32s - loss: 0.0159 - val_loss: 0.0091
 - val_f1: 0.9815
Epoch 2/200
 - 30s - loss: 0.0097 - val_loss: 0.0088
 - val_f1: 0.9817
Epoch 3/200
 - 30s - loss: 0.0094 - val_loss: 0.0086
 - val_f1: 0.9821
Epoch 4/200
 - 30s - loss: 0.0092 - val_loss: 0.0087
 - val_f1: 0.9801
Epoch 5/200
 - 30s - loss: 0.0091 - val_loss: 0.0084
 - val_f1: 0.9823
Epoch 6/200
 - 30s - loss: 0.0089 - val_loss: 0.0084
 - val_f1: 0.9823
Epoch 7/200
 - 30s - loss: 0.0088 - val_loss: 0.0083
 - val_f1: 0.9825
Epoch 8/200
 - 30s - loss: 0.0087 - val_loss: 0.0082
 - val_f1: 0.9822
Epoch 9/200
 - 30s - loss: 0.0087 - val_loss: 0.0092
 - val_f1: 0.9812
Epoch 10/200
 - 30s - loss: 0.0087 - val_loss: 0.0082
 - val_f1: 0.9823
Epoch 11/200
 - 30s - loss: 0.0088 - val_loss: 0.0083
 - val_f1: 0.9825
Epoch 12/200
 - 30s - loss: 0.0087 - val_loss: 0.0085
 - val_f1: 0.9799
Epoch 13/200
 - 30s - loss: 0.0086 - val_loss: 0.0085
 - val_f1: 0.9821
Epoch 14/200
 - 30s - loss: 0.0087 - val_loss: 0.0087
 - val_f1: 0.9816
Epoch 15/200
 - 30s - loss: 0.0086 - val_loss: 0.0082
 - val_f1: 0.9828
Epoch 16/200
 - 30s - loss: 0.0086 - val_loss: 0.0082
 - val_f1: 0.9829
Epoch 17/200
 - 30s - loss: 0.0086 - val_loss: 0.0083
 - val_f1: 0.9810
Epoch 18/200
 - 30s - loss: 0.0086 - val_loss: 0.0084
 - val_f1: 0.9827
Epoch 19/200
 - 30s - loss: 0.0086 - val_loss: 0.0081
 - val_f1: 0.9808
Epoch 20/200
 - 30s - loss: 0.0086 - val_loss: 0.0084
 - val_f1: 0.9822
Epoch 21/200
 - 30s - loss: 0.0085 - val_loss: 0.0084
2019-12-27 03:00:37,265 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep3/ann_model_epoch_20.pickle
 - val_f1: 0.9826
Epoch 22/200
 - 30s - loss: 0.0085 - val_loss: 0.0083
 - val_f1: 0.9828
Epoch 23/200
 - 30s - loss: 0.0085 - val_loss: 0.0082
 - val_f1: 0.9828
Epoch 24/200
 - 30s - loss: 0.0085 - val_loss: 0.0081
 - val_f1: 0.9826
Epoch 25/200
 - 30s - loss: 0.0085 - val_loss: 0.0087
 - val_f1: 0.9815
Epoch 26/200
 - 30s - loss: 0.0085 - val_loss: 0.0083
 - val_f1: 0.9803
Epoch 27/200
 - 30s - loss: 0.0085 - val_loss: 0.0083
 - val_f1: 0.9826
Epoch 28/200
 - 30s - loss: 0.0085 - val_loss: 0.0083
 - val_f1: 0.9809
Epoch 29/200
 - 30s - loss: 0.0085 - val_loss: 0.0081
 - val_f1: 0.9833
Epoch 30/200
 - 30s - loss: 0.0085 - val_loss: 0.0081
 - val_f1: 0.9826
Epoch 31/200
 - 30s - loss: 0.0085 - val_loss: 0.0081
 - val_f1: 0.9829
Epoch 32/200
 - 30s - loss: 0.0085 - val_loss: 0.0082
 - val_f1: 0.9807
Epoch 33/200
 - 30s - loss: 0.0084 - val_loss: 0.0081
 - val_f1: 0.9827
Epoch 34/200
 - 30s - loss: 0.0085 - val_loss: 0.0081
 - val_f1: 0.9828
Epoch 35/200
 - 30s - loss: 0.0084 - val_loss: 0.0087
 - val_f1: 0.9813
Epoch 36/200
 - 30s - loss: 0.0084 - val_loss: 0.0082
 - val_f1: 0.9824
Epoch 37/200
 - 30s - loss: 0.0084 - val_loss: 0.0081
 - val_f1: 0.9828
Epoch 38/200
 - 30s - loss: 0.0084 - val_loss: 0.0081
 - val_f1: 0.9831
Epoch 39/200
 - 30s - loss: 0.0084 - val_loss: 0.0081
 - val_f1: 0.9811
Epoch 40/200
 - 30s - loss: 0.0084 - val_loss: 0.0081
 - val_f1: 0.9825
Epoch 41/200
 - 30s - loss: 0.0084 - val_loss: 0.0080
2019-12-27 03:22:06,318 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep3/ann_model_epoch_40.pickle
 - val_f1: 0.9829
Epoch 42/200
 - 30s - loss: 0.0084 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 43/200
 - 30s - loss: 0.0084 - val_loss: 0.0080
 - val_f1: 0.9826
Epoch 44/200
 - 30s - loss: 0.0084 - val_loss: 0.0081
 - val_f1: 0.9827
Epoch 45/200
 - 30s - loss: 0.0084 - val_loss: 0.0081
 - val_f1: 0.9830
Epoch 46/200
 - 30s - loss: 0.0084 - val_loss: 0.0082
 - val_f1: 0.9831
Epoch 47/200
 - 30s - loss: 0.0084 - val_loss: 0.0081
 - val_f1: 0.9829
Epoch 48/200
 - 30s - loss: 0.0084 - val_loss: 0.0081
 - val_f1: 0.9830
Epoch 49/200
 - 30s - loss: 0.0084 - val_loss: 0.0084
 - val_f1: 0.9819
Epoch 50/200
 - 30s - loss: 0.0084 - val_loss: 0.0081
 - val_f1: 0.9830
Epoch 51/200
 - 30s - loss: 0.0084 - val_loss: 0.0080
 - val_f1: 0.9810
Epoch 52/200
 - 30s - loss: 0.0084 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 53/200
 - 30s - loss: 0.0084 - val_loss: 0.0081
 - val_f1: 0.9829
Epoch 54/200
 - 30s - loss: 0.0084 - val_loss: 0.0081
 - val_f1: 0.9829
Epoch 55/200
 - 30s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9830
Epoch 56/200
 - 30s - loss: 0.0084 - val_loss: 0.0081
 - val_f1: 0.9830
Epoch 57/200
 - 30s - loss: 0.0084 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 58/200
 - 30s - loss: 0.0083 - val_loss: 0.0082
 - val_f1: 0.9825
Epoch 59/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 60/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9827
Epoch 61/200
 - 30s - loss: 0.0084 - val_loss: 0.0080
2019-12-27 03:43:35,030 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep3/ann_model_epoch_60.pickle
 - val_f1: 0.9829
Epoch 62/200
 - 30s - loss: 0.0084 - val_loss: 0.0088
 - val_f1: 0.9821
Epoch 63/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 64/200
 - 30s - loss: 0.0084 - val_loss: 0.0080
 - val_f1: 0.9834
Epoch 65/200
 - 30s - loss: 0.0084 - val_loss: 0.0081
 - val_f1: 0.9829
Epoch 66/200
 - 30s - loss: 0.0083 - val_loss: 0.0082
 - val_f1: 0.9829
Epoch 67/200
 - 30s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9808
Epoch 68/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 69/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 70/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 71/200
 - 30s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9830
Epoch 72/200
 - 30s - loss: 0.0083 - val_loss: 0.0084
 - val_f1: 0.9818
Epoch 73/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 74/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 75/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 76/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 77/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9834
Epoch 78/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 79/200
 - 30s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9834
Epoch 80/200
 - 30s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9828
Epoch 81/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
2019-12-27 04:05:03,210 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep3/ann_model_epoch_80.pickle
 - val_f1: 0.9830
Epoch 82/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 83/200
 - 30s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9834
Epoch 84/200
 - 30s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9833
Epoch 85/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 86/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 87/200
 - 30s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9829
Epoch 88/200
 - 30s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9829
Epoch 89/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 90/200
 - 30s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9832
Epoch 91/200
 - 30s - loss: 0.0083 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 92/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 93/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 94/200
 - 30s - loss: 0.0083 - val_loss: 0.0082
 - val_f1: 0.9828
Epoch 95/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 96/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9833
Epoch 97/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 98/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9834
Epoch 99/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 100/200
 - 30s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9831
Epoch 101/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
2019-12-27 04:26:31,943 [INFO] epoch = 100. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep3/ann_model_epoch_100.pickle
 - val_f1: 0.9830
Epoch 102/200
 - 30s - loss: 0.0083 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 103/200
 - 30s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9830
Epoch 104/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 105/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 106/200
 - 30s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9829
Epoch 107/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 108/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 109/200
 - 30s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9831
Epoch 110/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 111/200
 - 30s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9830
Epoch 112/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 113/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9813
Epoch 114/200
 - 30s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9832
Epoch 115/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 116/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 117/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 118/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 119/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 120/200
 - 30s - loss: 0.0083 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 121/200
 - 30s - loss: 0.0083 - val_loss: 0.0079
2019-12-27 04:48:00,904 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep3/ann_model_epoch_120.pickle
 - val_f1: 0.9831
Epoch 122/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9835
Epoch 123/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 124/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 125/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 126/200
 - 30s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9828
Epoch 127/200
 - 30s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9828
Epoch 128/200
 - 30s - loss: 0.0083 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 129/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9835
Epoch 130/200
 - 30s - loss: 0.0083 - val_loss: 0.0082
 - val_f1: 0.9827
Epoch 131/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 132/200
 - 30s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9831
Epoch 133/200
 - 30s - loss: 0.0083 - val_loss: 0.0079
 - val_f1: 0.9833
Epoch 134/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 135/200
 - 30s - loss: 0.0083 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 136/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 137/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9835
Epoch 138/200
 - 30s - loss: 0.0083 - val_loss: 0.0079
 - val_f1: 0.9830
Epoch 139/200
 - 30s - loss: 0.0083 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 140/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 141/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
2019-12-27 05:09:29,450 [INFO] epoch = 140. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep3/ann_model_epoch_140.pickle
 - val_f1: 0.9836
Epoch 142/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9833
Epoch 143/200
 - 30s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9829
Epoch 144/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9834
Epoch 145/200
 - 30s - loss: 0.0083 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 146/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9833
Epoch 147/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 148/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 149/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9835
Epoch 150/200
 - 30s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9829
Epoch 151/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9836
Epoch 152/200
 - 30s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9827
Epoch 153/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 154/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 155/200
 - 30s - loss: 0.0083 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 156/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 157/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 158/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 159/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9827
Epoch 160/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9830
Epoch 161/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
2019-12-27 05:31:00,193 [INFO] epoch = 160. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep3/ann_model_epoch_160.pickle
 - val_f1: 0.9832
Epoch 162/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9833
Epoch 163/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9833
Epoch 164/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 165/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 166/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9833
Epoch 167/200
 - 30s - loss: 0.0083 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 168/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 169/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 170/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 171/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9833
Epoch 172/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 173/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9834
Epoch 174/200
 - 30s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9829
Epoch 175/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 176/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9827
Epoch 177/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 178/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 179/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 180/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9833
Epoch 181/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
2019-12-27 05:52:28,299 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep3/ann_model_epoch_180.pickle
 - val_f1: 0.9832
Epoch 182/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 183/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 184/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 185/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 186/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 187/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 188/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 189/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9833
Epoch 190/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9834
Epoch 191/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9835
Epoch 192/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 193/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9819
Epoch 194/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9814
Epoch 195/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9833
Epoch 196/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 197/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9835
Epoch 198/200
 - 30s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9828
Epoch 199/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9834
Epoch 200/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
2019-12-27 06:13:26,593 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-27 06:15:23,473 [INFO] Last epoch loss evaluation: train_loss = 0.007886, val_loss = 0.007905
2019-12-27 06:15:23,482 [INFO] Training complete. time_to_train = 19907.00 sec, 331.78 min
2019-12-27 06:15:23,518 [INFO] Model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep3/best_model.pickle
/home/sunanda/research/ids_experiments/utility.py:268: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
2019-12-27 06:15:23,719 [INFO] Plot saved to: results_selected_models/selected_ids18_subset_ae_ann_deep_rep3/training_error_history.png
/home/sunanda/research/ids_experiments/utility.py:285: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
2019-12-27 06:15:23,912 [INFO] Plot saved to: results_selected_models/selected_ids18_subset_ae_ann_deep_rep3/training_f1_history.png
2019-12-27 06:15:23,912 [INFO] Making predictions on training, validation, testing data
2019-12-27 06:21:17,397 [INFO] Evaluating predictions (results)
/home/sunanda/anaconda3/envs/ml_env_cpu/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2019-12-27 06:21:40,538 [INFO] Dataset: Testing. Classification report below
2019-12-27 06:21:40,538 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99    535650
                     Bot       1.00      1.00      1.00     11465
        Brute Force -Web       0.00      0.00      0.00        24
        Brute Force -XSS       1.00      0.33      0.50         9
        DDOS attack-HOIC       1.00      1.00      1.00     27447
    DDOS attack-LOIC-UDP       0.73      0.99      0.84        67
  DDoS attacks-LOIC-HTTP       0.99      0.99      0.99     23010
   DoS attacks-GoldenEye       0.99      1.00      0.99      1651
        DoS attacks-Hulk       1.00      1.00      1.00     18478
DoS attacks-SlowHTTPTest       0.75      0.49      0.59      5596
   DoS attacks-Slowloris       0.94      0.95      0.94       440
          FTP-BruteForce       0.70      0.88      0.78      7718
           Infilteration       0.38      0.00      0.01      6404
           SQL Injection       0.00      0.00      0.00         4
          SSH-Bruteforce       1.00      1.00      1.00      7525

                accuracy                           0.98    645488
               macro avg       0.76      0.71      0.71    645488
            weighted avg       0.98      0.98      0.98    645488

2019-12-27 06:21:40,538 [INFO] Overall accuracy (micro avg): 0.9834295912549885
/home/sunanda/research/ids_experiments/utility.py:307: RuntimeWarning: invalid value encountered in true_divide
  PPV = TP / (TP + FP)    # Precision or positive predictive value
/home/sunanda/research/ids_experiments/utility.py:311: RuntimeWarning: invalid value encountered in true_divide
  FDR = FP / (TP + FP)    # False discovery rate
2019-12-27 06:22:05,454 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9834         0.9834                       0.9834                0.0012                   0.0166  0.9834
1     Macro avg        0.9978         0.7646                       0.7083                0.0045                   0.2917  0.7091
2  Weighted avg        0.9909         0.9776                       0.9834                0.0502                   0.0166  0.9783
2019-12-27 06:22:28,555 [INFO] Dataset: Validation. Classification report below
2019-12-27 06:22:28,555 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99    535650
                     Bot       1.00      1.00      1.00     11465
        Brute Force -Web       0.00      0.00      0.00        25
        Brute Force -XSS       1.00      0.67      0.80         9
        DDOS attack-HOIC       1.00      1.00      1.00     27447
    DDOS attack-LOIC-UDP       0.78      0.97      0.86        68
  DDoS attacks-LOIC-HTTP       0.99      0.99      0.99     23009
   DoS attacks-GoldenEye       0.99      1.00      0.99      1651
        DoS attacks-Hulk       1.00      1.00      1.00     18478
DoS attacks-SlowHTTPTest       0.76      0.48      0.59      5596
   DoS attacks-Slowloris       0.93      0.94      0.93       439
          FTP-BruteForce       0.70      0.89      0.78      7718
           Infilteration       0.40      0.00      0.01      6403
           SQL Injection       0.00      0.00      0.00         4
          SSH-Bruteforce       1.00      1.00      1.00      7525

                accuracy                           0.98    645487
               macro avg       0.77      0.73      0.73    645487
            weighted avg       0.98      0.98      0.98    645487

2019-12-27 06:22:28,555 [INFO] Overall accuracy (micro avg): 0.9834651975949942
2019-12-27 06:22:53,448 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9835         0.9835                       0.9835                0.0012                   0.0165  0.9835
1     Macro avg        0.9978         0.7688                       0.7290                0.0044                   0.2710  0.7302
2  Weighted avg        0.9909         0.9778                       0.9835                0.0501                   0.0165  0.9783
2019-12-27 06:24:09,246 [INFO] Dataset: Training. Classification report below
2019-12-27 06:24:09,246 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99   1606949
                     Bot       1.00      1.00      1.00     34396
        Brute Force -Web       0.00      0.00      0.00        73
        Brute Force -XSS       1.00      0.46      0.63        26
        DDOS attack-HOIC       1.00      1.00      1.00     82341
    DDOS attack-LOIC-UDP       0.75      0.97      0.84       203
  DDoS attacks-LOIC-HTTP       0.99      0.99      0.99     69029
   DoS attacks-GoldenEye       0.99      1.00      0.99      4954
        DoS attacks-Hulk       1.00      1.00      1.00     55435
DoS attacks-SlowHTTPTest       0.75      0.48      0.59     16787
   DoS attacks-Slowloris       0.95      0.97      0.96      1318
          FTP-BruteForce       0.70      0.88      0.78     23153
           Infilteration       0.49      0.00      0.01     19210
           SQL Injection       0.00      0.00      0.00        12
          SSH-Bruteforce       1.00      1.00      1.00     22576

                accuracy                           0.98   1936462
               macro avg       0.77      0.72      0.72   1936462
            weighted avg       0.98      0.98      0.98   1936462

2019-12-27 06:24:09,246 [INFO] Overall accuracy (micro avg): 0.9834522959913492
/home/sunanda/research/ids_experiments/utility.py:313: RuntimeWarning: invalid value encountered in true_divide
  F1 = (2 * PPV * TPR) / (PPV + TPR)  # F1 score
2019-12-27 06:25:30,948 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9835         0.9835                       0.9835                0.0012                   0.0165  0.9835
1     Macro avg        0.9978         0.7737                       0.7165                0.0044                   0.2835  0.7189
2  Weighted avg        0.9910         0.9787                       0.9835                0.0500                   0.0165  0.9783
2019-12-27 06:25:31,023 [INFO] Results saved to: results_selected_models/selected_ids18_subset_ae_ann_deep_rep3/selected_ids18_subset_ae_ann_deep_rep3_results.xlsx
2019-12-27 06:25:31,027 [INFO] ================= Finished running experiment no. 3 ================= 

2019-12-27 06:25:31,078 [INFO] Created directory: results_selected_models/selected_ids18_subset_ae_ann_deep_rep4
2019-12-27 06:25:31,078 [INFO] Initialized logging. log_filename = results_selected_models/selected_ids18_subset_ae_ann_deep_rep4/run_log.log
2019-12-27 06:25:31,078 [INFO] ================= Running experiment no. 4  ================= 

2019-12-27 06:25:31,078 [INFO] Experiment parameters given below
2019-12-27 06:25:31,078 [INFO] 
{'experiment_num': 4, 'results_dir': 'results_selected_models/selected_ids18_subset_ae_ann_deep_rep4', 'model_type': 'classifier', 'model': 'ae_ann', 'normal_label': 'BENIGN', 'scaling_type': 'NA', 'unsupervised_ratio': 0.25, 'ae_encoder_units': [128, 64, 32], 'ae_encoder_activations': ['relu', 'relu', 'relu'], 'ae_encoder_dropout_rates': [0.2, 0.2, 0.2], 'ae_encoder_l1_param': -1, 'ae_decoder_units': [64, 128], 'ae_decoder_activations': ['relu', 'relu'], 'ae_decoder_dropout_rates': [0.2, 0.2], 'output_activation': 'sigmoid', 'loss_function': 'binary_crossentropy', 'ae_epochs': 200, 'ann_layer_units': [64], 'ann_layer_activations': ['relu'], 'ann_layer_dropout_rates': [0.2], 'ann_epochs': 200, 'early_stop_patience': 50, 'batch_size': 256, 'dataset_dir': '../Datasets/small_datasets/ids2018', 'description': 'selected_ids18_subset_ae_ann_deep_rep4'}
2019-12-27 06:25:31,078 [INFO] Created tensorboard log directory: results_selected_models/selected_ids18_subset_ae_ann_deep_rep4/tf_logs_run_2019_12_27-06_25_31
2019-12-27 06:25:31,078 [INFO] Loading datsets from: ../Datasets/small_datasets/ids2018
2019-12-27 06:25:31,079 [INFO] Reading X, y files
2019-12-27 06:25:31,079 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_train.h5
2019-12-27 06:25:35,140 [INFO] Reading complete. time_to_read=4.06 seconds
2019-12-27 06:25:35,140 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_val.h5
2019-12-27 06:25:36,531 [INFO] Reading complete. time_to_read=1.39 seconds
2019-12-27 06:25:36,531 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_test.h5
2019-12-27 06:25:37,921 [INFO] Reading complete. time_to_read=1.39 seconds
2019-12-27 06:25:37,922 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_train.h5
2019-12-27 06:25:38,196 [INFO] Reading complete. time_to_read=0.27 seconds
2019-12-27 06:25:38,196 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_val.h5
2019-12-27 06:25:38,292 [INFO] Reading complete. time_to_read=0.10 seconds
2019-12-27 06:25:38,293 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_test.h5
2019-12-27 06:25:38,389 [INFO] Reading complete. time_to_read=0.10 seconds
2019-12-27 06:25:41,726 [INFO] Initializing model
2019-12-27 06:25:42,428 [INFO] _________________________________________________________________
2019-12-27 06:25:42,428 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-27 06:25:42,429 [INFO] =================================================================
2019-12-27 06:25:42,429 [INFO] dense_165 (Dense)            (None, 128)               9984      
2019-12-27 06:25:42,429 [INFO] _________________________________________________________________
2019-12-27 06:25:42,429 [INFO] batch_normalization_119 (Bat (None, 128)               512       
2019-12-27 06:25:42,429 [INFO] _________________________________________________________________
2019-12-27 06:25:42,429 [INFO] dropout_119 (Dropout)        (None, 128)               0         
2019-12-27 06:25:42,429 [INFO] _________________________________________________________________
2019-12-27 06:25:42,429 [INFO] dense_166 (Dense)            (None, 64)                8256      
2019-12-27 06:25:42,429 [INFO] _________________________________________________________________
2019-12-27 06:25:42,429 [INFO] batch_normalization_120 (Bat (None, 64)                256       
2019-12-27 06:25:42,429 [INFO] _________________________________________________________________
2019-12-27 06:25:42,429 [INFO] dropout_120 (Dropout)        (None, 64)                0         
2019-12-27 06:25:42,430 [INFO] _________________________________________________________________
2019-12-27 06:25:42,430 [INFO] dense_167 (Dense)            (None, 32)                2080      
2019-12-27 06:25:42,430 [INFO] _________________________________________________________________
2019-12-27 06:25:42,430 [INFO] batch_normalization_121 (Bat (None, 32)                128       
2019-12-27 06:25:42,430 [INFO] _________________________________________________________________
2019-12-27 06:25:42,430 [INFO] dropout_121 (Dropout)        (None, 32)                0         
2019-12-27 06:25:42,430 [INFO] _________________________________________________________________
2019-12-27 06:25:42,430 [INFO] dense_168 (Dense)            (None, 64)                2112      
2019-12-27 06:25:42,430 [INFO] _________________________________________________________________
2019-12-27 06:25:42,430 [INFO] batch_normalization_122 (Bat (None, 64)                256       
2019-12-27 06:25:42,430 [INFO] _________________________________________________________________
2019-12-27 06:25:42,430 [INFO] dropout_122 (Dropout)        (None, 64)                0         
2019-12-27 06:25:42,431 [INFO] _________________________________________________________________
2019-12-27 06:25:42,431 [INFO] dense_169 (Dense)            (None, 128)               8320      
2019-12-27 06:25:42,431 [INFO] _________________________________________________________________
2019-12-27 06:25:42,431 [INFO] batch_normalization_123 (Bat (None, 128)               512       
2019-12-27 06:25:42,431 [INFO] _________________________________________________________________
2019-12-27 06:25:42,431 [INFO] dropout_123 (Dropout)        (None, 128)               0         
2019-12-27 06:25:42,431 [INFO] _________________________________________________________________
2019-12-27 06:25:42,431 [INFO] dense_170 (Dense)            (None, 77)                9933      
2019-12-27 06:25:42,431 [INFO] =================================================================
2019-12-27 06:25:42,432 [INFO] Total params: 42,349
2019-12-27 06:25:42,432 [INFO] Trainable params: 41,517
2019-12-27 06:25:42,432 [INFO] Non-trainable params: 832
2019-12-27 06:25:42,432 [INFO] _________________________________________________________________
2019-12-27 06:25:42,599 [INFO] _________________________________________________________________
2019-12-27 06:25:42,600 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-27 06:25:42,600 [INFO] =================================================================
2019-12-27 06:25:42,600 [INFO] dense_171 (Dense)            (None, 64)                2112      
2019-12-27 06:25:42,600 [INFO] _________________________________________________________________
2019-12-27 06:25:42,600 [INFO] batch_normalization_124 (Bat (None, 64)                256       
2019-12-27 06:25:42,600 [INFO] _________________________________________________________________
2019-12-27 06:25:42,600 [INFO] dropout_124 (Dropout)        (None, 64)                0         
2019-12-27 06:25:42,600 [INFO] _________________________________________________________________
2019-12-27 06:25:42,600 [INFO] dense_172 (Dense)            (None, 15)                975       
2019-12-27 06:25:42,600 [INFO] =================================================================
2019-12-27 06:25:42,601 [INFO] Total params: 3,343
2019-12-27 06:25:42,601 [INFO] Trainable params: 3,215
2019-12-27 06:25:42,601 [INFO] Non-trainable params: 128
2019-12-27 06:25:42,601 [INFO] _________________________________________________________________
2019-12-27 06:25:42,601 [INFO] Training model
2019-12-27 06:25:42,601 [INFO] Splitting train set into 2 sets (unsupervised, supervised), random_seed = None
2019-12-27 06:26:14,142 [INFO] Split sizes (instances). total = 1936462, unsupervised = 484115, supervised = 1452347, unsupervised dataset hash = 98285dd0fbc87d4b14206f71ead8df971d2c98b5
2019-12-27 06:26:14,142 [INFO] Training autoencoder
 - val_f1: 0.9829
Train on 484115 samples, validate on 645487 samples
Epoch 1/200
 - 37s - loss: -2.9176e+00 - val_loss: -3.5557e+00
Epoch 2/200
 - 33s - loss: -3.4880e+00 - val_loss: -3.5929e+00
Epoch 3/200
 - 33s - loss: -3.5292e+00 - val_loss: -3.6094e+00
Epoch 4/200
 - 33s - loss: -3.5493e+00 - val_loss: -3.6181e+00
Epoch 5/200
 - 33s - loss: -3.5618e+00 - val_loss: -3.6237e+00
Epoch 6/200
 - 33s - loss: -3.5709e+00 - val_loss: -3.6316e+00
Epoch 7/200
 - 33s - loss: -3.5776e+00 - val_loss: -3.6330e+00
Epoch 8/200
 - 33s - loss: -3.5837e+00 - val_loss: -3.6376e+00
Epoch 9/200
 - 33s - loss: -3.5870e+00 - val_loss: -3.6427e+00
Epoch 10/200
 - 33s - loss: -3.5918e+00 - val_loss: -3.6451e+00
Epoch 11/200
 - 33s - loss: -3.5946e+00 - val_loss: -3.6457e+00
Epoch 12/200
 - 33s - loss: -3.5966e+00 - val_loss: -3.6460e+00
Epoch 13/200
 - 33s - loss: -3.5996e+00 - val_loss: -3.6471e+00
Epoch 14/200
 - 33s - loss: -3.6019e+00 - val_loss: -3.6482e+00
Epoch 15/200
 - 33s - loss: -3.6025e+00 - val_loss: -3.6488e+00
Epoch 16/200
 - 33s - loss: -3.6051e+00 - val_loss: -3.6508e+00
Epoch 17/200
 - 33s - loss: -3.6058e+00 - val_loss: -3.6482e+00
Epoch 18/200
 - 33s - loss: -3.6078e+00 - val_loss: -3.6522e+00
Epoch 19/200
 - 34s - loss: -3.6082e+00 - val_loss: -3.6524e+00
Epoch 20/200
 - 33s - loss: -3.6097e+00 - val_loss: -3.6542e+00
Epoch 21/200
 - 33s - loss: -3.6098e+00 - val_loss: -3.6531e+00
2019-12-27 06:38:26,622 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep4/ae_model_epoch_20.pickle
Epoch 22/200
 - 33s - loss: -3.6117e+00 - val_loss: -3.6536e+00
Epoch 23/200
 - 33s - loss: -3.6120e+00 - val_loss: -3.6550e+00
Epoch 24/200
 - 33s - loss: -3.6134e+00 - val_loss: -3.6555e+00
Epoch 25/200
 - 33s - loss: -3.6135e+00 - val_loss: -3.6568e+00
Epoch 26/200
 - 33s - loss: -3.6143e+00 - val_loss: -3.6577e+00
Epoch 27/200
 - 33s - loss: -3.6140e+00 - val_loss: -3.6574e+00
Epoch 28/200
 - 33s - loss: -3.6150e+00 - val_loss: -3.6564e+00
Epoch 29/200
 - 33s - loss: -3.6166e+00 - val_loss: -3.6557e+00
Epoch 30/200
 - 33s - loss: -3.6159e+00 - val_loss: -3.6553e+00
Epoch 31/200
 - 33s - loss: -3.6172e+00 - val_loss: -3.6565e+00
Epoch 32/200
 - 33s - loss: -3.6164e+00 - val_loss: -3.6570e+00
Epoch 33/200
 - 33s - loss: -3.6173e+00 - val_loss: -3.6572e+00
Epoch 34/200
 - 33s - loss: -3.6179e+00 - val_loss: -3.6569e+00
Epoch 35/200
 - 33s - loss: -3.6169e+00 - val_loss: -3.6573e+00
Epoch 36/200
 - 33s - loss: -3.6186e+00 - val_loss: -3.6587e+00
Epoch 37/200
 - 34s - loss: -3.6173e+00 - val_loss: -3.6566e+00
Epoch 38/200
 - 33s - loss: -3.6193e+00 - val_loss: -3.6572e+00
Epoch 39/200
 - 33s - loss: -3.6199e+00 - val_loss: -3.6573e+00
Epoch 40/200
 - 33s - loss: -3.6194e+00 - val_loss: -3.6582e+00
Epoch 41/200
 - 33s - loss: -3.6211e+00 - val_loss: -3.6587e+00
2019-12-27 06:49:34,129 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep4/ae_model_epoch_40.pickle
Epoch 42/200
 - 33s - loss: -3.6207e+00 - val_loss: -3.6578e+00
Epoch 43/200
 - 33s - loss: -3.6216e+00 - val_loss: -3.6584e+00
Epoch 44/200
 - 33s - loss: -3.6207e+00 - val_loss: -3.6577e+00
Epoch 45/200
 - 33s - loss: -3.6212e+00 - val_loss: -3.6589e+00
Epoch 46/200
 - 33s - loss: -3.6214e+00 - val_loss: -3.6576e+00
Epoch 47/200
 - 33s - loss: -3.6215e+00 - val_loss: -3.6581e+00
Epoch 48/200
 - 33s - loss: -3.6222e+00 - val_loss: -3.6577e+00
Epoch 49/200
 - 33s - loss: -3.6213e+00 - val_loss: -3.6591e+00
Epoch 50/200
 - 33s - loss: -3.6232e+00 - val_loss: -3.6585e+00
Epoch 51/200
 - 33s - loss: -3.6224e+00 - val_loss: -3.6590e+00
Epoch 52/200
 - 33s - loss: -3.6223e+00 - val_loss: -3.6599e+00
Epoch 53/200
 - 33s - loss: -3.6231e+00 - val_loss: -3.6587e+00
Epoch 54/200
 - 33s - loss: -3.6236e+00 - val_loss: -3.6596e+00
Epoch 55/200
 - 33s - loss: -3.6238e+00 - val_loss: -3.6599e+00
Epoch 56/200
 - 33s - loss: -3.6229e+00 - val_loss: -3.6599e+00
Epoch 57/200
 - 33s - loss: -3.6234e+00 - val_loss: -3.6608e+00
Epoch 58/200
 - 33s - loss: -3.6238e+00 - val_loss: -3.6606e+00
Epoch 59/200
 - 33s - loss: -3.6239e+00 - val_loss: -3.6638e+00
Epoch 60/200
 - 33s - loss: -3.6257e+00 - val_loss: -3.6633e+00
Epoch 61/200
 - 33s - loss: -3.6258e+00 - val_loss: -3.6623e+00
2019-12-27 07:00:40,523 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep4/ae_model_epoch_60.pickle
Epoch 62/200
 - 33s - loss: -3.6261e+00 - val_loss: -3.6623e+00
Epoch 63/200
 - 33s - loss: -3.6270e+00 - val_loss: -3.6633e+00
Epoch 64/200
 - 33s - loss: -3.6270e+00 - val_loss: -3.6640e+00
Epoch 65/200
 - 33s - loss: -3.6259e+00 - val_loss: -3.6633e+00
Epoch 66/200
 - 33s - loss: -3.6273e+00 - val_loss: -3.6637e+00
Epoch 67/200
 - 33s - loss: -3.6279e+00 - val_loss: -3.6644e+00
Epoch 68/200
 - 33s - loss: -3.6272e+00 - val_loss: -3.6629e+00
Epoch 69/200
 - 33s - loss: -3.6280e+00 - val_loss: -3.6637e+00
Epoch 70/200
 - 33s - loss: -3.6284e+00 - val_loss: -3.6640e+00
Epoch 71/200
 - 33s - loss: -3.6276e+00 - val_loss: -3.6637e+00
Epoch 72/200
 - 33s - loss: -3.6279e+00 - val_loss: -3.6646e+00
Epoch 73/200
 - 33s - loss: -3.6288e+00 - val_loss: -3.6648e+00
Epoch 74/200
 - 33s - loss: -3.6278e+00 - val_loss: -3.6631e+00
Epoch 75/200
 - 33s - loss: -3.6286e+00 - val_loss: -3.6640e+00
Epoch 76/200
 - 33s - loss: -3.6287e+00 - val_loss: -3.6641e+00
Epoch 77/200
 - 33s - loss: -3.6286e+00 - val_loss: -3.6641e+00
Epoch 78/200
 - 33s - loss: -3.6289e+00 - val_loss: -3.6639e+00
Epoch 79/200
 - 33s - loss: -3.6286e+00 - val_loss: -3.6615e+00
Epoch 80/200
 - 33s - loss: -3.6294e+00 - val_loss: -3.6642e+00
Epoch 81/200
 - 33s - loss: -3.6297e+00 - val_loss: -3.6642e+00
2019-12-27 07:11:48,251 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep4/ae_model_epoch_80.pickle
Epoch 82/200
 - 33s - loss: -3.6298e+00 - val_loss: -3.6645e+00
Epoch 83/200
 - 33s - loss: -3.6289e+00 - val_loss: -3.6637e+00
Epoch 84/200
 - 33s - loss: -3.6295e+00 - val_loss: -3.6644e+00
Epoch 85/200
 - 33s - loss: -3.6298e+00 - val_loss: -3.6641e+00
Epoch 86/200
 - 33s - loss: -3.6294e+00 - val_loss: -3.6643e+00
Epoch 87/200
 - 33s - loss: -3.6288e+00 - val_loss: -3.6641e+00
Epoch 88/200
 - 33s - loss: -3.6292e+00 - val_loss: -3.6633e+00
Epoch 89/200
 - 33s - loss: -3.6286e+00 - val_loss: -3.6641e+00
Epoch 90/200
 - 33s - loss: -3.6306e+00 - val_loss: -3.6644e+00
Epoch 91/200
 - 33s - loss: -3.6289e+00 - val_loss: -3.6644e+00
Epoch 92/200
 - 33s - loss: -3.6305e+00 - val_loss: -3.6647e+00
Epoch 93/200
 - 33s - loss: -3.6299e+00 - val_loss: -3.6641e+00
Epoch 94/200
 - 33s - loss: -3.6307e+00 - val_loss: -3.6645e+00
Epoch 95/200
 - 33s - loss: -3.6302e+00 - val_loss: -3.6646e+00
Epoch 96/200
 - 33s - loss: -3.6310e+00 - val_loss: -3.6644e+00
Epoch 97/200
 - 33s - loss: -3.6302e+00 - val_loss: -3.6648e+00
Epoch 98/200
 - 33s - loss: -3.6302e+00 - val_loss: -3.6650e+00
Epoch 99/200
 - 33s - loss: -3.6303e+00 - val_loss: -3.6643e+00
Epoch 100/200
 - 33s - loss: -3.6296e+00 - val_loss: -3.6647e+00
Epoch 101/200
 - 33s - loss: -3.6308e+00 - val_loss: -3.6650e+00
2019-12-27 07:22:54,685 [INFO] epoch = 100. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep4/ae_model_epoch_100.pickle
Epoch 102/200
 - 33s - loss: -3.6306e+00 - val_loss: -3.6647e+00
Epoch 103/200
 - 33s - loss: -3.6302e+00 - val_loss: -3.6650e+00
Epoch 104/200
 - 33s - loss: -3.6309e+00 - val_loss: -3.6650e+00
Epoch 105/200
 - 33s - loss: -3.6309e+00 - val_loss: -3.6644e+00
Epoch 106/200
 - 33s - loss: -3.6303e+00 - val_loss: -3.6646e+00
Epoch 107/200
 - 33s - loss: -3.6308e+00 - val_loss: -3.6654e+00
Epoch 108/200
 - 33s - loss: -3.6313e+00 - val_loss: -3.6641e+00
Epoch 109/200
 - 34s - loss: -3.6314e+00 - val_loss: -3.6650e+00
Epoch 110/200
 - 33s - loss: -3.6306e+00 - val_loss: -3.6649e+00
Epoch 111/200
 - 33s - loss: -3.6315e+00 - val_loss: -3.6644e+00
Epoch 112/200
 - 33s - loss: -3.6310e+00 - val_loss: -3.6649e+00
Epoch 113/200
 - 33s - loss: -3.6314e+00 - val_loss: -3.6641e+00
Epoch 114/200
 - 33s - loss: -3.6312e+00 - val_loss: -3.6649e+00
Epoch 115/200
 - 33s - loss: -3.6317e+00 - val_loss: -3.6650e+00
Epoch 116/200
 - 33s - loss: -3.6311e+00 - val_loss: -3.6650e+00
Epoch 117/200
 - 33s - loss: -3.6318e+00 - val_loss: -3.6649e+00
Epoch 118/200
 - 33s - loss: -3.6317e+00 - val_loss: -3.6643e+00
Epoch 119/200
 - 33s - loss: -3.6316e+00 - val_loss: -3.6644e+00
Epoch 120/200
 - 33s - loss: -3.6318e+00 - val_loss: -3.6647e+00
Epoch 121/200
 - 33s - loss: -3.6313e+00 - val_loss: -3.6650e+00
2019-12-27 07:34:02,195 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep4/ae_model_epoch_120.pickle
Epoch 122/200
 - 33s - loss: -3.6323e+00 - val_loss: -3.6641e+00
Epoch 123/200
 - 33s - loss: -3.6325e+00 - val_loss: -3.6662e+00
Epoch 124/200
 - 33s - loss: -3.6320e+00 - val_loss: -3.6657e+00
Epoch 125/200
 - 33s - loss: -3.6320e+00 - val_loss: -3.6660e+00
Epoch 126/200
 - 33s - loss: -3.6317e+00 - val_loss: -3.6663e+00
Epoch 127/200
 - 33s - loss: -3.6307e+00 - val_loss: -3.6661e+00
Epoch 128/200
 - 33s - loss: -3.6322e+00 - val_loss: -3.6654e+00
Epoch 129/200
 - 33s - loss: -3.6312e+00 - val_loss: -3.6652e+00
Epoch 130/200
 - 33s - loss: -3.6316e+00 - val_loss: -3.6654e+00
Epoch 131/200
 - 33s - loss: -3.6328e+00 - val_loss: -3.6649e+00
Epoch 132/200
 - 33s - loss: -3.6316e+00 - val_loss: -3.6653e+00
Epoch 133/200
 - 33s - loss: -3.6327e+00 - val_loss: -3.6652e+00
Epoch 134/200
 - 33s - loss: -3.6327e+00 - val_loss: -3.6652e+00
Epoch 135/200
 - 33s - loss: -3.6325e+00 - val_loss: -3.6652e+00
Epoch 136/200
 - 33s - loss: -3.6331e+00 - val_loss: -3.6655e+00
Epoch 137/200
 - 33s - loss: -3.6326e+00 - val_loss: -3.6657e+00
Epoch 138/200
 - 33s - loss: -3.6333e+00 - val_loss: -3.6655e+00
Epoch 139/200
 - 33s - loss: -3.6328e+00 - val_loss: -3.6667e+00
Epoch 140/200
 - 33s - loss: -3.6323e+00 - val_loss: -3.6664e+00
Epoch 141/200
 - 33s - loss: -3.6326e+00 - val_loss: -3.6662e+00
2019-12-27 07:45:08,562 [INFO] epoch = 140. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep4/ae_model_epoch_140.pickle
Epoch 142/200
 - 33s - loss: -3.6334e+00 - val_loss: -3.6665e+00
Epoch 143/200
 - 33s - loss: -3.6329e+00 - val_loss: -3.6661e+00
Epoch 144/200
 - 33s - loss: -3.6330e+00 - val_loss: -3.6662e+00
Epoch 145/200
 - 33s - loss: -3.6337e+00 - val_loss: -3.6665e+00
Epoch 146/200
 - 33s - loss: -3.6328e+00 - val_loss: -3.6666e+00
Epoch 147/200
 - 33s - loss: -3.6327e+00 - val_loss: -3.6651e+00
Epoch 148/200
 - 33s - loss: -3.6329e+00 - val_loss: -3.6667e+00
Epoch 149/200
 - 33s - loss: -3.6337e+00 - val_loss: -3.6666e+00
Epoch 150/200
 - 33s - loss: -3.6330e+00 - val_loss: -3.6657e+00
Epoch 151/200
 - 33s - loss: -3.6319e+00 - val_loss: -3.6656e+00
Epoch 152/200
 - 33s - loss: -3.6323e+00 - val_loss: -3.6655e+00
Epoch 153/200
 - 33s - loss: -3.6325e+00 - val_loss: -3.6654e+00
Epoch 154/200
 - 33s - loss: -3.6332e+00 - val_loss: -3.6662e+00
Epoch 155/200
 - 33s - loss: -3.6335e+00 - val_loss: -3.6655e+00
Epoch 156/200
 - 33s - loss: -3.6337e+00 - val_loss: -3.6653e+00
Epoch 157/200
 - 33s - loss: -3.6334e+00 - val_loss: -3.6664e+00
Epoch 158/200
 - 33s - loss: -3.6330e+00 - val_loss: -3.6665e+00
Epoch 159/200
 - 33s - loss: -3.6335e+00 - val_loss: -3.6668e+00
Epoch 160/200
 - 33s - loss: -3.6321e+00 - val_loss: -3.6663e+00
Epoch 161/200
 - 33s - loss: -3.6330e+00 - val_loss: -3.6666e+00
2019-12-27 07:56:16,129 [INFO] epoch = 160. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep4/ae_model_epoch_160.pickle
Epoch 162/200
 - 33s - loss: -3.6336e+00 - val_loss: -3.6656e+00
Epoch 163/200
 - 33s - loss: -3.6337e+00 - val_loss: -3.6661e+00
Epoch 164/200
 - 33s - loss: -3.6330e+00 - val_loss: -3.6666e+00
Epoch 165/200
 - 33s - loss: -3.6336e+00 - val_loss: -3.6656e+00
Epoch 166/200
 - 33s - loss: -3.6335e+00 - val_loss: -3.6668e+00
Epoch 167/200
 - 33s - loss: -3.6318e+00 - val_loss: -3.6659e+00
Epoch 168/200
 - 33s - loss: -3.6334e+00 - val_loss: -3.6655e+00
Epoch 169/200
 - 33s - loss: -3.6335e+00 - val_loss: -3.6664e+00
Epoch 170/200
 - 33s - loss: -3.6333e+00 - val_loss: -3.6652e+00
Epoch 171/200
 - 33s - loss: -3.6337e+00 - val_loss: -3.6665e+00
Epoch 172/200
 - 33s - loss: -3.6332e+00 - val_loss: -3.6645e+00
Epoch 173/200
 - 33s - loss: -3.6329e+00 - val_loss: -3.6668e+00
Epoch 174/200
 - 33s - loss: -3.6331e+00 - val_loss: -3.6665e+00
Epoch 175/200
 - 33s - loss: -3.6341e+00 - val_loss: -3.6667e+00
Epoch 176/200
 - 33s - loss: -3.6333e+00 - val_loss: -3.6666e+00
Epoch 177/200
 - 33s - loss: -3.6318e+00 - val_loss: -3.6661e+00
Epoch 178/200
 - 33s - loss: -3.6341e+00 - val_loss: -3.6657e+00
Epoch 179/200
 - 33s - loss: -3.6342e+00 - val_loss: -3.6666e+00
Epoch 180/200
 - 33s - loss: -3.6339e+00 - val_loss: -3.6667e+00
Epoch 181/200
 - 33s - loss: -3.6342e+00 - val_loss: -3.6668e+00
2019-12-27 08:07:22,895 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep4/ae_model_epoch_180.pickle
Epoch 182/200
 - 33s - loss: -3.6338e+00 - val_loss: -3.6674e+00
Epoch 183/200
 - 33s - loss: -3.6334e+00 - val_loss: -3.6670e+00
Epoch 184/200
 - 33s - loss: -3.6340e+00 - val_loss: -3.6671e+00
Epoch 185/200
 - 33s - loss: -3.6345e+00 - val_loss: -3.6670e+00
Epoch 186/200
 - 33s - loss: -3.6334e+00 - val_loss: -3.6664e+00
Epoch 187/200
 - 33s - loss: -3.6348e+00 - val_loss: -3.6669e+00
Epoch 188/200
 - 33s - loss: -3.6331e+00 - val_loss: -3.6684e+00
Epoch 189/200
 - 33s - loss: -3.6344e+00 - val_loss: -3.6668e+00
Epoch 190/200
 - 33s - loss: -3.6343e+00 - val_loss: -3.6665e+00
Epoch 191/200
 - 33s - loss: -3.6342e+00 - val_loss: -3.6653e+00
Epoch 192/200
 - 33s - loss: -3.6335e+00 - val_loss: -3.6658e+00
Epoch 193/200
 - 33s - loss: -3.6334e+00 - val_loss: -3.6664e+00
Epoch 194/200
 - 33s - loss: -3.6344e+00 - val_loss: -3.6682e+00
Epoch 195/200
 - 33s - loss: -3.6341e+00 - val_loss: -3.6671e+00
Epoch 196/200
 - 33s - loss: -3.6335e+00 - val_loss: -3.6674e+00
Epoch 197/200
 - 33s - loss: -3.6338e+00 - val_loss: -3.6683e+00
Epoch 198/200
 - 33s - loss: -3.6342e+00 - val_loss: -3.6671e+00
Epoch 199/200
 - 34s - loss: -3.6339e+00 - val_loss: -3.6681e+00
Epoch 200/200
 - 33s - loss: -3.6346e+00 - val_loss: -3.6673e+00
2019-12-27 08:17:57,256 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-27 08:19:33,798 [INFO] Last epoch loss evaluation: train_loss = -3.664756, val_loss = -3.668352
2019-12-27 08:19:33,798 [INFO] Training autoencoder complete
2019-12-27 08:19:33,798 [INFO] Encoding data for supervised training
2019-12-27 08:21:42,225 [INFO] Encoding complete
2019-12-27 08:21:42,226 [INFO] Training neural network layers (after autoencoder)
Train on 1452347 samples, validate on 645487 samples
Epoch 1/200
 - 33s - loss: 0.0170 - val_loss: 0.0098
 - val_f1: 0.9809
Epoch 2/200
 - 30s - loss: 0.0095 - val_loss: 0.0090
 - val_f1: 0.9797
Epoch 3/200
 - 30s - loss: 0.0092 - val_loss: 0.0086
 - val_f1: 0.9819
Epoch 4/200
 - 30s - loss: 0.0090 - val_loss: 0.0086
 - val_f1: 0.9819
Epoch 5/200
 - 30s - loss: 0.0089 - val_loss: 0.0086
 - val_f1: 0.9803
Epoch 6/200
 - 30s - loss: 0.0088 - val_loss: 0.0084
 - val_f1: 0.9807
Epoch 7/200
 - 30s - loss: 0.0088 - val_loss: 0.0083
 - val_f1: 0.9825
Epoch 8/200
 - 30s - loss: 0.0088 - val_loss: 0.0082
 - val_f1: 0.9829
Epoch 9/200
 - 30s - loss: 0.0087 - val_loss: 0.0083
 - val_f1: 0.9827
Epoch 10/200
 - 30s - loss: 0.0086 - val_loss: 0.0082
 - val_f1: 0.9825
Epoch 11/200
 - 30s - loss: 0.0086 - val_loss: 0.0082
 - val_f1: 0.9826
Epoch 12/200
 - 30s - loss: 0.0086 - val_loss: 0.0108
 - val_f1: 0.9772
Epoch 13/200
 - 30s - loss: 0.0086 - val_loss: 0.0083
 - val_f1: 0.9822
Epoch 14/200
 - 30s - loss: 0.0085 - val_loss: 0.0082
 - val_f1: 0.9826
Epoch 15/200
 - 30s - loss: 0.0086 - val_loss: 0.0082
 - val_f1: 0.9825
Epoch 16/200
 - 30s - loss: 0.0085 - val_loss: 0.0084
 - val_f1: 0.9826
Epoch 17/200
 - 30s - loss: 0.0085 - val_loss: 0.0081
 - val_f1: 0.9830
Epoch 18/200
 - 30s - loss: 0.0085 - val_loss: 0.0082
 - val_f1: 0.9830
Epoch 19/200
 - 30s - loss: 0.0085 - val_loss: 0.0091
 - val_f1: 0.9803
Epoch 20/200
 - 30s - loss: 0.0085 - val_loss: 0.0082
 - val_f1: 0.9826
Epoch 21/200
 - 30s - loss: 0.0085 - val_loss: 0.0082
2019-12-27 08:44:43,199 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep4/ann_model_epoch_20.pickle
 - val_f1: 0.9832
Epoch 22/200
 - 30s - loss: 0.0085 - val_loss: 0.0083
 - val_f1: 0.9830
Epoch 23/200
 - 30s - loss: 0.0085 - val_loss: 0.0081
 - val_f1: 0.9829
Epoch 24/200
 - 30s - loss: 0.0084 - val_loss: 0.0083
 - val_f1: 0.9827
Epoch 25/200
 - 30s - loss: 0.0084 - val_loss: 0.0082
 - val_f1: 0.9831
Epoch 26/200
 - 30s - loss: 0.0084 - val_loss: 0.0081
 - val_f1: 0.9829
Epoch 27/200
 - 30s - loss: 0.0084 - val_loss: 0.0082
 - val_f1: 0.9829
Epoch 28/200
 - 30s - loss: 0.0084 - val_loss: 0.0081
 - val_f1: 0.9832
Epoch 29/200
 - 30s - loss: 0.0084 - val_loss: 0.0082
 - val_f1: 0.9825
Epoch 30/200
 - 30s - loss: 0.0084 - val_loss: 0.0082
 - val_f1: 0.9833
Epoch 31/200
 - 30s - loss: 0.0084 - val_loss: 0.0082
 - val_f1: 0.9828
Epoch 32/200
 - 30s - loss: 0.0084 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 33/200
 - 30s - loss: 0.0084 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 34/200
 - 30s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9828
Epoch 35/200
 - 30s - loss: 0.0084 - val_loss: 0.0081
 - val_f1: 0.9831
Epoch 36/200
 - 30s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9829
Epoch 37/200
 - 30s - loss: 0.0084 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 38/200
 - 31s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9830
Epoch 39/200
 - 30s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9829
Epoch 40/200
 - 31s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9830
Epoch 41/200
 - 30s - loss: 0.0083 - val_loss: 0.0081
2019-12-27 09:06:45,227 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep4/ann_model_epoch_40.pickle
 - val_f1: 0.9827
Epoch 42/200
 - 30s - loss: 0.0083 - val_loss: 0.0082
 - val_f1: 0.9828
Epoch 43/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 44/200
 - 30s - loss: 0.0083 - val_loss: 0.0082
 - val_f1: 0.9828
Epoch 45/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 46/200
 - 31s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 47/200
 - 30s - loss: 0.0083 - val_loss: 0.0116
 - val_f1: 0.9762
Epoch 48/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9833
Epoch 49/200
 - 31s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9814
Epoch 50/200
 - 30s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9816
Epoch 51/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 52/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9836
Epoch 53/200
 - 30s - loss: 0.0083 - val_loss: 0.0083
 - val_f1: 0.9821
Epoch 54/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9834
Epoch 55/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 56/200
 - 31s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 57/200
 - 31s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9831
Epoch 58/200
 - 31s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9833
Epoch 59/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 60/200
 - 31s - loss: 0.0083 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 61/200
 - 31s - loss: 0.0083 - val_loss: 0.0082
2019-12-27 09:28:59,582 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep4/ann_model_epoch_60.pickle
 - val_f1: 0.9826
Epoch 62/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 63/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 64/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9828
Epoch 65/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 66/200
 - 30s - loss: 0.0083 - val_loss: 0.0093
 - val_f1: 0.9801
Epoch 67/200
 - 31s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 68/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9834
Epoch 69/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9834
Epoch 70/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 71/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 72/200
 - 30s - loss: 0.0083 - val_loss: 0.0107
 - val_f1: 0.9784
Epoch 73/200
 - 30s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9830
Epoch 74/200
 - 31s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 75/200
 - 31s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 76/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 77/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 78/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 79/200
 - 30s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9833
Epoch 80/200
 - 30s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9816
Epoch 81/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
2019-12-27 09:51:13,640 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep4/ann_model_epoch_80.pickle
 - val_f1: 0.9835
Epoch 82/200
 - 31s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9835
Epoch 83/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 84/200
 - 31s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9835
Epoch 85/200
 - 30s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9828
Epoch 86/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 87/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9819
Epoch 88/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 89/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9834
Epoch 90/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9834
Epoch 91/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9835
Epoch 92/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9835
Epoch 93/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 94/200
 - 31s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9833
Epoch 95/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 96/200
 - 30s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9833
Epoch 97/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 98/200
 - 31s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 99/200
 - 30s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9833
Epoch 100/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9833
Epoch 101/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
2019-12-27 10:13:23,804 [INFO] epoch = 100. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep4/ann_model_epoch_100.pickle
 - val_f1: 0.9832
Epoch 102/200
 - 31s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 103/200
 - 31s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9828
Epoch 104/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 105/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 106/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 107/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 108/200
 - 31s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 109/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 110/200
 - 31s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 111/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 112/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 113/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9834
Epoch 114/200
 - 31s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 115/200
 - 30s - loss: 0.0082 - val_loss: 0.0082
 - val_f1: 0.9812
Epoch 116/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9836
Epoch 117/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 118/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 119/200
 - 31s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 120/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 121/200
 - 31s - loss: 0.0082 - val_loss: 0.0079
2019-12-27 10:35:39,590 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep4/ann_model_epoch_120.pickle
 - val_f1: 0.9834
Epoch 122/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 123/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9835
Epoch 124/200
 - 30s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9830
Epoch 125/200
 - 31s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 126/200
 - 31s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 127/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 128/200
 - 30s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9829
Epoch 129/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 130/200
 - 31s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 131/200
 - 31s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9834
Epoch 132/200
 - 31s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9817
Epoch 133/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 134/200
 - 30s - loss: 0.0082 - val_loss: 0.0093
 - val_f1: 0.9800
Epoch 135/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9833
Epoch 136/200
 - 31s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 137/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 138/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9834
Epoch 139/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 140/200
 - 31s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9830
Epoch 141/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
2019-12-27 10:57:56,040 [INFO] epoch = 140. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep4/ann_model_epoch_140.pickle
 - val_f1: 0.9834
Epoch 142/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9834
Epoch 143/200
 - 30s - loss: 0.0082 - val_loss: 0.0082
 - val_f1: 0.9821
Epoch 144/200
 - 30s - loss: 0.0082 - val_loss: 0.0083
 - val_f1: 0.9829
Epoch 145/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9835
Epoch 146/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 147/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 148/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 149/200
 - 30s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9817
Epoch 150/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 151/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 152/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9833
Epoch 153/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 154/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9836
Epoch 155/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9835
Epoch 156/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9833
Epoch 157/200
 - 30s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9815
Epoch 158/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 159/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 160/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 161/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
2019-12-27 11:20:07,048 [INFO] epoch = 160. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep4/ann_model_epoch_160.pickle
 - val_f1: 0.9830
Epoch 162/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 163/200
 - 31s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9834
Epoch 164/200
 - 31s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9835
Epoch 165/200
 - 31s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 166/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9835
Epoch 167/200
 - 30s - loss: 0.0082 - val_loss: 0.0082
 - val_f1: 0.9831
Epoch 168/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 169/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 170/200
 - 31s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9833
Epoch 171/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9833
Epoch 172/200
 - 31s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9833
Epoch 173/200
 - 31s - loss: 0.0082 - val_loss: 0.0099
 - val_f1: 0.9786
Epoch 174/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 175/200
 - 30s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9829
Epoch 176/200
 - 31s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9818
Epoch 177/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9835
Epoch 178/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 179/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9835
Epoch 180/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 181/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
2019-12-27 11:42:21,342 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep4/ann_model_epoch_180.pickle
 - val_f1: 0.9831
Epoch 182/200
 - 31s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 183/200
 - 30s - loss: 0.0082 - val_loss: 0.0083
 - val_f1: 0.9823
Epoch 184/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 185/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9834
Epoch 186/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 187/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9835
Epoch 188/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 189/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9834
Epoch 190/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9837
Epoch 191/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 192/200
 - 31s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 193/200
 - 30s - loss: 0.0082 - val_loss: 0.0086
 - val_f1: 0.9818
Epoch 194/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9815
Epoch 195/200
 - 30s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9830
Epoch 196/200
 - 31s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9835
Epoch 197/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 198/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 199/200
 - 31s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 200/200
 - 30s - loss: 0.0081 - val_loss: 0.0093
2019-12-27 12:04:05,700 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-27 12:06:08,827 [INFO] Last epoch loss evaluation: train_loss = 0.007848, val_loss = 0.007874
2019-12-27 12:06:08,836 [INFO] Training complete. time_to_train = 20426.24 sec, 340.44 min
2019-12-27 12:06:08,873 [INFO] Model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep4/best_model.pickle
/home/sunanda/research/ids_experiments/utility.py:268: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
2019-12-27 12:06:09,059 [INFO] Plot saved to: results_selected_models/selected_ids18_subset_ae_ann_deep_rep4/training_error_history.png
/home/sunanda/research/ids_experiments/utility.py:285: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
2019-12-27 12:06:09,253 [INFO] Plot saved to: results_selected_models/selected_ids18_subset_ae_ann_deep_rep4/training_f1_history.png
2019-12-27 12:06:09,254 [INFO] Making predictions on training, validation, testing data
2019-12-27 12:12:18,273 [INFO] Evaluating predictions (results)
/home/sunanda/anaconda3/envs/ml_env_cpu/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2019-12-27 12:12:41,440 [INFO] Dataset: Testing. Classification report below
2019-12-27 12:12:41,440 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99    535650
                     Bot       1.00      1.00      1.00     11465
        Brute Force -Web       0.00      0.00      0.00        24
        Brute Force -XSS       1.00      0.33      0.50         9
        DDOS attack-HOIC       1.00      1.00      1.00     27447
    DDOS attack-LOIC-UDP       0.71      1.00      0.83        67
  DDoS attacks-LOIC-HTTP       0.99      0.99      0.99     23010
   DoS attacks-GoldenEye       0.99      1.00      0.99      1651
        DoS attacks-Hulk       1.00      1.00      1.00     18478
DoS attacks-SlowHTTPTest       0.75      0.49      0.59      5596
   DoS attacks-Slowloris       0.94      0.96      0.95       440
          FTP-BruteForce       0.70      0.88      0.78      7718
           Infilteration       0.39      0.00      0.01      6404
           SQL Injection       0.00      0.00      0.00         4
          SSH-Bruteforce       1.00      1.00      1.00      7525

                accuracy                           0.98    645488
               macro avg       0.76      0.71      0.71    645488
            weighted avg       0.98      0.98      0.98    645488

2019-12-27 12:12:41,440 [INFO] Overall accuracy (micro avg): 0.9833893116525791
/home/sunanda/research/ids_experiments/utility.py:307: RuntimeWarning: invalid value encountered in true_divide
  PPV = TP / (TP + FP)    # Precision or positive predictive value
/home/sunanda/research/ids_experiments/utility.py:311: RuntimeWarning: invalid value encountered in true_divide
  FDR = FP / (TP + FP)    # False discovery rate
2019-12-27 12:13:06,375 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9834         0.9834                       0.9834                0.0012                   0.0166  0.9834
1     Macro avg        0.9978         0.7630                       0.7098                0.0044                   0.2902  0.7085
2  Weighted avg        0.9909         0.9776                       0.9834                0.0500                   0.0166  0.9782
2019-12-27 12:13:29,509 [INFO] Dataset: Validation. Classification report below
2019-12-27 12:13:29,509 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99    535650
                     Bot       1.00      1.00      1.00     11465
        Brute Force -Web       0.00      0.00      0.00        25
        Brute Force -XSS       1.00      0.67      0.80         9
        DDOS attack-HOIC       1.00      1.00      1.00     27447
    DDOS attack-LOIC-UDP       0.76      0.99      0.86        68
  DDoS attacks-LOIC-HTTP       0.99      0.99      0.99     23009
   DoS attacks-GoldenEye       0.99      0.99      0.99      1651
        DoS attacks-Hulk       1.00      1.00      1.00     18478
DoS attacks-SlowHTTPTest       0.76      0.48      0.59      5596
   DoS attacks-Slowloris       0.93      0.95      0.94       439
          FTP-BruteForce       0.70      0.89      0.78      7718
           Infilteration       0.46      0.00      0.00      6403
           SQL Injection       0.00      0.00      0.00         4
          SSH-Bruteforce       1.00      1.00      1.00      7525

                accuracy                           0.98    645487
               macro avg       0.77      0.73      0.73    645487
            weighted avg       0.98      0.98      0.98    645487

2019-12-27 12:13:29,509 [INFO] Overall accuracy (micro avg): 0.9834837882095224
2019-12-27 12:13:54,415 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9835         0.9835                       0.9835                0.0012                   0.0165  0.9835
1     Macro avg        0.9978         0.7720                       0.7306                0.0044                   0.2694  0.7304
2  Weighted avg        0.9909         0.9784                       0.9835                0.0498                   0.0165  0.9783
2019-12-27 12:15:10,259 [INFO] Dataset: Training. Classification report below
2019-12-27 12:15:10,260 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99   1606949
                     Bot       1.00      1.00      1.00     34396
        Brute Force -Web       0.00      0.00      0.00        73
        Brute Force -XSS       1.00      0.50      0.67        26
        DDOS attack-HOIC       1.00      1.00      1.00     82341
    DDOS attack-LOIC-UDP       0.73      0.98      0.84       203
  DDoS attacks-LOIC-HTTP       0.99      0.99      0.99     69029
   DoS attacks-GoldenEye       0.99      1.00      0.99      4954
        DoS attacks-Hulk       1.00      1.00      1.00     55435
DoS attacks-SlowHTTPTest       0.75      0.48      0.59     16787
   DoS attacks-Slowloris       0.95      0.97      0.96      1318
          FTP-BruteForce       0.70      0.88      0.78     23153
           Infilteration       0.45      0.00      0.01     19210
           SQL Injection       0.00      0.00      0.00        12
          SSH-Bruteforce       1.00      1.00      1.00     22576

                accuracy                           0.98   1936462
               macro avg       0.77      0.72      0.72   1936462
            weighted avg       0.98      0.98      0.98   1936462

2019-12-27 12:15:10,260 [INFO] Overall accuracy (micro avg): 0.9834073686961066
2019-12-27 12:16:31,978 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9834         0.9834                       0.9834                0.0012                   0.0166  0.9834
1     Macro avg        0.9978         0.7699                       0.7203                0.0044                   0.2797  0.7209
2  Weighted avg        0.9909         0.9783                       0.9834                0.0498                   0.0166  0.9782
2019-12-27 12:16:32,051 [INFO] Results saved to: results_selected_models/selected_ids18_subset_ae_ann_deep_rep4/selected_ids18_subset_ae_ann_deep_rep4_results.xlsx
2019-12-27 12:16:32,056 [INFO] ================= Finished running experiment no. 4 ================= 

2019-12-27 12:16:32,107 [INFO] Created directory: results_selected_models/selected_ids18_subset_ae_ann_deep_rep5
2019-12-27 12:16:32,107 [INFO] Initialized logging. log_filename = results_selected_models/selected_ids18_subset_ae_ann_deep_rep5/run_log.log
2019-12-27 12:16:32,107 [INFO] ================= Running experiment no. 5  ================= 

2019-12-27 12:16:32,107 [INFO] Experiment parameters given below
2019-12-27 12:16:32,107 [INFO] 
{'experiment_num': 5, 'results_dir': 'results_selected_models/selected_ids18_subset_ae_ann_deep_rep5', 'model_type': 'classifier', 'model': 'ae_ann', 'normal_label': 'BENIGN', 'scaling_type': 'NA', 'unsupervised_ratio': 0.25, 'ae_encoder_units': [128, 64, 32], 'ae_encoder_activations': ['relu', 'relu', 'relu'], 'ae_encoder_dropout_rates': [0.2, 0.2, 0.2], 'ae_encoder_l1_param': -1, 'ae_decoder_units': [64, 128], 'ae_decoder_activations': ['relu', 'relu'], 'ae_decoder_dropout_rates': [0.2, 0.2], 'output_activation': 'sigmoid', 'loss_function': 'binary_crossentropy', 'ae_epochs': 200, 'ann_layer_units': [64], 'ann_layer_activations': ['relu'], 'ann_layer_dropout_rates': [0.2], 'ann_epochs': 200, 'early_stop_patience': 50, 'batch_size': 256, 'dataset_dir': '../Datasets/small_datasets/ids2018', 'description': 'selected_ids18_subset_ae_ann_deep_rep5'}
2019-12-27 12:16:32,107 [INFO] Created tensorboard log directory: results_selected_models/selected_ids18_subset_ae_ann_deep_rep5/tf_logs_run_2019_12_27-12_16_32
2019-12-27 12:16:32,107 [INFO] Loading datsets from: ../Datasets/small_datasets/ids2018
2019-12-27 12:16:32,108 [INFO] Reading X, y files
2019-12-27 12:16:32,108 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_train.h5
2019-12-27 12:16:36,151 [INFO] Reading complete. time_to_read=4.04 seconds
2019-12-27 12:16:36,151 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_val.h5
2019-12-27 12:16:37,535 [INFO] Reading complete. time_to_read=1.38 seconds
2019-12-27 12:16:37,535 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_test.h5
2019-12-27 12:16:38,918 [INFO] Reading complete. time_to_read=1.38 seconds
2019-12-27 12:16:38,918 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_train.h5
2019-12-27 12:16:39,195 [INFO] Reading complete. time_to_read=0.28 seconds
2019-12-27 12:16:39,195 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_val.h5
2019-12-27 12:16:39,291 [INFO] Reading complete. time_to_read=0.10 seconds
2019-12-27 12:16:39,292 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_test.h5
2019-12-27 12:16:39,389 [INFO] Reading complete. time_to_read=0.10 seconds
2019-12-27 12:16:42,720 [INFO] Initializing model
2019-12-27 12:16:43,440 [INFO] _________________________________________________________________
2019-12-27 12:16:43,440 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-27 12:16:43,440 [INFO] =================================================================
2019-12-27 12:16:43,440 [INFO] dense_173 (Dense)            (None, 128)               9984      
2019-12-27 12:16:43,440 [INFO] _________________________________________________________________
2019-12-27 12:16:43,440 [INFO] batch_normalization_125 (Bat (None, 128)               512       
2019-12-27 12:16:43,440 [INFO] _________________________________________________________________
2019-12-27 12:16:43,441 [INFO] dropout_125 (Dropout)        (None, 128)               0         
2019-12-27 12:16:43,441 [INFO] _________________________________________________________________
2019-12-27 12:16:43,441 [INFO] dense_174 (Dense)            (None, 64)                8256      
2019-12-27 12:16:43,441 [INFO] _________________________________________________________________
2019-12-27 12:16:43,441 [INFO] batch_normalization_126 (Bat (None, 64)                256       
2019-12-27 12:16:43,441 [INFO] _________________________________________________________________
2019-12-27 12:16:43,441 [INFO] dropout_126 (Dropout)        (None, 64)                0         
2019-12-27 12:16:43,441 [INFO] _________________________________________________________________
2019-12-27 12:16:43,441 [INFO] dense_175 (Dense)            (None, 32)                2080      
2019-12-27 12:16:43,441 [INFO] _________________________________________________________________
2019-12-27 12:16:43,441 [INFO] batch_normalization_127 (Bat (None, 32)                128       
2019-12-27 12:16:43,442 [INFO] _________________________________________________________________
2019-12-27 12:16:43,442 [INFO] dropout_127 (Dropout)        (None, 32)                0         
2019-12-27 12:16:43,442 [INFO] _________________________________________________________________
2019-12-27 12:16:43,442 [INFO] dense_176 (Dense)            (None, 64)                2112      
2019-12-27 12:16:43,442 [INFO] _________________________________________________________________
2019-12-27 12:16:43,442 [INFO] batch_normalization_128 (Bat (None, 64)                256       
2019-12-27 12:16:43,442 [INFO] _________________________________________________________________
2019-12-27 12:16:43,442 [INFO] dropout_128 (Dropout)        (None, 64)                0         
2019-12-27 12:16:43,442 [INFO] _________________________________________________________________
2019-12-27 12:16:43,442 [INFO] dense_177 (Dense)            (None, 128)               8320      
2019-12-27 12:16:43,442 [INFO] _________________________________________________________________
2019-12-27 12:16:43,443 [INFO] batch_normalization_129 (Bat (None, 128)               512       
2019-12-27 12:16:43,443 [INFO] _________________________________________________________________
2019-12-27 12:16:43,443 [INFO] dropout_129 (Dropout)        (None, 128)               0         
2019-12-27 12:16:43,443 [INFO] _________________________________________________________________
2019-12-27 12:16:43,443 [INFO] dense_178 (Dense)            (None, 77)                9933      
2019-12-27 12:16:43,443 [INFO] =================================================================
2019-12-27 12:16:43,443 [INFO] Total params: 42,349
2019-12-27 12:16:43,443 [INFO] Trainable params: 41,517
2019-12-27 12:16:43,444 [INFO] Non-trainable params: 832
2019-12-27 12:16:43,444 [INFO] _________________________________________________________________
2019-12-27 12:16:43,616 [INFO] _________________________________________________________________
2019-12-27 12:16:43,616 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-27 12:16:43,616 [INFO] =================================================================
2019-12-27 12:16:43,616 [INFO] dense_179 (Dense)            (None, 64)                2112      
2019-12-27 12:16:43,617 [INFO] _________________________________________________________________
2019-12-27 12:16:43,617 [INFO] batch_normalization_130 (Bat (None, 64)                256       
2019-12-27 12:16:43,617 [INFO] _________________________________________________________________
2019-12-27 12:16:43,617 [INFO] dropout_130 (Dropout)        (None, 64)                0         
2019-12-27 12:16:43,617 [INFO] _________________________________________________________________
2019-12-27 12:16:43,617 [INFO] dense_180 (Dense)            (None, 15)                975       
2019-12-27 12:16:43,617 [INFO] =================================================================
2019-12-27 12:16:43,617 [INFO] Total params: 3,343
2019-12-27 12:16:43,617 [INFO] Trainable params: 3,215
2019-12-27 12:16:43,617 [INFO] Non-trainable params: 128
2019-12-27 12:16:43,617 [INFO] _________________________________________________________________
2019-12-27 12:16:43,618 [INFO] Training model
2019-12-27 12:16:43,618 [INFO] Splitting train set into 2 sets (unsupervised, supervised), random_seed = None
2019-12-27 12:17:15,348 [INFO] Split sizes (instances). total = 1936462, unsupervised = 484115, supervised = 1452347, unsupervised dataset hash = 1295ed91814dabd04a336ed6327c97fcba16a7e6
2019-12-27 12:17:15,348 [INFO] Training autoencoder
 - val_f1: 0.9805
Train on 484115 samples, validate on 645487 samples
Epoch 1/200
 - 38s - loss: -2.9263e+00 - val_loss: -3.5496e+00
Epoch 2/200
 - 34s - loss: -3.4795e+00 - val_loss: -3.5887e+00
Epoch 3/200
 - 34s - loss: -3.5206e+00 - val_loss: -3.6045e+00
Epoch 4/200
 - 34s - loss: -3.5427e+00 - val_loss: -3.6165e+00
Epoch 5/200
 - 34s - loss: -3.5557e+00 - val_loss: -3.6209e+00
Epoch 6/200
 - 34s - loss: -3.5666e+00 - val_loss: -3.6275e+00
Epoch 7/200
 - 34s - loss: -3.5727e+00 - val_loss: -3.6311e+00
Epoch 8/200
 - 34s - loss: -3.5781e+00 - val_loss: -3.6338e+00
Epoch 9/200
 - 34s - loss: -3.5831e+00 - val_loss: -3.6377e+00
Epoch 10/200
 - 34s - loss: -3.5869e+00 - val_loss: -3.6405e+00
Epoch 11/200
 - 34s - loss: -3.5900e+00 - val_loss: -3.6420e+00
Epoch 12/200
 - 34s - loss: -3.5926e+00 - val_loss: -3.6405e+00
Epoch 13/200
 - 34s - loss: -3.5945e+00 - val_loss: -3.6455e+00
Epoch 14/200
 - 34s - loss: -3.5972e+00 - val_loss: -3.6450e+00
Epoch 15/200
 - 34s - loss: -3.5985e+00 - val_loss: -3.6464e+00
Epoch 16/200
 - 34s - loss: -3.6005e+00 - val_loss: -3.6467e+00
Epoch 17/200
 - 34s - loss: -3.6014e+00 - val_loss: -3.6487e+00
Epoch 18/200
 - 34s - loss: -3.6024e+00 - val_loss: -3.6472e+00
Epoch 19/200
 - 34s - loss: -3.6041e+00 - val_loss: -3.6507e+00
Epoch 20/200
 - 34s - loss: -3.6051e+00 - val_loss: -3.6513e+00
Epoch 21/200
 - 34s - loss: -3.6069e+00 - val_loss: -3.6508e+00
2019-12-27 12:29:50,141 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep5/ae_model_epoch_20.pickle
Epoch 22/200
 - 34s - loss: -3.6076e+00 - val_loss: -3.6528e+00
Epoch 23/200
 - 34s - loss: -3.6090e+00 - val_loss: -3.6522e+00
Epoch 24/200
 - 34s - loss: -3.6098e+00 - val_loss: -3.6542e+00
Epoch 25/200
 - 34s - loss: -3.6105e+00 - val_loss: -3.6538e+00
Epoch 26/200
 - 34s - loss: -3.6109e+00 - val_loss: -3.6555e+00
Epoch 27/200
 - 34s - loss: -3.6107e+00 - val_loss: -3.6543e+00
Epoch 28/200
 - 34s - loss: -3.6130e+00 - val_loss: -3.6569e+00
Epoch 29/200
 - 34s - loss: -3.6125e+00 - val_loss: -3.6567e+00
Epoch 30/200
 - 34s - loss: -3.6146e+00 - val_loss: -3.6575e+00
Epoch 31/200
 - 34s - loss: -3.6145e+00 - val_loss: -3.6572e+00
Epoch 32/200
 - 34s - loss: -3.6139e+00 - val_loss: -3.6578e+00
Epoch 33/200
 - 34s - loss: -3.6165e+00 - val_loss: -3.6574e+00
Epoch 34/200
 - 34s - loss: -3.6161e+00 - val_loss: -3.6602e+00
Epoch 35/200
 - 34s - loss: -3.6166e+00 - val_loss: -3.6604e+00
Epoch 36/200
 - 34s - loss: -3.6170e+00 - val_loss: -3.6587e+00
Epoch 37/200
 - 34s - loss: -3.6162e+00 - val_loss: -3.6587e+00
Epoch 38/200
 - 34s - loss: -3.6175e+00 - val_loss: -3.6594e+00
Epoch 39/200
 - 34s - loss: -3.6173e+00 - val_loss: -3.6596e+00
Epoch 40/200
 - 34s - loss: -3.6189e+00 - val_loss: -3.6603e+00
Epoch 41/200
 - 34s - loss: -3.6182e+00 - val_loss: -3.6617e+00
2019-12-27 12:41:14,094 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep5/ae_model_epoch_40.pickle
Epoch 42/200
 - 34s - loss: -3.6188e+00 - val_loss: -3.6621e+00
Epoch 43/200
 - 34s - loss: -3.6190e+00 - val_loss: -3.6599e+00
Epoch 44/200
 - 34s - loss: -3.6186e+00 - val_loss: -3.6615e+00
Epoch 45/200
 - 34s - loss: -3.6197e+00 - val_loss: -3.6628e+00
Epoch 46/200
 - 34s - loss: -3.6201e+00 - val_loss: -3.6620e+00
Epoch 47/200
 - 34s - loss: -3.6205e+00 - val_loss: -3.6634e+00
Epoch 48/200
 - 34s - loss: -3.6206e+00 - val_loss: -3.6617e+00
Epoch 49/200
 - 34s - loss: -3.6203e+00 - val_loss: -3.6618e+00
Epoch 50/200
 - 34s - loss: -3.6218e+00 - val_loss: -3.6603e+00
Epoch 51/200
 - 34s - loss: -3.6213e+00 - val_loss: -3.6613e+00
Epoch 52/200
 - 34s - loss: -3.6204e+00 - val_loss: -3.6623e+00
Epoch 53/200
 - 34s - loss: -3.6218e+00 - val_loss: -3.6611e+00
Epoch 54/200
 - 34s - loss: -3.6216e+00 - val_loss: -3.6625e+00
Epoch 55/200
 - 34s - loss: -3.6226e+00 - val_loss: -3.6633e+00
Epoch 56/200
 - 34s - loss: -3.6223e+00 - val_loss: -3.6626e+00
Epoch 57/200
 - 34s - loss: -3.6229e+00 - val_loss: -3.6590e+00
Epoch 58/200
 - 34s - loss: -3.6224e+00 - val_loss: -3.6636e+00
Epoch 59/200
 - 34s - loss: -3.6232e+00 - val_loss: -3.6620e+00
Epoch 60/200
 - 34s - loss: -3.6238e+00 - val_loss: -3.6617e+00
Epoch 61/200
 - 34s - loss: -3.6237e+00 - val_loss: -3.6632e+00
2019-12-27 12:52:38,298 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep5/ae_model_epoch_60.pickle
Epoch 62/200
 - 34s - loss: -3.6238e+00 - val_loss: -3.6630e+00
Epoch 63/200
 - 34s - loss: -3.6232e+00 - val_loss: -3.6606e+00
Epoch 64/200
 - 34s - loss: -3.6235e+00 - val_loss: -3.6612e+00
Epoch 65/200
 - 34s - loss: -3.6239e+00 - val_loss: -3.6633e+00
Epoch 66/200
 - 34s - loss: -3.6239e+00 - val_loss: -3.6631e+00
Epoch 67/200
 - 34s - loss: -3.6239e+00 - val_loss: -3.6639e+00
Epoch 68/200
 - 34s - loss: -3.6246e+00 - val_loss: -3.6645e+00
Epoch 69/200
 - 34s - loss: -3.6240e+00 - val_loss: -3.6635e+00
Epoch 70/200
 - 34s - loss: -3.6252e+00 - val_loss: -3.6631e+00
Epoch 71/200
 - 34s - loss: -3.6249e+00 - val_loss: -3.6630e+00
Epoch 72/200
 - 34s - loss: -3.6254e+00 - val_loss: -3.6635e+00
Epoch 73/200
 - 34s - loss: -3.6248e+00 - val_loss: -3.6628e+00
Epoch 74/200
 - 34s - loss: -3.6256e+00 - val_loss: -3.6625e+00
Epoch 75/200
 - 34s - loss: -3.6257e+00 - val_loss: -3.6627e+00
Epoch 76/200
 - 34s - loss: -3.6254e+00 - val_loss: -3.6631e+00
Epoch 77/200
 - 34s - loss: -3.6245e+00 - val_loss: -3.6637e+00
Epoch 78/200
 - 34s - loss: -3.6258e+00 - val_loss: -3.6631e+00
Epoch 79/200
 - 34s - loss: -3.6262e+00 - val_loss: -3.6633e+00
Epoch 80/200
 - 34s - loss: -3.6263e+00 - val_loss: -3.6637e+00
Epoch 81/200
 - 34s - loss: -3.6263e+00 - val_loss: -3.6638e+00
2019-12-27 13:04:02,016 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep5/ae_model_epoch_80.pickle
Epoch 82/200
 - 34s - loss: -3.6274e+00 - val_loss: -3.6629e+00
Epoch 83/200
 - 34s - loss: -3.6247e+00 - val_loss: -3.6633e+00
Epoch 84/200
 - 34s - loss: -3.6260e+00 - val_loss: -3.6634e+00
Epoch 85/200
 - 34s - loss: -3.6269e+00 - val_loss: -3.6644e+00
Epoch 86/200
 - 34s - loss: -3.6260e+00 - val_loss: -3.6624e+00
Epoch 87/200
 - 34s - loss: -3.6277e+00 - val_loss: -3.6650e+00
Epoch 88/200
 - 34s - loss: -3.6266e+00 - val_loss: -3.6643e+00
Epoch 89/200
 - 34s - loss: -3.6276e+00 - val_loss: -3.6641e+00
Epoch 90/200
 - 34s - loss: -3.6266e+00 - val_loss: -3.6643e+00
Epoch 91/200
 - 34s - loss: -3.6263e+00 - val_loss: -3.6642e+00
Epoch 92/200
 - 34s - loss: -3.6270e+00 - val_loss: -3.6646e+00
Epoch 93/200
 - 34s - loss: -3.6275e+00 - val_loss: -3.6639e+00
Epoch 94/200
 - 34s - loss: -3.6282e+00 - val_loss: -3.6632e+00
Epoch 95/200
 - 34s - loss: -3.6281e+00 - val_loss: -3.6636e+00
Epoch 96/200
 - 34s - loss: -3.6282e+00 - val_loss: -3.6645e+00
Epoch 97/200
 - 34s - loss: -3.6289e+00 - val_loss: -3.6646e+00
Epoch 98/200
 - 34s - loss: -3.6276e+00 - val_loss: -3.6646e+00
Epoch 99/200
 - 34s - loss: -3.6278e+00 - val_loss: -3.6642e+00
Epoch 100/200
 - 34s - loss: -3.6289e+00 - val_loss: -3.6653e+00
Epoch 101/200
 - 34s - loss: -3.6283e+00 - val_loss: -3.6653e+00
2019-12-27 13:15:26,056 [INFO] epoch = 100. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep5/ae_model_epoch_100.pickle
Epoch 102/200
 - 34s - loss: -3.6284e+00 - val_loss: -3.6630e+00
Epoch 103/200
 - 34s - loss: -3.6293e+00 - val_loss: -3.6645e+00
Epoch 104/200
 - 34s - loss: -3.6282e+00 - val_loss: -3.6660e+00
Epoch 105/200
 - 34s - loss: -3.6287e+00 - val_loss: -3.6655e+00
Epoch 106/200
 - 34s - loss: -3.6292e+00 - val_loss: -3.6653e+00
Epoch 107/200
 - 34s - loss: -3.6285e+00 - val_loss: -3.6650e+00
Epoch 108/200
 - 34s - loss: -3.6282e+00 - val_loss: -3.6645e+00
Epoch 109/200
 - 34s - loss: -3.6289e+00 - val_loss: -3.6652e+00
Epoch 110/200
 - 34s - loss: -3.6288e+00 - val_loss: -3.6649e+00
Epoch 111/200
 - 34s - loss: -3.6291e+00 - val_loss: -3.6646e+00
Epoch 112/200
 - 34s - loss: -3.6277e+00 - val_loss: -3.6645e+00
Epoch 113/200
 - 34s - loss: -3.6273e+00 - val_loss: -3.6655e+00
Epoch 114/200
 - 34s - loss: -3.6295e+00 - val_loss: -3.6661e+00
Epoch 115/200
 - 34s - loss: -3.6285e+00 - val_loss: -3.6654e+00
Epoch 116/200
 - 34s - loss: -3.6288e+00 - val_loss: -3.6654e+00
Epoch 117/200
 - 34s - loss: -3.6298e+00 - val_loss: -3.6645e+00
Epoch 118/200
 - 34s - loss: -3.6288e+00 - val_loss: -3.6656e+00
Epoch 119/200
 - 34s - loss: -3.6293e+00 - val_loss: -3.6663e+00
Epoch 120/200
 - 34s - loss: -3.6289e+00 - val_loss: -3.6660e+00
Epoch 121/200
 - 34s - loss: -3.6302e+00 - val_loss: -3.6660e+00
2019-12-27 13:26:50,391 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep5/ae_model_epoch_120.pickle
Epoch 122/200
 - 34s - loss: -3.6299e+00 - val_loss: -3.6653e+00
Epoch 123/200
 - 34s - loss: -3.6293e+00 - val_loss: -3.6646e+00
Epoch 124/200
 - 34s - loss: -3.6290e+00 - val_loss: -3.6659e+00
Epoch 125/200
 - 34s - loss: -3.6299e+00 - val_loss: -3.6636e+00
Epoch 126/200
 - 34s - loss: -3.6297e+00 - val_loss: -3.6657e+00
Epoch 127/200
 - 34s - loss: -3.6297e+00 - val_loss: -3.6646e+00
Epoch 128/200
 - 34s - loss: -3.6306e+00 - val_loss: -3.6631e+00
Epoch 129/200
 - 34s - loss: -3.6295e+00 - val_loss: -3.6649e+00
Epoch 130/200
 - 34s - loss: -3.6288e+00 - val_loss: -3.6636e+00
Epoch 131/200
 - 34s - loss: -3.6304e+00 - val_loss: -3.6650e+00
Epoch 132/200
 - 34s - loss: -3.6297e+00 - val_loss: -3.6643e+00
Epoch 133/200
 - 34s - loss: -3.6301e+00 - val_loss: -3.6637e+00
Epoch 134/200
 - 34s - loss: -3.6298e+00 - val_loss: -3.6657e+00
Epoch 135/200
 - 34s - loss: -3.6299e+00 - val_loss: -3.6663e+00
Epoch 136/200
 - 34s - loss: -3.6297e+00 - val_loss: -3.6664e+00
Epoch 137/200
 - 34s - loss: -3.6302e+00 - val_loss: -3.6663e+00
Epoch 138/200
 - 34s - loss: -3.6290e+00 - val_loss: -3.6659e+00
Epoch 139/200
 - 34s - loss: -3.6297e+00 - val_loss: -3.6651e+00
Epoch 140/200
 - 34s - loss: -3.6302e+00 - val_loss: -3.6654e+00
Epoch 141/200
 - 34s - loss: -3.6299e+00 - val_loss: -3.6663e+00
2019-12-27 13:38:14,578 [INFO] epoch = 140. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep5/ae_model_epoch_140.pickle
Epoch 142/200
 - 34s - loss: -3.6303e+00 - val_loss: -3.6667e+00
Epoch 143/200
 - 34s - loss: -3.6305e+00 - val_loss: -3.6663e+00
Epoch 144/200
 - 34s - loss: -3.6294e+00 - val_loss: -3.6664e+00
Epoch 145/200
 - 34s - loss: -3.6310e+00 - val_loss: -3.6663e+00
Epoch 146/200
 - 34s - loss: -3.6309e+00 - val_loss: -3.6657e+00
Epoch 147/200
 - 34s - loss: -3.6305e+00 - val_loss: -3.6660e+00
Epoch 148/200
 - 34s - loss: -3.6308e+00 - val_loss: -3.6678e+00
Epoch 149/200
 - 34s - loss: -3.6309e+00 - val_loss: -3.6659e+00
Epoch 150/200
 - 34s - loss: -3.6305e+00 - val_loss: -3.6664e+00
Epoch 151/200
 - 34s - loss: -3.6310e+00 - val_loss: -3.6666e+00
Epoch 152/200
 - 34s - loss: -3.6305e+00 - val_loss: -3.6664e+00
Epoch 153/200
 - 34s - loss: -3.6309e+00 - val_loss: -3.6668e+00
Epoch 154/200
 - 34s - loss: -3.6302e+00 - val_loss: -3.6660e+00
Epoch 155/200
 - 34s - loss: -3.6304e+00 - val_loss: -3.6664e+00
Epoch 156/200
 - 34s - loss: -3.6311e+00 - val_loss: -3.6662e+00
Epoch 157/200
 - 34s - loss: -3.6311e+00 - val_loss: -3.6663e+00
Epoch 158/200
 - 34s - loss: -3.6304e+00 - val_loss: -3.6661e+00
Epoch 159/200
 - 34s - loss: -3.6310e+00 - val_loss: -3.6664e+00
Epoch 160/200
 - 34s - loss: -3.6304e+00 - val_loss: -3.6668e+00
Epoch 161/200
 - 34s - loss: -3.6319e+00 - val_loss: -3.6673e+00
2019-12-27 13:49:38,767 [INFO] epoch = 160. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep5/ae_model_epoch_160.pickle
Epoch 162/200
 - 34s - loss: -3.6315e+00 - val_loss: -3.6666e+00
Epoch 163/200
 - 34s - loss: -3.6299e+00 - val_loss: -3.6670e+00
Epoch 164/200
 - 34s - loss: -3.6312e+00 - val_loss: -3.6673e+00
Epoch 165/200
 - 34s - loss: -3.6315e+00 - val_loss: -3.6666e+00
Epoch 166/200
 - 34s - loss: -3.6311e+00 - val_loss: -3.6674e+00
Epoch 167/200
 - 34s - loss: -3.6303e+00 - val_loss: -3.6651e+00
Epoch 168/200
 - 34s - loss: -3.6316e+00 - val_loss: -3.6667e+00
Epoch 169/200
 - 34s - loss: -3.6321e+00 - val_loss: -3.6670e+00
Epoch 170/200
 - 34s - loss: -3.6311e+00 - val_loss: -3.6663e+00
Epoch 171/200
 - 34s - loss: -3.6315e+00 - val_loss: -3.6670e+00
Epoch 172/200
 - 34s - loss: -3.6311e+00 - val_loss: -3.6668e+00
Epoch 173/200
 - 34s - loss: -3.6318e+00 - val_loss: -3.6670e+00
Epoch 174/200
 - 34s - loss: -3.6301e+00 - val_loss: -3.6669e+00
Epoch 175/200
 - 34s - loss: -3.6316e+00 - val_loss: -3.6666e+00
Epoch 176/200
 - 34s - loss: -3.6315e+00 - val_loss: -3.6669e+00
Epoch 177/200
 - 34s - loss: -3.6316e+00 - val_loss: -3.6671e+00
Epoch 178/200
 - 34s - loss: -3.6310e+00 - val_loss: -3.6670e+00
Epoch 179/200
 - 34s - loss: -3.6316e+00 - val_loss: -3.6656e+00
Epoch 180/200
 - 34s - loss: -3.6307e+00 - val_loss: -3.6658e+00
Epoch 181/200
 - 34s - loss: -3.6317e+00 - val_loss: -3.6665e+00
2019-12-27 14:01:02,576 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep5/ae_model_epoch_180.pickle
Epoch 182/200
 - 34s - loss: -3.6304e+00 - val_loss: -3.6664e+00
Epoch 183/200
 - 34s - loss: -3.6319e+00 - val_loss: -3.6664e+00
Epoch 184/200
 - 34s - loss: -3.6314e+00 - val_loss: -3.6667e+00
Epoch 185/200
 - 34s - loss: -3.6311e+00 - val_loss: -3.6666e+00
Epoch 186/200
 - 34s - loss: -3.6312e+00 - val_loss: -3.6666e+00
Epoch 187/200
 - 34s - loss: -3.6317e+00 - val_loss: -3.6651e+00
Epoch 188/200
 - 34s - loss: -3.6311e+00 - val_loss: -3.6646e+00
Epoch 189/200
 - 34s - loss: -3.6314e+00 - val_loss: -3.6655e+00
Epoch 190/200
 - 34s - loss: -3.6316e+00 - val_loss: -3.6646e+00
Epoch 191/200
 - 34s - loss: -3.6316e+00 - val_loss: -3.6664e+00
Epoch 192/200
 - 34s - loss: -3.6317e+00 - val_loss: -3.6661e+00
Epoch 193/200
 - 34s - loss: -3.6319e+00 - val_loss: -3.6667e+00
Epoch 194/200
 - 34s - loss: -3.6317e+00 - val_loss: -3.6655e+00
Epoch 195/200
 - 34s - loss: -3.6320e+00 - val_loss: -3.6661e+00
Epoch 196/200
 - 34s - loss: -3.6320e+00 - val_loss: -3.6654e+00
Epoch 197/200
 - 34s - loss: -3.6320e+00 - val_loss: -3.6668e+00
Epoch 198/200
 - 34s - loss: -3.6309e+00 - val_loss: -3.6663e+00
2019-12-27 14:10:43,713 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-27 14:12:22,565 [INFO] Last epoch loss evaluation: train_loss = -3.661068, val_loss = -3.667838
2019-12-27 14:12:22,565 [INFO] Training autoencoder complete
2019-12-27 14:12:22,565 [INFO] Encoding data for supervised training
2019-12-27 14:14:34,574 [INFO] Encoding complete
2019-12-27 14:14:34,574 [INFO] Training neural network layers (after autoencoder)
Epoch 00198: early stopping
Train on 1452347 samples, validate on 645487 samples
Epoch 1/200
 - 33s - loss: 0.0163 - val_loss: 0.0093
 - val_f1: 0.9814
Epoch 2/200
 - 30s - loss: 0.0094 - val_loss: 0.0088
 - val_f1: 0.9817
Epoch 3/200
 - 30s - loss: 0.0090 - val_loss: 0.0083
 - val_f1: 0.9808
Epoch 4/200
 - 30s - loss: 0.0088 - val_loss: 0.0082
 - val_f1: 0.9827
Epoch 5/200
 - 31s - loss: 0.0087 - val_loss: 0.0084
 - val_f1: 0.9826
Epoch 6/200
 - 31s - loss: 0.0086 - val_loss: 0.0083
 - val_f1: 0.9825
Epoch 7/200
 - 31s - loss: 0.0086 - val_loss: 0.0082
 - val_f1: 0.9824
Epoch 8/200
 - 30s - loss: 0.0086 - val_loss: 0.0082
 - val_f1: 0.9827
Epoch 9/200
 - 30s - loss: 0.0086 - val_loss: 0.0088
 - val_f1: 0.9816
Epoch 10/200
 - 31s - loss: 0.0085 - val_loss: 0.0082
 - val_f1: 0.9825
Epoch 11/200
 - 30s - loss: 0.0085 - val_loss: 0.0082
 - val_f1: 0.9826
Epoch 12/200
 - 30s - loss: 0.0085 - val_loss: 0.0082
 - val_f1: 0.9828
Epoch 13/200
 - 31s - loss: 0.0085 - val_loss: 0.0082
 - val_f1: 0.9828
Epoch 14/200
 - 30s - loss: 0.0085 - val_loss: 0.0083
 - val_f1: 0.9807
Epoch 15/200
 - 30s - loss: 0.0085 - val_loss: 0.0082
 - val_f1: 0.9826
Epoch 16/200
 - 30s - loss: 0.0084 - val_loss: 0.0081
 - val_f1: 0.9828
Epoch 17/200
 - 31s - loss: 0.0084 - val_loss: 0.0082
 - val_f1: 0.9823
Epoch 18/200
 - 30s - loss: 0.0084 - val_loss: 0.0083
 - val_f1: 0.9824
Epoch 19/200
 - 30s - loss: 0.0084 - val_loss: 0.0082
 - val_f1: 0.9827
Epoch 20/200
 - 30s - loss: 0.0084 - val_loss: 0.0081
 - val_f1: 0.9829
Epoch 21/200
 - 31s - loss: 0.0084 - val_loss: 0.0081
2019-12-27 14:38:09,774 [INFO] epoch = 20. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep5/ann_model_epoch_20.pickle
 - val_f1: 0.9812
Epoch 22/200
 - 31s - loss: 0.0084 - val_loss: 0.0082
 - val_f1: 0.9826
Epoch 23/200
 - 31s - loss: 0.0084 - val_loss: 0.0081
 - val_f1: 0.9828
Epoch 24/200
 - 31s - loss: 0.0084 - val_loss: 0.0081
 - val_f1: 0.9827
Epoch 25/200
 - 30s - loss: 0.0084 - val_loss: 0.0082
 - val_f1: 0.9829
Epoch 26/200
 - 30s - loss: 0.0084 - val_loss: 0.0081
 - val_f1: 0.9809
Epoch 27/200
 - 30s - loss: 0.0084 - val_loss: 0.0080
 - val_f1: 0.9828
Epoch 28/200
 - 30s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9812
Epoch 29/200
 - 31s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9827
Epoch 30/200
 - 30s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9830
Epoch 31/200
 - 30s - loss: 0.0083 - val_loss: 0.0082
 - val_f1: 0.9826
Epoch 32/200
 - 31s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9828
Epoch 33/200
 - 30s - loss: 0.0083 - val_loss: 0.0082
 - val_f1: 0.9823
Epoch 34/200
 - 30s - loss: 0.0083 - val_loss: 0.0083
 - val_f1: 0.9821
Epoch 35/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9828
Epoch 36/200
 - 31s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9812
Epoch 37/200
 - 30s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9812
Epoch 38/200
 - 31s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9827
Epoch 39/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9826
Epoch 40/200
 - 30s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9827
Epoch 41/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
2019-12-27 15:00:41,308 [INFO] epoch = 40. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep5/ann_model_epoch_40.pickle
 - val_f1: 0.9826
Epoch 42/200
 - 31s - loss: 0.0083 - val_loss: 0.0082
 - val_f1: 0.9826
Epoch 43/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 44/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9813
Epoch 45/200
 - 31s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9828
Epoch 46/200
 - 30s - loss: 0.0083 - val_loss: 0.0082
 - val_f1: 0.9810
Epoch 47/200
 - 30s - loss: 0.0082 - val_loss: 0.0094
 - val_f1: 0.9803
Epoch 48/200
 - 30s - loss: 0.0083 - val_loss: 0.0082
 - val_f1: 0.9806
Epoch 49/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 50/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 51/200
 - 30s - loss: 0.0083 - val_loss: 0.0082
 - val_f1: 0.9808
Epoch 52/200
 - 30s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9828
Epoch 53/200
 - 30s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9813
Epoch 54/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9827
Epoch 55/200
 - 31s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9828
Epoch 56/200
 - 31s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 57/200
 - 31s - loss: 0.0082 - val_loss: 0.0087
 - val_f1: 0.9824
Epoch 58/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 59/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 60/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 61/200
 - 30s - loss: 0.0082 - val_loss: 0.0082
2019-12-27 15:23:12,836 [INFO] epoch = 60. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep5/ann_model_epoch_60.pickle
 - val_f1: 0.9829
Epoch 62/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9828
Epoch 63/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 64/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9825
Epoch 65/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 66/200
 - 30s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9831
Epoch 67/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 68/200
 - 31s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9828
Epoch 69/200
 - 31s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 70/200
 - 31s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9827
Epoch 71/200
 - 31s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9834
Epoch 72/200
 - 30s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9828
Epoch 73/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 74/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 75/200
 - 31s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9827
Epoch 76/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 77/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 78/200
 - 31s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 79/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 80/200
 - 31s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9827
Epoch 81/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
2019-12-27 15:45:46,126 [INFO] epoch = 80. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep5/ann_model_epoch_80.pickle
 - val_f1: 0.9830
Epoch 82/200
 - 30s - loss: 0.0082 - val_loss: 0.0084
 - val_f1: 0.9826
Epoch 83/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9830
Epoch 84/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9828
Epoch 85/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9830
Epoch 86/200
 - 31s - loss: 0.0082 - val_loss: 0.0091
 - val_f1: 0.9815
Epoch 87/200
 - 31s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9829
Epoch 88/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 89/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 90/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 91/200
 - 31s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 92/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9835
Epoch 93/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 94/200
 - 31s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9813
Epoch 95/200
 - 31s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 96/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 97/200
 - 31s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 98/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 99/200
 - 31s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9817
Epoch 100/200
 - 31s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 101/200
 - 31s - loss: 0.0082 - val_loss: 0.0080
2019-12-27 16:08:17,629 [INFO] epoch = 100. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep5/ann_model_epoch_100.pickle
 - val_f1: 0.9830
Epoch 102/200
 - 31s - loss: 0.0082 - val_loss: 0.0082
 - val_f1: 0.9830
Epoch 103/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 104/200
 - 31s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 105/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 106/200
 - 30s - loss: 0.0082 - val_loss: 0.0083
 - val_f1: 0.9825
Epoch 107/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 108/200
 - 31s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 109/200
 - 31s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9830
Epoch 110/200
 - 31s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9829
Epoch 111/200
 - 31s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 112/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 113/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 114/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 115/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9835
Epoch 116/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 117/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9834
Epoch 118/200
 - 31s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 119/200
 - 31s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9828
Epoch 120/200
 - 31s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 121/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
2019-12-27 16:30:50,286 [INFO] epoch = 120. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep5/ann_model_epoch_120.pickle
 - val_f1: 0.9830
Epoch 122/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9828
Epoch 123/200
 - 30s - loss: 0.0082 - val_loss: 0.0089
 - val_f1: 0.9810
Epoch 124/200
 - 31s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 125/200
 - 31s - loss: 0.0081 - val_loss: 0.0104
 - val_f1: 0.9808
Epoch 126/200
 - 31s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9832
Epoch 127/200
 - 31s - loss: 0.0082 - val_loss: 0.0093
 - val_f1: 0.9812
Epoch 128/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9830
Epoch 129/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9830
Epoch 130/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9832
Epoch 131/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9833
Epoch 132/200
 - 31s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 133/200
 - 31s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 134/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9833
Epoch 135/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 136/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 137/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9829
Epoch 138/200
 - 31s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 139/200
 - 31s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9830
Epoch 140/200
 - 31s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9833
Epoch 141/200
 - 30s - loss: 0.0081 - val_loss: 0.0080
2019-12-27 16:53:23,961 [INFO] epoch = 140. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep5/ann_model_epoch_140.pickle
 - val_f1: 0.9813
Epoch 142/200
 - 30s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 143/200
 - 30s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9811
Epoch 144/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 145/200
 - 31s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9815
Epoch 146/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9829
Epoch 147/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 148/200
 - 31s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 149/200
 - 31s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9829
Epoch 150/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9810
Epoch 151/200
 - 30s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 152/200
 - 31s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 153/200
 - 30s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9830
Epoch 154/200
 - 30s - loss: 0.0081 - val_loss: 0.0082
 - val_f1: 0.9831
Epoch 155/200
 - 31s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9830
Epoch 156/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9829
Epoch 157/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9834
Epoch 158/200
 - 31s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9833
Epoch 159/200
 - 31s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 160/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9833
Epoch 161/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
2019-12-27 17:15:55,965 [INFO] epoch = 160. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep5/ann_model_epoch_160.pickle
 - val_f1: 0.9830
Epoch 162/200
 - 31s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9813
Epoch 163/200
 - 31s - loss: 0.0081 - val_loss: 0.0081
 - val_f1: 0.9830
Epoch 164/200
 - 31s - loss: 0.0081 - val_loss: 0.0081
 - val_f1: 0.9834
Epoch 165/200
 - 31s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9829
Epoch 166/200
 - 31s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9834
Epoch 167/200
 - 30s - loss: 0.0081 - val_loss: 0.0081
 - val_f1: 0.9806
Epoch 168/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 169/200
 - 30s - loss: 0.0081 - val_loss: 0.0081
 - val_f1: 0.9831
Epoch 170/200
 - 30s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9814
Epoch 171/200
 - 31s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9829
Epoch 172/200
 - 31s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 173/200
 - 30s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 174/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 175/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 176/200
 - 31s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9834
Epoch 177/200
 - 30s - loss: 0.0081 - val_loss: 0.0094
 - val_f1: 0.9807
Epoch 178/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 179/200
 - 31s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9829
Epoch 180/200
 - 31s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9830
Epoch 181/200
 - 30s - loss: 0.0081 - val_loss: 0.0080
2019-12-27 17:38:28,476 [INFO] epoch = 180. Intermediate model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep5/ann_model_epoch_180.pickle
 - val_f1: 0.9829
Epoch 182/200
 - 31s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9830
Epoch 183/200
 - 30s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9829
Epoch 184/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9830
Epoch 185/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9835
Epoch 186/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 187/200
 - 30s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9812
Epoch 188/200
 - 31s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9813
Epoch 189/200
 - 31s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9830
Epoch 190/200
 - 31s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 191/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 192/200
 - 31s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 193/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 194/200
 - 31s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9833
Epoch 195/200
 - 31s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9832
Epoch 196/200
 - 30s - loss: 0.0081 - val_loss: 0.0082
 - val_f1: 0.9831
Epoch 197/200
 - 31s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 198/200
 - 30s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9831
Epoch 199/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9831
Epoch 200/200
 - 30s - loss: 0.0081 - val_loss: 0.0079
2019-12-27 18:00:30,048 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-27 18:02:38,608 [INFO] Last epoch loss evaluation: train_loss = 0.007866, val_loss = 0.007886
2019-12-27 18:02:38,618 [INFO] Training complete. time_to_train = 20755.00 sec, 345.92 min
2019-12-27 18:02:38,656 [INFO] Model saved to results_selected_models/selected_ids18_subset_ae_ann_deep_rep5/best_model.pickle
/home/sunanda/research/ids_experiments/utility.py:268: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
2019-12-27 18:02:38,842 [INFO] Plot saved to: results_selected_models/selected_ids18_subset_ae_ann_deep_rep5/training_error_history.png
/home/sunanda/research/ids_experiments/utility.py:285: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
2019-12-27 18:02:39,037 [INFO] Plot saved to: results_selected_models/selected_ids18_subset_ae_ann_deep_rep5/training_f1_history.png
2019-12-27 18:02:39,037 [INFO] Making predictions on training, validation, testing data
2019-12-27 18:08:54,430 [INFO] Evaluating predictions (results)
/home/sunanda/anaconda3/envs/ml_env_cpu/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2019-12-27 18:09:17,598 [INFO] Dataset: Testing. Classification report below
2019-12-27 18:09:17,598 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99    535650
                     Bot       1.00      1.00      1.00     11465
        Brute Force -Web       0.00      0.00      0.00        24
        Brute Force -XSS       1.00      0.33      0.50         9
        DDOS attack-HOIC       1.00      1.00      1.00     27447
    DDOS attack-LOIC-UDP       0.72      0.99      0.83        67
  DDoS attacks-LOIC-HTTP       0.99      0.99      0.99     23010
   DoS attacks-GoldenEye       0.99      0.99      0.99      1651
        DoS attacks-Hulk       1.00      1.00      1.00     18478
DoS attacks-SlowHTTPTest       0.73      0.45      0.56      5596
   DoS attacks-Slowloris       0.95      0.97      0.96       440
          FTP-BruteForce       0.69      0.88      0.77      7718
           Infilteration       0.45      0.00      0.01      6404
           SQL Injection       0.00      0.00      0.00         4
          SSH-Bruteforce       1.00      1.00      1.00      7525

                accuracy                           0.98    645488
               macro avg       0.77      0.71      0.71    645488
            weighted avg       0.98      0.98      0.98    645488

2019-12-27 18:09:17,598 [INFO] Overall accuracy (micro avg): 0.9831212973750093
/home/sunanda/research/ids_experiments/utility.py:307: RuntimeWarning: invalid value encountered in true_divide
  PPV = TP / (TP + FP)    # Precision or positive predictive value
/home/sunanda/research/ids_experiments/utility.py:311: RuntimeWarning: invalid value encountered in true_divide
  FDR = FP / (TP + FP)    # False discovery rate
/home/sunanda/research/ids_experiments/utility.py:313: RuntimeWarning: invalid value encountered in true_divide
  F1 = (2 * PPV * TPR) / (PPV + TPR)  # F1 score
2019-12-27 18:09:42,529 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9831         0.9831                       0.9831                0.0012                   0.0169  0.9831
1     Macro avg        0.9977         0.7671                       0.7069                0.0045                   0.2931  0.7071
2  Weighted avg        0.9909         0.9779                       0.9831                0.0502                   0.0169  0.9779
2019-12-27 18:10:05,629 [INFO] Dataset: Validation. Classification report below
2019-12-27 18:10:05,629 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99    535650
                     Bot       1.00      1.00      1.00     11465
        Brute Force -Web       0.00      0.00      0.00        25
        Brute Force -XSS       1.00      0.67      0.80         9
        DDOS attack-HOIC       1.00      1.00      1.00     27447
    DDOS attack-LOIC-UDP       0.79      0.97      0.87        68
  DDoS attacks-LOIC-HTTP       0.99      0.99      0.99     23009
   DoS attacks-GoldenEye       0.99      0.99      0.99      1651
        DoS attacks-Hulk       1.00      1.00      1.00     18478
DoS attacks-SlowHTTPTest       0.74      0.45      0.56      5596
   DoS attacks-Slowloris       0.94      0.97      0.96       439
          FTP-BruteForce       0.69      0.89      0.78      7718
           Infilteration       0.37      0.00      0.01      6403
           SQL Injection       0.00      0.00      0.00         4
          SSH-Bruteforce       1.00      1.00      1.00      7525

                accuracy                           0.98    645487
               macro avg       0.77      0.73      0.73    645487
            weighted avg       0.98      0.98      0.98    645487

2019-12-27 18:10:05,629 [INFO] Overall accuracy (micro avg): 0.9831723954161742
2019-12-27 18:10:30,512 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9832         0.9832                       0.9832                0.0012                   0.0168  0.9832
1     Macro avg        0.9978         0.7664                       0.7288                0.0045                   0.2712  0.7297
2  Weighted avg        0.9909         0.9772                       0.9832                0.0502                   0.0168  0.9779
2019-12-27 18:11:46,261 [INFO] Dataset: Training. Classification report below
2019-12-27 18:11:46,261 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99   1606949
                     Bot       1.00      1.00      1.00     34396
        Brute Force -Web       0.00      0.00      0.00        73
        Brute Force -XSS       1.00      0.50      0.67        26
        DDOS attack-HOIC       1.00      1.00      1.00     82341
    DDOS attack-LOIC-UDP       0.73      0.97      0.83       203
  DDoS attacks-LOIC-HTTP       0.99      0.99      0.99     69029
   DoS attacks-GoldenEye       0.99      0.99      0.99      4954
        DoS attacks-Hulk       1.00      1.00      1.00     55435
DoS attacks-SlowHTTPTest       0.74      0.45      0.56     16787
   DoS attacks-Slowloris       0.96      0.99      0.97      1318
          FTP-BruteForce       0.69      0.88      0.77     23153
           Infilteration       0.46      0.00      0.01     19210
           SQL Injection       0.00      0.00      0.00        12
          SSH-Bruteforce       1.00      1.00      1.00     22576

                accuracy                           0.98   1936462
               macro avg       0.77      0.72      0.72   1936462
            weighted avg       0.98      0.98      0.98   1936462

2019-12-27 18:11:46,261 [INFO] Overall accuracy (micro avg): 0.983176018945892
2019-12-27 18:13:07,896 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9832         0.9832                       0.9832                0.0012                   0.0168  0.9832
1     Macro avg        0.9978         0.7697                       0.7183                0.0045                   0.2817  0.7195
2  Weighted avg        0.9909         0.9781                       0.9832                0.0501                   0.0168  0.9780
2019-12-27 18:13:07,969 [INFO] Results saved to: results_selected_models/selected_ids18_subset_ae_ann_deep_rep5/selected_ids18_subset_ae_ann_deep_rep5_results.xlsx
2019-12-27 18:13:07,974 [INFO] ================= Finished running experiment no. 5 ================= 

2019-12-27 18:13:08,023 [INFO] ================= Finished running 25 experiments ================= 

 - val_f1: 0.9830
