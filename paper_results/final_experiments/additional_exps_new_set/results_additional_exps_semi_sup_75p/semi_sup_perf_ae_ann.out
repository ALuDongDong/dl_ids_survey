Using TensorFlow backend.
/home/sunanda/anaconda3/envs/ml_env_cpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/sunanda/anaconda3/envs/ml_env_cpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/sunanda/anaconda3/envs/ml_env_cpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/sunanda/anaconda3/envs/ml_env_cpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/sunanda/anaconda3/envs/ml_env_cpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/sunanda/anaconda3/envs/ml_env_cpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
2020-01-07 11:13:42,163 [INFO] Read 6 experiments from file: experiment_specs/additional_exps/semi_sup_perf_ae_ann.csv
2020-01-07 11:13:42,163 [INFO] ================= Started running experiments ================= 

2020-01-07 11:13:42,163 [INFO] Created directory: results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1
2020-01-07 11:13:42,163 [INFO] Initialized logging. log_filename = results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/run_log.log
2020-01-07 11:13:42,164 [INFO] ================= Running experiment no. 1  ================= 

2020-01-07 11:13:42,164 [INFO] Experiment parameters given below
2020-01-07 11:13:42,164 [INFO] 
{'experiment_num': 1, 'results_dir': 'results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1', 'model_type': 'classifier', 'model': 'ae_ann', 'normal_label': 'normal', 'split_random_seed': 42, 'unsupervised_ratio': 0.75, 'ae_encoder_units': [64], 'ae_encoder_activations': ['relu'], 'ae_encoder_dropout_rates': [0.2], 'ae_encoder_l1_param': -1, 'ae_decoder_units': '', 'ae_decoder_activations': '', 'ae_decoder_dropout_rates': '', 'output_activation': 'sigmoid', 'loss_function': 'binary_crossentropy', 'ae_epochs': 200, 'ann_layer_units': [64], 'ann_layer_activations': ['relu'], 'ann_layer_dropout_rates': [0.2], 'ann_epochs': 200, 'early_stop_patience': 50, 'batch_size': 256, 'dataset_dir': '../Datasets/small_datasets/nsl_kdd_five_classes', 'description': 'semi_sup_perf_nsl_ae_ann_rep1'}
2020-01-07 11:13:42,164 [INFO] Created tensorboard log directory: results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/tf_logs_run_2020_01_07-11_13_42
2020-01-07 11:13:42,164 [INFO] Loading datsets from: ../Datasets/small_datasets/nsl_kdd_five_classes
2020-01-07 11:13:42,165 [INFO] Reading X, y files
2020-01-07 11:13:42,165 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_train.h5
2020-01-07 11:13:42,173 [INFO] NumExpr defaulting to 8 threads.
2020-01-07 11:13:42,412 [INFO] Reading complete. time_to_read=0.25 seconds
2020-01-07 11:13:42,412 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_val.h5
2020-01-07 11:13:42,473 [INFO] Reading complete. time_to_read=0.06 seconds
2020-01-07 11:13:42,473 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_test.h5
2020-01-07 11:13:42,530 [INFO] Reading complete. time_to_read=0.06 seconds
2020-01-07 11:13:42,530 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_train.h5
2020-01-07 11:13:42,538 [INFO] Reading complete. time_to_read=0.01 seconds
2020-01-07 11:13:42,539 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_val.h5
2020-01-07 11:13:42,543 [INFO] Reading complete. time_to_read=0.00 seconds
2020-01-07 11:13:42,543 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_test.h5
2020-01-07 11:13:42,547 [INFO] Reading complete. time_to_read=0.00 seconds
2020-01-07 11:13:42,662 [INFO] Initializing model
WARNING:tensorflow:From /home/sunanda/anaconda3/envs/ml_env_cpu/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
2020-01-07 11:13:42,677 [WARNING] From /home/sunanda/anaconda3/envs/ml_env_cpu/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From /home/sunanda/anaconda3/envs/ml_env_cpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
2020-01-07 11:13:42,749 [WARNING] From /home/sunanda/anaconda3/envs/ml_env_cpu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
2020-01-07 11:13:42,795 [INFO] _________________________________________________________________
2020-01-07 11:13:42,795 [INFO] Layer (type)                 Output Shape              Param #   
2020-01-07 11:13:42,795 [INFO] =================================================================
2020-01-07 11:13:42,795 [INFO] dense_1 (Dense)              (None, 64)                7872      
2020-01-07 11:13:42,795 [INFO] _________________________________________________________________
2020-01-07 11:13:42,795 [INFO] batch_normalization_1 (Batch (None, 64)                256       
2020-01-07 11:13:42,795 [INFO] _________________________________________________________________
2020-01-07 11:13:42,795 [INFO] dropout_1 (Dropout)          (None, 64)                0         
2020-01-07 11:13:42,795 [INFO] _________________________________________________________________
2020-01-07 11:13:42,795 [INFO] dense_2 (Dense)              (None, 122)               7930      
2020-01-07 11:13:42,795 [INFO] =================================================================
2020-01-07 11:13:42,796 [INFO] Total params: 16,058
2020-01-07 11:13:42,796 [INFO] Trainable params: 15,930
2020-01-07 11:13:42,796 [INFO] Non-trainable params: 128
2020-01-07 11:13:42,796 [INFO] _________________________________________________________________
2020-01-07 11:13:42,914 [INFO] _________________________________________________________________
2020-01-07 11:13:42,914 [INFO] Layer (type)                 Output Shape              Param #   
2020-01-07 11:13:42,914 [INFO] =================================================================
2020-01-07 11:13:42,915 [INFO] dense_3 (Dense)              (None, 64)                4160      
2020-01-07 11:13:42,915 [INFO] _________________________________________________________________
2020-01-07 11:13:42,915 [INFO] batch_normalization_2 (Batch (None, 64)                256       
2020-01-07 11:13:42,915 [INFO] _________________________________________________________________
2020-01-07 11:13:42,915 [INFO] dropout_2 (Dropout)          (None, 64)                0         
2020-01-07 11:13:42,915 [INFO] _________________________________________________________________
2020-01-07 11:13:42,915 [INFO] dense_4 (Dense)              (None, 5)                 325       
2020-01-07 11:13:42,915 [INFO] =================================================================
2020-01-07 11:13:42,915 [INFO] Total params: 4,741
2020-01-07 11:13:42,915 [INFO] Trainable params: 4,613
2020-01-07 11:13:42,916 [INFO] Non-trainable params: 128
2020-01-07 11:13:42,916 [INFO] _________________________________________________________________
2020-01-07 11:13:42,916 [INFO] Training model
2020-01-07 11:13:42,916 [INFO] Splitting train set into 2 sets (unsupervised, supervised), random_seed = 42
OMP: Info #212: KMP_AFFINITY: decoding x2APIC ids.
OMP: Info #210: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info
OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0-7
OMP: Info #156: KMP_AFFINITY: 8 available OS procs
OMP: Info #157: KMP_AFFINITY: Uniform topology
OMP: Info #179: KMP_AFFINITY: 1 packages x 4 cores/pkg x 2 threads/core (4 total cores)
OMP: Info #214: KMP_AFFINITY: OS proc to physical thread map:
OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 4 maps to package 0 core 0 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 5 maps to package 0 core 1 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 2 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 6 maps to package 0 core 2 thread 1 
OMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 3 thread 0 
OMP: Info #171: KMP_AFFINITY: OS proc 7 maps to package 0 core 3 thread 1 
OMP: Info #250: KMP_AFFINITY: pid 28928 tid 28928 thread 0 bound to OS proc set 0
OMP: Info #250: KMP_AFFINITY: pid 28928 tid 28940 thread 1 bound to OS proc set 1
OMP: Info #250: KMP_AFFINITY: pid 28928 tid 28941 thread 2 bound to OS proc set 2
OMP: Info #250: KMP_AFFINITY: pid 28928 tid 28942 thread 3 bound to OS proc set 3
2020-01-07 11:13:43,985 [INFO] Split sizes (instances). total = 100778, unsupervised = 75583, supervised = 25195, unsupervised dataset hash = ae4213cf8e706d08135096bdf69e103f9b07b1e6
2020-01-07 11:13:43,985 [INFO] Training autoencoder
WARNING:tensorflow:From /home/sunanda/anaconda3/envs/ml_env_cpu/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
2020-01-07 11:13:44,389 [WARNING] From /home/sunanda/anaconda3/envs/ml_env_cpu/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
2020-01-07 11:13:44.701180: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX
2020-01-07 11:13:44.703880: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3701985000 Hz
2020-01-07 11:13:44.703974: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5591db224e10 executing computations on platform Host. Devices:
2020-01-07 11:13:44.703988: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2020-01-07 11:13:44.704049: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
OMP: Info #250: KMP_AFFINITY: pid 28928 tid 28950 thread 4 bound to OS proc set 4
OMP: Info #250: KMP_AFFINITY: pid 28928 tid 28968 thread 5 bound to OS proc set 5
OMP: Info #250: KMP_AFFINITY: pid 28928 tid 28971 thread 8 bound to OS proc set 0
OMP: Info #250: KMP_AFFINITY: pid 28928 tid 28970 thread 7 bound to OS proc set 7
OMP: Info #250: KMP_AFFINITY: pid 28928 tid 28969 thread 6 bound to OS proc set 6
OMP: Info #250: KMP_AFFINITY: pid 28928 tid 28949 thread 9 bound to OS proc set 1
OMP: Info #250: KMP_AFFINITY: pid 28928 tid 28973 thread 11 bound to OS proc set 3
OMP: Info #250: KMP_AFFINITY: pid 28928 tid 28974 thread 12 bound to OS proc set 4
OMP: Info #250: KMP_AFFINITY: pid 28928 tid 28972 thread 10 bound to OS proc set 2
OMP: Info #250: KMP_AFFINITY: pid 28928 tid 28976 thread 14 bound to OS proc set 6
OMP: Info #250: KMP_AFFINITY: pid 28928 tid 28975 thread 13 bound to OS proc set 5
OMP: Info #250: KMP_AFFINITY: pid 28928 tid 28977 thread 15 bound to OS proc set 7
OMP: Info #250: KMP_AFFINITY: pid 28928 tid 28978 thread 16 bound to OS proc set 0
Train on 75583 samples, validate on 25195 samples
Epoch 1/200
 - 2s - loss: -3.7831e-01 - val_loss: -1.6385e+00
Epoch 2/200
 - 1s - loss: -2.2156e+00 - val_loss: -2.7615e+00
Epoch 3/200
 - 1s - loss: -2.8932e+00 - val_loss: -3.0824e+00
Epoch 4/200
 - 1s - loss: -3.0725e+00 - val_loss: -3.1615e+00
Epoch 5/200
 - 1s - loss: -3.1337e+00 - val_loss: -3.1906e+00
Epoch 6/200
 - 1s - loss: -3.1620e+00 - val_loss: -3.2054e+00
Epoch 7/200
 - 1s - loss: -3.1775e+00 - val_loss: -3.2139e+00
Epoch 8/200
 - 1s - loss: -3.1897e+00 - val_loss: -3.2190e+00
Epoch 9/200
 - 1s - loss: -3.1970e+00 - val_loss: -3.2233e+00
Epoch 10/200
 - 1s - loss: -3.2030e+00 - val_loss: -3.2261e+00
Epoch 11/200
 - 1s - loss: -3.2076e+00 - val_loss: -3.2278e+00
Epoch 12/200
 - 1s - loss: -3.2117e+00 - val_loss: -3.2297e+00
Epoch 13/200
 - 1s - loss: -3.2151e+00 - val_loss: -3.2317e+00
Epoch 14/200
 - 1s - loss: -3.2175e+00 - val_loss: -3.2326e+00
Epoch 15/200
 - 1s - loss: -3.2210e+00 - val_loss: -3.2340e+00
Epoch 16/200
 - 1s - loss: -3.2232e+00 - val_loss: -3.2351e+00
Epoch 17/200
 - 1s - loss: -3.2247e+00 - val_loss: -3.2363e+00
Epoch 18/200
 - 1s - loss: -3.2258e+00 - val_loss: -3.2366e+00
Epoch 19/200
 - 1s - loss: -3.2279e+00 - val_loss: -3.2375e+00
Epoch 20/200
 - 1s - loss: -3.2291e+00 - val_loss: -3.2382e+00
Epoch 21/200
 - 1s - loss: -3.2307e+00 - val_loss: -3.2382e+00
2020-01-07 11:14:14,263 [INFO] epoch = 20. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/ae_model_epoch_20.pickle
Epoch 22/200
 - 1s - loss: -3.2315e+00 - val_loss: -3.2389e+00
Epoch 23/200
 - 1s - loss: -3.2329e+00 - val_loss: -3.2391e+00
Epoch 24/200
 - 1s - loss: -3.2338e+00 - val_loss: -3.2398e+00
Epoch 25/200
 - 1s - loss: -3.2351e+00 - val_loss: -3.2394e+00
Epoch 26/200
 - 1s - loss: -3.2360e+00 - val_loss: -3.2405e+00
Epoch 27/200
 - 1s - loss: -3.2364e+00 - val_loss: -3.2404e+00
Epoch 28/200
 - 1s - loss: -3.2369e+00 - val_loss: -3.2408e+00
Epoch 29/200
 - 1s - loss: -3.2379e+00 - val_loss: -3.2408e+00
Epoch 30/200
 - 1s - loss: -3.2382e+00 - val_loss: -3.2410e+00
Epoch 31/200
 - 1s - loss: -3.2385e+00 - val_loss: -3.2413e+00
Epoch 32/200
 - 1s - loss: -3.2398e+00 - val_loss: -3.2414e+00
Epoch 33/200
 - 1s - loss: -3.2402e+00 - val_loss: -3.2415e+00
Epoch 34/200
 - 1s - loss: -3.2408e+00 - val_loss: -3.2418e+00
Epoch 35/200
 - 1s - loss: -3.2412e+00 - val_loss: -3.2420e+00
Epoch 36/200
 - 1s - loss: -3.2419e+00 - val_loss: -3.2423e+00
Epoch 37/200
 - 1s - loss: -3.2422e+00 - val_loss: -3.2428e+00
Epoch 38/200
 - 1s - loss: -3.2428e+00 - val_loss: -3.2428e+00
Epoch 39/200
 - 1s - loss: -3.2430e+00 - val_loss: -3.2431e+00
Epoch 40/200
 - 1s - loss: -3.2434e+00 - val_loss: -3.2434e+00
Epoch 41/200
 - 1s - loss: -3.2439e+00 - val_loss: -3.2432e+00
2020-01-07 11:14:41,498 [INFO] epoch = 40. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/ae_model_epoch_40.pickle
Epoch 42/200
 - 1s - loss: -3.2440e+00 - val_loss: -3.2434e+00
Epoch 43/200
 - 1s - loss: -3.2445e+00 - val_loss: -3.2434e+00
Epoch 44/200
 - 1s - loss: -3.2450e+00 - val_loss: -3.2433e+00
Epoch 45/200
 - 1s - loss: -3.2444e+00 - val_loss: -3.2440e+00
Epoch 46/200
 - 1s - loss: -3.2444e+00 - val_loss: -3.2435e+00
Epoch 47/200
 - 1s - loss: -3.2456e+00 - val_loss: -3.2441e+00
Epoch 48/200
 - 1s - loss: -3.2457e+00 - val_loss: -3.2439e+00
Epoch 49/200
 - 1s - loss: -3.2459e+00 - val_loss: -3.2440e+00
Epoch 50/200
 - 1s - loss: -3.2461e+00 - val_loss: -3.2443e+00
Epoch 51/200
 - 1s - loss: -3.2462e+00 - val_loss: -3.2440e+00
Epoch 52/200
 - 1s - loss: -3.2460e+00 - val_loss: -3.2445e+00
Epoch 53/200
 - 1s - loss: -3.2459e+00 - val_loss: -3.2442e+00
Epoch 54/200
 - 1s - loss: -3.2469e+00 - val_loss: -3.2447e+00
Epoch 55/200
 - 1s - loss: -3.2473e+00 - val_loss: -3.2446e+00
Epoch 56/200
 - 1s - loss: -3.2475e+00 - val_loss: -3.2439e+00
Epoch 57/200
 - 1s - loss: -3.2476e+00 - val_loss: -3.2449e+00
Epoch 58/200
 - 1s - loss: -3.2477e+00 - val_loss: -3.2444e+00
Epoch 59/200
 - 1s - loss: -3.2476e+00 - val_loss: -3.2448e+00
Epoch 60/200
 - 1s - loss: -3.2464e+00 - val_loss: -3.2446e+00
Epoch 61/200
 - 1s - loss: -3.2479e+00 - val_loss: -3.2449e+00
2020-01-07 11:15:08,699 [INFO] epoch = 60. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/ae_model_epoch_60.pickle
Epoch 62/200
 - 1s - loss: -3.2482e+00 - val_loss: -3.2449e+00
Epoch 63/200
 - 1s - loss: -3.2484e+00 - val_loss: -3.2449e+00
Epoch 64/200
 - 1s - loss: -3.2488e+00 - val_loss: -3.2449e+00
Epoch 65/200
 - 1s - loss: -3.2486e+00 - val_loss: -3.2452e+00
Epoch 66/200
 - 1s - loss: -3.2490e+00 - val_loss: -3.2449e+00
Epoch 67/200
 - 1s - loss: -3.2487e+00 - val_loss: -3.2451e+00
Epoch 68/200
 - 1s - loss: -3.2489e+00 - val_loss: -3.2452e+00
Epoch 69/200
 - 1s - loss: -3.2490e+00 - val_loss: -3.2452e+00
Epoch 70/200
 - 1s - loss: -3.2488e+00 - val_loss: -3.2449e+00
Epoch 71/200
 - 1s - loss: -3.2492e+00 - val_loss: -3.2454e+00
Epoch 72/200
 - 1s - loss: -3.2494e+00 - val_loss: -3.2454e+00
Epoch 73/200
 - 1s - loss: -3.2494e+00 - val_loss: -3.2456e+00
Epoch 74/200
 - 1s - loss: -3.2493e+00 - val_loss: -3.2456e+00
Epoch 75/200
 - 1s - loss: -3.2496e+00 - val_loss: -3.2456e+00
Epoch 76/200
 - 1s - loss: -3.2498e+00 - val_loss: -3.2454e+00
Epoch 77/200
 - 1s - loss: -3.2500e+00 - val_loss: -3.2456e+00
Epoch 78/200
 - 1s - loss: -3.2502e+00 - val_loss: -3.2455e+00
Epoch 79/200
 - 1s - loss: -3.2499e+00 - val_loss: -3.2455e+00
Epoch 80/200
 - 1s - loss: -3.2498e+00 - val_loss: -3.2454e+00
Epoch 81/200
 - 1s - loss: -3.2494e+00 - val_loss: -3.2452e+00
2020-01-07 11:15:35,900 [INFO] epoch = 80. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/ae_model_epoch_80.pickle
Epoch 82/200
 - 1s - loss: -3.2503e+00 - val_loss: -3.2457e+00
Epoch 83/200
 - 1s - loss: -3.2503e+00 - val_loss: -3.2459e+00
Epoch 84/200
 - 1s - loss: -3.2505e+00 - val_loss: -3.2460e+00
Epoch 85/200
 - 1s - loss: -3.2505e+00 - val_loss: -3.2461e+00
Epoch 86/200
 - 1s - loss: -3.2506e+00 - val_loss: -3.2460e+00
Epoch 87/200
 - 1s - loss: -3.2500e+00 - val_loss: -3.2460e+00
Epoch 88/200
 - 1s - loss: -3.2512e+00 - val_loss: -3.2462e+00
Epoch 89/200
 - 1s - loss: -3.2510e+00 - val_loss: -3.2462e+00
Epoch 90/200
 - 1s - loss: -3.2510e+00 - val_loss: -3.2462e+00
Epoch 91/200
 - 1s - loss: -3.2508e+00 - val_loss: -3.2464e+00
Epoch 92/200
 - 1s - loss: -3.2507e+00 - val_loss: -3.2463e+00
Epoch 93/200
 - 1s - loss: -3.2511e+00 - val_loss: -3.2463e+00
Epoch 94/200
 - 1s - loss: -3.2510e+00 - val_loss: -3.2465e+00
Epoch 95/200
 - 1s - loss: -3.2516e+00 - val_loss: -3.2463e+00
Epoch 96/200
 - 1s - loss: -3.2507e+00 - val_loss: -3.2464e+00
Epoch 97/200
 - 1s - loss: -3.2509e+00 - val_loss: -3.2463e+00
Epoch 98/200
 - 1s - loss: -3.2515e+00 - val_loss: -3.2465e+00
Epoch 99/200
 - 1s - loss: -3.2514e+00 - val_loss: -3.2466e+00
Epoch 100/200
 - 1s - loss: -3.2513e+00 - val_loss: -3.2465e+00
Epoch 101/200
 - 1s - loss: -3.2514e+00 - val_loss: -3.2460e+00
2020-01-07 11:16:03,093 [INFO] epoch = 100. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/ae_model_epoch_100.pickle
Epoch 102/200
 - 1s - loss: -3.2515e+00 - val_loss: -3.2463e+00
Epoch 103/200
 - 1s - loss: -3.2515e+00 - val_loss: -3.2462e+00
Epoch 104/200
 - 1s - loss: -3.2520e+00 - val_loss: -3.2467e+00
Epoch 105/200
 - 1s - loss: -3.2517e+00 - val_loss: -3.2467e+00
Epoch 106/200
 - 1s - loss: -3.2518e+00 - val_loss: -3.2465e+00
Epoch 107/200
 - 1s - loss: -3.2519e+00 - val_loss: -3.2464e+00
Epoch 108/200
 - 1s - loss: -3.2521e+00 - val_loss: -3.2467e+00
Epoch 109/200
 - 1s - loss: -3.2519e+00 - val_loss: -3.2463e+00
Epoch 110/200
 - 1s - loss: -3.2523e+00 - val_loss: -3.2464e+00
Epoch 111/200
 - 1s - loss: -3.2523e+00 - val_loss: -3.2463e+00
Epoch 112/200
 - 1s - loss: -3.2509e+00 - val_loss: -3.2464e+00
Epoch 113/200
 - 1s - loss: -3.2516e+00 - val_loss: -3.2464e+00
Epoch 114/200
 - 1s - loss: -3.2518e+00 - val_loss: -3.2467e+00
Epoch 115/200
 - 1s - loss: -3.2520e+00 - val_loss: -3.2466e+00
Epoch 116/200
 - 1s - loss: -3.2522e+00 - val_loss: -3.2466e+00
Epoch 117/200
 - 1s - loss: -3.2525e+00 - val_loss: -3.2465e+00
Epoch 118/200
 - 1s - loss: -3.2522e+00 - val_loss: -3.2465e+00
Epoch 119/200
 - 1s - loss: -3.2524e+00 - val_loss: -3.2467e+00
Epoch 120/200
 - 1s - loss: -3.2525e+00 - val_loss: -3.2465e+00
Epoch 121/200
 - 1s - loss: -3.2527e+00 - val_loss: -3.2468e+00
2020-01-07 11:16:30,288 [INFO] epoch = 120. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/ae_model_epoch_120.pickle
Epoch 122/200
 - 1s - loss: -3.2522e+00 - val_loss: -3.2467e+00
Epoch 123/200
 - 1s - loss: -3.2524e+00 - val_loss: -3.2464e+00
Epoch 124/200
 - 1s - loss: -3.2527e+00 - val_loss: -3.2465e+00
Epoch 125/200
 - 1s - loss: -3.2524e+00 - val_loss: -3.2466e+00
Epoch 126/200
 - 1s - loss: -3.2524e+00 - val_loss: -3.2469e+00
Epoch 127/200
 - 1s - loss: -3.2525e+00 - val_loss: -3.2467e+00
Epoch 128/200
 - 1s - loss: -3.2528e+00 - val_loss: -3.2467e+00
Epoch 129/200
 - 1s - loss: -3.2527e+00 - val_loss: -3.2469e+00
Epoch 130/200
 - 1s - loss: -3.2528e+00 - val_loss: -3.2468e+00
Epoch 131/200
 - 1s - loss: -3.2525e+00 - val_loss: -3.2465e+00
Epoch 132/200
 - 1s - loss: -3.2527e+00 - val_loss: -3.2468e+00
Epoch 133/200
 - 1s - loss: -3.2531e+00 - val_loss: -3.2469e+00
Epoch 134/200
 - 1s - loss: -3.2529e+00 - val_loss: -3.2470e+00
Epoch 135/200
 - 1s - loss: -3.2530e+00 - val_loss: -3.2469e+00
Epoch 136/200
 - 1s - loss: -3.2528e+00 - val_loss: -3.2469e+00
Epoch 137/200
 - 1s - loss: -3.2528e+00 - val_loss: -3.2469e+00
Epoch 138/200
 - 1s - loss: -3.2531e+00 - val_loss: -3.2469e+00
Epoch 139/200
 - 1s - loss: -3.2528e+00 - val_loss: -3.2470e+00
Epoch 140/200
 - 1s - loss: -3.2533e+00 - val_loss: -3.2471e+00
Epoch 141/200
 - 1s - loss: -3.2530e+00 - val_loss: -3.2470e+00
2020-01-07 11:16:57,513 [INFO] epoch = 140. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/ae_model_epoch_140.pickle
Epoch 142/200
 - 1s - loss: -3.2524e+00 - val_loss: -3.2467e+00
Epoch 143/200
 - 1s - loss: -3.2531e+00 - val_loss: -3.2472e+00
Epoch 144/200
 - 1s - loss: -3.2533e+00 - val_loss: -3.2471e+00
Epoch 145/200
 - 1s - loss: -3.2536e+00 - val_loss: -3.2470e+00
Epoch 146/200
 - 1s - loss: -3.2535e+00 - val_loss: -3.2471e+00
Epoch 147/200
 - 1s - loss: -3.2533e+00 - val_loss: -3.2470e+00
Epoch 148/200
 - 1s - loss: -3.2534e+00 - val_loss: -3.2472e+00
Epoch 149/200
 - 1s - loss: -3.2532e+00 - val_loss: -3.2470e+00
Epoch 150/200
 - 1s - loss: -3.2533e+00 - val_loss: -3.2469e+00
Epoch 151/200
 - 1s - loss: -3.2534e+00 - val_loss: -3.2472e+00
Epoch 152/200
 - 1s - loss: -3.2535e+00 - val_loss: -3.2471e+00
Epoch 153/200
 - 1s - loss: -3.2536e+00 - val_loss: -3.2472e+00
Epoch 154/200
 - 1s - loss: -3.2532e+00 - val_loss: -3.2471e+00
Epoch 155/200
 - 1s - loss: -3.2540e+00 - val_loss: -3.2470e+00
Epoch 156/200
 - 1s - loss: -3.2532e+00 - val_loss: -3.2470e+00
Epoch 157/200
 - 1s - loss: -3.2533e+00 - val_loss: -3.2472e+00
Epoch 158/200
 - 1s - loss: -3.2538e+00 - val_loss: -3.2467e+00
Epoch 159/200
 - 1s - loss: -3.2537e+00 - val_loss: -3.2471e+00
Epoch 160/200
 - 1s - loss: -3.2536e+00 - val_loss: -3.2470e+00
Epoch 161/200
 - 1s - loss: -3.2537e+00 - val_loss: -3.2474e+00
2020-01-07 11:17:24,745 [INFO] epoch = 160. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/ae_model_epoch_160.pickle
Epoch 162/200
 - 1s - loss: -3.2537e+00 - val_loss: -3.2470e+00
Epoch 163/200
 - 1s - loss: -3.2537e+00 - val_loss: -3.2473e+00
Epoch 164/200
 - 1s - loss: -3.2535e+00 - val_loss: -3.2473e+00
Epoch 165/200
 - 1s - loss: -3.2532e+00 - val_loss: -3.2472e+00
Epoch 166/200
 - 1s - loss: -3.2536e+00 - val_loss: -3.2471e+00
Epoch 167/200
 - 1s - loss: -3.2538e+00 - val_loss: -3.2472e+00
Epoch 168/200
 - 1s - loss: -3.2541e+00 - val_loss: -3.2471e+00
Epoch 169/200
 - 1s - loss: -3.2543e+00 - val_loss: -3.2473e+00
Epoch 170/200
 - 1s - loss: -3.2538e+00 - val_loss: -3.2472e+00
Epoch 171/200
 - 1s - loss: -3.2540e+00 - val_loss: -3.2471e+00
Epoch 172/200
 - 1s - loss: -3.2541e+00 - val_loss: -3.2473e+00
Epoch 173/200
 - 1s - loss: -3.2542e+00 - val_loss: -3.2468e+00
Epoch 174/200
 - 1s - loss: -3.2536e+00 - val_loss: -3.2472e+00
Epoch 175/200
 - 1s - loss: -3.2538e+00 - val_loss: -3.2473e+00
Epoch 176/200
 - 1s - loss: -3.2541e+00 - val_loss: -3.2471e+00
Epoch 177/200
 - 1s - loss: -3.2540e+00 - val_loss: -3.2470e+00
Epoch 178/200
 - 1s - loss: -3.2542e+00 - val_loss: -3.2472e+00
Epoch 179/200
 - 1s - loss: -3.2541e+00 - val_loss: -3.2470e+00
Epoch 180/200
 - 1s - loss: -3.2543e+00 - val_loss: -3.2472e+00
Epoch 181/200
 - 1s - loss: -3.2540e+00 - val_loss: -3.2472e+00
2020-01-07 11:17:51,943 [INFO] epoch = 180. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/ae_model_epoch_180.pickle
Epoch 182/200
 - 1s - loss: -3.2542e+00 - val_loss: -3.2474e+00
Epoch 183/200
 - 1s - loss: -3.2540e+00 - val_loss: -3.2472e+00
Epoch 184/200
 - 1s - loss: -3.2542e+00 - val_loss: -3.2469e+00
Epoch 185/200
 - 1s - loss: -3.2540e+00 - val_loss: -3.2473e+00
Epoch 186/200
 - 1s - loss: -3.2541e+00 - val_loss: -3.2475e+00
Epoch 187/200
 - 1s - loss: -3.2544e+00 - val_loss: -3.2473e+00
Epoch 188/200
 - 1s - loss: -3.2544e+00 - val_loss: -3.2472e+00
Epoch 189/200
 - 1s - loss: -3.2543e+00 - val_loss: -3.2472e+00
Epoch 190/200
 - 1s - loss: -3.2545e+00 - val_loss: -3.2471e+00
Epoch 191/200
 - 1s - loss: -3.2542e+00 - val_loss: -3.2474e+00
Epoch 192/200
 - 1s - loss: -3.2546e+00 - val_loss: -3.2474e+00
Epoch 193/200
 - 1s - loss: -3.2527e+00 - val_loss: -3.2471e+00
Epoch 194/200
 - 1s - loss: -3.2543e+00 - val_loss: -3.2474e+00
Epoch 195/200
 - 1s - loss: -3.2543e+00 - val_loss: -3.2470e+00
Epoch 196/200
 - 1s - loss: -3.2543e+00 - val_loss: -3.2472e+00
Epoch 197/200
 - 1s - loss: -3.2543e+00 - val_loss: -3.2474e+00
Epoch 198/200
 - 1s - loss: -3.2546e+00 - val_loss: -3.2471e+00
Epoch 199/200
 - 1s - loss: -3.2548e+00 - val_loss: -3.2472e+00
Epoch 200/200
 - 1s - loss: -3.2547e+00 - val_loss: -3.2473e+00
2020-01-07 11:18:17,797 [INFO] WeightRestorer::on_train_end(): restoring best weights
2020-01-07 11:18:19,731 [INFO] Last epoch loss evaluation: train_loss = -3.267596, val_loss = -3.247461
2020-01-07 11:18:19,731 [INFO] Training autoencoder complete
2020-01-07 11:18:19,731 [INFO] Encoding data for supervised training
2020-01-07 11:18:20,556 [INFO] Encoding complete
2020-01-07 11:18:20,556 [INFO] Training neural network layers (after autoencoder)
Train on 25195 samples, validate on 25195 samples
Epoch 1/200
 - 1s - loss: 0.2076 - val_loss: 0.0523
 - val_f1: 0.9645
Epoch 2/200
 - 0s - loss: 0.0515 - val_loss: 0.0328
 - val_f1: 0.9685
Epoch 3/200
 - 0s - loss: 0.0360 - val_loss: 0.0239
 - val_f1: 0.9788
Epoch 4/200
 - 0s - loss: 0.0268 - val_loss: 0.0191
 - val_f1: 0.9849
Epoch 5/200
 - 0s - loss: 0.0216 - val_loss: 0.0159
 - val_f1: 0.9874
Epoch 6/200
 - 0s - loss: 0.0200 - val_loss: 0.0145
 - val_f1: 0.9885
Epoch 7/200
 - 0s - loss: 0.0178 - val_loss: 0.0134
 - val_f1: 0.9889
Epoch 8/200
 - 0s - loss: 0.0162 - val_loss: 0.0124
 - val_f1: 0.9905
Epoch 9/200
 - 0s - loss: 0.0146 - val_loss: 0.0122
 - val_f1: 0.9888
Epoch 10/200
 - 0s - loss: 0.0137 - val_loss: 0.0118
 - val_f1: 0.9903
Epoch 11/200
 - 0s - loss: 0.0135 - val_loss: 0.0113
 - val_f1: 0.9909
Epoch 12/200
 - 0s - loss: 0.0130 - val_loss: 0.0108
 - val_f1: 0.9914
Epoch 13/200
 - 0s - loss: 0.0124 - val_loss: 0.0106
 - val_f1: 0.9915
Epoch 14/200
 - 0s - loss: 0.0118 - val_loss: 0.0102
 - val_f1: 0.9918
Epoch 15/200
 - 0s - loss: 0.0111 - val_loss: 0.0103
 - val_f1: 0.9924
Epoch 16/200
 - 0s - loss: 0.0113 - val_loss: 0.0101
 - val_f1: 0.9916
Epoch 17/200
 - 0s - loss: 0.0109 - val_loss: 0.0105
 - val_f1: 0.9922
Epoch 18/200
 - 0s - loss: 0.0104 - val_loss: 0.0100
 - val_f1: 0.9923
Epoch 19/200
 - 0s - loss: 0.0098 - val_loss: 0.0096
 - val_f1: 0.9926
Epoch 20/200
 - 0s - loss: 0.0100 - val_loss: 0.0098
 - val_f1: 0.9925
Epoch 21/200
 - 0s - loss: 0.0096 - val_loss: 0.0099
/home/sunanda/anaconda3/envs/ml_env_cpu/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2020-01-07 11:18:35,394 [INFO] epoch = 20. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/ann_model_epoch_20.pickle
 - val_f1: 0.9922
Epoch 22/200
 - 0s - loss: 0.0099 - val_loss: 0.0094
 - val_f1: 0.9931
Epoch 23/200
 - 0s - loss: 0.0089 - val_loss: 0.0099
 - val_f1: 0.9921
Epoch 24/200
 - 0s - loss: 0.0092 - val_loss: 0.0097
 - val_f1: 0.9920
Epoch 25/200
 - 0s - loss: 0.0092 - val_loss: 0.0091
 - val_f1: 0.9924
Epoch 26/200
 - 0s - loss: 0.0084 - val_loss: 0.0094
 - val_f1: 0.9924
Epoch 27/200
 - 0s - loss: 0.0083 - val_loss: 0.0088
 - val_f1: 0.9932
Epoch 28/200
 - 0s - loss: 0.0084 - val_loss: 0.0089
 - val_f1: 0.9928
Epoch 29/200
 - 0s - loss: 0.0082 - val_loss: 0.0090
 - val_f1: 0.9927
Epoch 30/200
 - 0s - loss: 0.0082 - val_loss: 0.0089
 - val_f1: 0.9938
Epoch 31/200
 - 0s - loss: 0.0083 - val_loss: 0.0088
 - val_f1: 0.9936
Epoch 32/200
 - 0s - loss: 0.0078 - val_loss: 0.0094
 - val_f1: 0.9931
Epoch 33/200
 - 0s - loss: 0.0077 - val_loss: 0.0087
 - val_f1: 0.9938
Epoch 34/200
 - 0s - loss: 0.0079 - val_loss: 0.0087
 - val_f1: 0.9935
Epoch 35/200
 - 0s - loss: 0.0075 - val_loss: 0.0087
 - val_f1: 0.9931
Epoch 36/200
 - 0s - loss: 0.0075 - val_loss: 0.0086
 - val_f1: 0.9937
Epoch 37/200
 - 0s - loss: 0.0076 - val_loss: 0.0090
 - val_f1: 0.9938
Epoch 38/200
 - 0s - loss: 0.0074 - val_loss: 0.0087
 - val_f1: 0.9943
Epoch 39/200
 - 0s - loss: 0.0071 - val_loss: 0.0089
 - val_f1: 0.9935
Epoch 40/200
 - 0s - loss: 0.0072 - val_loss: 0.0086
 - val_f1: 0.9942
Epoch 41/200
 - 0s - loss: 0.0072 - val_loss: 0.0086
2020-01-07 11:18:48,327 [INFO] epoch = 40. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/ann_model_epoch_40.pickle
 - val_f1: 0.9934
Epoch 42/200
 - 0s - loss: 0.0077 - val_loss: 0.0086
 - val_f1: 0.9936
Epoch 43/200
 - 0s - loss: 0.0068 - val_loss: 0.0092
 - val_f1: 0.9934
Epoch 44/200
 - 0s - loss: 0.0069 - val_loss: 0.0086
 - val_f1: 0.9944
Epoch 45/200
 - 0s - loss: 0.0071 - val_loss: 0.0086
 - val_f1: 0.9935
Epoch 46/200
 - 0s - loss: 0.0070 - val_loss: 0.0087
 - val_f1: 0.9944
Epoch 47/200
 - 0s - loss: 0.0065 - val_loss: 0.0087
 - val_f1: 0.9942
Epoch 48/200
 - 0s - loss: 0.0064 - val_loss: 0.0091
 - val_f1: 0.9934
Epoch 49/200
 - 0s - loss: 0.0066 - val_loss: 0.0086
 - val_f1: 0.9939
Epoch 50/200
 - 0s - loss: 0.0068 - val_loss: 0.0091
 - val_f1: 0.9935
Epoch 51/200
 - 0s - loss: 0.0057 - val_loss: 0.0087
 - val_f1: 0.9941
Epoch 52/200
 - 0s - loss: 0.0066 - val_loss: 0.0083
 - val_f1: 0.9943
Epoch 53/200
 - 0s - loss: 0.0066 - val_loss: 0.0094
 - val_f1: 0.9937
Epoch 54/200
 - 0s - loss: 0.0066 - val_loss: 0.0089
 - val_f1: 0.9940
Epoch 55/200
 - 0s - loss: 0.0069 - val_loss: 0.0085
 - val_f1: 0.9940
Epoch 56/200
 - 0s - loss: 0.0060 - val_loss: 0.0091
 - val_f1: 0.9934
Epoch 57/200
 - 0s - loss: 0.0057 - val_loss: 0.0086
 - val_f1: 0.9941
Epoch 58/200
 - 0s - loss: 0.0059 - val_loss: 0.0089
 - val_f1: 0.9941
Epoch 59/200
 - 0s - loss: 0.0055 - val_loss: 0.0095
 - val_f1: 0.9932
Epoch 60/200
 - 0s - loss: 0.0061 - val_loss: 0.0093
 - val_f1: 0.9940
Epoch 61/200
 - 0s - loss: 0.0060 - val_loss: 0.0091
2020-01-07 11:19:01,240 [INFO] epoch = 60. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/ann_model_epoch_60.pickle
 - val_f1: 0.9941
Epoch 62/200
 - 0s - loss: 0.0054 - val_loss: 0.0086
 - val_f1: 0.9945
Epoch 63/200
 - 0s - loss: 0.0056 - val_loss: 0.0091
 - val_f1: 0.9938
Epoch 64/200
 - 0s - loss: 0.0053 - val_loss: 0.0093
 - val_f1: 0.9936
Epoch 65/200
 - 0s - loss: 0.0054 - val_loss: 0.0089
 - val_f1: 0.9943
Epoch 66/200
 - 0s - loss: 0.0056 - val_loss: 0.0090
 - val_f1: 0.9938
Epoch 67/200
 - 0s - loss: 0.0051 - val_loss: 0.0086
 - val_f1: 0.9938
Epoch 68/200
 - 0s - loss: 0.0049 - val_loss: 0.0084
 - val_f1: 0.9947
Epoch 69/200
 - 0s - loss: 0.0054 - val_loss: 0.0085
 - val_f1: 0.9947
Epoch 70/200
 - 0s - loss: 0.0050 - val_loss: 0.0089
 - val_f1: 0.9939
Epoch 71/200
 - 0s - loss: 0.0056 - val_loss: 0.0086
 - val_f1: 0.9946
Epoch 72/200
 - 0s - loss: 0.0045 - val_loss: 0.0090
 - val_f1: 0.9941
Epoch 73/200
 - 0s - loss: 0.0050 - val_loss: 0.0092
 - val_f1: 0.9936
Epoch 74/200
 - 0s - loss: 0.0050 - val_loss: 0.0091
 - val_f1: 0.9942
Epoch 75/200
 - 0s - loss: 0.0053 - val_loss: 0.0094
 - val_f1: 0.9933
Epoch 76/200
 - 0s - loss: 0.0051 - val_loss: 0.0093
 - val_f1: 0.9938
Epoch 77/200
 - 0s - loss: 0.0055 - val_loss: 0.0094
 - val_f1: 0.9934
Epoch 78/200
 - 0s - loss: 0.0050 - val_loss: 0.0086
 - val_f1: 0.9944
Epoch 79/200
 - 0s - loss: 0.0050 - val_loss: 0.0088
 - val_f1: 0.9938
Epoch 80/200
 - 0s - loss: 0.0048 - val_loss: 0.0085
 - val_f1: 0.9946
Epoch 81/200
 - 0s - loss: 0.0045 - val_loss: 0.0090
2020-01-07 11:19:14,161 [INFO] epoch = 80. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/ann_model_epoch_80.pickle
 - val_f1: 0.9940
Epoch 82/200
 - 0s - loss: 0.0051 - val_loss: 0.0094
 - val_f1: 0.9942
Epoch 83/200
 - 0s - loss: 0.0048 - val_loss: 0.0089
 - val_f1: 0.9938
Epoch 84/200
 - 0s - loss: 0.0049 - val_loss: 0.0092
 - val_f1: 0.9936
Epoch 85/200
 - 0s - loss: 0.0049 - val_loss: 0.0089
 - val_f1: 0.9935
Epoch 86/200
 - 0s - loss: 0.0046 - val_loss: 0.0090
 - val_f1: 0.9945
Epoch 87/200
 - 0s - loss: 0.0044 - val_loss: 0.0090
 - val_f1: 0.9941
Epoch 88/200
 - 0s - loss: 0.0050 - val_loss: 0.0094
 - val_f1: 0.9938
Epoch 89/200
 - 0s - loss: 0.0046 - val_loss: 0.0091
 - val_f1: 0.9938
Epoch 90/200
 - 0s - loss: 0.0044 - val_loss: 0.0090
 - val_f1: 0.9934
Epoch 91/200
 - 0s - loss: 0.0044 - val_loss: 0.0091
 - val_f1: 0.9940
Epoch 92/200
 - 0s - loss: 0.0048 - val_loss: 0.0094
 - val_f1: 0.9935
Epoch 93/200
 - 0s - loss: 0.0048 - val_loss: 0.0093
 - val_f1: 0.9939
Epoch 94/200
 - 0s - loss: 0.0046 - val_loss: 0.0094
 - val_f1: 0.9937
Epoch 95/200
 - 0s - loss: 0.0048 - val_loss: 0.0091
 - val_f1: 0.9938
Epoch 96/200
 - 0s - loss: 0.0045 - val_loss: 0.0090
 - val_f1: 0.9940
Epoch 97/200
 - 0s - loss: 0.0044 - val_loss: 0.0095
 - val_f1: 0.9943
Epoch 98/200
 - 0s - loss: 0.0044 - val_loss: 0.0092
 - val_f1: 0.9943
Epoch 99/200
 - 0s - loss: 0.0050 - val_loss: 0.0089
 - val_f1: 0.9946
Epoch 100/200
 - 0s - loss: 0.0049 - val_loss: 0.0091
 - val_f1: 0.9941
Epoch 101/200
 - 0s - loss: 0.0044 - val_loss: 0.0091
2020-01-07 11:19:27,061 [INFO] epoch = 100. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/ann_model_epoch_100.pickle
 - val_f1: 0.9942
Epoch 102/200
 - 0s - loss: 0.0045 - val_loss: 0.0090
2020-01-07 11:19:28,063 [INFO] WeightRestorer::on_train_end(): restoring best weights
2020-01-07 11:19:28,996 [INFO] Last epoch loss evaluation: train_loss = 0.004301, val_loss = 0.008347
2020-01-07 11:19:28,997 [INFO] Training complete. time_to_train = 346.08 sec, 5.77 min
2020-01-07 11:19:29,006 [INFO] Model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/best_model.pickle
2020-01-07 11:19:29,009 [INFO] Training history saved to: results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/training_error_history.csv
2020-01-07 11:19:29,202 [INFO] Plot saved to: results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/training_error_history.png
2020-01-07 11:19:29,371 [INFO] Plot saved to: results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/training_f1_history.png
2020-01-07 11:19:29,371 [INFO] Making predictions on training, validation, testing data
2020-01-07 11:19:33,681 [INFO] Evaluating predictions (results)
2020-01-07 11:19:34,337 [INFO] Dataset: Testing. Classification report below
2020-01-07 11:19:34,337 [INFO] 
              precision    recall  f1-score   support

         dos       0.97      0.84      0.90      7458
      normal       0.68      0.93      0.79      9711
       probe       0.71      0.74      0.73      2421
         r2l       0.52      0.07      0.13      2421
         u2r       0.85      0.03      0.06       533

    accuracy                           0.77     22544
   macro avg       0.75      0.52      0.52     22544
weighted avg       0.77      0.77      0.73     22544

2020-01-07 11:19:34,337 [INFO] Overall accuracy (micro avg): 0.7671220723917672
2020-01-07 11:19:34,933 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.7671         0.7671                       0.7671                0.0582                   0.2329  0.7671
1     Macro avg        0.9068         0.7462                       0.5242                0.0762                   0.4758  0.5213
2  Weighted avg        0.8689         0.7678                       0.7671                0.1483                   0.2329  0.7311
2020-01-07 11:19:35,603 [INFO] Dataset: Validation. Classification report below
2020-01-07 11:19:35,603 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00      9186
      normal       0.99      1.00      1.00     13469
       probe       0.99      0.99      0.99      2331
         r2l       0.87      0.83      0.85       199
         u2r       1.00      0.40      0.57        10

    accuracy                           0.99     25195
   macro avg       0.97      0.84      0.88     25195
weighted avg       0.99      0.99      0.99     25195

2020-01-07 11:19:35,603 [INFO] Overall accuracy (micro avg): 0.9943242706886287
2020-01-07 11:19:36,271 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9943         0.9943                       0.9943                0.0014                   0.0057  0.9943
1     Macro avg        0.9977         0.9699                       0.8431                0.0019                   0.1569  0.8807
2  Weighted avg        0.9965         0.9943                       0.9943                0.0038                   0.0057  0.9943
2020-01-07 11:19:39,095 [INFO] Dataset: Training. Classification report below
2020-01-07 11:19:39,096 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00     36741
      normal       0.99      1.00      1.00     53874
       probe       0.99      0.99      0.99      9325
         r2l       0.88      0.83      0.86       796
         u2r       0.70      0.45      0.55        42

    accuracy                           0.99    100778
   macro avg       0.91      0.85      0.88    100778
weighted avg       0.99      0.99      0.99    100778

2020-01-07 11:19:39,096 [INFO] Overall accuracy (micro avg): 0.9949889856913215
2020-01-07 11:19:42,123 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9950         0.9950                       0.9950                0.0013                   0.0050  0.9950
1     Macro avg        0.9980         0.9131                       0.8543                0.0017                   0.1457  0.8781
2  Weighted avg        0.9969         0.9949                       0.9950                0.0035                   0.0050  0.9949
2020-01-07 11:19:42,171 [INFO] Results saved to: results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/semi_sup_perf_nsl_ae_ann_rep1_results.xlsx
2020-01-07 11:19:42,171 [INFO] ================= Finished running experiment no. 1 ================= 

2020-01-07 11:19:42,172 [INFO] Created directory: results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2
2020-01-07 11:19:42,173 [INFO] Initialized logging. log_filename = results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2/run_log.log
2020-01-07 11:19:42,173 [INFO] ================= Running experiment no. 2  ================= 

2020-01-07 11:19:42,173 [INFO] Experiment parameters given below
2020-01-07 11:19:42,173 [INFO] 
{'experiment_num': 2, 'results_dir': 'results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2', 'model_type': 'classifier', 'model': 'ae_ann', 'normal_label': 'normal', 'split_random_seed': 42, 'unsupervised_ratio': 0.75, 'ae_encoder_units': [64], 'ae_encoder_activations': ['relu'], 'ae_encoder_dropout_rates': [0.2], 'ae_encoder_l1_param': -1, 'ae_decoder_units': '', 'ae_decoder_activations': '', 'ae_decoder_dropout_rates': '', 'output_activation': 'sigmoid', 'loss_function': 'binary_crossentropy', 'ae_epochs': 200, 'ann_layer_units': [64], 'ann_layer_activations': ['relu'], 'ann_layer_dropout_rates': [0.2], 'ann_epochs': 200, 'early_stop_patience': 50, 'batch_size': 256, 'dataset_dir': '../Datasets/small_datasets/nsl_kdd_five_classes', 'description': 'semi_sup_perf_nsl_ae_ann_rep2'}
2020-01-07 11:19:42,173 [INFO] Created tensorboard log directory: results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2/tf_logs_run_2020_01_07-11_19_42
2020-01-07 11:19:42,173 [INFO] Loading datsets from: ../Datasets/small_datasets/nsl_kdd_five_classes
2020-01-07 11:19:42,173 [INFO] Reading X, y files
2020-01-07 11:19:42,173 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_train.h5
2020-01-07 11:19:42,401 [INFO] Reading complete. time_to_read=0.23 seconds
2020-01-07 11:19:42,402 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_val.h5
2020-01-07 11:19:42,464 [INFO] Reading complete. time_to_read=0.06 seconds
2020-01-07 11:19:42,464 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_test.h5
2020-01-07 11:19:42,519 [INFO] Reading complete. time_to_read=0.06 seconds
2020-01-07 11:19:42,519 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_train.h5
2020-01-07 11:19:42,528 [INFO] Reading complete. time_to_read=0.01 seconds
2020-01-07 11:19:42,528 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_val.h5
2020-01-07 11:19:42,532 [INFO] Reading complete. time_to_read=0.00 seconds
2020-01-07 11:19:42,532 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_test.h5
2020-01-07 11:19:42,536 [INFO] Reading complete. time_to_read=0.00 seconds
2020-01-07 11:19:42,651 [INFO] Initializing model
2020-01-07 11:19:42,772 [INFO] _________________________________________________________________
2020-01-07 11:19:42,772 [INFO] Layer (type)                 Output Shape              Param #   
2020-01-07 11:19:42,772 [INFO] =================================================================
2020-01-07 11:19:42,772 [INFO] dense_5 (Dense)              (None, 64)                7872      
2020-01-07 11:19:42,773 [INFO] _________________________________________________________________
2020-01-07 11:19:42,773 [INFO] batch_normalization_3 (Batch (None, 64)                256       
2020-01-07 11:19:42,773 [INFO] _________________________________________________________________
2020-01-07 11:19:42,773 [INFO] dropout_3 (Dropout)          (None, 64)                0         
2020-01-07 11:19:42,773 [INFO] _________________________________________________________________
2020-01-07 11:19:42,773 [INFO] dense_6 (Dense)              (None, 122)               7930      
2020-01-07 11:19:42,773 [INFO] =================================================================
2020-01-07 11:19:42,773 [INFO] Total params: 16,058
2020-01-07 11:19:42,773 [INFO] Trainable params: 15,930
2020-01-07 11:19:42,773 [INFO] Non-trainable params: 128
2020-01-07 11:19:42,773 [INFO] _________________________________________________________________
2020-01-07 11:19:42,891 [INFO] _________________________________________________________________
2020-01-07 11:19:42,892 [INFO] Layer (type)                 Output Shape              Param #   
2020-01-07 11:19:42,892 [INFO] =================================================================
2020-01-07 11:19:42,892 [INFO] dense_7 (Dense)              (None, 64)                4160      
2020-01-07 11:19:42,892 [INFO] _________________________________________________________________
2020-01-07 11:19:42,892 [INFO] batch_normalization_4 (Batch (None, 64)                256       
2020-01-07 11:19:42,892 [INFO] _________________________________________________________________
2020-01-07 11:19:42,892 [INFO] dropout_4 (Dropout)          (None, 64)                0         
2020-01-07 11:19:42,892 [INFO] _________________________________________________________________
2020-01-07 11:19:42,892 [INFO] dense_8 (Dense)              (None, 5)                 325       
2020-01-07 11:19:42,892 [INFO] =================================================================
2020-01-07 11:19:42,893 [INFO] Total params: 4,741
2020-01-07 11:19:42,893 [INFO] Trainable params: 4,613
2020-01-07 11:19:42,893 [INFO] Non-trainable params: 128
2020-01-07 11:19:42,893 [INFO] _________________________________________________________________
2020-01-07 11:19:42,893 [INFO] Training model
2020-01-07 11:19:42,893 [INFO] Splitting train set into 2 sets (unsupervised, supervised), random_seed = 42
2020-01-07 11:19:43,911 [INFO] Split sizes (instances). total = 100778, unsupervised = 75583, supervised = 25195, unsupervised dataset hash = 40fcc5b399aca9d2e6f9581990505c20b3f01fda
2020-01-07 11:19:43,912 [INFO] Training autoencoder
 - val_f1: 0.9946
Epoch 00102: early stopping
Train on 75583 samples, validate on 25195 samples
Epoch 1/200
 - 2s - loss: -3.7097e-01 - val_loss: -1.6821e+00
Epoch 2/200
 - 1s - loss: -2.2271e+00 - val_loss: -2.7701e+00
Epoch 3/200
 - 1s - loss: -2.8927e+00 - val_loss: -3.0868e+00
Epoch 4/200
 - 1s - loss: -3.0734e+00 - val_loss: -3.1602e+00
Epoch 5/200
 - 1s - loss: -3.1322e+00 - val_loss: -3.1892e+00
Epoch 6/200
 - 1s - loss: -3.1611e+00 - val_loss: -3.2038e+00
Epoch 7/200
 - 1s - loss: -3.1776e+00 - val_loss: -3.2132e+00
Epoch 8/200
 - 1s - loss: -3.1891e+00 - val_loss: -3.2186e+00
Epoch 9/200
 - 1s - loss: -3.1973e+00 - val_loss: -3.2226e+00
Epoch 10/200
 - 1s - loss: -3.2032e+00 - val_loss: -3.2253e+00
Epoch 11/200
 - 1s - loss: -3.2083e+00 - val_loss: -3.2271e+00
Epoch 12/200
 - 1s - loss: -3.2114e+00 - val_loss: -3.2299e+00
Epoch 13/200
 - 1s - loss: -3.2155e+00 - val_loss: -3.2312e+00
Epoch 14/200
 - 1s - loss: -3.2185e+00 - val_loss: -3.2323e+00
Epoch 15/200
 - 1s - loss: -3.2208e+00 - val_loss: -3.2342e+00
Epoch 16/200
 - 1s - loss: -3.2229e+00 - val_loss: -3.2353e+00
Epoch 17/200
 - 1s - loss: -3.2253e+00 - val_loss: -3.2361e+00
Epoch 18/200
 - 1s - loss: -3.2270e+00 - val_loss: -3.2367e+00
Epoch 19/200
 - 1s - loss: -3.2284e+00 - val_loss: -3.2369e+00
Epoch 20/200
 - 1s - loss: -3.2295e+00 - val_loss: -3.2379e+00
Epoch 21/200
 - 1s - loss: -3.2313e+00 - val_loss: -3.2383e+00
2020-01-07 11:20:14,912 [INFO] epoch = 20. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2/ae_model_epoch_20.pickle
Epoch 22/200
 - 1s - loss: -3.2317e+00 - val_loss: -3.2389e+00
Epoch 23/200
 - 1s - loss: -3.2332e+00 - val_loss: -3.2392e+00
Epoch 24/200
 - 1s - loss: -3.2338e+00 - val_loss: -3.2395e+00
Epoch 25/200
 - 1s - loss: -3.2354e+00 - val_loss: -3.2399e+00
Epoch 26/200
 - 1s - loss: -3.2359e+00 - val_loss: -3.2402e+00
Epoch 27/200
 - 1s - loss: -3.2362e+00 - val_loss: -3.2403e+00
Epoch 28/200
 - 1s - loss: -3.2370e+00 - val_loss: -3.2406e+00
Epoch 29/200
 - 1s - loss: -3.2380e+00 - val_loss: -3.2410e+00
Epoch 30/200
 - 1s - loss: -3.2387e+00 - val_loss: -3.2411e+00
Epoch 31/200
 - 1s - loss: -3.2393e+00 - val_loss: -3.2413e+00
Epoch 32/200
 - 1s - loss: -3.2400e+00 - val_loss: -3.2416e+00
Epoch 33/200
 - 1s - loss: -3.2401e+00 - val_loss: -3.2419e+00
Epoch 34/200
 - 1s - loss: -3.2405e+00 - val_loss: -3.2414e+00
Epoch 35/200
 - 1s - loss: -3.2405e+00 - val_loss: -3.2421e+00
Epoch 36/200
 - 1s - loss: -3.2416e+00 - val_loss: -3.2422e+00
Epoch 37/200
 - 1s - loss: -3.2421e+00 - val_loss: -3.2426e+00
Epoch 38/200
 - 1s - loss: -3.2401e+00 - val_loss: -3.2423e+00
Epoch 39/200
 - 1s - loss: -3.2413e+00 - val_loss: -3.2426e+00
Epoch 40/200
 - 1s - loss: -3.2426e+00 - val_loss: -3.2423e+00
Epoch 41/200
 - 1s - loss: -3.2435e+00 - val_loss: -3.2428e+00
2020-01-07 11:20:42,741 [INFO] epoch = 40. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2/ae_model_epoch_40.pickle
Epoch 42/200
 - 1s - loss: -3.2429e+00 - val_loss: -3.2428e+00
Epoch 43/200
 - 1s - loss: -3.2442e+00 - val_loss: -3.2430e+00
Epoch 44/200
 - 1s - loss: -3.2441e+00 - val_loss: -3.2427e+00
Epoch 45/200
 - 1s - loss: -3.2445e+00 - val_loss: -3.2431e+00
Epoch 46/200
 - 1s - loss: -3.2444e+00 - val_loss: -3.2434e+00
Epoch 47/200
 - 1s - loss: -3.2447e+00 - val_loss: -3.2431e+00
Epoch 48/200
 - 1s - loss: -3.2454e+00 - val_loss: -3.2434e+00
Epoch 49/200
 - 1s - loss: -3.2453e+00 - val_loss: -3.2434e+00
Epoch 50/200
 - 1s - loss: -3.2450e+00 - val_loss: -3.2433e+00
Epoch 51/200
 - 1s - loss: -3.2456e+00 - val_loss: -3.2436e+00
Epoch 52/200
 - 1s - loss: -3.2462e+00 - val_loss: -3.2438e+00
Epoch 53/200
 - 1s - loss: -3.2460e+00 - val_loss: -3.2438e+00
Epoch 54/200
 - 1s - loss: -3.2466e+00 - val_loss: -3.2436e+00
Epoch 55/200
 - 1s - loss: -3.2466e+00 - val_loss: -3.2438e+00
Epoch 56/200
 - 1s - loss: -3.2468e+00 - val_loss: -3.2438e+00
Epoch 57/200
 - 1s - loss: -3.2451e+00 - val_loss: -3.2442e+00
Epoch 58/200
 - 1s - loss: -3.2458e+00 - val_loss: -3.2442e+00
Epoch 59/200
 - 1s - loss: -3.2475e+00 - val_loss: -3.2445e+00
Epoch 60/200
 - 1s - loss: -3.2474e+00 - val_loss: -3.2444e+00
Epoch 61/200
 - 1s - loss: -3.2476e+00 - val_loss: -3.2441e+00
2020-01-07 11:21:10,476 [INFO] epoch = 60. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2/ae_model_epoch_60.pickle
Epoch 62/200
 - 1s - loss: -3.2473e+00 - val_loss: -3.2447e+00
Epoch 63/200
 - 1s - loss: -3.2479e+00 - val_loss: -3.2445e+00
Epoch 64/200
 - 1s - loss: -3.2481e+00 - val_loss: -3.2448e+00
Epoch 65/200
 - 1s - loss: -3.2482e+00 - val_loss: -3.2449e+00
Epoch 66/200
 - 1s - loss: -3.2489e+00 - val_loss: -3.2448e+00
Epoch 67/200
 - 1s - loss: -3.2490e+00 - val_loss: -3.2448e+00
Epoch 68/200
 - 1s - loss: -3.2491e+00 - val_loss: -3.2449e+00
Epoch 69/200
 - 1s - loss: -3.2494e+00 - val_loss: -3.2448e+00
Epoch 70/200
 - 1s - loss: -3.2492e+00 - val_loss: -3.2450e+00
Epoch 71/200
 - 1s - loss: -3.2491e+00 - val_loss: -3.2450e+00
Epoch 72/200
 - 1s - loss: -3.2497e+00 - val_loss: -3.2452e+00
Epoch 73/200
 - 1s - loss: -3.2497e+00 - val_loss: -3.2451e+00
Epoch 74/200
 - 1s - loss: -3.2500e+00 - val_loss: -3.2450e+00
Epoch 75/200
 - 1s - loss: -3.2498e+00 - val_loss: -3.2456e+00
Epoch 76/200
 - 1s - loss: -3.2501e+00 - val_loss: -3.2456e+00
Epoch 77/200
 - 1s - loss: -3.2502e+00 - val_loss: -3.2456e+00
Epoch 78/200
 - 1s - loss: -3.2504e+00 - val_loss: -3.2455e+00
Epoch 79/200
 - 1s - loss: -3.2503e+00 - val_loss: -3.2455e+00
Epoch 80/200
 - 1s - loss: -3.2502e+00 - val_loss: -3.2454e+00
Epoch 81/200
 - 1s - loss: -3.2501e+00 - val_loss: -3.2451e+00
2020-01-07 11:21:38,186 [INFO] epoch = 80. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2/ae_model_epoch_80.pickle
Epoch 82/200
 - 1s - loss: -3.2511e+00 - val_loss: -3.2453e+00
Epoch 83/200
 - 1s - loss: -3.2503e+00 - val_loss: -3.2455e+00
Epoch 84/200
 - 1s - loss: -3.2508e+00 - val_loss: -3.2454e+00
Epoch 85/200
 - 1s - loss: -3.2498e+00 - val_loss: -3.2455e+00
Epoch 86/200
 - 1s - loss: -3.2509e+00 - val_loss: -3.2456e+00
Epoch 87/200
 - 1s - loss: -3.2510e+00 - val_loss: -3.2450e+00
Epoch 88/200
 - 1s - loss: -3.2509e+00 - val_loss: -3.2459e+00
Epoch 89/200
 - 1s - loss: -3.2511e+00 - val_loss: -3.2455e+00
Epoch 90/200
 - 1s - loss: -3.2508e+00 - val_loss: -3.2459e+00
Epoch 91/200
 - 1s - loss: -3.2510e+00 - val_loss: -3.2457e+00
Epoch 92/200
 - 1s - loss: -3.2495e+00 - val_loss: -3.2458e+00
Epoch 93/200
 - 1s - loss: -3.2510e+00 - val_loss: -3.2458e+00
Epoch 94/200
 - 1s - loss: -3.2515e+00 - val_loss: -3.2461e+00
Epoch 95/200
 - 1s - loss: -3.2513e+00 - val_loss: -3.2459e+00
Epoch 96/200
 - 1s - loss: -3.2515e+00 - val_loss: -3.2459e+00
Epoch 97/200
 - 1s - loss: -3.2516e+00 - val_loss: -3.2459e+00
Epoch 98/200
 - 1s - loss: -3.2521e+00 - val_loss: -3.2459e+00
Epoch 99/200
 - 1s - loss: -3.2517e+00 - val_loss: -3.2461e+00
Epoch 100/200
 - 1s - loss: -3.2519e+00 - val_loss: -3.2464e+00
Epoch 101/200
 - 1s - loss: -3.2524e+00 - val_loss: -3.2459e+00
2020-01-07 11:22:05,944 [INFO] epoch = 100. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2/ae_model_epoch_100.pickle
Epoch 102/200
 - 1s - loss: -3.2507e+00 - val_loss: -3.2459e+00
Epoch 103/200
 - 1s - loss: -3.2521e+00 - val_loss: -3.2462e+00
Epoch 104/200
 - 1s - loss: -3.2525e+00 - val_loss: -3.2461e+00
Epoch 105/200
 - 1s - loss: -3.2520e+00 - val_loss: -3.2462e+00
Epoch 106/200
 - 1s - loss: -3.2522e+00 - val_loss: -3.2464e+00
Epoch 107/200
 - 1s - loss: -3.2520e+00 - val_loss: -3.2464e+00
Epoch 108/200
 - 1s - loss: -3.2519e+00 - val_loss: -3.2463e+00
Epoch 109/200
 - 1s - loss: -3.2521e+00 - val_loss: -3.2463e+00
Epoch 110/200
 - 1s - loss: -3.2521e+00 - val_loss: -3.2459e+00
Epoch 111/200
 - 1s - loss: -3.2525e+00 - val_loss: -3.2463e+00
Epoch 112/200
 - 1s - loss: -3.2523e+00 - val_loss: -3.2463e+00
Epoch 113/200
 - 1s - loss: -3.2527e+00 - val_loss: -3.2463e+00
Epoch 114/200
 - 1s - loss: -3.2522e+00 - val_loss: -3.2461e+00
Epoch 115/200
 - 1s - loss: -3.2523e+00 - val_loss: -3.2466e+00
Epoch 116/200
 - 1s - loss: -3.2530e+00 - val_loss: -3.2466e+00
Epoch 117/200
 - 1s - loss: -3.2526e+00 - val_loss: -3.2463e+00
Epoch 118/200
 - 1s - loss: -3.2525e+00 - val_loss: -3.2464e+00
Epoch 119/200
 - 1s - loss: -3.2527e+00 - val_loss: -3.2464e+00
Epoch 120/200
 - 1s - loss: -3.2525e+00 - val_loss: -3.2463e+00
Epoch 121/200
 - 1s - loss: -3.2530e+00 - val_loss: -3.2460e+00
2020-01-07 11:22:33,617 [INFO] epoch = 120. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2/ae_model_epoch_120.pickle
Epoch 122/200
 - 1s - loss: -3.2530e+00 - val_loss: -3.2461e+00
Epoch 123/200
 - 1s - loss: -3.2527e+00 - val_loss: -3.2459e+00
Epoch 124/200
 - 1s - loss: -3.2532e+00 - val_loss: -3.2455e+00
Epoch 125/200
 - 1s - loss: -3.2529e+00 - val_loss: -3.2459e+00
Epoch 126/200
 - 1s - loss: -3.2532e+00 - val_loss: -3.2465e+00
Epoch 127/200
 - 1s - loss: -3.2528e+00 - val_loss: -3.2463e+00
Epoch 128/200
 - 1s - loss: -3.2529e+00 - val_loss: -3.2465e+00
Epoch 129/200
 - 1s - loss: -3.2528e+00 - val_loss: -3.2466e+00
Epoch 130/200
 - 1s - loss: -3.2533e+00 - val_loss: -3.2466e+00
Epoch 131/200
 - 1s - loss: -3.2533e+00 - val_loss: -3.2464e+00
Epoch 132/200
 - 1s - loss: -3.2531e+00 - val_loss: -3.2466e+00
Epoch 133/200
 - 1s - loss: -3.2537e+00 - val_loss: -3.2462e+00
Epoch 134/200
 - 1s - loss: -3.2536e+00 - val_loss: -3.2464e+00
Epoch 135/200
 - 1s - loss: -3.2532e+00 - val_loss: -3.2466e+00
Epoch 136/200
 - 1s - loss: -3.2536e+00 - val_loss: -3.2466e+00
Epoch 137/200
 - 1s - loss: -3.2538e+00 - val_loss: -3.2465e+00
Epoch 138/200
 - 1s - loss: -3.2534e+00 - val_loss: -3.2465e+00
Epoch 139/200
 - 1s - loss: -3.2535e+00 - val_loss: -3.2464e+00
Epoch 140/200
 - 1s - loss: -3.2539e+00 - val_loss: -3.2465e+00
Epoch 141/200
 - 1s - loss: -3.2535e+00 - val_loss: -3.2464e+00
2020-01-07 11:23:01,359 [INFO] epoch = 140. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2/ae_model_epoch_140.pickle
Epoch 142/200
 - 1s - loss: -3.2539e+00 - val_loss: -3.2466e+00
Epoch 143/200
 - 1s - loss: -3.2538e+00 - val_loss: -3.2466e+00
Epoch 144/200
 - 1s - loss: -3.2539e+00 - val_loss: -3.2464e+00
Epoch 145/200
 - 1s - loss: -3.2541e+00 - val_loss: -3.2466e+00
Epoch 146/200
 - 1s - loss: -3.2536e+00 - val_loss: -3.2465e+00
Epoch 147/200
 - 1s - loss: -3.2536e+00 - val_loss: -3.2467e+00
Epoch 148/200
 - 1s - loss: -3.2539e+00 - val_loss: -3.2466e+00
Epoch 149/200
 - 1s - loss: -3.2537e+00 - val_loss: -3.2467e+00
Epoch 150/200
 - 1s - loss: -3.2537e+00 - val_loss: -3.2464e+00
Epoch 151/200
 - 1s - loss: -3.2543e+00 - val_loss: -3.2463e+00
Epoch 152/200
 - 1s - loss: -3.2535e+00 - val_loss: -3.2466e+00
Epoch 153/200
 - 1s - loss: -3.2542e+00 - val_loss: -3.2470e+00
Epoch 154/200
 - 1s - loss: -3.2538e+00 - val_loss: -3.2469e+00
Epoch 155/200
 - 1s - loss: -3.2542e+00 - val_loss: -3.2470e+00
Epoch 156/200
 - 1s - loss: -3.2529e+00 - val_loss: -3.2469e+00
Epoch 157/200
 - 1s - loss: -3.2542e+00 - val_loss: -3.2470e+00
Epoch 158/200
 - 1s - loss: -3.2539e+00 - val_loss: -3.2467e+00
Epoch 159/200
 - 1s - loss: -3.2541e+00 - val_loss: -3.2471e+00
Epoch 160/200
 - 1s - loss: -3.2544e+00 - val_loss: -3.2471e+00
Epoch 161/200
 - 1s - loss: -3.2543e+00 - val_loss: -3.2471e+00
2020-01-07 11:23:29,142 [INFO] epoch = 160. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2/ae_model_epoch_160.pickle
Epoch 162/200
 - 1s - loss: -3.2543e+00 - val_loss: -3.2470e+00
Epoch 163/200
 - 1s - loss: -3.2539e+00 - val_loss: -3.2468e+00
Epoch 164/200
 - 1s - loss: -3.2545e+00 - val_loss: -3.2471e+00
Epoch 165/200
 - 1s - loss: -3.2543e+00 - val_loss: -3.2469e+00
Epoch 166/200
 - 1s - loss: -3.2539e+00 - val_loss: -3.2469e+00
Epoch 167/200
 - 1s - loss: -3.2544e+00 - val_loss: -3.2472e+00
Epoch 168/200
 - 1s - loss: -3.2545e+00 - val_loss: -3.2470e+00
Epoch 169/200
 - 1s - loss: -3.2543e+00 - val_loss: -3.2470e+00
Epoch 170/200
 - 1s - loss: -3.2539e+00 - val_loss: -3.2469e+00
Epoch 171/200
 - 1s - loss: -3.2547e+00 - val_loss: -3.2465e+00
Epoch 172/200
 - 1s - loss: -3.2544e+00 - val_loss: -3.2469e+00
Epoch 173/200
 - 1s - loss: -3.2543e+00 - val_loss: -3.2462e+00
Epoch 174/200
 - 1s - loss: -3.2540e+00 - val_loss: -3.2469e+00
Epoch 175/200
 - 1s - loss: -3.2540e+00 - val_loss: -3.2467e+00
Epoch 176/200
 - 1s - loss: -3.2548e+00 - val_loss: -3.2467e+00
Epoch 177/200
 - 1s - loss: -3.2535e+00 - val_loss: -3.2471e+00
Epoch 178/200
 - 1s - loss: -3.2544e+00 - val_loss: -3.2468e+00
Epoch 179/200
 - 1s - loss: -3.2546e+00 - val_loss: -3.2469e+00
Epoch 180/200
 - 1s - loss: -3.2541e+00 - val_loss: -3.2470e+00
Epoch 181/200
 - 1s - loss: -3.2542e+00 - val_loss: -3.2469e+00
2020-01-07 11:23:56,869 [INFO] epoch = 180. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2/ae_model_epoch_180.pickle
Epoch 182/200
 - 1s - loss: -3.2544e+00 - val_loss: -3.2472e+00
Epoch 183/200
 - 1s - loss: -3.2546e+00 - val_loss: -3.2468e+00
Epoch 184/200
 - 1s - loss: -3.2548e+00 - val_loss: -3.2470e+00
Epoch 185/200
 - 1s - loss: -3.2548e+00 - val_loss: -3.2470e+00
Epoch 186/200
 - 1s - loss: -3.2548e+00 - val_loss: -3.2469e+00
Epoch 187/200
 - 1s - loss: -3.2548e+00 - val_loss: -3.2471e+00
Epoch 188/200
 - 1s - loss: -3.2545e+00 - val_loss: -3.2473e+00
Epoch 189/200
 - 1s - loss: -3.2547e+00 - val_loss: -3.2468e+00
Epoch 190/200
 - 1s - loss: -3.2542e+00 - val_loss: -3.2468e+00
Epoch 191/200
 - 1s - loss: -3.2551e+00 - val_loss: -3.2468e+00
Epoch 192/200
 - 1s - loss: -3.2545e+00 - val_loss: -3.2469e+00
Epoch 193/200
 - 1s - loss: -3.2543e+00 - val_loss: -3.2471e+00
Epoch 194/200
 - 1s - loss: -3.2541e+00 - val_loss: -3.2470e+00
Epoch 195/200
 - 1s - loss: -3.2552e+00 - val_loss: -3.2471e+00
Epoch 196/200
 - 1s - loss: -3.2550e+00 - val_loss: -3.2466e+00
Epoch 197/200
 - 1s - loss: -3.2550e+00 - val_loss: -3.2470e+00
Epoch 198/200
 - 1s - loss: -3.2546e+00 - val_loss: -3.2471e+00
Epoch 199/200
 - 1s - loss: -3.2551e+00 - val_loss: -3.2466e+00
Epoch 200/200
 - 1s - loss: -3.2549e+00 - val_loss: -3.2470e+00
2020-01-07 11:24:23,299 [INFO] WeightRestorer::on_train_end(): restoring best weights
2020-01-07 11:24:25,472 [INFO] Last epoch loss evaluation: train_loss = -3.267713, val_loss = -3.247325
2020-01-07 11:24:25,473 [INFO] Training autoencoder complete
2020-01-07 11:24:25,473 [INFO] Encoding data for supervised training
2020-01-07 11:24:26,432 [INFO] Encoding complete
2020-01-07 11:24:26,433 [INFO] Training neural network layers (after autoencoder)
Train on 25195 samples, validate on 25195 samples
Epoch 1/200
 - 1s - loss: 0.2381 - val_loss: 0.0585
 - val_f1: 0.9636
Epoch 2/200
 - 0s - loss: 0.0562 - val_loss: 0.0366
 - val_f1: 0.9727
Epoch 3/200
 - 0s - loss: 0.0392 - val_loss: 0.0260
 - val_f1: 0.9782
Epoch 4/200
 - 0s - loss: 0.0293 - val_loss: 0.0204
 - val_f1: 0.9829
Epoch 5/200
 - 0s - loss: 0.0239 - val_loss: 0.0172
 - val_f1: 0.9853
Epoch 6/200
 - 0s - loss: 0.0201 - val_loss: 0.0151
 - val_f1: 0.9869
Epoch 7/200
 - 0s - loss: 0.0181 - val_loss: 0.0129
 - val_f1: 0.9901
Epoch 8/200
 - 0s - loss: 0.0168 - val_loss: 0.0120
 - val_f1: 0.9901
Epoch 9/200
 - 0s - loss: 0.0155 - val_loss: 0.0111
 - val_f1: 0.9912
Epoch 10/200
 - 0s - loss: 0.0145 - val_loss: 0.0108
 - val_f1: 0.9909
Epoch 11/200
 - 0s - loss: 0.0131 - val_loss: 0.0100
 - val_f1: 0.9921
Epoch 12/200
 - 0s - loss: 0.0127 - val_loss: 0.0099
 - val_f1: 0.9921
Epoch 13/200
 - 0s - loss: 0.0122 - val_loss: 0.0096
 - val_f1: 0.9920
Epoch 14/200
 - 0s - loss: 0.0116 - val_loss: 0.0093
 - val_f1: 0.9926
Epoch 15/200
 - 0s - loss: 0.0113 - val_loss: 0.0097
 - val_f1: 0.9917
Epoch 16/200
 - 0s - loss: 0.0112 - val_loss: 0.0093
 - val_f1: 0.9924
Epoch 17/200
 - 0s - loss: 0.0102 - val_loss: 0.0087
 - val_f1: 0.9928
Epoch 18/200
 - 0s - loss: 0.0100 - val_loss: 0.0085
 - val_f1: 0.9932
Epoch 19/200
 - 0s - loss: 0.0099 - val_loss: 0.0086
 - val_f1: 0.9933
Epoch 20/200
 - 0s - loss: 0.0094 - val_loss: 0.0088
 - val_f1: 0.9928
Epoch 21/200
 - 0s - loss: 0.0101 - val_loss: 0.0084
/home/sunanda/anaconda3/envs/ml_env_cpu/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2020-01-07 11:24:43,063 [INFO] epoch = 20. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2/ann_model_epoch_20.pickle
 - val_f1: 0.9927
Epoch 22/200
 - 0s - loss: 0.0088 - val_loss: 0.0084
 - val_f1: 0.9927
Epoch 23/200
 - 0s - loss: 0.0091 - val_loss: 0.0086
 - val_f1: 0.9925
Epoch 24/200
 - 0s - loss: 0.0090 - val_loss: 0.0084
 - val_f1: 0.9936
Epoch 25/200
 - 0s - loss: 0.0088 - val_loss: 0.0080
 - val_f1: 0.9939
Epoch 26/200
 - 0s - loss: 0.0084 - val_loss: 0.0084
 - val_f1: 0.9933
Epoch 27/200
 - 0s - loss: 0.0084 - val_loss: 0.0081
 - val_f1: 0.9932
Epoch 28/200
 - 0s - loss: 0.0085 - val_loss: 0.0079
 - val_f1: 0.9935
Epoch 29/200
 - 0s - loss: 0.0082 - val_loss: 0.0083
 - val_f1: 0.9940
Epoch 30/200
 - 0s - loss: 0.0082 - val_loss: 0.0077
 - val_f1: 0.9937
Epoch 31/200
 - 0s - loss: 0.0076 - val_loss: 0.0085
 - val_f1: 0.9925
Epoch 32/200
 - 0s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9932
Epoch 33/200
 - 0s - loss: 0.0071 - val_loss: 0.0079
 - val_f1: 0.9931
Epoch 34/200
 - 0s - loss: 0.0072 - val_loss: 0.0080
 - val_f1: 0.9936
Epoch 35/200
 - 0s - loss: 0.0073 - val_loss: 0.0077
 - val_f1: 0.9938
Epoch 36/200
 - 0s - loss: 0.0070 - val_loss: 0.0075
 - val_f1: 0.9941
Epoch 37/200
 - 0s - loss: 0.0073 - val_loss: 0.0081
 - val_f1: 0.9935
Epoch 38/200
 - 0s - loss: 0.0075 - val_loss: 0.0081
 - val_f1: 0.9940
Epoch 39/200
 - 0s - loss: 0.0070 - val_loss: 0.0078
 - val_f1: 0.9934
Epoch 40/200
 - 0s - loss: 0.0068 - val_loss: 0.0075
 - val_f1: 0.9938
Epoch 41/200
 - 0s - loss: 0.0067 - val_loss: 0.0080
2020-01-07 11:24:57,417 [INFO] epoch = 40. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2/ann_model_epoch_40.pickle
 - val_f1: 0.9936
Epoch 42/200
 - 0s - loss: 0.0064 - val_loss: 0.0076
 - val_f1: 0.9940
Epoch 43/200
 - 0s - loss: 0.0070 - val_loss: 0.0079
 - val_f1: 0.9931
Epoch 44/200
 - 0s - loss: 0.0063 - val_loss: 0.0082
 - val_f1: 0.9939
Epoch 45/200
 - 0s - loss: 0.0061 - val_loss: 0.0076
 - val_f1: 0.9946
Epoch 46/200
 - 0s - loss: 0.0065 - val_loss: 0.0074
 - val_f1: 0.9938
Epoch 47/200
 - 0s - loss: 0.0069 - val_loss: 0.0080
 - val_f1: 0.9935
Epoch 48/200
 - 0s - loss: 0.0060 - val_loss: 0.0073
 - val_f1: 0.9943
Epoch 49/200
 - 0s - loss: 0.0066 - val_loss: 0.0075
 - val_f1: 0.9943
Epoch 50/200
 - 0s - loss: 0.0063 - val_loss: 0.0075
 - val_f1: 0.9942
Epoch 51/200
 - 0s - loss: 0.0060 - val_loss: 0.0076
 - val_f1: 0.9936
Epoch 52/200
 - 0s - loss: 0.0059 - val_loss: 0.0076
 - val_f1: 0.9943
Epoch 53/200
 - 0s - loss: 0.0059 - val_loss: 0.0075
 - val_f1: 0.9946
Epoch 54/200
 - 0s - loss: 0.0060 - val_loss: 0.0075
 - val_f1: 0.9948
Epoch 55/200
 - 0s - loss: 0.0058 - val_loss: 0.0076
 - val_f1: 0.9949
Epoch 56/200
 - 0s - loss: 0.0056 - val_loss: 0.0075
 - val_f1: 0.9945
Epoch 57/200
 - 0s - loss: 0.0056 - val_loss: 0.0080
 - val_f1: 0.9938
Epoch 58/200
 - 0s - loss: 0.0058 - val_loss: 0.0075
 - val_f1: 0.9943
Epoch 59/200
 - 0s - loss: 0.0056 - val_loss: 0.0077
 - val_f1: 0.9941
Epoch 60/200
 - 0s - loss: 0.0057 - val_loss: 0.0079
 - val_f1: 0.9942
Epoch 61/200
 - 0s - loss: 0.0058 - val_loss: 0.0083
2020-01-07 11:25:11,825 [INFO] epoch = 60. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2/ann_model_epoch_60.pickle
 - val_f1: 0.9932
Epoch 62/200
 - 0s - loss: 0.0056 - val_loss: 0.0084
 - val_f1: 0.9938
Epoch 63/200
 - 0s - loss: 0.0060 - val_loss: 0.0080
 - val_f1: 0.9943
Epoch 64/200
 - 0s - loss: 0.0054 - val_loss: 0.0075
 - val_f1: 0.9948
Epoch 65/200
 - 0s - loss: 0.0059 - val_loss: 0.0080
 - val_f1: 0.9947
Epoch 66/200
 - 0s - loss: 0.0054 - val_loss: 0.0077
 - val_f1: 0.9945
Epoch 67/200
 - 0s - loss: 0.0054 - val_loss: 0.0082
 - val_f1: 0.9939
Epoch 68/200
 - 0s - loss: 0.0053 - val_loss: 0.0085
 - val_f1: 0.9944
Epoch 69/200
 - 0s - loss: 0.0046 - val_loss: 0.0077
 - val_f1: 0.9949
Epoch 70/200
 - 0s - loss: 0.0051 - val_loss: 0.0080
 - val_f1: 0.9940
Epoch 71/200
 - 0s - loss: 0.0052 - val_loss: 0.0078
 - val_f1: 0.9944
Epoch 72/200
 - 0s - loss: 0.0049 - val_loss: 0.0080
 - val_f1: 0.9944
Epoch 73/200
 - 0s - loss: 0.0050 - val_loss: 0.0078
 - val_f1: 0.9949
Epoch 74/200
 - 0s - loss: 0.0058 - val_loss: 0.0089
 - val_f1: 0.9934
Epoch 75/200
 - 0s - loss: 0.0050 - val_loss: 0.0080
 - val_f1: 0.9942
Epoch 76/200
 - 0s - loss: 0.0047 - val_loss: 0.0080
 - val_f1: 0.9938
Epoch 77/200
 - 0s - loss: 0.0047 - val_loss: 0.0078
 - val_f1: 0.9946
Epoch 78/200
 - 0s - loss: 0.0049 - val_loss: 0.0080
 - val_f1: 0.9946
Epoch 79/200
 - 0s - loss: 0.0049 - val_loss: 0.0081
 - val_f1: 0.9948
Epoch 80/200
 - 0s - loss: 0.0048 - val_loss: 0.0079
 - val_f1: 0.9948
Epoch 81/200
 - 0s - loss: 0.0050 - val_loss: 0.0082
2020-01-07 11:25:26,200 [INFO] epoch = 80. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2/ann_model_epoch_80.pickle
 - val_f1: 0.9939
Epoch 82/200
 - 0s - loss: 0.0045 - val_loss: 0.0079
 - val_f1: 0.9949
Epoch 83/200
 - 0s - loss: 0.0044 - val_loss: 0.0080
 - val_f1: 0.9947
Epoch 84/200
 - 0s - loss: 0.0047 - val_loss: 0.0081
 - val_f1: 0.9943
Epoch 85/200
 - 0s - loss: 0.0051 - val_loss: 0.0083
 - val_f1: 0.9937
Epoch 86/200
 - 0s - loss: 0.0051 - val_loss: 0.0081
 - val_f1: 0.9946
Epoch 87/200
 - 0s - loss: 0.0047 - val_loss: 0.0089
 - val_f1: 0.9937
Epoch 88/200
 - 0s - loss: 0.0044 - val_loss: 0.0082
 - val_f1: 0.9944
Epoch 89/200
 - 0s - loss: 0.0047 - val_loss: 0.0081
 - val_f1: 0.9944
Epoch 90/200
 - 0s - loss: 0.0048 - val_loss: 0.0081
 - val_f1: 0.9948
Epoch 91/200
 - 0s - loss: 0.0043 - val_loss: 0.0085
 - val_f1: 0.9936
Epoch 92/200
 - 0s - loss: 0.0048 - val_loss: 0.0089
 - val_f1: 0.9940
Epoch 93/200
 - 0s - loss: 0.0046 - val_loss: 0.0085
 - val_f1: 0.9940
Epoch 94/200
 - 0s - loss: 0.0041 - val_loss: 0.0081
 - val_f1: 0.9942
Epoch 95/200
 - 0s - loss: 0.0045 - val_loss: 0.0088
 - val_f1: 0.9942
Epoch 96/200
 - 0s - loss: 0.0044 - val_loss: 0.0082
 - val_f1: 0.9948
Epoch 97/200
 - 0s - loss: 0.0045 - val_loss: 0.0083
 - val_f1: 0.9943
Epoch 98/200
 - 0s - loss: 0.0048 - val_loss: 0.0082
2020-01-07 11:25:38,882 [INFO] WeightRestorer::on_train_end(): restoring best weights
2020-01-07 11:25:39,951 [INFO] Last epoch loss evaluation: train_loss = 0.004275, val_loss = 0.007331
2020-01-07 11:25:39,952 [INFO] Training complete. time_to_train = 357.06 sec, 5.95 min
2020-01-07 11:25:39,962 [INFO] Model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2/best_model.pickle
2020-01-07 11:25:39,964 [INFO] Training history saved to: results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2/training_error_history.csv
2020-01-07 11:25:40,134 [INFO] Plot saved to: results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2/training_error_history.png
2020-01-07 11:25:40,302 [INFO] Plot saved to: results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2/training_f1_history.png
2020-01-07 11:25:40,302 [INFO] Making predictions on training, validation, testing data
2020-01-07 11:25:45,243 [INFO] Evaluating predictions (results)
2020-01-07 11:25:45,794 [INFO] Dataset: Testing. Classification report below
2020-01-07 11:25:45,794 [INFO] 
              precision    recall  f1-score   support

         dos       0.96      0.85      0.90      7458
      normal       0.68      0.93      0.79      9711
       probe       0.71      0.75      0.73      2421
         r2l       0.96      0.04      0.08      2421
         u2r       0.70      0.03      0.05       533

    accuracy                           0.77     22544
   macro avg       0.80      0.52      0.51     22544
weighted avg       0.81      0.77      0.73     22544

2020-01-07 11:25:45,794 [INFO] Overall accuracy (micro avg): 0.7679648687012065
2020-01-07 11:25:46,390 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.7680         0.7680                       0.7680                0.0580                   0.2320  0.7680
1     Macro avg        0.9072         0.8031                       0.5208                0.0767                   0.4792  0.5108
2  Weighted avg        0.8681         0.8069                       0.7680                0.1512                   0.2320  0.7257
2020-01-07 11:25:47,062 [INFO] Dataset: Validation. Classification report below
2020-01-07 11:25:47,063 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00      9186
      normal       0.99      1.00      0.99     13469
       probe       0.99      0.99      0.99      2331
         r2l       0.90      0.82      0.86       199
         u2r       0.50      0.30      0.37        10

    accuracy                           0.99     25195
   macro avg       0.88      0.82      0.84     25195
weighted avg       0.99      0.99      0.99     25195

2020-01-07 11:25:47,063 [INFO] Overall accuracy (micro avg): 0.9943639611033935
2020-01-07 11:25:47,732 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9944         0.9944                       0.9944                0.0014                   0.0056  0.9944
1     Macro avg        0.9977         0.8759                       0.8201                0.0019                   0.1799  0.8426
2  Weighted avg        0.9966         0.9942                       0.9944                0.0039                   0.0056  0.9943
2020-01-07 11:25:50,557 [INFO] Dataset: Training. Classification report below
2020-01-07 11:25:50,560 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00     36741
      normal       0.99      1.00      1.00     53874
       probe       0.99      0.99      0.99      9325
         r2l       0.90      0.80      0.84       796
         u2r       0.62      0.38      0.47        42

    accuracy                           0.99    100778
   macro avg       0.90      0.83      0.86    100778
weighted avg       0.99      0.99      0.99    100778

2020-01-07 11:25:50,560 [INFO] Overall accuracy (micro avg): 0.994681378872373
2020-01-07 11:25:53,588 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9947         0.9947                       0.9947                0.0013                   0.0053  0.9947
1     Macro avg        0.9979         0.8992                       0.8318                0.0019                   0.1682  0.8594
2  Weighted avg        0.9968         0.9945                       0.9947                0.0041                   0.0053  0.9946
2020-01-07 11:25:53,636 [INFO] Results saved to: results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2/semi_sup_perf_nsl_ae_ann_rep2_results.xlsx
2020-01-07 11:25:53,636 [INFO] ================= Finished running experiment no. 2 ================= 

2020-01-07 11:25:53,637 [INFO] Created directory: results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3
2020-01-07 11:25:53,637 [INFO] Initialized logging. log_filename = results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/run_log.log
2020-01-07 11:25:53,637 [INFO] ================= Running experiment no. 3  ================= 

2020-01-07 11:25:53,637 [INFO] Experiment parameters given below
2020-01-07 11:25:53,637 [INFO] 
{'experiment_num': 3, 'results_dir': 'results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3', 'model_type': 'classifier', 'model': 'ae_ann', 'normal_label': 'normal', 'split_random_seed': 42, 'unsupervised_ratio': 0.75, 'ae_encoder_units': [64], 'ae_encoder_activations': ['relu'], 'ae_encoder_dropout_rates': [0.2], 'ae_encoder_l1_param': -1, 'ae_decoder_units': '', 'ae_decoder_activations': '', 'ae_decoder_dropout_rates': '', 'output_activation': 'sigmoid', 'loss_function': 'binary_crossentropy', 'ae_epochs': 200, 'ann_layer_units': [64], 'ann_layer_activations': ['relu'], 'ann_layer_dropout_rates': [0.2], 'ann_epochs': 200, 'early_stop_patience': 50, 'batch_size': 256, 'dataset_dir': '../Datasets/small_datasets/nsl_kdd_five_classes', 'description': 'semi_sup_perf_nsl_ae_ann_rep3'}
2020-01-07 11:25:53,637 [INFO] Created tensorboard log directory: results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/tf_logs_run_2020_01_07-11_25_53
2020-01-07 11:25:53,638 [INFO] Loading datsets from: ../Datasets/small_datasets/nsl_kdd_five_classes
2020-01-07 11:25:53,638 [INFO] Reading X, y files
2020-01-07 11:25:53,638 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_train.h5
2020-01-07 11:25:53,869 [INFO] Reading complete. time_to_read=0.23 seconds
2020-01-07 11:25:53,869 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_val.h5
2020-01-07 11:25:53,932 [INFO] Reading complete. time_to_read=0.06 seconds
2020-01-07 11:25:53,932 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_test.h5
2020-01-07 11:25:53,989 [INFO] Reading complete. time_to_read=0.06 seconds
2020-01-07 11:25:53,989 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_train.h5
2020-01-07 11:25:53,997 [INFO] Reading complete. time_to_read=0.01 seconds
2020-01-07 11:25:53,997 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_val.h5
2020-01-07 11:25:54,002 [INFO] Reading complete. time_to_read=0.00 seconds
2020-01-07 11:25:54,002 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_test.h5
2020-01-07 11:25:54,006 [INFO] Reading complete. time_to_read=0.00 seconds
2020-01-07 11:25:54,121 [INFO] Initializing model
2020-01-07 11:25:54,244 [INFO] _________________________________________________________________
2020-01-07 11:25:54,244 [INFO] Layer (type)                 Output Shape              Param #   
2020-01-07 11:25:54,244 [INFO] =================================================================
2020-01-07 11:25:54,245 [INFO] dense_9 (Dense)              (None, 64)                7872      
2020-01-07 11:25:54,245 [INFO] _________________________________________________________________
2020-01-07 11:25:54,245 [INFO] batch_normalization_5 (Batch (None, 64)                256       
2020-01-07 11:25:54,245 [INFO] _________________________________________________________________
2020-01-07 11:25:54,245 [INFO] dropout_5 (Dropout)          (None, 64)                0         
2020-01-07 11:25:54,245 [INFO] _________________________________________________________________
2020-01-07 11:25:54,245 [INFO] dense_10 (Dense)             (None, 122)               7930      
2020-01-07 11:25:54,245 [INFO] =================================================================
2020-01-07 11:25:54,245 [INFO] Total params: 16,058
2020-01-07 11:25:54,245 [INFO] Trainable params: 15,930
2020-01-07 11:25:54,245 [INFO] Non-trainable params: 128
2020-01-07 11:25:54,245 [INFO] _________________________________________________________________
2020-01-07 11:25:54,364 [INFO] _________________________________________________________________
2020-01-07 11:25:54,364 [INFO] Layer (type)                 Output Shape              Param #   
2020-01-07 11:25:54,364 [INFO] =================================================================
2020-01-07 11:25:54,365 [INFO] dense_11 (Dense)             (None, 64)                4160      
2020-01-07 11:25:54,365 [INFO] _________________________________________________________________
2020-01-07 11:25:54,365 [INFO] batch_normalization_6 (Batch (None, 64)                256       
2020-01-07 11:25:54,365 [INFO] _________________________________________________________________
2020-01-07 11:25:54,365 [INFO] dropout_6 (Dropout)          (None, 64)                0         
2020-01-07 11:25:54,365 [INFO] _________________________________________________________________
2020-01-07 11:25:54,365 [INFO] dense_12 (Dense)             (None, 5)                 325       
2020-01-07 11:25:54,365 [INFO] =================================================================
2020-01-07 11:25:54,365 [INFO] Total params: 4,741
2020-01-07 11:25:54,365 [INFO] Trainable params: 4,613
2020-01-07 11:25:54,365 [INFO] Non-trainable params: 128
2020-01-07 11:25:54,365 [INFO] _________________________________________________________________
2020-01-07 11:25:54,366 [INFO] Training model
2020-01-07 11:25:54,366 [INFO] Splitting train set into 2 sets (unsupervised, supervised), random_seed = 42
2020-01-07 11:25:55,433 [INFO] Split sizes (instances). total = 100778, unsupervised = 75583, supervised = 25195, unsupervised dataset hash = 40fcc5b399aca9d2e6f9581990505c20b3f01fda
2020-01-07 11:25:55,434 [INFO] Training autoencoder
 - val_f1: 0.9945
Epoch 00098: early stopping
Train on 75583 samples, validate on 25195 samples
Epoch 1/200
 - 2s - loss: -3.8019e-01 - val_loss: -1.7084e+00
Epoch 2/200
 - 1s - loss: -2.2301e+00 - val_loss: -2.7838e+00
Epoch 3/200
 - 1s - loss: -2.8991e+00 - val_loss: -3.0875e+00
Epoch 4/200
 - 1s - loss: -3.0744e+00 - val_loss: -3.1605e+00
Epoch 5/200
 - 1s - loss: -3.1327e+00 - val_loss: -3.1898e+00
Epoch 6/200
 - 1s - loss: -3.1612e+00 - val_loss: -3.2051e+00
Epoch 7/200
 - 1s - loss: -3.1785e+00 - val_loss: -3.2142e+00
Epoch 8/200
 - 1s - loss: -3.1896e+00 - val_loss: -3.2196e+00
Epoch 9/200
 - 1s - loss: -3.1976e+00 - val_loss: -3.2232e+00
Epoch 10/200
 - 1s - loss: -3.2042e+00 - val_loss: -3.2266e+00
Epoch 11/200
 - 1s - loss: -3.2089e+00 - val_loss: -3.2289e+00
Epoch 12/200
 - 1s - loss: -3.2123e+00 - val_loss: -3.2304e+00
Epoch 13/200
 - 1s - loss: -3.2158e+00 - val_loss: -3.2317e+00
Epoch 14/200
 - 1s - loss: -3.2190e+00 - val_loss: -3.2329e+00
Epoch 15/200
 - 1s - loss: -3.2211e+00 - val_loss: -3.2345e+00
Epoch 16/200
 - 1s - loss: -3.2245e+00 - val_loss: -3.2350e+00
Epoch 17/200
 - 1s - loss: -3.2254e+00 - val_loss: -3.2363e+00
Epoch 18/200
 - 1s - loss: -3.2275e+00 - val_loss: -3.2370e+00
Epoch 19/200
 - 1s - loss: -3.2287e+00 - val_loss: -3.2377e+00
Epoch 20/200
 - 1s - loss: -3.2301e+00 - val_loss: -3.2380e+00
Epoch 21/200
 - 1s - loss: -3.2318e+00 - val_loss: -3.2386e+00
2020-01-07 11:26:27,270 [INFO] epoch = 20. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/ae_model_epoch_20.pickle
Epoch 22/200
 - 1s - loss: -3.2323e+00 - val_loss: -3.2385e+00
Epoch 23/200
 - 1s - loss: -3.2340e+00 - val_loss: -3.2393e+00
Epoch 24/200
 - 1s - loss: -3.2346e+00 - val_loss: -3.2394e+00
Epoch 25/200
 - 1s - loss: -3.2356e+00 - val_loss: -3.2399e+00
Epoch 26/200
 - 1s - loss: -3.2369e+00 - val_loss: -3.2408e+00
Epoch 27/200
 - 1s - loss: -3.2374e+00 - val_loss: -3.2410e+00
Epoch 28/200
 - 1s - loss: -3.2382e+00 - val_loss: -3.2412e+00
Epoch 29/200
 - 1s - loss: -3.2383e+00 - val_loss: -3.2414e+00
Epoch 30/200
 - 1s - loss: -3.2388e+00 - val_loss: -3.2415e+00
Epoch 31/200
 - 1s - loss: -3.2398e+00 - val_loss: -3.2416e+00
Epoch 32/200
 - 1s - loss: -3.2407e+00 - val_loss: -3.2420e+00
Epoch 33/200
 - 1s - loss: -3.2412e+00 - val_loss: -3.2420e+00
Epoch 34/200
 - 1s - loss: -3.2409e+00 - val_loss: -3.2424e+00
Epoch 35/200
 - 1s - loss: -3.2416e+00 - val_loss: -3.2425e+00
Epoch 36/200
 - 1s - loss: -3.2423e+00 - val_loss: -3.2429e+00
Epoch 37/200
 - 1s - loss: -3.2424e+00 - val_loss: -3.2429e+00
Epoch 38/200
 - 1s - loss: -3.2427e+00 - val_loss: -3.2428e+00
Epoch 39/200
 - 1s - loss: -3.2435e+00 - val_loss: -3.2431e+00
Epoch 40/200
 - 1s - loss: -3.2438e+00 - val_loss: -3.2432e+00
Epoch 41/200
 - 1s - loss: -3.2440e+00 - val_loss: -3.2434e+00
2020-01-07 11:26:55,299 [INFO] epoch = 40. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/ae_model_epoch_40.pickle
Epoch 42/200
 - 1s - loss: -3.2446e+00 - val_loss: -3.2435e+00
Epoch 43/200
 - 1s - loss: -3.2436e+00 - val_loss: -3.2434e+00
Epoch 44/200
 - 1s - loss: -3.2449e+00 - val_loss: -3.2437e+00
Epoch 45/200
 - 1s - loss: -3.2458e+00 - val_loss: -3.2438e+00
Epoch 46/200
 - 1s - loss: -3.2456e+00 - val_loss: -3.2441e+00
Epoch 47/200
 - 1s - loss: -3.2460e+00 - val_loss: -3.2438e+00
Epoch 48/200
 - 1s - loss: -3.2451e+00 - val_loss: -3.2441e+00
Epoch 49/200
 - 1s - loss: -3.2456e+00 - val_loss: -3.2440e+00
Epoch 50/200
 - 1s - loss: -3.2466e+00 - val_loss: -3.2442e+00
Epoch 51/200
 - 1s - loss: -3.2467e+00 - val_loss: -3.2442e+00
Epoch 52/200
 - 1s - loss: -3.2469e+00 - val_loss: -3.2445e+00
Epoch 53/200
 - 1s - loss: -3.2466e+00 - val_loss: -3.2447e+00
Epoch 54/200
 - 1s - loss: -3.2476e+00 - val_loss: -3.2449e+00
Epoch 55/200
 - 1s - loss: -3.2475e+00 - val_loss: -3.2449e+00
Epoch 56/200
 - 1s - loss: -3.2469e+00 - val_loss: -3.2449e+00
Epoch 57/200
 - 1s - loss: -3.2478e+00 - val_loss: -3.2446e+00
Epoch 58/200
 - 1s - loss: -3.2478e+00 - val_loss: -3.2447e+00
Epoch 59/200
 - 1s - loss: -3.2482e+00 - val_loss: -3.2451e+00
Epoch 60/200
 - 1s - loss: -3.2486e+00 - val_loss: -3.2447e+00
Epoch 61/200
 - 1s - loss: -3.2485e+00 - val_loss: -3.2452e+00
2020-01-07 11:27:23,532 [INFO] epoch = 60. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/ae_model_epoch_60.pickle
Epoch 62/200
 - 1s - loss: -3.2485e+00 - val_loss: -3.2449e+00
Epoch 63/200
 - 1s - loss: -3.2489e+00 - val_loss: -3.2447e+00
Epoch 64/200
 - 1s - loss: -3.2486e+00 - val_loss: -3.2448e+00
Epoch 65/200
 - 1s - loss: -3.2492e+00 - val_loss: -3.2452e+00
Epoch 66/200
 - 1s - loss: -3.2489e+00 - val_loss: -3.2450e+00
Epoch 67/200
 - 1s - loss: -3.2487e+00 - val_loss: -3.2449e+00
Epoch 68/200
 - 1s - loss: -3.2490e+00 - val_loss: -3.2448e+00
Epoch 69/200
 - 1s - loss: -3.2496e+00 - val_loss: -3.2448e+00
Epoch 70/200
 - 1s - loss: -3.2498e+00 - val_loss: -3.2451e+00
Epoch 71/200
 - 1s - loss: -3.2496e+00 - val_loss: -3.2452e+00
Epoch 72/200
 - 1s - loss: -3.2499e+00 - val_loss: -3.2451e+00
Epoch 73/200
 - 1s - loss: -3.2499e+00 - val_loss: -3.2453e+00
Epoch 74/200
 - 1s - loss: -3.2498e+00 - val_loss: -3.2452e+00
Epoch 75/200
 - 1s - loss: -3.2504e+00 - val_loss: -3.2451e+00
Epoch 76/200
 - 1s - loss: -3.2499e+00 - val_loss: -3.2456e+00
Epoch 77/200
 - 1s - loss: -3.2503e+00 - val_loss: -3.2458e+00
Epoch 78/200
 - 1s - loss: -3.2505e+00 - val_loss: -3.2455e+00
Epoch 79/200
 - 1s - loss: -3.2508e+00 - val_loss: -3.2459e+00
Epoch 80/200
 - 1s - loss: -3.2507e+00 - val_loss: -3.2456e+00
Epoch 81/200
 - 1s - loss: -3.2508e+00 - val_loss: -3.2460e+00
2020-01-07 11:27:51,742 [INFO] epoch = 80. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/ae_model_epoch_80.pickle
Epoch 82/200
 - 1s - loss: -3.2511e+00 - val_loss: -3.2454e+00
Epoch 83/200
 - 1s - loss: -3.2512e+00 - val_loss: -3.2457e+00
Epoch 84/200
 - 1s - loss: -3.2513e+00 - val_loss: -3.2458e+00
Epoch 85/200
 - 1s - loss: -3.2511e+00 - val_loss: -3.2459e+00
Epoch 86/200
 - 1s - loss: -3.2502e+00 - val_loss: -3.2458e+00
Epoch 87/200
 - 1s - loss: -3.2511e+00 - val_loss: -3.2460e+00
Epoch 88/200
 - 1s - loss: -3.2512e+00 - val_loss: -3.2460e+00
Epoch 89/200
 - 1s - loss: -3.2515e+00 - val_loss: -3.2458e+00
Epoch 90/200
 - 1s - loss: -3.2507e+00 - val_loss: -3.2462e+00
Epoch 91/200
 - 1s - loss: -3.2516e+00 - val_loss: -3.2461e+00
Epoch 92/200
 - 1s - loss: -3.2515e+00 - val_loss: -3.2462e+00
Epoch 93/200
 - 1s - loss: -3.2518e+00 - val_loss: -3.2461e+00
Epoch 94/200
 - 1s - loss: -3.2516e+00 - val_loss: -3.2463e+00
Epoch 95/200
 - 1s - loss: -3.2514e+00 - val_loss: -3.2462e+00
Epoch 96/200
 - 1s - loss: -3.2519e+00 - val_loss: -3.2460e+00
Epoch 97/200
 - 1s - loss: -3.2520e+00 - val_loss: -3.2461e+00
Epoch 98/200
 - 1s - loss: -3.2519e+00 - val_loss: -3.2461e+00
Epoch 99/200
 - 1s - loss: -3.2523e+00 - val_loss: -3.2463e+00
Epoch 100/200
 - 1s - loss: -3.2524e+00 - val_loss: -3.2465e+00
Epoch 101/200
 - 1s - loss: -3.2521e+00 - val_loss: -3.2466e+00
2020-01-07 11:28:19,984 [INFO] epoch = 100. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/ae_model_epoch_100.pickle
Epoch 102/200
 - 1s - loss: -3.2523e+00 - val_loss: -3.2466e+00
Epoch 103/200
 - 1s - loss: -3.2522e+00 - val_loss: -3.2468e+00
Epoch 104/200
 - 1s - loss: -3.2519e+00 - val_loss: -3.2467e+00
Epoch 105/200
 - 1s - loss: -3.2525e+00 - val_loss: -3.2464e+00
Epoch 106/200
 - 1s - loss: -3.2526e+00 - val_loss: -3.2460e+00
Epoch 107/200
 - 1s - loss: -3.2521e+00 - val_loss: -3.2465e+00
Epoch 108/200
 - 1s - loss: -3.2524e+00 - val_loss: -3.2465e+00
Epoch 109/200
 - 1s - loss: -3.2524e+00 - val_loss: -3.2464e+00
Epoch 110/200
 - 1s - loss: -3.2526e+00 - val_loss: -3.2466e+00
Epoch 111/200
 - 1s - loss: -3.2524e+00 - val_loss: -3.2464e+00
Epoch 112/200
 - 1s - loss: -3.2532e+00 - val_loss: -3.2463e+00
Epoch 113/200
 - 1s - loss: -3.2530e+00 - val_loss: -3.2464e+00
Epoch 114/200
 - 1s - loss: -3.2527e+00 - val_loss: -3.2467e+00
Epoch 115/200
 - 1s - loss: -3.2519e+00 - val_loss: -3.2461e+00
Epoch 116/200
 - 1s - loss: -3.2530e+00 - val_loss: -3.2466e+00
Epoch 117/200
 - 1s - loss: -3.2530e+00 - val_loss: -3.2468e+00
Epoch 118/200
 - 1s - loss: -3.2530e+00 - val_loss: -3.2466e+00
Epoch 119/200
 - 1s - loss: -3.2531e+00 - val_loss: -3.2465e+00
Epoch 120/200
 - 1s - loss: -3.2531e+00 - val_loss: -3.2466e+00
Epoch 121/200
 - 1s - loss: -3.2530e+00 - val_loss: -3.2467e+00
2020-01-07 11:28:48,204 [INFO] epoch = 120. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/ae_model_epoch_120.pickle
Epoch 122/200
 - 1s - loss: -3.2535e+00 - val_loss: -3.2470e+00
Epoch 123/200
 - 1s - loss: -3.2530e+00 - val_loss: -3.2466e+00
Epoch 124/200
 - 1s - loss: -3.2531e+00 - val_loss: -3.2468e+00
Epoch 125/200
 - 1s - loss: -3.2534e+00 - val_loss: -3.2467e+00
Epoch 126/200
 - 1s - loss: -3.2534e+00 - val_loss: -3.2464e+00
Epoch 127/200
 - 1s - loss: -3.2526e+00 - val_loss: -3.2465e+00
Epoch 128/200
 - 1s - loss: -3.2534e+00 - val_loss: -3.2463e+00
Epoch 129/200
 - 1s - loss: -3.2537e+00 - val_loss: -3.2462e+00
Epoch 130/200
 - 1s - loss: -3.2536e+00 - val_loss: -3.2467e+00
Epoch 131/200
 - 1s - loss: -3.2535e+00 - val_loss: -3.2467e+00
Epoch 132/200
 - 1s - loss: -3.2535e+00 - val_loss: -3.2464e+00
Epoch 133/200
 - 1s - loss: -3.2536e+00 - val_loss: -3.2466e+00
Epoch 134/200
 - 1s - loss: -3.2537e+00 - val_loss: -3.2467e+00
Epoch 135/200
 - 1s - loss: -3.2539e+00 - val_loss: -3.2467e+00
Epoch 136/200
 - 1s - loss: -3.2539e+00 - val_loss: -3.2467e+00
Epoch 137/200
 - 1s - loss: -3.2535e+00 - val_loss: -3.2466e+00
Epoch 138/200
 - 1s - loss: -3.2537e+00 - val_loss: -3.2467e+00
Epoch 139/200
 - 1s - loss: -3.2536e+00 - val_loss: -3.2467e+00
Epoch 140/200
 - 1s - loss: -3.2538e+00 - val_loss: -3.2466e+00
Epoch 141/200
 - 1s - loss: -3.2539e+00 - val_loss: -3.2466e+00
2020-01-07 11:29:16,392 [INFO] epoch = 140. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/ae_model_epoch_140.pickle
Epoch 142/200
 - 1s - loss: -3.2537e+00 - val_loss: -3.2467e+00
Epoch 143/200
 - 1s - loss: -3.2542e+00 - val_loss: -3.2469e+00
Epoch 144/200
 - 1s - loss: -3.2544e+00 - val_loss: -3.2472e+00
Epoch 145/200
 - 1s - loss: -3.2543e+00 - val_loss: -3.2470e+00
Epoch 146/200
 - 1s - loss: -3.2541e+00 - val_loss: -3.2472e+00
Epoch 147/200
 - 1s - loss: -3.2542e+00 - val_loss: -3.2469e+00
Epoch 148/200
 - 1s - loss: -3.2543e+00 - val_loss: -3.2469e+00
Epoch 149/200
 - 1s - loss: -3.2537e+00 - val_loss: -3.2468e+00
Epoch 150/200
 - 1s - loss: -3.2540e+00 - val_loss: -3.2469e+00
Epoch 151/200
 - 1s - loss: -3.2546e+00 - val_loss: -3.2470e+00
Epoch 152/200
 - 1s - loss: -3.2533e+00 - val_loss: -3.2470e+00
Epoch 153/200
 - 1s - loss: -3.2542e+00 - val_loss: -3.2470e+00
Epoch 154/200
 - 1s - loss: -3.2545e+00 - val_loss: -3.2470e+00
Epoch 155/200
 - 1s - loss: -3.2542e+00 - val_loss: -3.2470e+00
Epoch 156/200
 - 1s - loss: -3.2543e+00 - val_loss: -3.2470e+00
Epoch 157/200
 - 1s - loss: -3.2544e+00 - val_loss: -3.2469e+00
Epoch 158/200
 - 1s - loss: -3.2544e+00 - val_loss: -3.2472e+00
Epoch 159/200
 - 1s - loss: -3.2543e+00 - val_loss: -3.2468e+00
Epoch 160/200
 - 1s - loss: -3.2547e+00 - val_loss: -3.2473e+00
Epoch 161/200
 - 1s - loss: -3.2546e+00 - val_loss: -3.2473e+00
2020-01-07 11:29:44,632 [INFO] epoch = 160. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/ae_model_epoch_160.pickle
Epoch 162/200
 - 1s - loss: -3.2543e+00 - val_loss: -3.2473e+00
Epoch 163/200
 - 1s - loss: -3.2544e+00 - val_loss: -3.2472e+00
Epoch 164/200
 - 1s - loss: -3.2548e+00 - val_loss: -3.2473e+00
Epoch 165/200
 - 1s - loss: -3.2549e+00 - val_loss: -3.2474e+00
Epoch 166/200
 - 1s - loss: -3.2547e+00 - val_loss: -3.2471e+00
Epoch 167/200
 - 1s - loss: -3.2547e+00 - val_loss: -3.2474e+00
Epoch 168/200
 - 1s - loss: -3.2548e+00 - val_loss: -3.2473e+00
Epoch 169/200
 - 1s - loss: -3.2549e+00 - val_loss: -3.2474e+00
Epoch 170/200
 - 1s - loss: -3.2547e+00 - val_loss: -3.2474e+00
Epoch 171/200
 - 1s - loss: -3.2549e+00 - val_loss: -3.2473e+00
Epoch 172/200
 - 1s - loss: -3.2546e+00 - val_loss: -3.2469e+00
Epoch 173/200
 - 1s - loss: -3.2548e+00 - val_loss: -3.2473e+00
Epoch 174/200
 - 1s - loss: -3.2546e+00 - val_loss: -3.2471e+00
Epoch 175/200
 - 1s - loss: -3.2546e+00 - val_loss: -3.2475e+00
Epoch 176/200
 - 1s - loss: -3.2541e+00 - val_loss: -3.2475e+00
Epoch 177/200
 - 1s - loss: -3.2551e+00 - val_loss: -3.2474e+00
Epoch 178/200
 - 1s - loss: -3.2549e+00 - val_loss: -3.2475e+00
Epoch 179/200
 - 1s - loss: -3.2550e+00 - val_loss: -3.2476e+00
Epoch 180/200
 - 1s - loss: -3.2550e+00 - val_loss: -3.2473e+00
Epoch 181/200
 - 1s - loss: -3.2550e+00 - val_loss: -3.2475e+00
2020-01-07 11:30:12,835 [INFO] epoch = 180. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/ae_model_epoch_180.pickle
Epoch 182/200
 - 1s - loss: -3.2551e+00 - val_loss: -3.2473e+00
Epoch 183/200
 - 1s - loss: -3.2551e+00 - val_loss: -3.2473e+00
Epoch 184/200
 - 1s - loss: -3.2553e+00 - val_loss: -3.2476e+00
Epoch 185/200
 - 1s - loss: -3.2551e+00 - val_loss: -3.2475e+00
Epoch 186/200
 - 1s - loss: -3.2554e+00 - val_loss: -3.2473e+00
Epoch 187/200
 - 1s - loss: -3.2552e+00 - val_loss: -3.2474e+00
Epoch 188/200
 - 1s - loss: -3.2547e+00 - val_loss: -3.2473e+00
Epoch 189/200
 - 1s - loss: -3.2548e+00 - val_loss: -3.2472e+00
Epoch 190/200
 - 1s - loss: -3.2553e+00 - val_loss: -3.2474e+00
Epoch 191/200
 - 1s - loss: -3.2551e+00 - val_loss: -3.2475e+00
Epoch 192/200
 - 1s - loss: -3.2549e+00 - val_loss: -3.2474e+00
Epoch 193/200
 - 1s - loss: -3.2554e+00 - val_loss: -3.2470e+00
Epoch 194/200
 - 1s - loss: -3.2555e+00 - val_loss: -3.2473e+00
Epoch 195/200
 - 1s - loss: -3.2553e+00 - val_loss: -3.2476e+00
Epoch 196/200
 - 1s - loss: -3.2556e+00 - val_loss: -3.2474e+00
Epoch 197/200
 - 1s - loss: -3.2555e+00 - val_loss: -3.2475e+00
Epoch 198/200
 - 1s - loss: -3.2556e+00 - val_loss: -3.2476e+00
Epoch 199/200
 - 1s - loss: -3.2553e+00 - val_loss: -3.2474e+00
Epoch 200/200
 - 1s - loss: -3.2553e+00 - val_loss: -3.2474e+00
2020-01-07 11:30:39,703 [INFO] WeightRestorer::on_train_end(): restoring best weights
2020-01-07 11:30:42,154 [INFO] Last epoch loss evaluation: train_loss = -3.267841, val_loss = -3.247647
2020-01-07 11:30:42,154 [INFO] Training autoencoder complete
2020-01-07 11:30:42,154 [INFO] Encoding data for supervised training
2020-01-07 11:30:43,217 [INFO] Encoding complete
2020-01-07 11:30:43,217 [INFO] Training neural network layers (after autoencoder)
Train on 25195 samples, validate on 25195 samples
Epoch 1/200
 - 1s - loss: 0.1724 - val_loss: 0.0542
 - val_f1: 0.9612
Epoch 2/200
 - 0s - loss: 0.0512 - val_loss: 0.0329
 - val_f1: 0.9722
Epoch 3/200
 - 0s - loss: 0.0347 - val_loss: 0.0222
 - val_f1: 0.9807
Epoch 4/200
 - 0s - loss: 0.0255 - val_loss: 0.0165
 - val_f1: 0.9882
Epoch 5/200
 - 0s - loss: 0.0206 - val_loss: 0.0140
 - val_f1: 0.9902
Epoch 6/200
 - 0s - loss: 0.0176 - val_loss: 0.0128
 - val_f1: 0.9903
Epoch 7/200
 - 0s - loss: 0.0158 - val_loss: 0.0121
 - val_f1: 0.9912
Epoch 8/200
 - 0s - loss: 0.0149 - val_loss: 0.0112
 - val_f1: 0.9911
Epoch 9/200
 - 0s - loss: 0.0135 - val_loss: 0.0108
 - val_f1: 0.9916
Epoch 10/200
 - 0s - loss: 0.0132 - val_loss: 0.0104
 - val_f1: 0.9910
Epoch 11/200
 - 0s - loss: 0.0119 - val_loss: 0.0101
 - val_f1: 0.9922
Epoch 12/200
 - 0s - loss: 0.0114 - val_loss: 0.0099
 - val_f1: 0.9916
Epoch 13/200
 - 0s - loss: 0.0109 - val_loss: 0.0101
 - val_f1: 0.9913
Epoch 14/200
 - 0s - loss: 0.0106 - val_loss: 0.0099
 - val_f1: 0.9916
Epoch 15/200
 - 0s - loss: 0.0098 - val_loss: 0.0099
 - val_f1: 0.9919
Epoch 16/200
 - 0s - loss: 0.0099 - val_loss: 0.0093
 - val_f1: 0.9933
Epoch 17/200
 - 0s - loss: 0.0098 - val_loss: 0.0099
 - val_f1: 0.9925
Epoch 18/200
 - 0s - loss: 0.0093 - val_loss: 0.0093
 - val_f1: 0.9931
Epoch 19/200
 - 0s - loss: 0.0087 - val_loss: 0.0095
 - val_f1: 0.9928
Epoch 20/200
 - 0s - loss: 0.0089 - val_loss: 0.0093
 - val_f1: 0.9927
Epoch 21/200
 - 0s - loss: 0.0086 - val_loss: 0.0092
/home/sunanda/anaconda3/envs/ml_env_cpu/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2020-01-07 11:31:01,609 [INFO] epoch = 20. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/ann_model_epoch_20.pickle
 - val_f1: 0.9931
Epoch 22/200
 - 0s - loss: 0.0084 - val_loss: 0.0092
 - val_f1: 0.9926
Epoch 23/200
 - 0s - loss: 0.0091 - val_loss: 0.0091
 - val_f1: 0.9931
Epoch 24/200
 - 0s - loss: 0.0085 - val_loss: 0.0097
 - val_f1: 0.9932
Epoch 25/200
 - 0s - loss: 0.0081 - val_loss: 0.0085
 - val_f1: 0.9932
Epoch 26/200
 - 0s - loss: 0.0083 - val_loss: 0.0091
 - val_f1: 0.9928
Epoch 27/200
 - 0s - loss: 0.0077 - val_loss: 0.0088
 - val_f1: 0.9936
Epoch 28/200
 - 0s - loss: 0.0082 - val_loss: 0.0088
 - val_f1: 0.9929
Epoch 29/200
 - 0s - loss: 0.0078 - val_loss: 0.0089
 - val_f1: 0.9927
Epoch 30/200
 - 0s - loss: 0.0075 - val_loss: 0.0091
 - val_f1: 0.9933
Epoch 31/200
 - 0s - loss: 0.0080 - val_loss: 0.0091
 - val_f1: 0.9930
Epoch 32/200
 - 0s - loss: 0.0077 - val_loss: 0.0091
 - val_f1: 0.9935
Epoch 33/200
 - 0s - loss: 0.0071 - val_loss: 0.0092
 - val_f1: 0.9937
Epoch 34/200
 - 0s - loss: 0.0069 - val_loss: 0.0099
 - val_f1: 0.9933
Epoch 35/200
 - 0s - loss: 0.0068 - val_loss: 0.0088
 - val_f1: 0.9941
Epoch 36/200
 - 0s - loss: 0.0070 - val_loss: 0.0091
 - val_f1: 0.9935
Epoch 37/200
 - 0s - loss: 0.0066 - val_loss: 0.0090
 - val_f1: 0.9938
Epoch 38/200
 - 0s - loss: 0.0066 - val_loss: 0.0092
 - val_f1: 0.9941
Epoch 39/200
 - 0s - loss: 0.0066 - val_loss: 0.0090
 - val_f1: 0.9939
Epoch 40/200
 - 0s - loss: 0.0065 - val_loss: 0.0092
 - val_f1: 0.9935
Epoch 41/200
 - 0s - loss: 0.0069 - val_loss: 0.0097
2020-01-07 11:31:17,309 [INFO] epoch = 40. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/ann_model_epoch_40.pickle
 - val_f1: 0.9937
Epoch 42/200
 - 0s - loss: 0.0065 - val_loss: 0.0090
 - val_f1: 0.9934
Epoch 43/200
 - 0s - loss: 0.0063 - val_loss: 0.0095
 - val_f1: 0.9930
Epoch 44/200
 - 0s - loss: 0.0064 - val_loss: 0.0087
 - val_f1: 0.9941
Epoch 45/200
 - 0s - loss: 0.0063 - val_loss: 0.0091
 - val_f1: 0.9928
Epoch 46/200
 - 0s - loss: 0.0065 - val_loss: 0.0085
 - val_f1: 0.9941
Epoch 47/200
 - 0s - loss: 0.0062 - val_loss: 0.0084
 - val_f1: 0.9942
Epoch 48/200
 - 0s - loss: 0.0057 - val_loss: 0.0090
 - val_f1: 0.9934
Epoch 49/200
 - 0s - loss: 0.0058 - val_loss: 0.0087
 - val_f1: 0.9943
Epoch 50/200
 - 0s - loss: 0.0059 - val_loss: 0.0088
 - val_f1: 0.9941
Epoch 51/200
 - 0s - loss: 0.0055 - val_loss: 0.0086
 - val_f1: 0.9943
Epoch 52/200
 - 0s - loss: 0.0055 - val_loss: 0.0088
 - val_f1: 0.9931
Epoch 53/200
 - 0s - loss: 0.0059 - val_loss: 0.0087
 - val_f1: 0.9944
Epoch 54/200
 - 0s - loss: 0.0056 - val_loss: 0.0088
 - val_f1: 0.9939
Epoch 55/200
 - 0s - loss: 0.0050 - val_loss: 0.0091
 - val_f1: 0.9935
Epoch 56/200
 - 0s - loss: 0.0059 - val_loss: 0.0092
 - val_f1: 0.9941
Epoch 57/200
 - 0s - loss: 0.0054 - val_loss: 0.0087
 - val_f1: 0.9937
Epoch 58/200
 - 0s - loss: 0.0056 - val_loss: 0.0088
 - val_f1: 0.9942
Epoch 59/200
 - 0s - loss: 0.0053 - val_loss: 0.0095
 - val_f1: 0.9932
Epoch 60/200
 - 0s - loss: 0.0054 - val_loss: 0.0084
 - val_f1: 0.9947
Epoch 61/200
 - 0s - loss: 0.0054 - val_loss: 0.0091
2020-01-07 11:31:32,957 [INFO] epoch = 60. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/ann_model_epoch_60.pickle
 - val_f1: 0.9939
Epoch 62/200
 - 0s - loss: 0.0048 - val_loss: 0.0083
 - val_f1: 0.9946
Epoch 63/200
 - 0s - loss: 0.0050 - val_loss: 0.0086
 - val_f1: 0.9945
Epoch 64/200
 - 0s - loss: 0.0052 - val_loss: 0.0086
 - val_f1: 0.9944
Epoch 65/200
 - 0s - loss: 0.0056 - val_loss: 0.0095
 - val_f1: 0.9934
Epoch 66/200
 - 0s - loss: 0.0054 - val_loss: 0.0090
 - val_f1: 0.9937
Epoch 67/200
 - 0s - loss: 0.0053 - val_loss: 0.0101
 - val_f1: 0.9935
Epoch 68/200
 - 0s - loss: 0.0051 - val_loss: 0.0094
 - val_f1: 0.9940
Epoch 69/200
 - 0s - loss: 0.0052 - val_loss: 0.0097
 - val_f1: 0.9941
Epoch 70/200
 - 0s - loss: 0.0052 - val_loss: 0.0096
 - val_f1: 0.9939
Epoch 71/200
 - 0s - loss: 0.0052 - val_loss: 0.0093
 - val_f1: 0.9945
Epoch 72/200
 - 0s - loss: 0.0051 - val_loss: 0.0092
 - val_f1: 0.9941
Epoch 73/200
 - 0s - loss: 0.0048 - val_loss: 0.0097
 - val_f1: 0.9940
Epoch 74/200
 - 0s - loss: 0.0051 - val_loss: 0.0093
 - val_f1: 0.9939
Epoch 75/200
 - 0s - loss: 0.0049 - val_loss: 0.0087
 - val_f1: 0.9947
Epoch 76/200
 - 0s - loss: 0.0049 - val_loss: 0.0092
 - val_f1: 0.9940
Epoch 77/200
 - 0s - loss: 0.0053 - val_loss: 0.0090
 - val_f1: 0.9943
Epoch 78/200
 - 0s - loss: 0.0049 - val_loss: 0.0095
 - val_f1: 0.9938
Epoch 79/200
 - 0s - loss: 0.0052 - val_loss: 0.0089
 - val_f1: 0.9941
Epoch 80/200
 - 0s - loss: 0.0049 - val_loss: 0.0089
 - val_f1: 0.9938
Epoch 81/200
 - 0s - loss: 0.0048 - val_loss: 0.0094
2020-01-07 11:31:48,633 [INFO] epoch = 80. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/ann_model_epoch_80.pickle
 - val_f1: 0.9943
Epoch 82/200
 - 0s - loss: 0.0052 - val_loss: 0.0096
 - val_f1: 0.9934
Epoch 83/200
 - 0s - loss: 0.0047 - val_loss: 0.0090
 - val_f1: 0.9942
Epoch 84/200
 - 0s - loss: 0.0044 - val_loss: 0.0095
 - val_f1: 0.9938
Epoch 85/200
 - 0s - loss: 0.0046 - val_loss: 0.0091
 - val_f1: 0.9942
Epoch 86/200
 - 0s - loss: 0.0047 - val_loss: 0.0089
 - val_f1: 0.9939
Epoch 87/200
 - 0s - loss: 0.0045 - val_loss: 0.0089
 - val_f1: 0.9944
Epoch 88/200
 - 0s - loss: 0.0046 - val_loss: 0.0090
 - val_f1: 0.9945
Epoch 89/200
 - 0s - loss: 0.0044 - val_loss: 0.0095
 - val_f1: 0.9942
Epoch 90/200
 - 0s - loss: 0.0045 - val_loss: 0.0095
 - val_f1: 0.9940
Epoch 91/200
 - 0s - loss: 0.0047 - val_loss: 0.0094
 - val_f1: 0.9942
Epoch 92/200
 - 0s - loss: 0.0045 - val_loss: 0.0097
 - val_f1: 0.9939
Epoch 93/200
 - 0s - loss: 0.0049 - val_loss: 0.0094
 - val_f1: 0.9939
Epoch 94/200
 - 0s - loss: 0.0044 - val_loss: 0.0092
 - val_f1: 0.9933
Epoch 95/200
 - 0s - loss: 0.0042 - val_loss: 0.0087
 - val_f1: 0.9949
Epoch 96/200
 - 0s - loss: 0.0044 - val_loss: 0.0091
 - val_f1: 0.9942
Epoch 97/200
 - 0s - loss: 0.0044 - val_loss: 0.0092
 - val_f1: 0.9943
Epoch 98/200
 - 0s - loss: 0.0041 - val_loss: 0.0089
 - val_f1: 0.9944
Epoch 99/200
 - 0s - loss: 0.0040 - val_loss: 0.0093
 - val_f1: 0.9943
Epoch 100/200
 - 0s - loss: 0.0042 - val_loss: 0.0082
 - val_f1: 0.9949
Epoch 101/200
 - 0s - loss: 0.0046 - val_loss: 0.0086
2020-01-07 11:32:04,338 [INFO] epoch = 100. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/ann_model_epoch_100.pickle
 - val_f1: 0.9942
Epoch 102/200
 - 0s - loss: 0.0040 - val_loss: 0.0086
 - val_f1: 0.9946
Epoch 103/200
 - 0s - loss: 0.0041 - val_loss: 0.0086
 - val_f1: 0.9946
Epoch 104/200
 - 0s - loss: 0.0044 - val_loss: 0.0087
 - val_f1: 0.9945
Epoch 105/200
 - 0s - loss: 0.0040 - val_loss: 0.0086
 - val_f1: 0.9947
Epoch 106/200
 - 0s - loss: 0.0044 - val_loss: 0.0089
 - val_f1: 0.9944
Epoch 107/200
 - 0s - loss: 0.0044 - val_loss: 0.0091
 - val_f1: 0.9938
Epoch 108/200
 - 0s - loss: 0.0038 - val_loss: 0.0089
 - val_f1: 0.9942
Epoch 109/200
 - 0s - loss: 0.0046 - val_loss: 0.0090
 - val_f1: 0.9947
Epoch 110/200
 - 0s - loss: 0.0046 - val_loss: 0.0095
 - val_f1: 0.9941
Epoch 111/200
 - 0s - loss: 0.0041 - val_loss: 0.0094
 - val_f1: 0.9941
Epoch 112/200
 - 0s - loss: 0.0043 - val_loss: 0.0088
 - val_f1: 0.9943
Epoch 113/200
 - 0s - loss: 0.0041 - val_loss: 0.0087
 - val_f1: 0.9946
Epoch 114/200
 - 0s - loss: 0.0038 - val_loss: 0.0092
 - val_f1: 0.9944
Epoch 115/200
 - 0s - loss: 0.0041 - val_loss: 0.0089
 - val_f1: 0.9947
Epoch 116/200
 - 0s - loss: 0.0044 - val_loss: 0.0089
 - val_f1: 0.9950
Epoch 117/200
 - 0s - loss: 0.0044 - val_loss: 0.0091
 - val_f1: 0.9946
Epoch 118/200
 - 0s - loss: 0.0039 - val_loss: 0.0088
 - val_f1: 0.9945
Epoch 119/200
 - 0s - loss: 0.0041 - val_loss: 0.0090
 - val_f1: 0.9946
Epoch 120/200
 - 0s - loss: 0.0038 - val_loss: 0.0093
 - val_f1: 0.9947
Epoch 121/200
 - 0s - loss: 0.0042 - val_loss: 0.0094
2020-01-07 11:32:20,062 [INFO] epoch = 120. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/ann_model_epoch_120.pickle
 - val_f1: 0.9938
Epoch 122/200
 - 0s - loss: 0.0038 - val_loss: 0.0091
 - val_f1: 0.9948
Epoch 123/200
 - 0s - loss: 0.0042 - val_loss: 0.0095
 - val_f1: 0.9937
Epoch 124/200
 - 0s - loss: 0.0042 - val_loss: 0.0088
 - val_f1: 0.9948
Epoch 125/200
 - 0s - loss: 0.0036 - val_loss: 0.0093
 - val_f1: 0.9950
Epoch 126/200
 - 0s - loss: 0.0039 - val_loss: 0.0094
 - val_f1: 0.9950
Epoch 127/200
 - 0s - loss: 0.0043 - val_loss: 0.0094
 - val_f1: 0.9945
Epoch 128/200
 - 0s - loss: 0.0041 - val_loss: 0.0094
 - val_f1: 0.9943
Epoch 129/200
 - 0s - loss: 0.0040 - val_loss: 0.0098
 - val_f1: 0.9937
Epoch 130/200
 - 0s - loss: 0.0044 - val_loss: 0.0100
 - val_f1: 0.9933
Epoch 131/200
 - 0s - loss: 0.0040 - val_loss: 0.0092
 - val_f1: 0.9947
Epoch 132/200
 - 0s - loss: 0.0039 - val_loss: 0.0094
 - val_f1: 0.9943
Epoch 133/200
 - 0s - loss: 0.0036 - val_loss: 0.0089
 - val_f1: 0.9949
Epoch 134/200
 - 0s - loss: 0.0034 - val_loss: 0.0092
 - val_f1: 0.9942
Epoch 135/200
 - 0s - loss: 0.0044 - val_loss: 0.0091
 - val_f1: 0.9949
Epoch 136/200
 - 0s - loss: 0.0038 - val_loss: 0.0089
 - val_f1: 0.9949
Epoch 137/200
 - 0s - loss: 0.0038 - val_loss: 0.0092
 - val_f1: 0.9950
Epoch 138/200
 - 0s - loss: 0.0037 - val_loss: 0.0098
 - val_f1: 0.9935
Epoch 139/200
 - 0s - loss: 0.0041 - val_loss: 0.0091
 - val_f1: 0.9945
Epoch 140/200
 - 0s - loss: 0.0037 - val_loss: 0.0090
 - val_f1: 0.9944
Epoch 141/200
 - 0s - loss: 0.0036 - val_loss: 0.0092
2020-01-07 11:32:35,735 [INFO] epoch = 140. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/ann_model_epoch_140.pickle
 - val_f1: 0.9946
Epoch 142/200
 - 0s - loss: 0.0034 - val_loss: 0.0094
 - val_f1: 0.9940
Epoch 143/200
 - 0s - loss: 0.0037 - val_loss: 0.0088
 - val_f1: 0.9951
Epoch 144/200
 - 0s - loss: 0.0036 - val_loss: 0.0094
 - val_f1: 0.9946
Epoch 145/200
 - 0s - loss: 0.0036 - val_loss: 0.0090
 - val_f1: 0.9949
Epoch 146/200
 - 0s - loss: 0.0033 - val_loss: 0.0092
 - val_f1: 0.9951
Epoch 147/200
 - 0s - loss: 0.0041 - val_loss: 0.0097
 - val_f1: 0.9936
Epoch 148/200
 - 0s - loss: 0.0038 - val_loss: 0.0096
 - val_f1: 0.9938
Epoch 149/200
 - 0s - loss: 0.0038 - val_loss: 0.0094
 - val_f1: 0.9948
Epoch 150/200
 - 0s - loss: 0.0035 - val_loss: 0.0092
2020-01-07 11:32:43,259 [INFO] WeightRestorer::on_train_end(): restoring best weights
2020-01-07 11:32:44,523 [INFO] Last epoch loss evaluation: train_loss = 0.002790, val_loss = 0.008227
2020-01-07 11:32:44,524 [INFO] Training complete. time_to_train = 410.16 sec, 6.84 min
2020-01-07 11:32:44,537 [INFO] Model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/best_model.pickle
2020-01-07 11:32:44,541 [INFO] Training history saved to: results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/training_error_history.csv
2020-01-07 11:32:44,730 [INFO] Plot saved to: results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/training_error_history.png
2020-01-07 11:32:44,912 [INFO] Plot saved to: results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/training_f1_history.png
2020-01-07 11:32:44,912 [INFO] Making predictions on training, validation, testing data
2020-01-07 11:32:50,381 [INFO] Evaluating predictions (results)
2020-01-07 11:32:50,932 [INFO] Dataset: Testing. Classification report below
2020-01-07 11:32:50,932 [INFO] 
              precision    recall  f1-score   support

         dos       0.95      0.83      0.89      7458
      normal       0.68      0.95      0.79      9711
       probe       0.75      0.64      0.69      2421
         r2l       0.97      0.19      0.32      2421
         u2r       0.92      0.02      0.04       533

    accuracy                           0.77     22544
   macro avg       0.86      0.53      0.55     22544
weighted avg       0.82      0.77      0.74     22544

2020-01-07 11:32:50,932 [INFO] Overall accuracy (micro avg): 0.7728442157558553
2020-01-07 11:32:51,528 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.7728         0.7728                       0.7728                0.0568                   0.2272  0.7728
1     Macro avg        0.9091         0.8572                       0.5264                0.0761                   0.4736  0.5472
2  Weighted avg        0.8688         0.8163                       0.7728                0.1536                   0.2272  0.7449
2020-01-07 11:32:52,201 [INFO] Dataset: Validation. Classification report below
2020-01-07 11:32:52,201 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00      9186
      normal       1.00      1.00      1.00     13469
       probe       0.99      0.99      0.99      2331
         r2l       0.90      0.86      0.88       199
         u2r       1.00      0.20      0.33        10

    accuracy                           1.00     25195
   macro avg       0.98      0.81      0.84     25195
weighted avg       1.00      1.00      0.99     25195

2020-01-07 11:32:52,201 [INFO] Overall accuracy (micro avg): 0.9950783885691605
2020-01-07 11:32:52,871 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9951         0.9951                       0.9951                0.0012                   0.0049  0.9951
1     Macro avg        0.9980         0.9756                       0.8096                0.0016                   0.1904  0.8392
2  Weighted avg        0.9970         0.9951                       0.9951                0.0032                   0.0049  0.9950
2020-01-07 11:32:55,696 [INFO] Dataset: Training. Classification report below
2020-01-07 11:32:55,696 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00     36741
      normal       1.00      1.00      1.00     53874
       probe       0.99      0.99      0.99      9325
         r2l       0.89      0.84      0.87       796
         u2r       0.81      0.60      0.68        42

    accuracy                           1.00    100778
   macro avg       0.94      0.88      0.91    100778
weighted avg       1.00      1.00      1.00    100778

2020-01-07 11:32:55,696 [INFO] Overall accuracy (micro avg): 0.9955645081267737
2020-01-07 11:32:58,723 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9956         0.9956                       0.9956                0.0011                   0.0044  0.9956
1     Macro avg        0.9982         0.9369                       0.8850                0.0015                   0.1150  0.9076
2  Weighted avg        0.9973         0.9955                       0.9956                0.0032                   0.0044  0.9955
2020-01-07 11:32:58,771 [INFO] Results saved to: results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/semi_sup_perf_nsl_ae_ann_rep3_results.xlsx
2020-01-07 11:32:58,771 [INFO] ================= Finished running experiment no. 3 ================= 

2020-01-07 11:32:58,772 [INFO] Created directory: results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1
2020-01-07 11:32:58,772 [INFO] Initialized logging. log_filename = results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/run_log.log
2020-01-07 11:32:58,772 [INFO] ================= Running experiment no. 1  ================= 

2020-01-07 11:32:58,772 [INFO] Experiment parameters given below
2020-01-07 11:32:58,772 [INFO] 
{'experiment_num': 1, 'results_dir': 'results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1', 'model_type': 'classifier', 'model': 'ae_ann', 'normal_label': 'BENIGN', 'split_random_seed': 42, 'unsupervised_ratio': 0.75, 'ae_encoder_units': [64], 'ae_encoder_activations': ['relu'], 'ae_encoder_dropout_rates': [0.2], 'ae_encoder_l1_param': -1, 'ae_decoder_units': '', 'ae_decoder_activations': '', 'ae_decoder_dropout_rates': '', 'output_activation': 'sigmoid', 'loss_function': 'binary_crossentropy', 'ae_epochs': 200, 'ann_layer_units': [64], 'ann_layer_activations': ['relu'], 'ann_layer_dropout_rates': [0.2], 'ann_epochs': 200, 'early_stop_patience': 50, 'batch_size': 256, 'dataset_dir': '../Datasets/full_datasets/ids2017', 'description': 'semi_sup_perf_ids17_ae_ann_rep1'}
2020-01-07 11:32:58,772 [INFO] Created tensorboard log directory: results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/tf_logs_run_2020_01_07-11_32_58
2020-01-07 11:32:58,772 [INFO] Loading datsets from: ../Datasets/full_datasets/ids2017
2020-01-07 11:32:58,773 [INFO] Reading X, y files
2020-01-07 11:32:58,773 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_train.h5
2020-01-07 11:33:02,587 [INFO] Reading complete. time_to_read=3.81 seconds
2020-01-07 11:33:02,587 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_val.h5
2020-01-07 11:33:03,871 [INFO] Reading complete. time_to_read=1.28 seconds
2020-01-07 11:33:03,872 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_test.h5
2020-01-07 11:33:05,157 [INFO] Reading complete. time_to_read=1.29 seconds
2020-01-07 11:33:05,157 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_train.h5
2020-01-07 11:33:05,417 [INFO] Reading complete. time_to_read=0.26 seconds
2020-01-07 11:33:05,417 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_val.h5
2020-01-07 11:33:05,505 [INFO] Reading complete. time_to_read=0.09 seconds
2020-01-07 11:33:05,505 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_test.h5
2020-01-07 11:33:05,589 [INFO] Reading complete. time_to_read=0.08 seconds
2020-01-07 11:33:08,551 [INFO] Initializing model
2020-01-07 11:33:08,676 [INFO] _________________________________________________________________
2020-01-07 11:33:08,676 [INFO] Layer (type)                 Output Shape              Param #   
2020-01-07 11:33:08,676 [INFO] =================================================================
2020-01-07 11:33:08,676 [INFO] dense_13 (Dense)             (None, 64)                5056      
2020-01-07 11:33:08,676 [INFO] _________________________________________________________________
2020-01-07 11:33:08,676 [INFO] batch_normalization_7 (Batch (None, 64)                256       
2020-01-07 11:33:08,676 [INFO] _________________________________________________________________
2020-01-07 11:33:08,676 [INFO] dropout_7 (Dropout)          (None, 64)                0         
2020-01-07 11:33:08,676 [INFO] _________________________________________________________________
2020-01-07 11:33:08,677 [INFO] dense_14 (Dense)             (None, 78)                5070      
2020-01-07 11:33:08,677 [INFO] =================================================================
2020-01-07 11:33:08,677 [INFO] Total params: 10,382
2020-01-07 11:33:08,677 [INFO] Trainable params: 10,254
2020-01-07 11:33:08,677 [INFO] Non-trainable params: 128
2020-01-07 11:33:08,677 [INFO] _________________________________________________________________
2020-01-07 11:33:08,797 [INFO] _________________________________________________________________
2020-01-07 11:33:08,797 [INFO] Layer (type)                 Output Shape              Param #   
2020-01-07 11:33:08,797 [INFO] =================================================================
2020-01-07 11:33:08,797 [INFO] dense_15 (Dense)             (None, 64)                4160      
2020-01-07 11:33:08,797 [INFO] _________________________________________________________________
2020-01-07 11:33:08,797 [INFO] batch_normalization_8 (Batch (None, 64)                256       
2020-01-07 11:33:08,797 [INFO] _________________________________________________________________
2020-01-07 11:33:08,797 [INFO] dropout_8 (Dropout)          (None, 64)                0         
2020-01-07 11:33:08,797 [INFO] _________________________________________________________________
2020-01-07 11:33:08,798 [INFO] dense_16 (Dense)             (None, 12)                780       
2020-01-07 11:33:08,798 [INFO] =================================================================
2020-01-07 11:33:08,798 [INFO] Total params: 5,196
2020-01-07 11:33:08,798 [INFO] Trainable params: 5,068
2020-01-07 11:33:08,798 [INFO] Non-trainable params: 128
2020-01-07 11:33:08,798 [INFO] _________________________________________________________________
2020-01-07 11:33:08,798 [INFO] Training model
2020-01-07 11:33:08,798 [INFO] Splitting train set into 2 sets (unsupervised, supervised), random_seed = 42
2020-01-07 11:33:33,121 [INFO] Split sizes (instances). total = 1696684, unsupervised = 1272513, supervised = 424171, unsupervised dataset hash = 07eca414d7fa8ed059bf34ec454d369f9e002f85
2020-01-07 11:33:33,121 [INFO] Training autoencoder
 - val_f1: 0.9949
Epoch 00150: early stopping
Train on 1272513 samples, validate on 565562 samples
Epoch 1/200
 - 23s - loss: -3.7944e+00 - val_loss: -4.1430e+00
Epoch 2/200
 - 22s - loss: -4.1139e+00 - val_loss: -4.1526e+00
Epoch 3/200
 - 22s - loss: -4.1249e+00 - val_loss: -4.1571e+00
Epoch 4/200
 - 22s - loss: -4.1298e+00 - val_loss: -4.1588e+00
Epoch 5/200
 - 22s - loss: -4.1323e+00 - val_loss: -4.1599e+00
Epoch 6/200
 - 22s - loss: -4.1343e+00 - val_loss: -4.1619e+00
Epoch 7/200
 - 22s - loss: -4.1357e+00 - val_loss: -4.1599e+00
Epoch 8/200
 - 22s - loss: -4.1365e+00 - val_loss: -4.1619e+00
Epoch 9/200
 - 22s - loss: -4.1372e+00 - val_loss: -4.1614e+00
Epoch 10/200
 - 22s - loss: -4.1379e+00 - val_loss: -4.1608e+00
Epoch 11/200
 - 22s - loss: -4.1383e+00 - val_loss: -4.1616e+00
Epoch 12/200
 - 22s - loss: -4.1388e+00 - val_loss: -4.1607e+00
Epoch 13/200
 - 22s - loss: -4.1393e+00 - val_loss: -4.1632e+00
Epoch 14/200
 - 22s - loss: -4.1396e+00 - val_loss: -4.1630e+00
Epoch 15/200
 - 22s - loss: -4.1402e+00 - val_loss: -4.1651e+00
Epoch 16/200
 - 22s - loss: -4.1404e+00 - val_loss: -4.1637e+00
Epoch 17/200
 - 22s - loss: -4.1406e+00 - val_loss: -4.1653e+00
Epoch 18/200
 - 22s - loss: -4.1408e+00 - val_loss: -4.1651e+00
Epoch 19/200
 - 22s - loss: -4.1412e+00 - val_loss: -4.1644e+00
Epoch 20/200
 - 22s - loss: -4.1412e+00 - val_loss: -4.1647e+00
Epoch 21/200
 - 22s - loss: -4.1414e+00 - val_loss: -4.1656e+00
2020-01-07 11:41:19,763 [INFO] epoch = 20. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/ae_model_epoch_20.pickle
Epoch 22/200
 - 22s - loss: -4.1418e+00 - val_loss: -4.1650e+00
Epoch 23/200
 - 22s - loss: -4.1419e+00 - val_loss: -4.1627e+00
Epoch 24/200
 - 22s - loss: -4.1422e+00 - val_loss: -4.1648e+00
Epoch 25/200
 - 22s - loss: -4.1423e+00 - val_loss: -4.1640e+00
Epoch 26/200
 - 22s - loss: -4.1421e+00 - val_loss: -4.1646e+00
Epoch 27/200
 - 22s - loss: -4.1423e+00 - val_loss: -4.1641e+00
Epoch 28/200
 - 22s - loss: -4.1425e+00 - val_loss: -4.1643e+00
Epoch 29/200
 - 22s - loss: -4.1426e+00 - val_loss: -4.1646e+00
Epoch 30/200
 - 22s - loss: -4.1427e+00 - val_loss: -4.1642e+00
Epoch 31/200
 - 22s - loss: -4.1428e+00 - val_loss: -4.1666e+00
Epoch 32/200
 - 22s - loss: -4.1430e+00 - val_loss: -4.1650e+00
Epoch 33/200
 - 22s - loss: -4.1429e+00 - val_loss: -4.1667e+00
Epoch 34/200
 - 22s - loss: -4.1429e+00 - val_loss: -4.1654e+00
Epoch 35/200
 - 22s - loss: -4.1432e+00 - val_loss: -4.1657e+00
Epoch 36/200
 - 22s - loss: -4.1433e+00 - val_loss: -4.1640e+00
Epoch 37/200
 - 22s - loss: -4.1434e+00 - val_loss: -4.1661e+00
Epoch 38/200
 - 22s - loss: -4.1434e+00 - val_loss: -4.1666e+00
Epoch 39/200
 - 22s - loss: -4.1436e+00 - val_loss: -4.1666e+00
Epoch 40/200
 - 22s - loss: -4.1436e+00 - val_loss: -4.1658e+00
Epoch 41/200
 - 22s - loss: -4.1435e+00 - val_loss: -4.1651e+00
2020-01-07 11:48:45,756 [INFO] epoch = 40. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/ae_model_epoch_40.pickle
Epoch 42/200
 - 22s - loss: -4.1438e+00 - val_loss: -4.1671e+00
Epoch 43/200
 - 22s - loss: -4.1438e+00 - val_loss: -4.1643e+00
Epoch 44/200
 - 22s - loss: -4.1439e+00 - val_loss: -4.1642e+00
Epoch 45/200
 - 22s - loss: -4.1439e+00 - val_loss: -4.1651e+00
Epoch 46/200
 - 22s - loss: -4.1440e+00 - val_loss: -4.1657e+00
Epoch 47/200
 - 22s - loss: -4.1440e+00 - val_loss: -4.1663e+00
Epoch 48/200
 - 22s - loss: -4.1441e+00 - val_loss: -4.1663e+00
Epoch 49/200
 - 22s - loss: -4.1441e+00 - val_loss: -4.1665e+00
Epoch 50/200
 - 22s - loss: -4.1441e+00 - val_loss: -4.1663e+00
Epoch 51/200
 - 22s - loss: -4.1439e+00 - val_loss: -4.1662e+00
Epoch 52/200
 - 22s - loss: -4.1442e+00 - val_loss: -4.1655e+00
Epoch 53/200
 - 22s - loss: -4.1445e+00 - val_loss: -4.1660e+00
Epoch 54/200
 - 22s - loss: -4.1445e+00 - val_loss: -4.1658e+00
Epoch 55/200
 - 22s - loss: -4.1444e+00 - val_loss: -4.1663e+00
Epoch 56/200
 - 22s - loss: -4.1443e+00 - val_loss: -4.1656e+00
Epoch 57/200
 - 22s - loss: -4.1444e+00 - val_loss: -4.1656e+00
Epoch 58/200
 - 22s - loss: -4.1445e+00 - val_loss: -4.1664e+00
Epoch 59/200
 - 22s - loss: -4.1447e+00 - val_loss: -4.1646e+00
Epoch 60/200
 - 22s - loss: -4.1446e+00 - val_loss: -4.1651e+00
Epoch 61/200
 - 22s - loss: -4.1446e+00 - val_loss: -4.1662e+00
2020-01-07 11:56:11,677 [INFO] epoch = 60. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/ae_model_epoch_60.pickle
Epoch 62/200
 - 22s - loss: -4.1445e+00 - val_loss: -4.1665e+00
Epoch 63/200
 - 22s - loss: -4.1448e+00 - val_loss: -4.1669e+00
Epoch 64/200
 - 22s - loss: -4.1448e+00 - val_loss: -4.1666e+00
Epoch 65/200
 - 22s - loss: -4.1447e+00 - val_loss: -4.1647e+00
Epoch 66/200
 - 22s - loss: -4.1448e+00 - val_loss: -4.1660e+00
Epoch 67/200
 - 22s - loss: -4.1448e+00 - val_loss: -4.1653e+00
Epoch 68/200
 - 22s - loss: -4.1449e+00 - val_loss: -4.1657e+00
Epoch 69/200
 - 22s - loss: -4.1448e+00 - val_loss: -4.1602e+00
Epoch 70/200
 - 22s - loss: -4.1449e+00 - val_loss: -4.1655e+00
Epoch 71/200
 - 22s - loss: -4.1449e+00 - val_loss: -4.1661e+00
Epoch 72/200
 - 22s - loss: -4.1448e+00 - val_loss: -4.1654e+00
Epoch 73/200
 - 22s - loss: -4.1449e+00 - val_loss: -4.1631e+00
Epoch 74/200
 - 22s - loss: -4.1449e+00 - val_loss: -4.1650e+00
Epoch 75/200
 - 22s - loss: -4.1450e+00 - val_loss: -4.1663e+00
Epoch 76/200
 - 22s - loss: -4.1448e+00 - val_loss: -4.1668e+00
Epoch 77/200
 - 22s - loss: -4.1449e+00 - val_loss: -4.1668e+00
Epoch 78/200
 - 22s - loss: -4.1451e+00 - val_loss: -4.1614e+00
Epoch 79/200
 - 22s - loss: -4.1451e+00 - val_loss: -4.1657e+00
Epoch 80/200
 - 22s - loss: -4.1451e+00 - val_loss: -4.1668e+00
Epoch 81/200
 - 22s - loss: -4.1452e+00 - val_loss: -4.1657e+00
2020-01-07 12:03:37,875 [INFO] epoch = 80. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/ae_model_epoch_80.pickle
Epoch 82/200
 - 22s - loss: -4.1451e+00 - val_loss: -4.1648e+00
Epoch 83/200
 - 22s - loss: -4.1449e+00 - val_loss: -4.1665e+00
Epoch 84/200
 - 22s - loss: -4.1451e+00 - val_loss: -4.1656e+00
Epoch 85/200
 - 22s - loss: -4.1451e+00 - val_loss: -4.1663e+00
Epoch 86/200
 - 22s - loss: -4.1452e+00 - val_loss: -4.1599e+00
Epoch 87/200
 - 22s - loss: -4.1452e+00 - val_loss: -4.1661e+00
Epoch 88/200
 - 22s - loss: -4.1452e+00 - val_loss: -4.1663e+00
Epoch 89/200
 - 22s - loss: -4.1447e+00 - val_loss: -4.1666e+00
Epoch 90/200
 - 22s - loss: -4.1449e+00 - val_loss: -4.1667e+00
Epoch 91/200
 - 22s - loss: -4.1452e+00 - val_loss: -4.1671e+00
Epoch 92/200
 - 22s - loss: -4.1453e+00 - val_loss: -4.1665e+00
Epoch 93/200
 - 22s - loss: -4.1449e+00 - val_loss: -4.1659e+00
Epoch 94/200
 - 22s - loss: -4.1453e+00 - val_loss: -4.1654e+00
Epoch 95/200
 - 22s - loss: -4.1453e+00 - val_loss: -4.1659e+00
Epoch 96/200
 - 22s - loss: -4.1450e+00 - val_loss: -4.1630e+00
Epoch 97/200
 - 22s - loss: -4.1452e+00 - val_loss: -4.1665e+00
Epoch 98/200
 - 22s - loss: -4.1449e+00 - val_loss: -4.1667e+00
Epoch 99/200
 - 22s - loss: -4.1453e+00 - val_loss: -4.1653e+00
Epoch 100/200
 - 22s - loss: -4.1453e+00 - val_loss: -4.1662e+00
Epoch 101/200
 - 22s - loss: -4.1451e+00 - val_loss: -4.1662e+00
2020-01-07 12:11:03,812 [INFO] epoch = 100. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/ae_model_epoch_100.pickle
Epoch 102/200
 - 22s - loss: -4.1454e+00 - val_loss: -4.1633e+00
Epoch 103/200
 - 22s - loss: -4.1453e+00 - val_loss: -4.1661e+00
Epoch 104/200
 - 22s - loss: -4.1450e+00 - val_loss: -4.1670e+00
Epoch 105/200
 - 22s - loss: -4.1454e+00 - val_loss: -4.1656e+00
Epoch 106/200
 - 22s - loss: -4.1454e+00 - val_loss: -4.1663e+00
Epoch 107/200
 - 22s - loss: -4.1454e+00 - val_loss: -4.1667e+00
Epoch 108/200
 - 22s - loss: -4.1454e+00 - val_loss: -4.1661e+00
Epoch 109/200
 - 22s - loss: -4.1453e+00 - val_loss: -4.1586e+00
Epoch 110/200
 - 22s - loss: -4.1450e+00 - val_loss: -4.1651e+00
Epoch 111/200
 - 22s - loss: -4.1454e+00 - val_loss: -4.1653e+00
Epoch 112/200
 - 22s - loss: -4.1451e+00 - val_loss: -4.1662e+00
Epoch 113/200
 - 22s - loss: -4.1455e+00 - val_loss: -4.1671e+00
Epoch 114/200
 - 22s - loss: -4.1455e+00 - val_loss: -4.1662e+00
Epoch 115/200
 - 22s - loss: -4.1455e+00 - val_loss: -4.1651e+00
Epoch 116/200
 - 22s - loss: -4.1456e+00 - val_loss: -4.1658e+00
Epoch 117/200
 - 22s - loss: -4.1455e+00 - val_loss: -4.1635e+00
Epoch 118/200
 - 22s - loss: -4.1456e+00 - val_loss: -4.1658e+00
Epoch 119/200
 - 22s - loss: -4.1455e+00 - val_loss: -4.1646e+00
Epoch 120/200
 - 22s - loss: -4.1455e+00 - val_loss: -4.1593e+00
Epoch 121/200
 - 22s - loss: -4.1458e+00 - val_loss: -4.1663e+00
2020-01-07 12:18:29,940 [INFO] epoch = 120. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/ae_model_epoch_120.pickle
Epoch 122/200
 - 22s - loss: -4.1454e+00 - val_loss: -4.1663e+00
Epoch 123/200
 - 22s - loss: -4.1456e+00 - val_loss: -4.1652e+00
Epoch 124/200
 - 22s - loss: -4.1456e+00 - val_loss: -4.1669e+00
Epoch 125/200
 - 22s - loss: -4.1456e+00 - val_loss: -4.1639e+00
Epoch 126/200
 - 22s - loss: -4.1454e+00 - val_loss: -4.1624e+00
Epoch 127/200
 - 22s - loss: -4.1456e+00 - val_loss: -4.1667e+00
Epoch 128/200
 - 22s - loss: -4.1452e+00 - val_loss: -4.1592e+00
Epoch 129/200
 - 22s - loss: -4.1457e+00 - val_loss: -4.1648e+00
Epoch 130/200
 - 22s - loss: -4.1456e+00 - val_loss: -4.1667e+00
Epoch 131/200
 - 22s - loss: -4.1455e+00 - val_loss: -4.1661e+00
Epoch 132/200
 - 22s - loss: -4.1454e+00 - val_loss: -4.1641e+00
Epoch 133/200
 - 22s - loss: -4.1457e+00 - val_loss: -4.1651e+00
Epoch 134/200
 - 22s - loss: -4.1457e+00 - val_loss: -4.1658e+00
Epoch 135/200
 - 22s - loss: -4.1457e+00 - val_loss: -4.1663e+00
Epoch 136/200
 - 22s - loss: -4.1458e+00 - val_loss: -4.1587e+00
Epoch 137/200
 - 22s - loss: -4.1456e+00 - val_loss: -4.1588e+00
Epoch 138/200
 - 22s - loss: -4.1456e+00 - val_loss: -4.1589e+00
Epoch 139/200
 - 22s - loss: -4.1454e+00 - val_loss: -4.1657e+00
Epoch 140/200
 - 22s - loss: -4.1456e+00 - val_loss: -4.1590e+00
Epoch 141/200
 - 22s - loss: -4.1458e+00 - val_loss: -4.1657e+00
2020-01-07 12:25:55,874 [INFO] epoch = 140. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/ae_model_epoch_140.pickle
2020-01-07 12:25:55,874 [INFO] WeightRestorer::on_train_end(): restoring best weights
2020-01-07 12:26:43,318 [INFO] Last epoch loss evaluation: train_loss = -4.165374, val_loss = -4.167122
2020-01-07 12:26:43,318 [INFO] Training autoencoder complete
2020-01-07 12:26:43,318 [INFO] Encoding data for supervised training
2020-01-07 12:27:02,921 [INFO] Encoding complete
2020-01-07 12:27:02,922 [INFO] Training neural network layers (after autoencoder)
Epoch 00141: early stopping
Train on 424171 samples, validate on 565562 samples
Epoch 1/200
 - 7s - loss: 0.0306 - val_loss: 0.0113
OMP: Info #250: KMP_AFFINITY: pid 28928 tid 29502 thread 17 bound to OS proc set 1
OMP: Info #250: KMP_AFFINITY: pid 28928 tid 29504 thread 19 bound to OS proc set 3
OMP: Info #250: KMP_AFFINITY: pid 28928 tid 29503 thread 18 bound to OS proc set 2
 - val_f1: 0.9692
Epoch 2/200
 - 6s - loss: 0.0124 - val_loss: 0.0102
 - val_f1: 0.9732
Epoch 3/200
 - 6s - loss: 0.0107 - val_loss: 0.0089
 - val_f1: 0.9751
Epoch 4/200
 - 6s - loss: 0.0097 - val_loss: 0.0083
 - val_f1: 0.9776
Epoch 5/200
 - 6s - loss: 0.0092 - val_loss: 0.0097
 - val_f1: 0.9756
Epoch 6/200
 - 6s - loss: 0.0088 - val_loss: 0.0089
 - val_f1: 0.9767
Epoch 7/200
 - 6s - loss: 0.0085 - val_loss: 0.0073
 - val_f1: 0.9833
Epoch 8/200
 - 6s - loss: 0.0084 - val_loss: 0.0087
 - val_f1: 0.9769
Epoch 9/200
 - 6s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9789
Epoch 10/200
 - 6s - loss: 0.0079 - val_loss: 0.0072
 - val_f1: 0.9807
Epoch 11/200
 - 6s - loss: 0.0076 - val_loss: 0.0065
 - val_f1: 0.9841
Epoch 12/200
 - 6s - loss: 0.0075 - val_loss: 0.0089
 - val_f1: 0.9751
Epoch 13/200
 - 6s - loss: 0.0077 - val_loss: 0.0080
 - val_f1: 0.9747
Epoch 14/200
 - 6s - loss: 0.0076 - val_loss: 0.0096
 - val_f1: 0.9768
Epoch 15/200
 - 6s - loss: 0.0074 - val_loss: 0.0066
 - val_f1: 0.9847
Epoch 16/200
 - 6s - loss: 0.0073 - val_loss: 0.0059
 - val_f1: 0.9867
Epoch 17/200
 - 6s - loss: 0.0066 - val_loss: 0.0074
 - val_f1: 0.9814
Epoch 18/200
 - 6s - loss: 0.0071 - val_loss: 0.0077
 - val_f1: 0.9795
Epoch 19/200
 - 6s - loss: 0.0074 - val_loss: 0.0066
 - val_f1: 0.9833
Epoch 20/200
 - 6s - loss: 0.0072 - val_loss: 0.0064
 - val_f1: 0.9812
Epoch 21/200
 - 6s - loss: 0.0067 - val_loss: 0.0061
/home/sunanda/anaconda3/envs/ml_env_cpu/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2020-01-07 12:33:19,583 [INFO] epoch = 20. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/ann_model_epoch_20.pickle
 - val_f1: 0.9853
Epoch 22/200
 - 6s - loss: 0.0060 - val_loss: 0.0066
 - val_f1: 0.9844
Epoch 23/200
 - 6s - loss: 0.0058 - val_loss: 0.0056
 - val_f1: 0.9860
Epoch 24/200
 - 6s - loss: 0.0061 - val_loss: 0.0057
 - val_f1: 0.9872
Epoch 25/200
 - 6s - loss: 0.0058 - val_loss: 0.0051
 - val_f1: 0.9893
Epoch 26/200
 - 6s - loss: 0.0056 - val_loss: 0.0049
 - val_f1: 0.9878
Epoch 27/200
 - 6s - loss: 0.0056 - val_loss: 0.0056
 - val_f1: 0.9861
Epoch 28/200
 - 6s - loss: 0.0056 - val_loss: 0.0049
 - val_f1: 0.9875
Epoch 29/200
 - 6s - loss: 0.0055 - val_loss: 0.0056
 - val_f1: 0.9849
Epoch 30/200
 - 6s - loss: 0.0054 - val_loss: 0.0050
 - val_f1: 0.9866
Epoch 31/200
 - 6s - loss: 0.0063 - val_loss: 0.0075
 - val_f1: 0.9796
Epoch 32/200
 - 6s - loss: 0.0070 - val_loss: 0.0070
 - val_f1: 0.9803
Epoch 33/200
 - 6s - loss: 0.0068 - val_loss: 0.0074
 - val_f1: 0.9802
Epoch 34/200
 - 6s - loss: 0.0066 - val_loss: 0.0064
 - val_f1: 0.9858
Epoch 35/200
 - 6s - loss: 0.0067 - val_loss: 0.0069
 - val_f1: 0.9807
Epoch 36/200
 - 6s - loss: 0.0067 - val_loss: 0.0078
 - val_f1: 0.9798
Epoch 37/200
 - 6s - loss: 0.0064 - val_loss: 0.0077
 - val_f1: 0.9831
Epoch 38/200
 - 6s - loss: 0.0062 - val_loss: 0.0063
 - val_f1: 0.9833
Epoch 39/200
 - 6s - loss: 0.0062 - val_loss: 0.0061
 - val_f1: 0.9849
Epoch 40/200
 - 6s - loss: 0.0060 - val_loss: 0.0091
 - val_f1: 0.9801
Epoch 41/200
 - 6s - loss: 0.0065 - val_loss: 0.0111
2020-01-07 12:39:26,974 [INFO] epoch = 40. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/ann_model_epoch_40.pickle
 - val_f1: 0.9776
Epoch 42/200
 - 6s - loss: 0.0067 - val_loss: 0.0067
 - val_f1: 0.9821
Epoch 43/200
 - 6s - loss: 0.0063 - val_loss: 0.0078
 - val_f1: 0.9797
Epoch 44/200
 - 6s - loss: 0.0066 - val_loss: 0.0078
 - val_f1: 0.9788
Epoch 45/200
 - 6s - loss: 0.0066 - val_loss: 0.0080
 - val_f1: 0.9792
Epoch 46/200
 - 6s - loss: 0.0068 - val_loss: 0.0087
 - val_f1: 0.9800
Epoch 47/200
 - 6s - loss: 0.0068 - val_loss: 0.0081
 - val_f1: 0.9790
Epoch 48/200
 - 6s - loss: 0.0069 - val_loss: 0.0068
 - val_f1: 0.9810
Epoch 49/200
 - 6s - loss: 0.0065 - val_loss: 0.0072
 - val_f1: 0.9807
Epoch 50/200
 - 6s - loss: 0.0062 - val_loss: 0.0065
 - val_f1: 0.9840
Epoch 51/200
 - 6s - loss: 0.0059 - val_loss: 0.0053
 - val_f1: 0.9873
Epoch 52/200
 - 6s - loss: 0.0057 - val_loss: 0.0062
 - val_f1: 0.9839
Epoch 53/200
 - 6s - loss: 0.0062 - val_loss: 0.0069
 - val_f1: 0.9813
Epoch 54/200
 - 6s - loss: 0.0064 - val_loss: 0.0065
 - val_f1: 0.9828
Epoch 55/200
 - 6s - loss: 0.0063 - val_loss: 0.0062
 - val_f1: 0.9830
Epoch 56/200
 - 6s - loss: 0.0062 - val_loss: 0.0081
 - val_f1: 0.9833
Epoch 57/200
 - 6s - loss: 0.0059 - val_loss: 0.0066
 - val_f1: 0.9833
Epoch 58/200
 - 6s - loss: 0.0058 - val_loss: 0.0080
 - val_f1: 0.9784
Epoch 59/200
 - 6s - loss: 0.0057 - val_loss: 0.0064
 - val_f1: 0.9835
Epoch 60/200
 - 6s - loss: 0.0055 - val_loss: 0.0058
 - val_f1: 0.9842
Epoch 61/200
 - 6s - loss: 0.0062 - val_loss: 0.0084
2020-01-07 12:45:34,963 [INFO] epoch = 60. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/ann_model_epoch_60.pickle
 - val_f1: 0.9789
Epoch 62/200
 - 6s - loss: 0.0066 - val_loss: 0.0063
 - val_f1: 0.9816
Epoch 63/200
 - 6s - loss: 0.0064 - val_loss: 0.0068
 - val_f1: 0.9827
Epoch 64/200
 - 6s - loss: 0.0064 - val_loss: 0.0059
 - val_f1: 0.9858
Epoch 65/200
 - 6s - loss: 0.0064 - val_loss: 0.0086
 - val_f1: 0.9832
Epoch 66/200
 - 6s - loss: 0.0065 - val_loss: 0.0077
 - val_f1: 0.9819
Epoch 67/200
 - 6s - loss: 0.0064 - val_loss: 0.0070
 - val_f1: 0.9843
Epoch 68/200
 - 6s - loss: 0.0062 - val_loss: 0.0071
 - val_f1: 0.9835
Epoch 69/200
 - 6s - loss: 0.0061 - val_loss: 0.0052
 - val_f1: 0.9914
Epoch 70/200
 - 6s - loss: 0.0059 - val_loss: 0.0063
 - val_f1: 0.9842
Epoch 71/200
 - 6s - loss: 0.0060 - val_loss: 0.0062
 - val_f1: 0.9827
Epoch 72/200
 - 6s - loss: 0.0059 - val_loss: 0.0057
 - val_f1: 0.9872
Epoch 73/200
 - 6s - loss: 0.0058 - val_loss: 0.0046
 - val_f1: 0.9897
Epoch 74/200
 - 6s - loss: 0.0058 - val_loss: 0.0045
 - val_f1: 0.9915
Epoch 75/200
 - 6s - loss: 0.0057 - val_loss: 0.0051
 - val_f1: 0.9920
Epoch 76/200
 - 6s - loss: 0.0055 - val_loss: 0.0042
 - val_f1: 0.9912
Epoch 77/200
 - 6s - loss: 0.0053 - val_loss: 0.0048
 - val_f1: 0.9880
Epoch 78/200
 - 6s - loss: 0.0052 - val_loss: 0.0058
 - val_f1: 0.9870
Epoch 79/200
 - 6s - loss: 0.0052 - val_loss: 0.0054
 - val_f1: 0.9862
Epoch 80/200
 - 6s - loss: 0.0053 - val_loss: 0.0041
 - val_f1: 0.9924
Epoch 81/200
 - 6s - loss: 0.0052 - val_loss: 0.0040
2020-01-07 12:51:43,229 [INFO] epoch = 80. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/ann_model_epoch_80.pickle
 - val_f1: 0.9913
Epoch 82/200
 - 6s - loss: 0.0051 - val_loss: 0.0046
 - val_f1: 0.9917
Epoch 83/200
 - 6s - loss: 0.0050 - val_loss: 0.0039
 - val_f1: 0.9926
Epoch 84/200
 - 6s - loss: 0.0049 - val_loss: 0.0041
 - val_f1: 0.9913
Epoch 85/200
 - 6s - loss: 0.0050 - val_loss: 0.0041
 - val_f1: 0.9899
Epoch 86/200
 - 6s - loss: 0.0050 - val_loss: 0.0041
 - val_f1: 0.9912
Epoch 87/200
 - 6s - loss: 0.0049 - val_loss: 0.0043
 - val_f1: 0.9916
Epoch 88/200
 - 6s - loss: 0.0050 - val_loss: 0.0050
 - val_f1: 0.9885
Epoch 89/200
 - 6s - loss: 0.0047 - val_loss: 0.0039
 - val_f1: 0.9905
Epoch 90/200
 - 6s - loss: 0.0050 - val_loss: 0.0046
 - val_f1: 0.9875
Epoch 91/200
 - 6s - loss: 0.0048 - val_loss: 0.0039
 - val_f1: 0.9927
Epoch 92/200
 - 6s - loss: 0.0047 - val_loss: 0.0045
 - val_f1: 0.9888
Epoch 93/200
 - 6s - loss: 0.0048 - val_loss: 0.0037
 - val_f1: 0.9931
Epoch 94/200
 - 6s - loss: 0.0046 - val_loss: 0.0038
 - val_f1: 0.9909
Epoch 95/200
 - 6s - loss: 0.0047 - val_loss: 0.0043
 - val_f1: 0.9907
Epoch 96/200
 - 6s - loss: 0.0047 - val_loss: 0.0044
 - val_f1: 0.9879
Epoch 97/200
 - 6s - loss: 0.0047 - val_loss: 0.0037
 - val_f1: 0.9937
Epoch 98/200
 - 6s - loss: 0.0046 - val_loss: 0.0037
 - val_f1: 0.9927
Epoch 99/200
 - 6s - loss: 0.0045 - val_loss: 0.0039
 - val_f1: 0.9900
Epoch 100/200
 - 6s - loss: 0.0045 - val_loss: 0.0040
 - val_f1: 0.9911
Epoch 101/200
 - 6s - loss: 0.0046 - val_loss: 0.0041
2020-01-07 12:57:51,672 [INFO] epoch = 100. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/ann_model_epoch_100.pickle
 - val_f1: 0.9916
Epoch 102/200
 - 6s - loss: 0.0045 - val_loss: 0.0043
 - val_f1: 0.9923
Epoch 103/200
 - 6s - loss: 0.0045 - val_loss: 0.0041
 - val_f1: 0.9911
Epoch 104/200
 - 6s - loss: 0.0045 - val_loss: 0.0034
 - val_f1: 0.9941
Epoch 105/200
 - 6s - loss: 0.0044 - val_loss: 0.0039
 - val_f1: 0.9915
Epoch 106/200
 - 6s - loss: 0.0045 - val_loss: 0.0041
 - val_f1: 0.9890
Epoch 107/200
 - 6s - loss: 0.0045 - val_loss: 0.0045
 - val_f1: 0.9881
Epoch 108/200
 - 6s - loss: 0.0046 - val_loss: 0.0034
 - val_f1: 0.9941
Epoch 109/200
 - 6s - loss: 0.0044 - val_loss: 0.0040
 - val_f1: 0.9902
Epoch 110/200
 - 6s - loss: 0.0045 - val_loss: 0.0035
 - val_f1: 0.9925
Epoch 111/200
 - 6s - loss: 0.0044 - val_loss: 0.0044
 - val_f1: 0.9902
Epoch 112/200
 - 6s - loss: 0.0043 - val_loss: 0.0050
 - val_f1: 0.9874
Epoch 113/200
 - 6s - loss: 0.0044 - val_loss: 0.0037
 - val_f1: 0.9905
Epoch 114/200
 - 6s - loss: 0.0043 - val_loss: 0.0040
 - val_f1: 0.9901
Epoch 115/200
 - 6s - loss: 0.0044 - val_loss: 0.0038
 - val_f1: 0.9900
Epoch 116/200
 - 6s - loss: 0.0043 - val_loss: 0.0034
 - val_f1: 0.9942
Epoch 117/200
 - 6s - loss: 0.0043 - val_loss: 0.0053
 - val_f1: 0.9886
Epoch 118/200
 - 6s - loss: 0.0044 - val_loss: 0.0045
 - val_f1: 0.9890
Epoch 119/200
 - 6s - loss: 0.0044 - val_loss: 0.0038
 - val_f1: 0.9918
Epoch 120/200
 - 6s - loss: 0.0059 - val_loss: 0.0060
 - val_f1: 0.9843
Epoch 121/200
 - 6s - loss: 0.0056 - val_loss: 0.0046
2020-01-07 13:04:00,268 [INFO] epoch = 120. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/ann_model_epoch_120.pickle
 - val_f1: 0.9926
Epoch 122/200
 - 6s - loss: 0.0050 - val_loss: 0.0040
 - val_f1: 0.9931
Epoch 123/200
 - 6s - loss: 0.0047 - val_loss: 0.0042
 - val_f1: 0.9924
Epoch 124/200
 - 6s - loss: 0.0051 - val_loss: 0.0045
 - val_f1: 0.9897
Epoch 125/200
 - 6s - loss: 0.0051 - val_loss: 0.0050
 - val_f1: 0.9912
Epoch 126/200
 - 6s - loss: 0.0050 - val_loss: 0.0045
 - val_f1: 0.9902
Epoch 127/200
 - 6s - loss: 0.0049 - val_loss: 0.0052
 - val_f1: 0.9902
Epoch 128/200
 - 6s - loss: 0.0050 - val_loss: 0.0045
 - val_f1: 0.9912
Epoch 129/200
 - 6s - loss: 0.0050 - val_loss: 0.0048
 - val_f1: 0.9897
Epoch 130/200
 - 6s - loss: 0.0049 - val_loss: 0.0052
 - val_f1: 0.9880
Epoch 131/200
 - 6s - loss: 0.0048 - val_loss: 0.0050
 - val_f1: 0.9886
Epoch 132/200
 - 6s - loss: 0.0047 - val_loss: 0.0043
 - val_f1: 0.9917
Epoch 133/200
 - 6s - loss: 0.0046 - val_loss: 0.0041
 - val_f1: 0.9913
Epoch 134/200
 - 6s - loss: 0.0046 - val_loss: 0.0039
 - val_f1: 0.9939
Epoch 135/200
 - 6s - loss: 0.0049 - val_loss: 0.0051
 - val_f1: 0.9912
Epoch 136/200
 - 6s - loss: 0.0057 - val_loss: 0.0058
 - val_f1: 0.9845
Epoch 137/200
 - 6s - loss: 0.0061 - val_loss: 0.0040
 - val_f1: 0.9926
Epoch 138/200
 - 6s - loss: 0.0050 - val_loss: 0.0046
 - val_f1: 0.9907
Epoch 139/200
 - 6s - loss: 0.0047 - val_loss: 0.0050
 - val_f1: 0.9884
Epoch 140/200
 - 6s - loss: 0.0052 - val_loss: 0.0041
 - val_f1: 0.9927
Epoch 141/200
 - 6s - loss: 0.0048 - val_loss: 0.0043
2020-01-07 13:10:08,833 [INFO] epoch = 140. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/ann_model_epoch_140.pickle
 - val_f1: 0.9910
Epoch 142/200
 - 6s - loss: 0.0058 - val_loss: 0.0059
 - val_f1: 0.9833
Epoch 143/200
 - 6s - loss: 0.0063 - val_loss: 0.0056
 - val_f1: 0.9857
Epoch 144/200
 - 6s - loss: 0.0064 - val_loss: 0.0055
 - val_f1: 0.9857
Epoch 145/200
 - 6s - loss: 0.0063 - val_loss: 0.0051
 - val_f1: 0.9885
Epoch 146/200
 - 6s - loss: 0.0058 - val_loss: 0.0045
 - val_f1: 0.9878
Epoch 147/200
 - 6s - loss: 0.0056 - val_loss: 0.0054
 - val_f1: 0.9862
Epoch 148/200
 - 6s - loss: 0.0059 - val_loss: 0.0059
 - val_f1: 0.9863
Epoch 149/200
 - 6s - loss: 0.0056 - val_loss: 0.0046
 - val_f1: 0.9899
Epoch 150/200
 - 6s - loss: 0.0056 - val_loss: 0.0047
 - val_f1: 0.9918
Epoch 151/200
 - 6s - loss: 0.0057 - val_loss: 0.0047
 - val_f1: 0.9897
Epoch 152/200
 - 6s - loss: 0.0059 - val_loss: 0.0059
 - val_f1: 0.9888
Epoch 153/200
 - 6s - loss: 0.0064 - val_loss: 0.0056
 - val_f1: 0.9859
Epoch 154/200
 - 6s - loss: 0.0060 - val_loss: 0.0047
2020-01-07 13:14:20,942 [INFO] WeightRestorer::on_train_end(): restoring best weights
2020-01-07 13:14:43,807 [INFO] Last epoch loss evaluation: train_loss = 0.003118, val_loss = 0.003357
2020-01-07 13:14:43,814 [INFO] Training complete. time_to_train = 6095.02 sec, 101.58 min
2020-01-07 13:14:43,824 [INFO] Model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/best_model.pickle
2020-01-07 13:14:43,826 [INFO] Training history saved to: results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/training_error_history.csv
2020-01-07 13:14:44,016 [INFO] Plot saved to: results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/training_error_history.png
2020-01-07 13:14:44,198 [INFO] Plot saved to: results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/training_f1_history.png
2020-01-07 13:14:44,198 [INFO] Making predictions on training, validation, testing data
2020-01-07 13:16:37,777 [INFO] Evaluating predictions (results)
/home/sunanda/anaconda3/envs/ml_env_cpu/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2020-01-07 13:16:57,615 [INFO] Dataset: Testing. Classification report below
2020-01-07 13:16:57,615 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       1.00      1.00      1.00    454265
                   Bot       0.97      0.35      0.52       391
                  DDoS       1.00      1.00      1.00     25605
         DoS GoldenEye       0.99      0.97      0.98      2058
              DoS Hulk       0.98      0.99      0.98     46025
      DoS Slowhttptest       0.88      0.99      0.93      1100
         DoS slowloris       0.99      0.96      0.97      1159
           FTP-Patator       1.00      0.99      0.99      1587
              PortScan       0.99      0.99      0.99     31761
           SSH-Patator       0.96      0.97      0.96      1179
Web Attack Brute Force       1.00      0.08      0.15       302
        Web Attack XSS       0.00      0.00      0.00       130

              accuracy                           0.99    565562
             macro avg       0.90      0.77      0.79    565562
          weighted avg       0.99      0.99      0.99    565562

2020-01-07 13:16:57,615 [INFO] Overall accuracy (micro avg): 0.9944568411597667
/home/sunanda/research/ids_experiments/utility.py:328: RuntimeWarning: invalid value encountered in true_divide
  PPV = TP / (TP + FP)    # Precision or positive predictive value
/home/sunanda/research/ids_experiments/utility.py:332: RuntimeWarning: invalid value encountered in true_divide
  FDR = FP / (TP + FP)    # False discovery rate
2020-01-07 13:17:18,927 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9945         0.9945                       0.9945                0.0005                   0.0055  0.9945
1     Macro avg        0.9991         0.8957                       0.7738                0.0014                   0.2262  0.7897
2  Weighted avg        0.9954         0.9943                       0.9945                0.0113                   0.0055  0.9940
2020-01-07 13:17:38,913 [INFO] Dataset: Validation. Classification report below
2020-01-07 13:17:38,913 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       1.00      1.00      1.00    454264
                   Bot       0.98      0.31      0.47       391
                  DDoS       1.00      1.00      1.00     25605
         DoS GoldenEye       0.99      0.97      0.98      2059
              DoS Hulk       0.98      0.99      0.98     46025
      DoS Slowhttptest       0.88      0.98      0.93      1099
         DoS slowloris       0.98      0.95      0.97      1159
           FTP-Patator       1.00      0.99      0.99      1587
              PortScan       0.99      0.99      0.99     31761
           SSH-Patator       0.97      0.96      0.96      1180
Web Attack Brute Force       0.93      0.04      0.08       301
        Web Attack XSS       1.00      0.03      0.06       131

              accuracy                           0.99    565562
             macro avg       0.97      0.77      0.78    565562
          weighted avg       0.99      0.99      0.99    565562

2020-01-07 13:17:38,913 [INFO] Overall accuracy (micro avg): 0.9945646984769132
2020-01-07 13:18:00,439 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9946         0.9946                       0.9946                0.0005                   0.0054  0.9946
1     Macro avg        0.9991         0.9741                       0.7673                0.0014                   0.2327  0.7843
2  Weighted avg        0.9955         0.9946                       0.9946                0.0112                   0.0054  0.9941
2020-01-07 13:19:07,280 [INFO] Dataset: Training. Classification report below
2020-01-07 13:19:07,280 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       1.00      1.00      1.00   1362791
                   Bot       0.98      0.35      0.52      1174
                  DDoS       1.00      1.00      1.00     76815
         DoS GoldenEye       0.99      0.97      0.98      6176
              DoS Hulk       0.98      0.99      0.98    138074
      DoS Slowhttptest       0.90      0.98      0.94      3300
         DoS slowloris       0.99      0.97      0.98      3478
           FTP-Patator       1.00      0.99      0.99      4761
              PortScan       0.99      0.99      0.99     95282
           SSH-Patator       0.97      0.97      0.97      3538
Web Attack Brute Force       1.00      0.07      0.12       904
        Web Attack XSS       1.00      0.03      0.06       391

              accuracy                           0.99   1696684
             macro avg       0.98      0.78      0.79   1696684
          weighted avg       0.99      0.99      0.99   1696684

2020-01-07 13:19:07,280 [INFO] Overall accuracy (micro avg): 0.9947297198535496
2020-01-07 13:20:19,297 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9947         0.9947                       0.9947                0.0005                   0.0053  0.9947
1     Macro avg        0.9991         0.9826                       0.7755                0.0014                   0.2245  0.7944
2  Weighted avg        0.9956         0.9948                       0.9947                0.0110                   0.0053  0.9943
2020-01-07 13:20:19,362 [INFO] Results saved to: results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/semi_sup_perf_ids17_ae_ann_rep1_results.xlsx
2020-01-07 13:20:19,366 [INFO] ================= Finished running experiment no. 1 ================= 

2020-01-07 13:20:19,408 [INFO] Created directory: results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2
2020-01-07 13:20:19,409 [INFO] Initialized logging. log_filename = results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/run_log.log
2020-01-07 13:20:19,409 [INFO] ================= Running experiment no. 2  ================= 

2020-01-07 13:20:19,409 [INFO] Experiment parameters given below
2020-01-07 13:20:19,409 [INFO] 
{'experiment_num': 2, 'results_dir': 'results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2', 'model_type': 'classifier', 'model': 'ae_ann', 'normal_label': 'BENIGN', 'split_random_seed': 42, 'unsupervised_ratio': 0.75, 'ae_encoder_units': [64], 'ae_encoder_activations': ['relu'], 'ae_encoder_dropout_rates': [0.2], 'ae_encoder_l1_param': -1, 'ae_decoder_units': '', 'ae_decoder_activations': '', 'ae_decoder_dropout_rates': '', 'output_activation': 'sigmoid', 'loss_function': 'binary_crossentropy', 'ae_epochs': 200, 'ann_layer_units': [64], 'ann_layer_activations': ['relu'], 'ann_layer_dropout_rates': [0.2], 'ann_epochs': 200, 'early_stop_patience': 50, 'batch_size': 256, 'dataset_dir': '../Datasets/full_datasets/ids2017', 'description': 'semi_sup_perf_ids17_ae_ann_rep2'}
2020-01-07 13:20:19,409 [INFO] Created tensorboard log directory: results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/tf_logs_run_2020_01_07-13_20_19
2020-01-07 13:20:19,409 [INFO] Loading datsets from: ../Datasets/full_datasets/ids2017
2020-01-07 13:20:19,409 [INFO] Reading X, y files
2020-01-07 13:20:19,409 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_train.h5
2020-01-07 13:20:23,194 [INFO] Reading complete. time_to_read=3.78 seconds
2020-01-07 13:20:23,194 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_val.h5
2020-01-07 13:20:24,476 [INFO] Reading complete. time_to_read=1.28 seconds
2020-01-07 13:20:24,476 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_test.h5
2020-01-07 13:20:25,759 [INFO] Reading complete. time_to_read=1.28 seconds
2020-01-07 13:20:25,759 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_train.h5
2020-01-07 13:20:26,013 [INFO] Reading complete. time_to_read=0.25 seconds
2020-01-07 13:20:26,013 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_val.h5
2020-01-07 13:20:26,096 [INFO] Reading complete. time_to_read=0.08 seconds
2020-01-07 13:20:26,096 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_test.h5
2020-01-07 13:20:26,180 [INFO] Reading complete. time_to_read=0.08 seconds
2020-01-07 13:20:29,128 [INFO] Initializing model
2020-01-07 13:20:29,254 [INFO] _________________________________________________________________
2020-01-07 13:20:29,254 [INFO] Layer (type)                 Output Shape              Param #   
2020-01-07 13:20:29,254 [INFO] =================================================================
2020-01-07 13:20:29,254 [INFO] dense_17 (Dense)             (None, 64)                5056      
2020-01-07 13:20:29,254 [INFO] _________________________________________________________________
2020-01-07 13:20:29,254 [INFO] batch_normalization_9 (Batch (None, 64)                256       
2020-01-07 13:20:29,254 [INFO] _________________________________________________________________
2020-01-07 13:20:29,254 [INFO] dropout_9 (Dropout)          (None, 64)                0         
2020-01-07 13:20:29,254 [INFO] _________________________________________________________________
2020-01-07 13:20:29,255 [INFO] dense_18 (Dense)             (None, 78)                5070      
2020-01-07 13:20:29,255 [INFO] =================================================================
2020-01-07 13:20:29,255 [INFO] Total params: 10,382
2020-01-07 13:20:29,255 [INFO] Trainable params: 10,254
2020-01-07 13:20:29,255 [INFO] Non-trainable params: 128
2020-01-07 13:20:29,255 [INFO] _________________________________________________________________
2020-01-07 13:20:29,376 [INFO] _________________________________________________________________
2020-01-07 13:20:29,376 [INFO] Layer (type)                 Output Shape              Param #   
2020-01-07 13:20:29,376 [INFO] =================================================================
2020-01-07 13:20:29,377 [INFO] dense_19 (Dense)             (None, 64)                4160      
2020-01-07 13:20:29,377 [INFO] _________________________________________________________________
2020-01-07 13:20:29,377 [INFO] batch_normalization_10 (Batc (None, 64)                256       
2020-01-07 13:20:29,377 [INFO] _________________________________________________________________
2020-01-07 13:20:29,377 [INFO] dropout_10 (Dropout)         (None, 64)                0         
2020-01-07 13:20:29,377 [INFO] _________________________________________________________________
2020-01-07 13:20:29,377 [INFO] dense_20 (Dense)             (None, 12)                780       
2020-01-07 13:20:29,377 [INFO] =================================================================
2020-01-07 13:20:29,377 [INFO] Total params: 5,196
2020-01-07 13:20:29,377 [INFO] Trainable params: 5,068
2020-01-07 13:20:29,377 [INFO] Non-trainable params: 128
2020-01-07 13:20:29,377 [INFO] _________________________________________________________________
2020-01-07 13:20:29,378 [INFO] Training model
2020-01-07 13:20:29,378 [INFO] Splitting train set into 2 sets (unsupervised, supervised), random_seed = 42
2020-01-07 13:20:53,899 [INFO] Split sizes (instances). total = 1696684, unsupervised = 1272513, supervised = 424171, unsupervised dataset hash = 07eca414d7fa8ed059bf34ec454d369f9e002f85
2020-01-07 13:20:53,899 [INFO] Training autoencoder
 - val_f1: 0.9906
Epoch 00154: early stopping
Train on 1272513 samples, validate on 565562 samples
Epoch 1/200
 - 24s - loss: -3.7995e+00 - val_loss: -4.1457e+00
Epoch 2/200
 - 23s - loss: -4.1149e+00 - val_loss: -4.1518e+00
Epoch 3/200
 - 23s - loss: -4.1248e+00 - val_loss: -4.1575e+00
Epoch 4/200
 - 23s - loss: -4.1292e+00 - val_loss: -4.1566e+00
Epoch 5/200
 - 23s - loss: -4.1320e+00 - val_loss: -4.1601e+00
Epoch 6/200
 - 23s - loss: -4.1342e+00 - val_loss: -4.1611e+00
Epoch 7/200
 - 23s - loss: -4.1359e+00 - val_loss: -4.1621e+00
Epoch 8/200
 - 23s - loss: -4.1368e+00 - val_loss: -4.1621e+00
Epoch 9/200
 - 23s - loss: -4.1377e+00 - val_loss: -4.1630e+00
Epoch 10/200
 - 23s - loss: -4.1383e+00 - val_loss: -4.1627e+00
Epoch 11/200
 - 23s - loss: -4.1382e+00 - val_loss: -4.1637e+00
Epoch 12/200
 - 23s - loss: -4.1395e+00 - val_loss: -4.1642e+00
Epoch 13/200
 - 23s - loss: -4.1397e+00 - val_loss: -4.1645e+00
Epoch 14/200
 - 23s - loss: -4.1399e+00 - val_loss: -4.1641e+00
Epoch 15/200
 - 23s - loss: -4.1398e+00 - val_loss: -4.1643e+00
Epoch 16/200
 - 23s - loss: -4.1407e+00 - val_loss: -4.1639e+00
Epoch 17/200
 - 23s - loss: -4.1407e+00 - val_loss: -4.1647e+00
Epoch 18/200
 - 23s - loss: -4.1410e+00 - val_loss: -4.1649e+00
Epoch 19/200
 - 23s - loss: -4.1412e+00 - val_loss: -4.1649e+00
Epoch 20/200
 - 23s - loss: -4.1414e+00 - val_loss: -4.1638e+00
Epoch 21/200
 - 23s - loss: -4.1415e+00 - val_loss: -4.1631e+00
2020-01-07 13:28:54,412 [INFO] epoch = 20. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/ae_model_epoch_20.pickle
Epoch 22/200
 - 23s - loss: -4.1416e+00 - val_loss: -4.1648e+00
Epoch 23/200
 - 23s - loss: -4.1418e+00 - val_loss: -4.1651e+00
Epoch 24/200
 - 23s - loss: -4.1419e+00 - val_loss: -4.1656e+00
Epoch 25/200
 - 23s - loss: -4.1420e+00 - val_loss: -4.1650e+00
Epoch 26/200
 - 23s - loss: -4.1421e+00 - val_loss: -4.1652e+00
Epoch 27/200
 - 23s - loss: -4.1424e+00 - val_loss: -4.1653e+00
Epoch 28/200
 - 23s - loss: -4.1425e+00 - val_loss: -4.1659e+00
Epoch 29/200
 - 23s - loss: -4.1425e+00 - val_loss: -4.1659e+00
Epoch 30/200
 - 23s - loss: -4.1426e+00 - val_loss: -4.1660e+00
Epoch 31/200
 - 23s - loss: -4.1425e+00 - val_loss: -4.1661e+00
Epoch 32/200
 - 23s - loss: -4.1429e+00 - val_loss: -4.1656e+00
Epoch 33/200
 - 23s - loss: -4.1429e+00 - val_loss: -4.1645e+00
Epoch 34/200
 - 23s - loss: -4.1430e+00 - val_loss: -4.1657e+00
Epoch 35/200
 - 23s - loss: -4.1427e+00 - val_loss: -4.1656e+00
Epoch 36/200
 - 23s - loss: -4.1432e+00 - val_loss: -4.1655e+00
Epoch 37/200
 - 23s - loss: -4.1432e+00 - val_loss: -4.1650e+00
Epoch 38/200
 - 23s - loss: -4.1433e+00 - val_loss: -4.1655e+00
Epoch 39/200
 - 23s - loss: -4.1433e+00 - val_loss: -4.1654e+00
Epoch 40/200
 - 23s - loss: -4.1434e+00 - val_loss: -4.1660e+00
Epoch 41/200
 - 23s - loss: -4.1430e+00 - val_loss: -4.1660e+00
2020-01-07 13:36:29,429 [INFO] epoch = 40. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/ae_model_epoch_40.pickle
Epoch 42/200
 - 23s - loss: -4.1435e+00 - val_loss: -4.1661e+00
Epoch 43/200
 - 23s - loss: -4.1435e+00 - val_loss: -4.1661e+00
Epoch 44/200
 - 23s - loss: -4.1435e+00 - val_loss: -4.1647e+00
Epoch 45/200
 - 23s - loss: -4.1432e+00 - val_loss: -4.1657e+00
Epoch 46/200
 - 23s - loss: -4.1437e+00 - val_loss: -4.1658e+00
Epoch 47/200
 - 23s - loss: -4.1433e+00 - val_loss: -4.1654e+00
Epoch 48/200
 - 23s - loss: -4.1438e+00 - val_loss: -4.1667e+00
Epoch 49/200
 - 23s - loss: -4.1436e+00 - val_loss: -4.1666e+00
Epoch 50/200
 - 23s - loss: -4.1436e+00 - val_loss: -4.1640e+00
Epoch 51/200
 - 23s - loss: -4.1438e+00 - val_loss: -4.1662e+00
Epoch 52/200
 - 23s - loss: -4.1440e+00 - val_loss: -4.1669e+00
Epoch 53/200
 - 23s - loss: -4.1439e+00 - val_loss: -4.1654e+00
Epoch 54/200
 - 23s - loss: -4.1442e+00 - val_loss: -4.1669e+00
Epoch 55/200
 - 23s - loss: -4.1440e+00 - val_loss: -4.1660e+00
Epoch 56/200
 - 23s - loss: -4.1443e+00 - val_loss: -4.1648e+00
Epoch 57/200
 - 23s - loss: -4.1441e+00 - val_loss: -4.1666e+00
Epoch 58/200
 - 23s - loss: -4.1443e+00 - val_loss: -4.1661e+00
Epoch 59/200
 - 23s - loss: -4.1441e+00 - val_loss: -4.1665e+00
Epoch 60/200
 - 23s - loss: -4.1440e+00 - val_loss: -4.1662e+00
Epoch 61/200
 - 23s - loss: -4.1443e+00 - val_loss: -4.1668e+00
2020-01-07 13:44:04,698 [INFO] epoch = 60. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/ae_model_epoch_60.pickle
Epoch 62/200
 - 23s - loss: -4.1442e+00 - val_loss: -4.1669e+00
Epoch 63/200
 - 23s - loss: -4.1442e+00 - val_loss: -4.1668e+00
Epoch 64/200
 - 23s - loss: -4.1444e+00 - val_loss: -4.1661e+00
Epoch 65/200
 - 23s - loss: -4.1444e+00 - val_loss: -4.1662e+00
Epoch 66/200
 - 23s - loss: -4.1445e+00 - val_loss: -4.1671e+00
Epoch 67/200
 - 23s - loss: -4.1444e+00 - val_loss: -4.1670e+00
Epoch 68/200
 - 23s - loss: -4.1446e+00 - val_loss: -4.1666e+00
Epoch 69/200
 - 23s - loss: -4.1447e+00 - val_loss: -4.1665e+00
Epoch 70/200
 - 23s - loss: -4.1445e+00 - val_loss: -4.1671e+00
Epoch 71/200
 - 23s - loss: -4.1447e+00 - val_loss: -4.1671e+00
Epoch 72/200
 - 23s - loss: -4.1446e+00 - val_loss: -4.1672e+00
Epoch 73/200
 - 23s - loss: -4.1446e+00 - val_loss: -4.1658e+00
Epoch 74/200
 - 23s - loss: -4.1447e+00 - val_loss: -4.1667e+00
Epoch 75/200
 - 23s - loss: -4.1446e+00 - val_loss: -4.1659e+00
Epoch 76/200
 - 23s - loss: -4.1446e+00 - val_loss: -4.1668e+00
Epoch 77/200
 - 23s - loss: -4.1450e+00 - val_loss: -4.1664e+00
Epoch 78/200
 - 23s - loss: -4.1448e+00 - val_loss: -4.1665e+00
Epoch 79/200
 - 23s - loss: -4.1450e+00 - val_loss: -4.1672e+00
Epoch 80/200
 - 23s - loss: -4.1450e+00 - val_loss: -4.1668e+00
Epoch 81/200
 - 23s - loss: -4.1446e+00 - val_loss: -4.1663e+00
2020-01-07 13:51:39,544 [INFO] epoch = 80. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/ae_model_epoch_80.pickle
Epoch 82/200
 - 23s - loss: -4.1450e+00 - val_loss: -4.1657e+00
Epoch 83/200
 - 23s - loss: -4.1452e+00 - val_loss: -4.1668e+00
Epoch 84/200
 - 23s - loss: -4.1448e+00 - val_loss: -4.1667e+00
Epoch 85/200
 - 23s - loss: -4.1452e+00 - val_loss: -4.1674e+00
Epoch 86/200
 - 23s - loss: -4.1451e+00 - val_loss: -4.1671e+00
Epoch 87/200
 - 23s - loss: -4.1452e+00 - val_loss: -4.1667e+00
Epoch 88/200
 - 23s - loss: -4.1452e+00 - val_loss: -4.1666e+00
Epoch 89/200
 - 23s - loss: -4.1452e+00 - val_loss: -4.1669e+00
Epoch 90/200
 - 23s - loss: -4.1449e+00 - val_loss: -4.1673e+00
Epoch 91/200
 - 23s - loss: -4.1451e+00 - val_loss: -4.1666e+00
Epoch 92/200
 - 23s - loss: -4.1450e+00 - val_loss: -4.1672e+00
Epoch 93/200
 - 23s - loss: -4.1452e+00 - val_loss: -4.1670e+00
Epoch 94/200
 - 23s - loss: -4.1452e+00 - val_loss: -4.1672e+00
Epoch 95/200
 - 23s - loss: -4.1447e+00 - val_loss: -4.1671e+00
Epoch 96/200
 - 23s - loss: -4.1448e+00 - val_loss: -4.1664e+00
Epoch 97/200
 - 23s - loss: -4.1453e+00 - val_loss: -4.1670e+00
Epoch 98/200
 - 23s - loss: -4.1453e+00 - val_loss: -4.1597e+00
Epoch 99/200
 - 23s - loss: -4.1455e+00 - val_loss: -4.1665e+00
Epoch 100/200
 - 23s - loss: -4.1453e+00 - val_loss: -4.1669e+00
Epoch 101/200
 - 23s - loss: -4.1454e+00 - val_loss: -4.1666e+00
2020-01-07 13:59:14,885 [INFO] epoch = 100. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/ae_model_epoch_100.pickle
Epoch 102/200
 - 23s - loss: -4.1452e+00 - val_loss: -4.1667e+00
Epoch 103/200
 - 23s - loss: -4.1447e+00 - val_loss: -4.1676e+00
Epoch 104/200
 - 23s - loss: -4.1454e+00 - val_loss: -4.1674e+00
Epoch 105/200
 - 23s - loss: -4.1455e+00 - val_loss: -4.1672e+00
Epoch 106/200
 - 23s - loss: -4.1456e+00 - val_loss: -4.1664e+00
Epoch 107/200
 - 23s - loss: -4.1454e+00 - val_loss: -4.1668e+00
Epoch 108/200
 - 23s - loss: -4.1456e+00 - val_loss: -4.1664e+00
Epoch 109/200
 - 23s - loss: -4.1453e+00 - val_loss: -4.1672e+00
Epoch 110/200
 - 23s - loss: -4.1454e+00 - val_loss: -4.1665e+00
Epoch 111/200
 - 23s - loss: -4.1455e+00 - val_loss: -4.1670e+00
Epoch 112/200
 - 23s - loss: -4.1457e+00 - val_loss: -4.1660e+00
Epoch 113/200
 - 23s - loss: -4.1452e+00 - val_loss: -4.1671e+00
Epoch 114/200
 - 23s - loss: -4.1453e+00 - val_loss: -4.1679e+00
Epoch 115/200
 - 23s - loss: -4.1457e+00 - val_loss: -4.1669e+00
Epoch 116/200
 - 23s - loss: -4.1454e+00 - val_loss: -4.1672e+00
Epoch 117/200
 - 23s - loss: -4.1457e+00 - val_loss: -4.1672e+00
Epoch 118/200
 - 23s - loss: -4.1455e+00 - val_loss: -4.1678e+00
Epoch 119/200
 - 23s - loss: -4.1456e+00 - val_loss: -4.1667e+00
Epoch 120/200
 - 23s - loss: -4.1456e+00 - val_loss: -4.1676e+00
Epoch 121/200
 - 23s - loss: -4.1458e+00 - val_loss: -4.1672e+00
2020-01-07 14:06:50,026 [INFO] epoch = 120. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/ae_model_epoch_120.pickle
Epoch 122/200
 - 23s - loss: -4.1457e+00 - val_loss: -4.1667e+00
Epoch 123/200
 - 23s - loss: -4.1458e+00 - val_loss: -4.1680e+00
Epoch 124/200
 - 23s - loss: -4.1457e+00 - val_loss: -4.1669e+00
Epoch 125/200
 - 23s - loss: -4.1457e+00 - val_loss: -4.1666e+00
Epoch 126/200
 - 23s - loss: -4.1457e+00 - val_loss: -4.1668e+00
Epoch 127/200
 - 23s - loss: -4.1457e+00 - val_loss: -4.1669e+00
Epoch 128/200
 - 23s - loss: -4.1458e+00 - val_loss: -4.1669e+00
Epoch 129/200
 - 23s - loss: -4.1459e+00 - val_loss: -4.1678e+00
Epoch 130/200
 - 23s - loss: -4.1456e+00 - val_loss: -4.1664e+00
Epoch 131/200
 - 23s - loss: -4.1458e+00 - val_loss: -4.1679e+00
Epoch 132/200
 - 23s - loss: -4.1459e+00 - val_loss: -4.1668e+00
Epoch 133/200
 - 23s - loss: -4.1459e+00 - val_loss: -4.1669e+00
Epoch 134/200
 - 23s - loss: -4.1458e+00 - val_loss: -4.1657e+00
Epoch 135/200
 - 23s - loss: -4.1460e+00 - val_loss: -4.1667e+00
Epoch 136/200
 - 23s - loss: -4.1457e+00 - val_loss: -4.1598e+00
Epoch 137/200
 - 23s - loss: -4.1459e+00 - val_loss: -4.1668e+00
Epoch 138/200
 - 23s - loss: -4.1456e+00 - val_loss: -4.1667e+00
Epoch 139/200
 - 23s - loss: -4.1459e+00 - val_loss: -4.1612e+00
Epoch 140/200
 - 23s - loss: -4.1459e+00 - val_loss: -4.1677e+00
Epoch 141/200
 - 23s - loss: -4.1460e+00 - val_loss: -4.1675e+00
2020-01-07 14:14:25,603 [INFO] epoch = 140. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/ae_model_epoch_140.pickle
Epoch 142/200
 - 23s - loss: -4.1459e+00 - val_loss: -4.1674e+00
Epoch 143/200
 - 23s - loss: -4.1461e+00 - val_loss: -4.1666e+00
Epoch 144/200
 - 23s - loss: -4.1458e+00 - val_loss: -4.1675e+00
Epoch 145/200
 - 23s - loss: -4.1461e+00 - val_loss: -4.1673e+00
Epoch 146/200
 - 23s - loss: -4.1458e+00 - val_loss: -4.1632e+00
Epoch 147/200
 - 23s - loss: -4.1460e+00 - val_loss: -4.1671e+00
Epoch 148/200
 - 23s - loss: -4.1459e+00 - val_loss: -4.1681e+00
Epoch 149/200
 - 23s - loss: -4.1459e+00 - val_loss: -4.1673e+00
Epoch 150/200
 - 23s - loss: -4.1460e+00 - val_loss: -4.1667e+00
Epoch 151/200
 - 23s - loss: -4.1461e+00 - val_loss: -4.1674e+00
Epoch 152/200
 - 23s - loss: -4.1462e+00 - val_loss: -4.1677e+00
Epoch 153/200
 - 23s - loss: -4.1456e+00 - val_loss: -4.1676e+00
Epoch 154/200
 - 23s - loss: -4.1461e+00 - val_loss: -4.1671e+00
Epoch 155/200
 - 23s - loss: -4.1460e+00 - val_loss: -4.1652e+00
Epoch 156/200
 - 23s - loss: -4.1459e+00 - val_loss: -4.1675e+00
Epoch 157/200
 - 23s - loss: -4.1461e+00 - val_loss: -4.1635e+00
Epoch 158/200
 - 23s - loss: -4.1461e+00 - val_loss: -4.1664e+00
Epoch 159/200
 - 23s - loss: -4.1460e+00 - val_loss: -4.1670e+00
Epoch 160/200
 - 23s - loss: -4.1461e+00 - val_loss: -4.1674e+00
Epoch 161/200
 - 23s - loss: -4.1456e+00 - val_loss: -4.1669e+00
2020-01-07 14:22:00,760 [INFO] epoch = 160. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/ae_model_epoch_160.pickle
Epoch 162/200
 - 23s - loss: -4.1462e+00 - val_loss: -4.1669e+00
Epoch 163/200
 - 23s - loss: -4.1459e+00 - val_loss: -4.1668e+00
Epoch 164/200
 - 23s - loss: -4.1462e+00 - val_loss: -4.1674e+00
Epoch 165/200
 - 23s - loss: -4.1459e+00 - val_loss: -4.1666e+00
Epoch 166/200
 - 23s - loss: -4.1462e+00 - val_loss: -4.1664e+00
Epoch 167/200
 - 23s - loss: -4.1460e+00 - val_loss: -4.1671e+00
Epoch 168/200
 - 23s - loss: -4.1461e+00 - val_loss: -4.1679e+00
Epoch 169/200
 - 23s - loss: -4.1459e+00 - val_loss: -4.1671e+00
Epoch 170/200
 - 23s - loss: -4.1459e+00 - val_loss: -4.1655e+00
Epoch 171/200
 - 23s - loss: -4.1462e+00 - val_loss: -4.1671e+00
Epoch 172/200
 - 23s - loss: -4.1462e+00 - val_loss: -4.1674e+00
Epoch 173/200
 - 23s - loss: -4.1461e+00 - val_loss: -4.1670e+00
Epoch 174/200
 - 23s - loss: -4.1459e+00 - val_loss: -4.1670e+00
Epoch 175/200
 - 23s - loss: -4.1462e+00 - val_loss: -4.1673e+00
Epoch 176/200
 - 23s - loss: -4.1463e+00 - val_loss: -4.1674e+00
Epoch 177/200
 - 23s - loss: -4.1460e+00 - val_loss: -4.1672e+00
Epoch 178/200
 - 23s - loss: -4.1462e+00 - val_loss: -4.1679e+00
Epoch 179/200
 - 23s - loss: -4.1460e+00 - val_loss: -4.1671e+00
Epoch 180/200
 - 23s - loss: -4.1460e+00 - val_loss: -4.1670e+00
Epoch 181/200
 - 23s - loss: -4.1462e+00 - val_loss: -4.1677e+00
2020-01-07 14:29:36,025 [INFO] epoch = 180. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/ae_model_epoch_180.pickle
Epoch 182/200
 - 23s - loss: -4.1459e+00 - val_loss: -4.1639e+00
Epoch 183/200
 - 23s - loss: -4.1461e+00 - val_loss: -4.1588e+00
Epoch 184/200
 - 23s - loss: -4.1462e+00 - val_loss: -4.1678e+00
Epoch 185/200
 - 23s - loss: -4.1463e+00 - val_loss: -4.1661e+00
Epoch 186/200
 - 23s - loss: -4.1459e+00 - val_loss: -4.1670e+00
Epoch 187/200
 - 23s - loss: -4.1463e+00 - val_loss: -4.1669e+00
Epoch 188/200
 - 23s - loss: -4.1462e+00 - val_loss: -4.1671e+00
Epoch 189/200
 - 23s - loss: -4.1462e+00 - val_loss: -4.1673e+00
Epoch 190/200
 - 23s - loss: -4.1463e+00 - val_loss: -4.1673e+00
Epoch 191/200
 - 23s - loss: -4.1460e+00 - val_loss: -4.1669e+00
Epoch 192/200
 - 23s - loss: -4.1462e+00 - val_loss: -4.1673e+00
Epoch 193/200
 - 23s - loss: -4.1463e+00 - val_loss: -4.1678e+00
Epoch 194/200
 - 23s - loss: -4.1461e+00 - val_loss: -4.1670e+00
Epoch 195/200
 - 23s - loss: -4.1463e+00 - val_loss: -4.1674e+00
Epoch 196/200
 - 23s - loss: -4.1463e+00 - val_loss: -4.1624e+00
Epoch 197/200
 - 23s - loss: -4.1464e+00 - val_loss: -4.1668e+00
Epoch 198/200
 - 23s - loss: -4.1461e+00 - val_loss: -4.1679e+00
2020-01-07 14:36:03,091 [INFO] WeightRestorer::on_train_end(): restoring best weights
2020-01-07 14:36:53,579 [INFO] Last epoch loss evaluation: train_loss = -4.166212, val_loss = -4.168132
2020-01-07 14:36:53,579 [INFO] Training autoencoder complete
2020-01-07 14:36:53,579 [INFO] Encoding data for supervised training
2020-01-07 14:37:13,990 [INFO] Encoding complete
2020-01-07 14:37:13,990 [INFO] Training neural network layers (after autoencoder)
Epoch 00198: early stopping
Train on 424171 samples, validate on 565562 samples
Epoch 1/200
 - 7s - loss: 0.0327 - val_loss: 0.0121
 - val_f1: 0.9651
Epoch 2/200
 - 6s - loss: 0.0125 - val_loss: 0.0100
 - val_f1: 0.9728
Epoch 3/200
 - 6s - loss: 0.0106 - val_loss: 0.0101
 - val_f1: 0.9727
Epoch 4/200
 - 6s - loss: 0.0097 - val_loss: 0.0084
 - val_f1: 0.9761
Epoch 5/200
 - 6s - loss: 0.0093 - val_loss: 0.0096
 - val_f1: 0.9725
Epoch 6/200
 - 6s - loss: 0.0089 - val_loss: 0.0105
 - val_f1: 0.9656
Epoch 7/200
 - 6s - loss: 0.0086 - val_loss: 0.0081
 - val_f1: 0.9806
Epoch 8/200
 - 6s - loss: 0.0084 - val_loss: 0.0082
 - val_f1: 0.9813
Epoch 9/200
 - 6s - loss: 0.0083 - val_loss: 0.0093
 - val_f1: 0.9764
Epoch 10/200
 - 6s - loss: 0.0081 - val_loss: 0.0092
 - val_f1: 0.9719
Epoch 11/200
 - 6s - loss: 0.0079 - val_loss: 0.0088
 - val_f1: 0.9759
Epoch 12/200
 - 6s - loss: 0.0078 - val_loss: 0.0073
 - val_f1: 0.9828
Epoch 13/200
 - 6s - loss: 0.0079 - val_loss: 0.0072
 - val_f1: 0.9838
Epoch 14/200
 - 6s - loss: 0.0078 - val_loss: 0.0087
 - val_f1: 0.9772
Epoch 15/200
 - 6s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9768
Epoch 16/200
 - 6s - loss: 0.0076 - val_loss: 0.0086
 - val_f1: 0.9760
Epoch 17/200
 - 6s - loss: 0.0074 - val_loss: 0.0072
 - val_f1: 0.9807
Epoch 18/200
 - 6s - loss: 0.0074 - val_loss: 0.0077
 - val_f1: 0.9794
Epoch 19/200
 - 6s - loss: 0.0074 - val_loss: 0.0068
 - val_f1: 0.9815
Epoch 20/200
 - 6s - loss: 0.0075 - val_loss: 0.0072
 - val_f1: 0.9824
Epoch 21/200
 - 6s - loss: 0.0074 - val_loss: 0.0105
/home/sunanda/anaconda3/envs/ml_env_cpu/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2020-01-07 14:43:50,720 [INFO] epoch = 20. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/ann_model_epoch_20.pickle
 - val_f1: 0.9709
Epoch 22/200
 - 6s - loss: 0.0072 - val_loss: 0.0076
 - val_f1: 0.9790
Epoch 23/200
 - 6s - loss: 0.0073 - val_loss: 0.0069
 - val_f1: 0.9824
Epoch 24/200
 - 6s - loss: 0.0072 - val_loss: 0.0068
 - val_f1: 0.9835
Epoch 25/200
 - 6s - loss: 0.0070 - val_loss: 0.0084
 - val_f1: 0.9792
Epoch 26/200
 - 6s - loss: 0.0069 - val_loss: 0.0073
 - val_f1: 0.9815
Epoch 27/200
 - 6s - loss: 0.0071 - val_loss: 0.0083
 - val_f1: 0.9766
Epoch 28/200
 - 6s - loss: 0.0071 - val_loss: 0.0069
 - val_f1: 0.9815
Epoch 29/200
 - 6s - loss: 0.0070 - val_loss: 0.0073
 - val_f1: 0.9785
Epoch 30/200
 - 6s - loss: 0.0069 - val_loss: 0.0069
 - val_f1: 0.9850
Epoch 31/200
 - 6s - loss: 0.0068 - val_loss: 0.0069
 - val_f1: 0.9823
Epoch 32/200
 - 6s - loss: 0.0069 - val_loss: 0.0068
 - val_f1: 0.9840
Epoch 33/200
 - 6s - loss: 0.0068 - val_loss: 0.0067
 - val_f1: 0.9846
Epoch 34/200
 - 6s - loss: 0.0069 - val_loss: 0.0065
 - val_f1: 0.9845
Epoch 35/200
 - 6s - loss: 0.0068 - val_loss: 0.0080
 - val_f1: 0.9759
Epoch 36/200
 - 6s - loss: 0.0068 - val_loss: 0.0071
 - val_f1: 0.9807
Epoch 37/200
 - 6s - loss: 0.0069 - val_loss: 0.0071
 - val_f1: 0.9831
Epoch 38/200
 - 6s - loss: 0.0070 - val_loss: 0.0071
 - val_f1: 0.9806
Epoch 39/200
 - 6s - loss: 0.0069 - val_loss: 0.0082
 - val_f1: 0.9768
Epoch 40/200
 - 6s - loss: 0.0067 - val_loss: 0.0077
 - val_f1: 0.9754
Epoch 41/200
 - 6s - loss: 0.0066 - val_loss: 0.0069
2020-01-07 14:50:18,251 [INFO] epoch = 40. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/ann_model_epoch_40.pickle
 - val_f1: 0.9852
Epoch 42/200
 - 6s - loss: 0.0065 - val_loss: 0.0059
 - val_f1: 0.9873
Epoch 43/200
 - 6s - loss: 0.0066 - val_loss: 0.0066
 - val_f1: 0.9865
Epoch 44/200
 - 6s - loss: 0.0064 - val_loss: 0.0061
 - val_f1: 0.9851
Epoch 45/200
 - 6s - loss: 0.0064 - val_loss: 0.0055
 - val_f1: 0.9901
Epoch 46/200
 - 6s - loss: 0.0062 - val_loss: 0.0062
 - val_f1: 0.9874
Epoch 47/200
 - 6s - loss: 0.0060 - val_loss: 0.0057
 - val_f1: 0.9873
Epoch 48/200
 - 6s - loss: 0.0062 - val_loss: 0.0081
 - val_f1: 0.9747
Epoch 49/200
 - 6s - loss: 0.0063 - val_loss: 0.0058
 - val_f1: 0.9835
Epoch 50/200
 - 6s - loss: 0.0065 - val_loss: 0.0059
 - val_f1: 0.9864
Epoch 51/200
 - 6s - loss: 0.0062 - val_loss: 0.0067
 - val_f1: 0.9838
Epoch 52/200
 - 6s - loss: 0.0059 - val_loss: 0.0048
 - val_f1: 0.9911
Epoch 53/200
 - 6s - loss: 0.0057 - val_loss: 0.0053
 - val_f1: 0.9899
Epoch 54/200
 - 6s - loss: 0.0060 - val_loss: 0.0056
 - val_f1: 0.9897
Epoch 55/200
 - 6s - loss: 0.0056 - val_loss: 0.0050
 - val_f1: 0.9877
Epoch 56/200
 - 6s - loss: 0.0055 - val_loss: 0.0074
 - val_f1: 0.9760
Epoch 57/200
 - 6s - loss: 0.0051 - val_loss: 0.0097
 - val_f1: 0.9637
Epoch 58/200
 - 6s - loss: 0.0051 - val_loss: 0.0049
 - val_f1: 0.9897
Epoch 59/200
 - 6s - loss: 0.0051 - val_loss: 0.0053
 - val_f1: 0.9866
Epoch 60/200
 - 6s - loss: 0.0052 - val_loss: 0.0046
 - val_f1: 0.9908
Epoch 61/200
 - 6s - loss: 0.0050 - val_loss: 0.0046
2020-01-07 14:56:45,792 [INFO] epoch = 60. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/ann_model_epoch_60.pickle
 - val_f1: 0.9933
Epoch 62/200
 - 6s - loss: 0.0049 - val_loss: 0.0044
 - val_f1: 0.9913
Epoch 63/200
 - 6s - loss: 0.0050 - val_loss: 0.0044
 - val_f1: 0.9921
Epoch 64/200
 - 6s - loss: 0.0048 - val_loss: 0.0052
 - val_f1: 0.9867
Epoch 65/200
 - 6s - loss: 0.0049 - val_loss: 0.0072
 - val_f1: 0.9781
Epoch 66/200
 - 6s - loss: 0.0049 - val_loss: 0.0039
 - val_f1: 0.9919
Epoch 67/200
 - 6s - loss: 0.0050 - val_loss: 0.0126
 - val_f1: 0.9587
Epoch 68/200
 - 6s - loss: 0.0048 - val_loss: 0.0039
 - val_f1: 0.9925
Epoch 69/200
 - 6s - loss: 0.0048 - val_loss: 0.0038
 - val_f1: 0.9925
Epoch 70/200
 - 6s - loss: 0.0048 - val_loss: 0.0084
 - val_f1: 0.9675
Epoch 71/200
 - 6s - loss: 0.0047 - val_loss: 0.0038
 - val_f1: 0.9918
Epoch 72/200
 - 6s - loss: 0.0047 - val_loss: 0.0050
 - val_f1: 0.9897
Epoch 73/200
 - 6s - loss: 0.0048 - val_loss: 0.0061
 - val_f1: 0.9852
Epoch 74/200
 - 6s - loss: 0.0048 - val_loss: 0.0034
 - val_f1: 0.9936
Epoch 75/200
 - 6s - loss: 0.0046 - val_loss: 0.0038
 - val_f1: 0.9940
Epoch 76/200
 - 6s - loss: 0.0046 - val_loss: 0.0037
 - val_f1: 0.9928
Epoch 77/200
 - 6s - loss: 0.0046 - val_loss: 0.0047
 - val_f1: 0.9886
Epoch 78/200
 - 6s - loss: 0.0046 - val_loss: 0.0039
 - val_f1: 0.9896
Epoch 79/200
 - 6s - loss: 0.0047 - val_loss: 0.0037
 - val_f1: 0.9927
Epoch 80/200
 - 6s - loss: 0.0046 - val_loss: 0.0053
 - val_f1: 0.9879
Epoch 81/200
 - 6s - loss: 0.0045 - val_loss: 0.0038
2020-01-07 15:03:13,722 [INFO] epoch = 80. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/ann_model_epoch_80.pickle
 - val_f1: 0.9901
Epoch 82/200
 - 6s - loss: 0.0046 - val_loss: 0.0039
 - val_f1: 0.9925
Epoch 83/200
 - 6s - loss: 0.0046 - val_loss: 0.0034
 - val_f1: 0.9940
Epoch 84/200
 - 6s - loss: 0.0046 - val_loss: 0.0033
 - val_f1: 0.9931
Epoch 85/200
 - 6s - loss: 0.0045 - val_loss: 0.0083
 - val_f1: 0.9778
Epoch 86/200
 - 6s - loss: 0.0045 - val_loss: 0.0050
 - val_f1: 0.9882
Epoch 87/200
 - 6s - loss: 0.0045 - val_loss: 0.0042
 - val_f1: 0.9909
Epoch 88/200
 - 6s - loss: 0.0045 - val_loss: 0.0043
 - val_f1: 0.9924
Epoch 89/200
 - 6s - loss: 0.0044 - val_loss: 0.0060
 - val_f1: 0.9854
Epoch 90/200
 - 6s - loss: 0.0046 - val_loss: 0.0054
 - val_f1: 0.9840
Epoch 91/200
 - 6s - loss: 0.0047 - val_loss: 0.0045
 - val_f1: 0.9885
Epoch 92/200
 - 6s - loss: 0.0047 - val_loss: 0.0044
 - val_f1: 0.9915
Epoch 93/200
 - 6s - loss: 0.0044 - val_loss: 0.0091
 - val_f1: 0.9622
Epoch 94/200
 - 6s - loss: 0.0044 - val_loss: 0.0053
 - val_f1: 0.9863
Epoch 95/200
 - 6s - loss: 0.0044 - val_loss: 0.0063
 - val_f1: 0.9835
Epoch 96/200
 - 6s - loss: 0.0046 - val_loss: 0.0045
 - val_f1: 0.9910
Epoch 97/200
 - 6s - loss: 0.0044 - val_loss: 0.0065
 - val_f1: 0.9844
Epoch 98/200
 - 6s - loss: 0.0045 - val_loss: 0.0042
 - val_f1: 0.9920
Epoch 99/200
 - 6s - loss: 0.0044 - val_loss: 0.0041
 - val_f1: 0.9892
Epoch 100/200
 - 6s - loss: 0.0044 - val_loss: 0.0042
 - val_f1: 0.9886
Epoch 101/200
 - 6s - loss: 0.0045 - val_loss: 0.0093
2020-01-07 15:09:41,515 [INFO] epoch = 100. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/ann_model_epoch_100.pickle
 - val_f1: 0.9622
Epoch 102/200
 - 6s - loss: 0.0046 - val_loss: 0.0049
 - val_f1: 0.9880
Epoch 103/200
 - 6s - loss: 0.0045 - val_loss: 0.0062
 - val_f1: 0.9853
Epoch 104/200
 - 6s - loss: 0.0045 - val_loss: 0.0038
 - val_f1: 0.9933
Epoch 105/200
 - 6s - loss: 0.0046 - val_loss: 0.0040
 - val_f1: 0.9932
Epoch 106/200
 - 6s - loss: 0.0045 - val_loss: 0.0045
 - val_f1: 0.9925
Epoch 107/200
 - 6s - loss: 0.0045 - val_loss: 0.0041
 - val_f1: 0.9917
Epoch 108/200
 - 6s - loss: 0.0045 - val_loss: 0.0048
 - val_f1: 0.9902
Epoch 109/200
 - 6s - loss: 0.0045 - val_loss: 0.0052
 - val_f1: 0.9902
Epoch 110/200
 - 6s - loss: 0.0045 - val_loss: 0.0069
 - val_f1: 0.9827
Epoch 111/200
 - 6s - loss: 0.0045 - val_loss: 0.0040
 - val_f1: 0.9894
Epoch 112/200
 - 6s - loss: 0.0044 - val_loss: 0.0045
 - val_f1: 0.9897
Epoch 113/200
 - 6s - loss: 0.0044 - val_loss: 0.0041
 - val_f1: 0.9909
Epoch 114/200
 - 6s - loss: 0.0045 - val_loss: 0.0046
 - val_f1: 0.9895
Epoch 115/200
 - 6s - loss: 0.0044 - val_loss: 0.0045
 - val_f1: 0.9918
Epoch 116/200
 - 6s - loss: 0.0043 - val_loss: 0.0038
 - val_f1: 0.9905
Epoch 117/200
 - 6s - loss: 0.0043 - val_loss: 0.0046
 - val_f1: 0.9885
Epoch 118/200
 - 6s - loss: 0.0044 - val_loss: 0.0039
 - val_f1: 0.9933
Epoch 119/200
 - 6s - loss: 0.0044 - val_loss: 0.0041
 - val_f1: 0.9902
Epoch 120/200
 - 6s - loss: 0.0043 - val_loss: 0.0041
 - val_f1: 0.9900
Epoch 121/200
 - 6s - loss: 0.0044 - val_loss: 0.0046
2020-01-07 15:16:09,106 [INFO] epoch = 120. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/ann_model_epoch_120.pickle
 - val_f1: 0.9899
Epoch 122/200
 - 6s - loss: 0.0043 - val_loss: 0.0038
 - val_f1: 0.9916
Epoch 123/200
 - 6s - loss: 0.0045 - val_loss: 0.0040
 - val_f1: 0.9922
Epoch 124/200
 - 6s - loss: 0.0044 - val_loss: 0.0041
 - val_f1: 0.9917
Epoch 125/200
 - 6s - loss: 0.0043 - val_loss: 0.0039
 - val_f1: 0.9919
Epoch 126/200
 - 6s - loss: 0.0043 - val_loss: 0.0054
 - val_f1: 0.9852
Epoch 127/200
 - 6s - loss: 0.0043 - val_loss: 0.0036
 - val_f1: 0.9925
Epoch 128/200
 - 6s - loss: 0.0043 - val_loss: 0.0038
 - val_f1: 0.9911
Epoch 129/200
 - 6s - loss: 0.0042 - val_loss: 0.0043
 - val_f1: 0.9896
Epoch 130/200
 - 6s - loss: 0.0043 - val_loss: 0.0037
 - val_f1: 0.9924
Epoch 131/200
 - 6s - loss: 0.0044 - val_loss: 0.0040
 - val_f1: 0.9930
Epoch 132/200
 - 6s - loss: 0.0045 - val_loss: 0.0039
 - val_f1: 0.9912
Epoch 133/200
 - 6s - loss: 0.0043 - val_loss: 0.0069
 - val_f1: 0.9781
Epoch 134/200
 - 6s - loss: 0.0043 - val_loss: 0.0037
2020-01-07 15:20:34,070 [INFO] WeightRestorer::on_train_end(): restoring best weights
2020-01-07 15:20:59,104 [INFO] Last epoch loss evaluation: train_loss = 0.003084, val_loss = 0.003311
2020-01-07 15:20:59,111 [INFO] Training complete. time_to_train = 7229.73 sec, 120.50 min
2020-01-07 15:20:59,121 [INFO] Model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/best_model.pickle
2020-01-07 15:20:59,123 [INFO] Training history saved to: results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/training_error_history.csv
2020-01-07 15:20:59,306 [INFO] Plot saved to: results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/training_error_history.png
2020-01-07 15:20:59,490 [INFO] Plot saved to: results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/training_f1_history.png
2020-01-07 15:20:59,490 [INFO] Making predictions on training, validation, testing data
2020-01-07 15:22:58,158 [INFO] Evaluating predictions (results)
/home/sunanda/anaconda3/envs/ml_env_cpu/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2020-01-07 15:23:17,996 [INFO] Dataset: Testing. Classification report below
2020-01-07 15:23:17,996 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       1.00      0.99      1.00    454265
                   Bot       0.97      0.36      0.53       391
                  DDoS       1.00      1.00      1.00     25605
         DoS GoldenEye       0.99      0.97      0.98      2058
              DoS Hulk       0.96      0.99      0.98     46025
      DoS Slowhttptest       0.92      0.98      0.95      1100
         DoS slowloris       0.99      0.96      0.97      1159
           FTP-Patator       1.00      0.99      0.99      1587
              PortScan       0.99      1.00      0.99     31761
           SSH-Patator       0.93      0.98      0.96      1179
Web Attack Brute Force       1.00      0.09      0.16       302
        Web Attack XSS       0.00      0.00      0.00       130

              accuracy                           0.99    565562
             macro avg       0.90      0.78      0.79    565562
          weighted avg       0.99      0.99      0.99    565562

2020-01-07 15:23:17,996 [INFO] Overall accuracy (micro avg): 0.9935692284842333
/home/sunanda/research/ids_experiments/utility.py:328: RuntimeWarning: invalid value encountered in true_divide
  PPV = TP / (TP + FP)    # Precision or positive predictive value
/home/sunanda/research/ids_experiments/utility.py:332: RuntimeWarning: invalid value encountered in true_divide
  FDR = FP / (TP + FP)    # False discovery rate
2020-01-07 15:23:39,311 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9936         0.9936                       0.9936                0.0006                   0.0064  0.9936
1     Macro avg        0.9989         0.8953                       0.7759                0.0013                   0.2241  0.7920
2  Weighted avg        0.9946         0.9934                       0.9936                0.0094                   0.0064  0.9932
2020-01-07 15:23:59,320 [INFO] Dataset: Validation. Classification report below
2020-01-07 15:23:59,321 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       1.00      0.99      1.00    454264
                   Bot       0.98      0.32      0.48       391
                  DDoS       1.00      1.00      1.00     25605
         DoS GoldenEye       0.99      0.96      0.98      2059
              DoS Hulk       0.96      0.99      0.98     46025
      DoS Slowhttptest       0.91      0.97      0.94      1099
         DoS slowloris       0.98      0.96      0.97      1159
           FTP-Patator       0.99      0.98      0.99      1587
              PortScan       0.99      1.00      0.99     31761
           SSH-Patator       0.93      0.97      0.95      1180
Web Attack Brute Force       0.94      0.05      0.10       301
        Web Attack XSS       1.00      0.03      0.06       131

              accuracy                           0.99    565562
             macro avg       0.97      0.77      0.79    565562
          weighted avg       0.99      0.99      0.99    565562

2020-01-07 15:23:59,321 [INFO] Overall accuracy (micro avg): 0.9935038068328494
2020-01-07 15:24:20,879 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9935         0.9935                       0.9935                0.0006                   0.0065  0.9935
1     Macro avg        0.9989         0.9736                       0.7689                0.0014                   0.2311  0.7860
2  Weighted avg        0.9945         0.9936                       0.9935                0.0097                   0.0065  0.9931
2020-01-07 15:25:27,623 [INFO] Dataset: Training. Classification report below
2020-01-07 15:25:27,623 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       1.00      1.00      1.00   1362791
                   Bot       0.98      0.35      0.52      1174
                  DDoS       1.00      1.00      1.00     76815
         DoS GoldenEye       0.99      0.97      0.98      6176
              DoS Hulk       0.96      0.99      0.98    138074
      DoS Slowhttptest       0.93      0.98      0.95      3300
         DoS slowloris       0.98      0.97      0.98      3478
           FTP-Patator       1.00      0.99      0.99      4761
              PortScan       0.99      1.00      0.99     95282
           SSH-Patator       0.94      0.98      0.96      3538
Web Attack Brute Force       0.99      0.07      0.14       904
        Web Attack XSS       1.00      0.03      0.06       391

              accuracy                           0.99   1696684
             macro avg       0.98      0.78      0.80   1696684
          weighted avg       0.99      0.99      0.99   1696684

2020-01-07 15:25:27,623 [INFO] Overall accuracy (micro avg): 0.9936676481890558
2020-01-07 15:26:39,549 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9937         0.9937                       0.9937                0.0006                   0.0063  0.9937
1     Macro avg        0.9989         0.9799                       0.7765                0.0013                   0.2235  0.7950
2  Weighted avg        0.9947         0.9938                       0.9937                0.0093                   0.0063  0.9933
2020-01-07 15:26:39,615 [INFO] Results saved to: results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/semi_sup_perf_ids17_ae_ann_rep2_results.xlsx
2020-01-07 15:26:39,618 [INFO] ================= Finished running experiment no. 2 ================= 

2020-01-07 15:26:39,660 [INFO] Created directory: results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3
2020-01-07 15:26:39,660 [INFO] Initialized logging. log_filename = results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/run_log.log
2020-01-07 15:26:39,660 [INFO] ================= Running experiment no. 3  ================= 

2020-01-07 15:26:39,660 [INFO] Experiment parameters given below
2020-01-07 15:26:39,660 [INFO] 
{'experiment_num': 3, 'results_dir': 'results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3', 'model_type': 'classifier', 'model': 'ae_ann', 'normal_label': 'BENIGN', 'split_random_seed': 42, 'unsupervised_ratio': 0.75, 'ae_encoder_units': [64], 'ae_encoder_activations': ['relu'], 'ae_encoder_dropout_rates': [0.2], 'ae_encoder_l1_param': -1, 'ae_decoder_units': '', 'ae_decoder_activations': '', 'ae_decoder_dropout_rates': '', 'output_activation': 'sigmoid', 'loss_function': 'binary_crossentropy', 'ae_epochs': 200, 'ann_layer_units': [64], 'ann_layer_activations': ['relu'], 'ann_layer_dropout_rates': [0.2], 'ann_epochs': 200, 'early_stop_patience': 50, 'batch_size': 256, 'dataset_dir': '../Datasets/full_datasets/ids2017', 'description': 'semi_sup_perf_ids17_ae_ann_rep3'}
2020-01-07 15:26:39,660 [INFO] Created tensorboard log directory: results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/tf_logs_run_2020_01_07-15_26_39
2020-01-07 15:26:39,660 [INFO] Loading datsets from: ../Datasets/full_datasets/ids2017
2020-01-07 15:26:39,661 [INFO] Reading X, y files
2020-01-07 15:26:39,661 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_train.h5
2020-01-07 15:26:43,437 [INFO] Reading complete. time_to_read=3.78 seconds
2020-01-07 15:26:43,437 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_val.h5
2020-01-07 15:26:44,720 [INFO] Reading complete. time_to_read=1.28 seconds
2020-01-07 15:26:44,724 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_test.h5
2020-01-07 15:26:46,006 [INFO] Reading complete. time_to_read=1.28 seconds
2020-01-07 15:26:46,006 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_train.h5
2020-01-07 15:26:46,266 [INFO] Reading complete. time_to_read=0.26 seconds
2020-01-07 15:26:46,266 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_val.h5
2020-01-07 15:26:46,349 [INFO] Reading complete. time_to_read=0.08 seconds
2020-01-07 15:26:46,349 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_test.h5
2020-01-07 15:26:46,432 [INFO] Reading complete. time_to_read=0.08 seconds
2020-01-07 15:26:49,378 [INFO] Initializing model
2020-01-07 15:26:49,503 [INFO] _________________________________________________________________
2020-01-07 15:26:49,504 [INFO] Layer (type)                 Output Shape              Param #   
2020-01-07 15:26:49,504 [INFO] =================================================================
2020-01-07 15:26:49,504 [INFO] dense_21 (Dense)             (None, 64)                5056      
2020-01-07 15:26:49,504 [INFO] _________________________________________________________________
2020-01-07 15:26:49,504 [INFO] batch_normalization_11 (Batc (None, 64)                256       
2020-01-07 15:26:49,504 [INFO] _________________________________________________________________
2020-01-07 15:26:49,504 [INFO] dropout_11 (Dropout)         (None, 64)                0         
2020-01-07 15:26:49,504 [INFO] _________________________________________________________________
2020-01-07 15:26:49,504 [INFO] dense_22 (Dense)             (None, 78)                5070      
2020-01-07 15:26:49,504 [INFO] =================================================================
2020-01-07 15:26:49,505 [INFO] Total params: 10,382
2020-01-07 15:26:49,505 [INFO] Trainable params: 10,254
2020-01-07 15:26:49,505 [INFO] Non-trainable params: 128
2020-01-07 15:26:49,505 [INFO] _________________________________________________________________
2020-01-07 15:26:49,627 [INFO] _________________________________________________________________
2020-01-07 15:26:49,627 [INFO] Layer (type)                 Output Shape              Param #   
2020-01-07 15:26:49,627 [INFO] =================================================================
2020-01-07 15:26:49,627 [INFO] dense_23 (Dense)             (None, 64)                4160      
2020-01-07 15:26:49,627 [INFO] _________________________________________________________________
2020-01-07 15:26:49,627 [INFO] batch_normalization_12 (Batc (None, 64)                256       
2020-01-07 15:26:49,627 [INFO] _________________________________________________________________
2020-01-07 15:26:49,627 [INFO] dropout_12 (Dropout)         (None, 64)                0         
2020-01-07 15:26:49,627 [INFO] _________________________________________________________________
2020-01-07 15:26:49,627 [INFO] dense_24 (Dense)             (None, 12)                780       
2020-01-07 15:26:49,628 [INFO] =================================================================
2020-01-07 15:26:49,628 [INFO] Total params: 5,196
2020-01-07 15:26:49,628 [INFO] Trainable params: 5,068
2020-01-07 15:26:49,628 [INFO] Non-trainable params: 128
2020-01-07 15:26:49,628 [INFO] _________________________________________________________________
2020-01-07 15:26:49,628 [INFO] Training model
2020-01-07 15:26:49,628 [INFO] Splitting train set into 2 sets (unsupervised, supervised), random_seed = 42
2020-01-07 15:27:14,591 [INFO] Split sizes (instances). total = 1696684, unsupervised = 1272513, supervised = 424171, unsupervised dataset hash = 07eca414d7fa8ed059bf34ec454d369f9e002f85
2020-01-07 15:27:14,591 [INFO] Training autoencoder
 - val_f1: 0.9928
Epoch 00134: early stopping
Train on 1272513 samples, validate on 565562 samples
Epoch 1/200
 - 24s - loss: -3.7939e+00 - val_loss: -4.1448e+00
Epoch 2/200
 - 23s - loss: -4.1136e+00 - val_loss: -4.1518e+00
Epoch 3/200
 - 23s - loss: -4.1241e+00 - val_loss: -4.1553e+00
Epoch 4/200
 - 23s - loss: -4.1291e+00 - val_loss: -4.1594e+00
Epoch 5/200
 - 23s - loss: -4.1322e+00 - val_loss: -4.1515e+00
Epoch 6/200
 - 23s - loss: -4.1342e+00 - val_loss: -4.1602e+00
Epoch 7/200
 - 23s - loss: -4.1355e+00 - val_loss: -4.1600e+00
Epoch 8/200
 - 23s - loss: -4.1364e+00 - val_loss: -4.1583e+00
Epoch 9/200
 - 23s - loss: -4.1374e+00 - val_loss: -4.1615e+00
Epoch 10/200
 - 23s - loss: -4.1378e+00 - val_loss: -4.1618e+00
Epoch 11/200
 - 23s - loss: -4.1385e+00 - val_loss: -4.1636e+00
Epoch 12/200
 - 23s - loss: -4.1388e+00 - val_loss: -4.1621e+00
Epoch 13/200
 - 23s - loss: -4.1391e+00 - val_loss: -4.1624e+00
Epoch 14/200
 - 23s - loss: -4.1395e+00 - val_loss: -4.1627e+00
Epoch 15/200
 - 23s - loss: -4.1395e+00 - val_loss: -4.1628e+00
Epoch 16/200
 - 23s - loss: -4.1402e+00 - val_loss: -4.1630e+00
Epoch 17/200
 - 23s - loss: -4.1403e+00 - val_loss: -4.1638e+00
Epoch 18/200
 - 23s - loss: -4.1403e+00 - val_loss: -4.1649e+00
Epoch 19/200
 - 23s - loss: -4.1406e+00 - val_loss: -4.1627e+00
Epoch 20/200
 - 23s - loss: -4.1408e+00 - val_loss: -4.1636e+00
Epoch 21/200
 - 23s - loss: -4.1414e+00 - val_loss: -4.1636e+00
2020-01-07 15:35:21,729 [INFO] epoch = 20. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/ae_model_epoch_20.pickle
Epoch 22/200
 - 23s - loss: -4.1414e+00 - val_loss: -4.1636e+00
Epoch 23/200
 - 23s - loss: -4.1416e+00 - val_loss: -4.1647e+00
Epoch 24/200
 - 23s - loss: -4.1417e+00 - val_loss: -4.1633e+00
Epoch 25/200
 - 23s - loss: -4.1419e+00 - val_loss: -4.1639e+00
Epoch 26/200
 - 23s - loss: -4.1422e+00 - val_loss: -4.1635e+00
Epoch 27/200
 - 23s - loss: -4.1421e+00 - val_loss: -4.1641e+00
Epoch 28/200
 - 23s - loss: -4.1424e+00 - val_loss: -4.1642e+00
Epoch 29/200
 - 23s - loss: -4.1427e+00 - val_loss: -4.1647e+00
Epoch 30/200
 - 23s - loss: -4.1426e+00 - val_loss: -4.1659e+00
Epoch 31/200
 - 23s - loss: -4.1428e+00 - val_loss: -4.1640e+00
Epoch 32/200
 - 23s - loss: -4.1428e+00 - val_loss: -4.1640e+00
Epoch 33/200
 - 23s - loss: -4.1428e+00 - val_loss: -4.1651e+00
Epoch 34/200
 - 23s - loss: -4.1429e+00 - val_loss: -4.1641e+00
Epoch 35/200
 - 23s - loss: -4.1431e+00 - val_loss: -4.1645e+00
Epoch 36/200
 - 23s - loss: -4.1431e+00 - val_loss: -4.1637e+00
Epoch 37/200
 - 23s - loss: -4.1427e+00 - val_loss: -4.1657e+00
Epoch 38/200
 - 23s - loss: -4.1432e+00 - val_loss: -4.1651e+00
Epoch 39/200
 - 23s - loss: -4.1433e+00 - val_loss: -4.1647e+00
Epoch 40/200
 - 23s - loss: -4.1433e+00 - val_loss: -4.1657e+00
Epoch 41/200
 - 23s - loss: -4.1436e+00 - val_loss: -4.1658e+00
2020-01-07 15:43:02,485 [INFO] epoch = 40. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/ae_model_epoch_40.pickle
Epoch 42/200
 - 23s - loss: -4.1433e+00 - val_loss: -4.1646e+00
Epoch 43/200
 - 23s - loss: -4.1435e+00 - val_loss: -4.1651e+00
Epoch 44/200
 - 23s - loss: -4.1436e+00 - val_loss: -4.1660e+00
Epoch 45/200
 - 23s - loss: -4.1437e+00 - val_loss: -4.1661e+00
Epoch 46/200
 - 23s - loss: -4.1436e+00 - val_loss: -4.1623e+00
Epoch 47/200
 - 23s - loss: -4.1437e+00 - val_loss: -4.1643e+00
Epoch 48/200
 - 23s - loss: -4.1437e+00 - val_loss: -4.1653e+00
Epoch 49/200
 - 23s - loss: -4.1438e+00 - val_loss: -4.1644e+00
Epoch 50/200
 - 23s - loss: -4.1438e+00 - val_loss: -4.1661e+00
Epoch 51/200
 - 23s - loss: -4.1436e+00 - val_loss: -4.1657e+00
Epoch 52/200
 - 23s - loss: -4.1438e+00 - val_loss: -4.1645e+00
Epoch 53/200
 - 23s - loss: -4.1437e+00 - val_loss: -4.1649e+00
Epoch 54/200
 - 23s - loss: -4.1441e+00 - val_loss: -4.1627e+00
Epoch 55/200
 - 23s - loss: -4.1439e+00 - val_loss: -4.1645e+00
Epoch 56/200
 - 23s - loss: -4.1441e+00 - val_loss: -4.1656e+00
Epoch 57/200
 - 23s - loss: -4.1437e+00 - val_loss: -4.1655e+00
Epoch 58/200
 - 23s - loss: -4.1438e+00 - val_loss: -4.1649e+00
Epoch 59/200
 - 23s - loss: -4.1439e+00 - val_loss: -4.1642e+00
Epoch 60/200
 - 23s - loss: -4.1442e+00 - val_loss: -4.1652e+00
Epoch 61/200
 - 23s - loss: -4.1440e+00 - val_loss: -4.1664e+00
2020-01-07 15:50:43,419 [INFO] epoch = 60. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/ae_model_epoch_60.pickle
Epoch 62/200
 - 23s - loss: -4.1440e+00 - val_loss: -4.1651e+00
Epoch 63/200
 - 23s - loss: -4.1441e+00 - val_loss: -4.1641e+00
Epoch 64/200
 - 23s - loss: -4.1439e+00 - val_loss: -4.1652e+00
Epoch 65/200
 - 23s - loss: -4.1441e+00 - val_loss: -4.1661e+00
Epoch 66/200
 - 23s - loss: -4.1443e+00 - val_loss: -4.1654e+00
Epoch 67/200
 - 23s - loss: -4.1442e+00 - val_loss: -4.1661e+00
Epoch 68/200
 - 23s - loss: -4.1444e+00 - val_loss: -4.1654e+00
Epoch 69/200
 - 23s - loss: -4.1441e+00 - val_loss: -4.1660e+00
Epoch 70/200
 - 23s - loss: -4.1444e+00 - val_loss: -4.1657e+00
Epoch 71/200
 - 23s - loss: -4.1441e+00 - val_loss: -4.1663e+00
Epoch 72/200
 - 23s - loss: -4.1444e+00 - val_loss: -4.1653e+00
Epoch 73/200
 - 23s - loss: -4.1443e+00 - val_loss: -4.1651e+00
Epoch 74/200
 - 23s - loss: -4.1444e+00 - val_loss: -4.1654e+00
Epoch 75/200
 - 23s - loss: -4.1444e+00 - val_loss: -4.1655e+00
Epoch 76/200
 - 23s - loss: -4.1442e+00 - val_loss: -4.1662e+00
Epoch 77/200
 - 23s - loss: -4.1444e+00 - val_loss: -4.1654e+00
Epoch 78/200
 - 23s - loss: -4.1443e+00 - val_loss: -4.1656e+00
Epoch 79/200
 - 23s - loss: -4.1445e+00 - val_loss: -4.1653e+00
Epoch 80/200
 - 23s - loss: -4.1444e+00 - val_loss: -4.1658e+00
Epoch 81/200
 - 23s - loss: -4.1446e+00 - val_loss: -4.1584e+00
2020-01-07 15:58:23,515 [INFO] epoch = 80. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/ae_model_epoch_80.pickle
Epoch 82/200
 - 23s - loss: -4.1444e+00 - val_loss: -4.1644e+00
Epoch 83/200
 - 23s - loss: -4.1445e+00 - val_loss: -4.1650e+00
Epoch 84/200
 - 23s - loss: -4.1445e+00 - val_loss: -4.1588e+00
Epoch 85/200
 - 23s - loss: -4.1443e+00 - val_loss: -4.1653e+00
Epoch 86/200
 - 23s - loss: -4.1446e+00 - val_loss: -4.1662e+00
Epoch 87/200
 - 23s - loss: -4.1443e+00 - val_loss: -4.1655e+00
Epoch 88/200
 - 23s - loss: -4.1446e+00 - val_loss: -4.1653e+00
Epoch 89/200
 - 23s - loss: -4.1446e+00 - val_loss: -4.1606e+00
Epoch 90/200
 - 23s - loss: -4.1447e+00 - val_loss: -4.1668e+00
Epoch 91/200
 - 23s - loss: -4.1447e+00 - val_loss: -4.1664e+00
Epoch 92/200
 - 23s - loss: -4.1446e+00 - val_loss: -4.1658e+00
Epoch 93/200
 - 23s - loss: -4.1446e+00 - val_loss: -4.1662e+00
Epoch 94/200
 - 23s - loss: -4.1448e+00 - val_loss: -4.1663e+00
Epoch 95/200
 - 23s - loss: -4.1445e+00 - val_loss: -4.1643e+00
Epoch 96/200
 - 23s - loss: -4.1446e+00 - val_loss: -4.1666e+00
Epoch 97/200
 - 23s - loss: -4.1445e+00 - val_loss: -4.1659e+00
Epoch 98/200
 - 23s - loss: -4.1448e+00 - val_loss: -4.1648e+00
Epoch 99/200
 - 23s - loss: -4.1447e+00 - val_loss: -4.1653e+00
Epoch 100/200
 - 23s - loss: -4.1449e+00 - val_loss: -4.1655e+00
Epoch 101/200
 - 23s - loss: -4.1447e+00 - val_loss: -4.1649e+00
2020-01-07 16:06:04,068 [INFO] epoch = 100. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/ae_model_epoch_100.pickle
Epoch 102/200
 - 23s - loss: -4.1449e+00 - val_loss: -4.1656e+00
Epoch 103/200
 - 23s - loss: -4.1449e+00 - val_loss: -4.1646e+00
Epoch 104/200
 - 23s - loss: -4.1449e+00 - val_loss: -4.1663e+00
Epoch 105/200
 - 23s - loss: -4.1449e+00 - val_loss: -4.1666e+00
Epoch 106/200
 - 23s - loss: -4.1449e+00 - val_loss: -4.1588e+00
Epoch 107/200
 - 23s - loss: -4.1448e+00 - val_loss: -4.1649e+00
Epoch 108/200
 - 23s - loss: -4.1450e+00 - val_loss: -4.1650e+00
Epoch 109/200
 - 23s - loss: -4.1449e+00 - val_loss: -4.1654e+00
Epoch 110/200
 - 23s - loss: -4.1448e+00 - val_loss: -4.1650e+00
Epoch 111/200
 - 23s - loss: -4.1450e+00 - val_loss: -4.1645e+00
Epoch 112/200
 - 23s - loss: -4.1448e+00 - val_loss: -4.1642e+00
Epoch 113/200
 - 23s - loss: -4.1448e+00 - val_loss: -4.1651e+00
Epoch 114/200
 - 23s - loss: -4.1451e+00 - val_loss: -4.1667e+00
Epoch 115/200
 - 23s - loss: -4.1450e+00 - val_loss: -4.1647e+00
Epoch 116/200
 - 23s - loss: -4.1451e+00 - val_loss: -4.1667e+00
Epoch 117/200
 - 23s - loss: -4.1451e+00 - val_loss: -4.1643e+00
Epoch 118/200
 - 23s - loss: -4.1453e+00 - val_loss: -4.1655e+00
Epoch 119/200
 - 23s - loss: -4.1450e+00 - val_loss: -4.1663e+00
Epoch 120/200
 - 23s - loss: -4.1450e+00 - val_loss: -4.1666e+00
Epoch 121/200
 - 23s - loss: -4.1450e+00 - val_loss: -4.1656e+00
2020-01-07 16:13:44,897 [INFO] epoch = 120. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/ae_model_epoch_120.pickle
Epoch 122/200
 - 23s - loss: -4.1450e+00 - val_loss: -4.1654e+00
Epoch 123/200
 - 23s - loss: -4.1451e+00 - val_loss: -4.1657e+00
Epoch 124/200
 - 23s - loss: -4.1451e+00 - val_loss: -4.1665e+00
Epoch 125/200
 - 23s - loss: -4.1451e+00 - val_loss: -4.1660e+00
Epoch 126/200
 - 23s - loss: -4.1451e+00 - val_loss: -4.1665e+00
Epoch 127/200
 - 23s - loss: -4.1452e+00 - val_loss: -4.1672e+00
Epoch 128/200
 - 23s - loss: -4.1453e+00 - val_loss: -4.1664e+00
Epoch 129/200
 - 23s - loss: -4.1449e+00 - val_loss: -4.1666e+00
Epoch 130/200
 - 23s - loss: -4.1452e+00 - val_loss: -4.1669e+00
Epoch 131/200
 - 23s - loss: -4.1449e+00 - val_loss: -4.1668e+00
Epoch 132/200
 - 23s - loss: -4.1453e+00 - val_loss: -4.1659e+00
Epoch 133/200
 - 23s - loss: -4.1453e+00 - val_loss: -4.1667e+00
Epoch 134/200
 - 23s - loss: -4.1450e+00 - val_loss: -4.1667e+00
Epoch 135/200
 - 23s - loss: -4.1452e+00 - val_loss: -4.1663e+00
Epoch 136/200
 - 23s - loss: -4.1451e+00 - val_loss: -4.1664e+00
Epoch 137/200
 - 23s - loss: -4.1452e+00 - val_loss: -4.1586e+00
Epoch 138/200
 - 23s - loss: -4.1452e+00 - val_loss: -4.1653e+00
Epoch 139/200
 - 23s - loss: -4.1452e+00 - val_loss: -4.1655e+00
Epoch 140/200
 - 23s - loss: -4.1453e+00 - val_loss: -4.1666e+00
Epoch 141/200
 - 23s - loss: -4.1452e+00 - val_loss: -4.1654e+00
2020-01-07 16:21:25,361 [INFO] epoch = 140. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/ae_model_epoch_140.pickle
Epoch 142/200
 - 23s - loss: -4.1450e+00 - val_loss: -4.1661e+00
Epoch 143/200
 - 23s - loss: -4.1453e+00 - val_loss: -4.1672e+00
Epoch 144/200
 - 23s - loss: -4.1451e+00 - val_loss: -4.1659e+00
Epoch 145/200
 - 23s - loss: -4.1452e+00 - val_loss: -4.1635e+00
Epoch 146/200
 - 23s - loss: -4.1453e+00 - val_loss: -4.1666e+00
Epoch 147/200
 - 23s - loss: -4.1453e+00 - val_loss: -4.1672e+00
Epoch 148/200
 - 23s - loss: -4.1454e+00 - val_loss: -4.1668e+00
Epoch 149/200
 - 23s - loss: -4.1454e+00 - val_loss: -4.1669e+00
Epoch 150/200
 - 23s - loss: -4.1454e+00 - val_loss: -4.1650e+00
Epoch 151/200
 - 23s - loss: -4.1454e+00 - val_loss: -4.1657e+00
Epoch 152/200
 - 23s - loss: -4.1453e+00 - val_loss: -4.1665e+00
Epoch 153/200
 - 23s - loss: -4.1456e+00 - val_loss: -4.1648e+00
Epoch 154/200
 - 23s - loss: -4.1456e+00 - val_loss: -4.1651e+00
Epoch 155/200
 - 23s - loss: -4.1454e+00 - val_loss: -4.1664e+00
Epoch 156/200
 - 23s - loss: -4.1454e+00 - val_loss: -4.1671e+00
Epoch 157/200
 - 23s - loss: -4.1455e+00 - val_loss: -4.1651e+00
Epoch 158/200
 - 23s - loss: -4.1454e+00 - val_loss: -4.1655e+00
Epoch 159/200
 - 23s - loss: -4.1450e+00 - val_loss: -4.1658e+00
Epoch 160/200
 - 23s - loss: -4.1456e+00 - val_loss: -4.1660e+00
Epoch 161/200
 - 23s - loss: -4.1455e+00 - val_loss: -4.1660e+00
2020-01-07 16:29:05,660 [INFO] epoch = 160. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/ae_model_epoch_160.pickle
Epoch 162/200
 - 23s - loss: -4.1456e+00 - val_loss: -4.1651e+00
Epoch 163/200
 - 23s - loss: -4.1456e+00 - val_loss: -4.1656e+00
Epoch 164/200
 - 23s - loss: -4.1454e+00 - val_loss: -4.1651e+00
Epoch 165/200
 - 23s - loss: -4.1456e+00 - val_loss: -4.1671e+00
Epoch 166/200
 - 23s - loss: -4.1455e+00 - val_loss: -4.1664e+00
Epoch 167/200
 - 23s - loss: -4.1455e+00 - val_loss: -4.1658e+00
Epoch 168/200
 - 23s - loss: -4.1457e+00 - val_loss: -4.1669e+00
Epoch 169/200
 - 23s - loss: -4.1456e+00 - val_loss: -4.1659e+00
Epoch 170/200
 - 23s - loss: -4.1457e+00 - val_loss: -4.1668e+00
Epoch 171/200
 - 23s - loss: -4.1456e+00 - val_loss: -4.1655e+00
Epoch 172/200
 - 23s - loss: -4.1457e+00 - val_loss: -4.1657e+00
Epoch 173/200
 - 23s - loss: -4.1456e+00 - val_loss: -4.1658e+00
Epoch 174/200
 - 23s - loss: -4.1457e+00 - val_loss: -4.1671e+00
Epoch 175/200
 - 23s - loss: -4.1457e+00 - val_loss: -4.1668e+00
Epoch 176/200
 - 23s - loss: -4.1457e+00 - val_loss: -4.1672e+00
Epoch 177/200
 - 23s - loss: -4.1455e+00 - val_loss: -4.1668e+00
Epoch 178/200
 - 23s - loss: -4.1457e+00 - val_loss: -4.1669e+00
Epoch 179/200
 - 23s - loss: -4.1459e+00 - val_loss: -4.1667e+00
Epoch 180/200
 - 23s - loss: -4.1455e+00 - val_loss: -4.1663e+00
Epoch 181/200
 - 23s - loss: -4.1458e+00 - val_loss: -4.1657e+00
2020-01-07 16:36:45,501 [INFO] epoch = 180. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/ae_model_epoch_180.pickle
Epoch 182/200
 - 23s - loss: -4.1457e+00 - val_loss: -4.1647e+00
Epoch 183/200
 - 23s - loss: -4.1456e+00 - val_loss: -4.1674e+00
Epoch 184/200
 - 23s - loss: -4.1457e+00 - val_loss: -4.1655e+00
Epoch 185/200
 - 23s - loss: -4.1456e+00 - val_loss: -4.1662e+00
Epoch 186/200
 - 23s - loss: -4.1454e+00 - val_loss: -4.1668e+00
Epoch 187/200
 - 23s - loss: -4.1458e+00 - val_loss: -4.1670e+00
Epoch 188/200
 - 23s - loss: -4.1457e+00 - val_loss: -4.1667e+00
Epoch 189/200
 - 23s - loss: -4.1459e+00 - val_loss: -4.1668e+00
Epoch 190/200
 - 23s - loss: -4.1457e+00 - val_loss: -4.1658e+00
Epoch 191/200
 - 23s - loss: -4.1461e+00 - val_loss: -4.1667e+00
Epoch 192/200
 - 23s - loss: -4.1458e+00 - val_loss: -4.1665e+00
Epoch 193/200
 - 23s - loss: -4.1458e+00 - val_loss: -4.1661e+00
Epoch 194/200
 - 23s - loss: -4.1457e+00 - val_loss: -4.1659e+00
Epoch 195/200
 - 23s - loss: -4.1456e+00 - val_loss: -4.1672e+00
Epoch 196/200
 - 23s - loss: -4.1458e+00 - val_loss: -4.1660e+00
Epoch 197/200
 - 23s - loss: -4.1457e+00 - val_loss: -4.1669e+00
Epoch 198/200
 - 23s - loss: -4.1459e+00 - val_loss: -4.1660e+00
Epoch 199/200
 - 23s - loss: -4.1460e+00 - val_loss: -4.1657e+00
Epoch 200/200
 - 23s - loss: -4.1458e+00 - val_loss: -4.1663e+00
2020-01-07 16:44:02,640 [INFO] WeightRestorer::on_train_end(): restoring best weights
2020-01-07 16:44:54,493 [INFO] Last epoch loss evaluation: train_loss = -4.165631, val_loss = -4.167374
2020-01-07 16:44:54,493 [INFO] Training autoencoder complete
2020-01-07 16:44:54,493 [INFO] Encoding data for supervised training
2020-01-07 16:45:15,554 [INFO] Encoding complete
2020-01-07 16:45:15,555 [INFO] Training neural network layers (after autoencoder)
Train on 424171 samples, validate on 565562 samples
Epoch 1/200
 - 7s - loss: 0.0319 - val_loss: 0.0136
 - val_f1: 0.9616
Epoch 2/200
 - 6s - loss: 0.0128 - val_loss: 0.0122
 - val_f1: 0.9662
Epoch 3/200
 - 6s - loss: 0.0108 - val_loss: 0.0112
 - val_f1: 0.9660
Epoch 4/200
 - 6s - loss: 0.0098 - val_loss: 0.0081
 - val_f1: 0.9766
Epoch 5/200
 - 6s - loss: 0.0091 - val_loss: 0.0083
 - val_f1: 0.9782
Epoch 6/200
 - 6s - loss: 0.0086 - val_loss: 0.0080
 - val_f1: 0.9781
Epoch 7/200
 - 6s - loss: 0.0081 - val_loss: 0.0077
 - val_f1: 0.9804
Epoch 8/200
 - 6s - loss: 0.0081 - val_loss: 0.0076
 - val_f1: 0.9755
Epoch 9/200
 - 6s - loss: 0.0077 - val_loss: 0.0086
 - val_f1: 0.9742
Epoch 10/200
 - 6s - loss: 0.0076 - val_loss: 0.0072
 - val_f1: 0.9800
Epoch 11/200
 - 6s - loss: 0.0073 - val_loss: 0.0059
 - val_f1: 0.9882
Epoch 12/200
 - 6s - loss: 0.0072 - val_loss: 0.0065
 - val_f1: 0.9855
Epoch 13/200
 - 6s - loss: 0.0073 - val_loss: 0.0078
 - val_f1: 0.9803
Epoch 14/200
 - 6s - loss: 0.0066 - val_loss: 0.0073
 - val_f1: 0.9805
Epoch 15/200
 - 6s - loss: 0.0068 - val_loss: 0.0076
 - val_f1: 0.9767
Epoch 16/200
 - 6s - loss: 0.0077 - val_loss: 0.0066
 - val_f1: 0.9847
Epoch 17/200
 - 6s - loss: 0.0073 - val_loss: 0.0112
 - val_f1: 0.9637
Epoch 18/200
 - 6s - loss: 0.0072 - val_loss: 0.0084
 - val_f1: 0.9723
Epoch 19/200
 - 6s - loss: 0.0070 - val_loss: 0.0060
 - val_f1: 0.9846
Epoch 20/200
 - 6s - loss: 0.0066 - val_loss: 0.0067
 - val_f1: 0.9831
Epoch 21/200
 - 6s - loss: 0.0064 - val_loss: 0.0086
/home/sunanda/anaconda3/envs/ml_env_cpu/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2020-01-07 16:52:07,933 [INFO] epoch = 20. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/ann_model_epoch_20.pickle
 - val_f1: 0.9758
Epoch 22/200
 - 6s - loss: 0.0064 - val_loss: 0.0063
 - val_f1: 0.9853
Epoch 23/200
 - 6s - loss: 0.0060 - val_loss: 0.0061
 - val_f1: 0.9858
Epoch 24/200
 - 6s - loss: 0.0067 - val_loss: 0.0064
 - val_f1: 0.9833
Epoch 25/200
 - 6s - loss: 0.0071 - val_loss: 0.0068
 - val_f1: 0.9833
Epoch 26/200
 - 6s - loss: 0.0075 - val_loss: 0.0075
 - val_f1: 0.9817
Epoch 27/200
 - 6s - loss: 0.0070 - val_loss: 0.0067
 - val_f1: 0.9809
Epoch 28/200
 - 6s - loss: 0.0066 - val_loss: 0.0053
 - val_f1: 0.9906
Epoch 29/200
 - 6s - loss: 0.0062 - val_loss: 0.0062
 - val_f1: 0.9875
Epoch 30/200
 - 6s - loss: 0.0057 - val_loss: 0.0059
 - val_f1: 0.9855
Epoch 31/200
 - 6s - loss: 0.0053 - val_loss: 0.0046
 - val_f1: 0.9891
Epoch 32/200
 - 6s - loss: 0.0053 - val_loss: 0.0044
 - val_f1: 0.9903
Epoch 33/200
 - 6s - loss: 0.0052 - val_loss: 0.0057
 - val_f1: 0.9871
Epoch 34/200
 - 6s - loss: 0.0053 - val_loss: 0.0047
 - val_f1: 0.9902
Epoch 35/200
 - 6s - loss: 0.0053 - val_loss: 0.0053
 - val_f1: 0.9871
Epoch 36/200
 - 6s - loss: 0.0053 - val_loss: 0.0041
 - val_f1: 0.9924
Epoch 37/200
 - 6s - loss: 0.0052 - val_loss: 0.0040
 - val_f1: 0.9923
Epoch 38/200
 - 6s - loss: 0.0051 - val_loss: 0.0042
 - val_f1: 0.9914
Epoch 39/200
 - 6s - loss: 0.0051 - val_loss: 0.0040
 - val_f1: 0.9909
Epoch 40/200
 - 6s - loss: 0.0051 - val_loss: 0.0051
 - val_f1: 0.9877
Epoch 41/200
 - 6s - loss: 0.0049 - val_loss: 0.0037
2020-01-07 16:58:48,860 [INFO] epoch = 40. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/ann_model_epoch_40.pickle
 - val_f1: 0.9929
Epoch 42/200
 - 6s - loss: 0.0048 - val_loss: 0.0039
 - val_f1: 0.9921
Epoch 43/200
 - 6s - loss: 0.0047 - val_loss: 0.0037
 - val_f1: 0.9900
Epoch 44/200
 - 6s - loss: 0.0049 - val_loss: 0.0045
 - val_f1: 0.9896
Epoch 45/200
 - 6s - loss: 0.0050 - val_loss: 0.0044
 - val_f1: 0.9895
Epoch 46/200
 - 6s - loss: 0.0051 - val_loss: 0.0035
 - val_f1: 0.9934
Epoch 47/200
 - 6s - loss: 0.0049 - val_loss: 0.0038
 - val_f1: 0.9923
Epoch 48/200
 - 6s - loss: 0.0049 - val_loss: 0.0054
 - val_f1: 0.9878
Epoch 49/200
 - 6s - loss: 0.0050 - val_loss: 0.0038
 - val_f1: 0.9927
Epoch 50/200
 - 6s - loss: 0.0050 - val_loss: 0.0054
 - val_f1: 0.9899
Epoch 51/200
 - 6s - loss: 0.0050 - val_loss: 0.0048
 - val_f1: 0.9885
Epoch 52/200
 - 6s - loss: 0.0048 - val_loss: 0.0056
 - val_f1: 0.9846
Epoch 53/200
 - 6s - loss: 0.0047 - val_loss: 0.0037
 - val_f1: 0.9914
Epoch 54/200
 - 6s - loss: 0.0048 - val_loss: 0.0040
 - val_f1: 0.9928
Epoch 55/200
 - 6s - loss: 0.0048 - val_loss: 0.0035
 - val_f1: 0.9914
Epoch 56/200
 - 6s - loss: 0.0047 - val_loss: 0.0037
 - val_f1: 0.9904
Epoch 57/200
 - 6s - loss: 0.0047 - val_loss: 0.0064
 - val_f1: 0.9833
Epoch 58/200
 - 6s - loss: 0.0046 - val_loss: 0.0035
 - val_f1: 0.9940
Epoch 59/200
 - 6s - loss: 0.0046 - val_loss: 0.0051
 - val_f1: 0.9873
Epoch 60/200
 - 6s - loss: 0.0046 - val_loss: 0.0056
 - val_f1: 0.9879
Epoch 61/200
 - 6s - loss: 0.0053 - val_loss: 0.0064
2020-01-07 17:05:29,900 [INFO] epoch = 60. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/ann_model_epoch_60.pickle
 - val_f1: 0.9868
Epoch 62/200
 - 6s - loss: 0.0052 - val_loss: 0.0039
 - val_f1: 0.9925
Epoch 63/200
 - 6s - loss: 0.0048 - val_loss: 0.0052
 - val_f1: 0.9878
Epoch 64/200
 - 6s - loss: 0.0050 - val_loss: 0.0050
 - val_f1: 0.9886
Epoch 65/200
 - 6s - loss: 0.0049 - val_loss: 0.0042
 - val_f1: 0.9896
Epoch 66/200
 - 6s - loss: 0.0047 - val_loss: 0.0048
 - val_f1: 0.9883
Epoch 67/200
 - 6s - loss: 0.0052 - val_loss: 0.0040
 - val_f1: 0.9940
Epoch 68/200
 - 6s - loss: 0.0052 - val_loss: 0.0053
 - val_f1: 0.9864
Epoch 69/200
 - 6s - loss: 0.0048 - val_loss: 0.0037
 - val_f1: 0.9924
Epoch 70/200
 - 6s - loss: 0.0047 - val_loss: 0.0040
 - val_f1: 0.9909
Epoch 71/200
 - 6s - loss: 0.0047 - val_loss: 0.0039
 - val_f1: 0.9906
Epoch 72/200
 - 6s - loss: 0.0046 - val_loss: 0.0041
 - val_f1: 0.9915
Epoch 73/200
 - 6s - loss: 0.0045 - val_loss: 0.0055
 - val_f1: 0.9859
Epoch 74/200
 - 6s - loss: 0.0043 - val_loss: 0.0044
 - val_f1: 0.9893
Epoch 75/200
 - 6s - loss: 0.0044 - val_loss: 0.0037
 - val_f1: 0.9918
Epoch 76/200
 - 6s - loss: 0.0044 - val_loss: 0.0040
 - val_f1: 0.9924
Epoch 77/200
 - 6s - loss: 0.0044 - val_loss: 0.0039
 - val_f1: 0.9917
Epoch 78/200
 - 6s - loss: 0.0045 - val_loss: 0.0046
 - val_f1: 0.9876
Epoch 79/200
 - 6s - loss: 0.0045 - val_loss: 0.0040
 - val_f1: 0.9920
Epoch 80/200
 - 6s - loss: 0.0044 - val_loss: 0.0036
 - val_f1: 0.9923
Epoch 81/200
 - 6s - loss: 0.0043 - val_loss: 0.0036
2020-01-07 17:12:10,931 [INFO] epoch = 80. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/ann_model_epoch_80.pickle
 - val_f1: 0.9934
Epoch 82/200
 - 6s - loss: 0.0042 - val_loss: 0.0033
 - val_f1: 0.9935
Epoch 83/200
 - 6s - loss: 0.0044 - val_loss: 0.0035
 - val_f1: 0.9915
Epoch 84/200
 - 6s - loss: 0.0044 - val_loss: 0.0035
 - val_f1: 0.9919
Epoch 85/200
 - 6s - loss: 0.0043 - val_loss: 0.0041
 - val_f1: 0.9910
Epoch 86/200
 - 6s - loss: 0.0044 - val_loss: 0.0034
 - val_f1: 0.9918
Epoch 87/200
 - 6s - loss: 0.0045 - val_loss: 0.0050
 - val_f1: 0.9876
Epoch 88/200
 - 6s - loss: 0.0045 - val_loss: 0.0034
 - val_f1: 0.9920
Epoch 89/200
 - 6s - loss: 0.0043 - val_loss: 0.0036
 - val_f1: 0.9932
Epoch 90/200
 - 6s - loss: 0.0043 - val_loss: 0.0035
 - val_f1: 0.9923
Epoch 91/200
 - 6s - loss: 0.0043 - val_loss: 0.0041
 - val_f1: 0.9927
Epoch 92/200
 - 6s - loss: 0.0043 - val_loss: 0.0032
 - val_f1: 0.9945
Epoch 93/200
 - 6s - loss: 0.0044 - val_loss: 0.0033
 - val_f1: 0.9922
Epoch 94/200
 - 6s - loss: 0.0043 - val_loss: 0.0045
 - val_f1: 0.9885
Epoch 95/200
 - 6s - loss: 0.0043 - val_loss: 0.0034
 - val_f1: 0.9931
Epoch 96/200
 - 6s - loss: 0.0043 - val_loss: 0.0039
 - val_f1: 0.9933
Epoch 97/200
 - 6s - loss: 0.0041 - val_loss: 0.0033
 - val_f1: 0.9941
Epoch 98/200
 - 6s - loss: 0.0041 - val_loss: 0.0037
 - val_f1: 0.9906
Epoch 99/200
 - 6s - loss: 0.0042 - val_loss: 0.0037
 - val_f1: 0.9925
Epoch 100/200
 - 6s - loss: 0.0050 - val_loss: 0.0045
 - val_f1: 0.9905
Epoch 101/200
 - 6s - loss: 0.0046 - val_loss: 0.0039
2020-01-07 17:18:51,928 [INFO] epoch = 100. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/ann_model_epoch_100.pickle
 - val_f1: 0.9910
Epoch 102/200
 - 6s - loss: 0.0047 - val_loss: 0.0042
 - val_f1: 0.9904
Epoch 103/200
 - 6s - loss: 0.0047 - val_loss: 0.0038
 - val_f1: 0.9903
Epoch 104/200
 - 6s - loss: 0.0045 - val_loss: 0.0034
 - val_f1: 0.9937
Epoch 105/200
 - 6s - loss: 0.0048 - val_loss: 0.0039
 - val_f1: 0.9916
Epoch 106/200
 - 6s - loss: 0.0049 - val_loss: 0.0036
 - val_f1: 0.9935
Epoch 107/200
 - 6s - loss: 0.0046 - val_loss: 0.0059
 - val_f1: 0.9827
Epoch 108/200
 - 6s - loss: 0.0048 - val_loss: 0.0047
 - val_f1: 0.9904
Epoch 109/200
 - 6s - loss: 0.0047 - val_loss: 0.0034
 - val_f1: 0.9937
Epoch 110/200
 - 6s - loss: 0.0048 - val_loss: 0.0077
 - val_f1: 0.9792
Epoch 111/200
 - 6s - loss: 0.0059 - val_loss: 0.0043
 - val_f1: 0.9923
Epoch 112/200
 - 6s - loss: 0.0049 - val_loss: 0.0041
 - val_f1: 0.9908
Epoch 113/200
 - 6s - loss: 0.0060 - val_loss: 0.0068
 - val_f1: 0.9823
Epoch 114/200
 - 6s - loss: 0.0051 - val_loss: 0.0038
 - val_f1: 0.9913
Epoch 115/200
 - 6s - loss: 0.0047 - val_loss: 0.0044
 - val_f1: 0.9914
Epoch 116/200
 - 6s - loss: 0.0044 - val_loss: 0.0035
 - val_f1: 0.9904
Epoch 117/200
 - 6s - loss: 0.0051 - val_loss: 0.0043
 - val_f1: 0.9920
Epoch 118/200
 - 6s - loss: 0.0051 - val_loss: 0.0041
 - val_f1: 0.9918
Epoch 119/200
 - 6s - loss: 0.0051 - val_loss: 0.0037
 - val_f1: 0.9930
Epoch 120/200
 - 6s - loss: 0.0045 - val_loss: 0.0035
 - val_f1: 0.9931
Epoch 121/200
 - 6s - loss: 0.0042 - val_loss: 0.0039
2020-01-07 17:25:32,731 [INFO] epoch = 120. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/ann_model_epoch_120.pickle
 - val_f1: 0.9926
Epoch 122/200
 - 6s - loss: 0.0044 - val_loss: 0.0037
 - val_f1: 0.9936
Epoch 123/200
 - 6s - loss: 0.0045 - val_loss: 0.0039
 - val_f1: 0.9936
Epoch 124/200
 - 6s - loss: 0.0046 - val_loss: 0.0066
 - val_f1: 0.9832
Epoch 125/200
 - 6s - loss: 0.0057 - val_loss: 0.0044
 - val_f1: 0.9916
Epoch 126/200
 - 6s - loss: 0.0050 - val_loss: 0.0040
 - val_f1: 0.9916
Epoch 127/200
 - 6s - loss: 0.0045 - val_loss: 0.0033
 - val_f1: 0.9940
Epoch 128/200
 - 6s - loss: 0.0045 - val_loss: 0.0033
 - val_f1: 0.9933
Epoch 129/200
 - 6s - loss: 0.0046 - val_loss: 0.0034
 - val_f1: 0.9938
Epoch 130/200
 - 6s - loss: 0.0055 - val_loss: 0.0051
 - val_f1: 0.9877
Epoch 131/200
 - 6s - loss: 0.0047 - val_loss: 0.0036
 - val_f1: 0.9932
Epoch 132/200
 - 6s - loss: 0.0050 - val_loss: 0.0050
 - val_f1: 0.9895
Epoch 133/200
 - 6s - loss: 0.0053 - val_loss: 0.0044
 - val_f1: 0.9923
Epoch 134/200
 - 6s - loss: 0.0055 - val_loss: 0.0048
 - val_f1: 0.9898
Epoch 135/200
 - 6s - loss: 0.0051 - val_loss: 0.0040
 - val_f1: 0.9919
Epoch 136/200
 - 6s - loss: 0.0050 - val_loss: 0.0043
 - val_f1: 0.9906
Epoch 137/200
 - 6s - loss: 0.0048 - val_loss: 0.0039
 - val_f1: 0.9915
Epoch 138/200
 - 6s - loss: 0.0060 - val_loss: 0.0058
 - val_f1: 0.9880
Epoch 139/200
 - 6s - loss: 0.0060 - val_loss: 0.0055
 - val_f1: 0.9890
Epoch 140/200
 - 6s - loss: 0.0058 - val_loss: 0.0049
 - val_f1: 0.9912
Epoch 141/200
 - 6s - loss: 0.0057 - val_loss: 0.0055
2020-01-07 17:32:13,840 [INFO] epoch = 140. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/ann_model_epoch_140.pickle
 - val_f1: 0.9881
Epoch 142/200
 - 6s - loss: 0.0055 - val_loss: 0.0043
2020-01-07 17:32:47,541 [INFO] WeightRestorer::on_train_end(): restoring best weights
2020-01-07 17:33:14,160 [INFO] Last epoch loss evaluation: train_loss = 0.003016, val_loss = 0.003247
2020-01-07 17:33:14,168 [INFO] Training complete. time_to_train = 7584.54 sec, 126.41 min
2020-01-07 17:33:14,178 [INFO] Model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/best_model.pickle
2020-01-07 17:33:14,181 [INFO] Training history saved to: results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/training_error_history.csv
2020-01-07 17:33:14,366 [INFO] Plot saved to: results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/training_error_history.png
2020-01-07 17:33:14,547 [INFO] Plot saved to: results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/training_f1_history.png
2020-01-07 17:33:14,548 [INFO] Making predictions on training, validation, testing data
2020-01-07 17:35:17,750 [INFO] Evaluating predictions (results)
2020-01-07 17:35:37,556 [INFO] Dataset: Testing. Classification report below
2020-01-07 17:35:37,556 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       1.00      1.00      1.00    454265
                   Bot       0.97      0.36      0.53       391
                  DDoS       1.00      1.00      1.00     25605
         DoS GoldenEye       0.98      0.98      0.98      2058
              DoS Hulk       0.98      0.99      0.98     46025
      DoS Slowhttptest       0.89      0.98      0.93      1100
         DoS slowloris       0.98      0.98      0.98      1159
           FTP-Patator       1.00      0.99      0.99      1587
              PortScan       0.99      1.00      0.99     31761
           SSH-Patator       0.95      0.98      0.96      1179
Web Attack Brute Force       0.97      0.10      0.19       302
        Web Attack XSS       0.00      0.00      0.00       130

              accuracy                           0.99    565562
             macro avg       0.89      0.78      0.79    565562
          weighted avg       0.99      0.99      0.99    565562

2020-01-07 17:35:37,556 [INFO] Overall accuracy (micro avg): 0.9948422984571099
/home/sunanda/research/ids_experiments/utility.py:334: RuntimeWarning: invalid value encountered in true_divide
  F1 = (2 * PPV * TPR) / (PPV + TPR)  # F1 score
2020-01-07 17:35:58,841 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9948         0.9948                       0.9948                0.0005                   0.0052  0.9948
1     Macro avg        0.9991         0.8925                       0.7785                0.0013                   0.2215  0.7944
2  Weighted avg        0.9957         0.9946                       0.9948                0.0107                   0.0052  0.9944
2020-01-07 17:36:18,847 [INFO] Dataset: Validation. Classification report below
2020-01-07 17:36:18,847 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       1.00      1.00      1.00    454264
                   Bot       0.98      0.32      0.49       391
                  DDoS       1.00      1.00      1.00     25605
         DoS GoldenEye       0.98      0.98      0.98      2059
              DoS Hulk       0.98      0.99      0.98     46025
      DoS Slowhttptest       0.89      0.98      0.93      1099
         DoS slowloris       0.98      0.98      0.98      1159
           FTP-Patator       1.00      0.98      0.99      1587
              PortScan       0.99      1.00      0.99     31761
           SSH-Patator       0.96      0.97      0.96      1180
Web Attack Brute Force       0.95      0.07      0.12       301
        Web Attack XSS       0.67      0.03      0.06       131

              accuracy                           0.99    565562
             macro avg       0.95      0.77      0.79    565562
          weighted avg       0.99      0.99      0.99    565562

2020-01-07 17:36:18,847 [INFO] Overall accuracy (micro avg): 0.994976678065358
2020-01-07 17:36:40,403 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9950         0.9950                       0.9950                0.0005                   0.0050  0.9950
1     Macro avg        0.9992         0.9481                       0.7740                0.0013                   0.2260  0.7907
2  Weighted avg        0.9958         0.9949                       0.9950                0.0106                   0.0050  0.9945
2020-01-07 17:37:47,179 [INFO] Dataset: Training. Classification report below
2020-01-07 17:37:47,179 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       1.00      1.00      1.00   1362791
                   Bot       0.98      0.36      0.52      1174
                  DDoS       1.00      1.00      1.00     76815
         DoS GoldenEye       0.98      0.98      0.98      6176
              DoS Hulk       0.98      0.99      0.98    138074
      DoS Slowhttptest       0.90      0.98      0.94      3300
         DoS slowloris       0.98      0.99      0.98      3478
           FTP-Patator       1.00      0.99      0.99      4761
              PortScan       0.99      1.00      0.99     95282
           SSH-Patator       0.96      0.98      0.97      3538
Web Attack Brute Force       0.95      0.09      0.16       904
        Web Attack XSS       0.75      0.03      0.06       391

              accuracy                           1.00   1696684
             macro avg       0.96      0.78      0.80   1696684
          weighted avg       0.99      1.00      0.99   1696684

2020-01-07 17:37:47,180 [INFO] Overall accuracy (micro avg): 0.9950509346466402
2020-01-07 17:38:59,150 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9951         0.9951                       0.9951                0.0004                   0.0049  0.9951
1     Macro avg        0.9992         0.9565                       0.7800                0.0013                   0.2200  0.7983
2  Weighted avg        0.9959         0.9950                       0.9951                0.0105                   0.0049  0.9946
2020-01-07 17:38:59,215 [INFO] Results saved to: results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/semi_sup_perf_ids17_ae_ann_rep3_results.xlsx
2020-01-07 17:38:59,219 [INFO] ================= Finished running experiment no. 3 ================= 

2020-01-07 17:38:59,261 [INFO] ================= Finished running 6 experiments ================= 

 - val_f1: 0.9896
Epoch 00142: early stopping
Using TensorFlow backend.
2020-01-08 10:22:43,364 [INFO] Initialized logging. log_filename = results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/run_log.log
2020-01-08 10:22:43,364 [INFO] ================= Running experiment no. 1  ================= 

2020-01-08 10:22:43,364 [INFO] Experiment parameters given below
2020-01-08 10:22:43,364 [INFO] 
{'experiment_num': 1, 'results_dir': 'results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1', 'model_type': 'classifier', 'model': 'ae_ann', 'normal_label': 'BENIGN', 'split_random_seed': 42, 'unsupervised_ratio': 0.75, 'ae_encoder_units': [64], 'ae_encoder_activations': ['relu'], 'ae_encoder_dropout_rates': [0.2], 'ae_encoder_l1_param': -1, 'ae_decoder_units': '', 'ae_decoder_activations': '', 'ae_decoder_dropout_rates': '', 'output_activation': 'sigmoid', 'loss_function': 'binary_crossentropy', 'ae_epochs': 200, 'ann_layer_units': [64], 'ann_layer_activations': ['relu'], 'ann_layer_dropout_rates': [0.2], 'ann_epochs': 200, 'early_stop_patience': 50, 'batch_size': 256, 'dataset_dir': '../Datasets/small_datasets/ids2018', 'description': 'semi_sup_perf_ids18_subset_ae_ann_rep1'}
2020-01-08 10:22:43,364 [INFO] Created tensorboard log directory: results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/tf_logs_run_2020_01_08-10_22_43
2020-01-08 10:22:43,364 [INFO] Loading datsets from: ../Datasets/small_datasets/ids2018
2020-01-08 10:22:43,365 [INFO] Reading X, y files
2020-01-08 10:22:43,365 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_train.h5
2020-01-08 10:22:47,841 [INFO] Reading complete. time_to_read=4.48 seconds
2020-01-08 10:22:47,841 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_val.h5
2020-01-08 10:22:49,373 [INFO] Reading complete. time_to_read=1.53 seconds
2020-01-08 10:22:49,373 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_test.h5
2020-01-08 10:22:50,899 [INFO] Reading complete. time_to_read=1.53 seconds
2020-01-08 10:22:50,900 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_train.h5
2020-01-08 10:22:51,187 [INFO] Reading complete. time_to_read=0.29 seconds
2020-01-08 10:22:51,187 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_val.h5
2020-01-08 10:22:51,272 [INFO] Reading complete. time_to_read=0.08 seconds
2020-01-08 10:22:51,272 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_test.h5
2020-01-08 10:22:51,356 [INFO] Reading complete. time_to_read=0.08 seconds
2020-01-08 10:22:55,266 [INFO] Initializing model
2020-01-08 10:22:55,266 [WARNING] From /home/hasitha/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

2020-01-08 10:22:55,276 [WARNING] From /home/hasitha/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

2020-01-08 10:22:55,277 [WARNING] From /home/hasitha/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

2020-01-08 10:22:55,336 [WARNING] From /home/hasitha/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

2020-01-08 10:22:55,351 [WARNING] From /home/hasitha/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
2020-01-08 10:22:55,372 [WARNING] From /home/hasitha/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

2020-01-08 10:22:55,386 [WARNING] From /home/hasitha/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.

2020-01-08 10:22:55,389 [WARNING] From /home/hasitha/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2020-01-08 10:22:55,399 [INFO] _________________________________________________________________
2020-01-08 10:22:55,400 [INFO] Layer (type)                 Output Shape              Param #   
2020-01-08 10:22:55,400 [INFO] =================================================================
2020-01-08 10:22:55,400 [INFO] dense_1 (Dense)              (None, 64)                4992      
2020-01-08 10:22:55,400 [INFO] _________________________________________________________________
2020-01-08 10:22:55,400 [INFO] batch_normalization_1 (Batch (None, 64)                256       
2020-01-08 10:22:55,400 [INFO] _________________________________________________________________
2020-01-08 10:22:55,400 [INFO] dropout_1 (Dropout)          (None, 64)                0         
2020-01-08 10:22:55,400 [INFO] _________________________________________________________________
2020-01-08 10:22:55,400 [INFO] dense_2 (Dense)              (None, 77)                5005      
2020-01-08 10:22:55,400 [INFO] =================================================================
2020-01-08 10:22:55,400 [INFO] Total params: 10,253
2020-01-08 10:22:55,400 [INFO] Trainable params: 10,125
2020-01-08 10:22:55,400 [INFO] Non-trainable params: 128
2020-01-08 10:22:55,400 [INFO] _________________________________________________________________
2020-01-08 10:22:55,511 [INFO] _________________________________________________________________
2020-01-08 10:22:55,511 [INFO] Layer (type)                 Output Shape              Param #   
2020-01-08 10:22:55,511 [INFO] =================================================================
2020-01-08 10:22:55,511 [INFO] dense_3 (Dense)              (None, 64)                4160      
2020-01-08 10:22:55,511 [INFO] _________________________________________________________________
2020-01-08 10:22:55,511 [INFO] batch_normalization_2 (Batch (None, 64)                256       
2020-01-08 10:22:55,511 [INFO] _________________________________________________________________
2020-01-08 10:22:55,511 [INFO] dropout_2 (Dropout)          (None, 64)                0         
2020-01-08 10:22:55,511 [INFO] _________________________________________________________________
2020-01-08 10:22:55,511 [INFO] dense_4 (Dense)              (None, 15)                975       
2020-01-08 10:22:55,511 [INFO] =================================================================
2020-01-08 10:22:55,512 [INFO] Total params: 5,391
2020-01-08 10:22:55,512 [INFO] Trainable params: 5,263
2020-01-08 10:22:55,512 [INFO] Non-trainable params: 128
2020-01-08 10:22:55,512 [INFO] _________________________________________________________________
2020-01-08 10:22:55,512 [INFO] Training model
2020-01-08 10:22:55,512 [INFO] Splitting train set into 2 sets (unsupervised, supervised), random_seed = 42
2020-01-08 10:23:18,468 [INFO] Split sizes (instances). total = 1936462, unsupervised = 1452346, supervised = 484116, unsupervised dataset hash = 380d72269f5bf1f4655cd65f184fdaf215e4cb4b
2020-01-08 10:23:18,468 [INFO] Training autoencoder
2020-01-08 10:23:19.217679: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-01-08 10:23:19.240900: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2893425000 Hz
2020-01-08 10:23:19.241100: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556bc7d9fd20 executing computations on platform Host. Devices:
2020-01-08 10:23:19.241127: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2020-01-08 10:23:19.390613: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2020-01-08 10:23:19,396 [WARNING] From /home/hasitha/anaconda3/lib/python3.7/site-packages/keras/callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

2020-01-08 10:23:19,396 [WARNING] From /home/hasitha/anaconda3/lib/python3.7/site-packages/keras/callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

Train on 1452346 samples, validate on 645487 samples
Epoch 1/200
 - 23s - loss: -3.3744e+00 - val_loss: -3.5151e+00
Epoch 2/200
 - 23s - loss: -3.6285e+00 - val_loss: -3.6670e+00
Epoch 3/200
 - 23s - loss: -3.6376e+00 - val_loss: -3.6702e+00
Epoch 4/200
 - 23s - loss: -3.6401e+00 - val_loss: -3.6708e+00
Epoch 5/200
 - 23s - loss: -3.6431e+00 - val_loss: -3.6715e+00
Epoch 6/200
 - 23s - loss: -3.6451e+00 - val_loss: -3.6729e+00
Epoch 7/200
 - 23s - loss: -3.6458e+00 - val_loss: -3.6718e+00
Epoch 8/200
 - 23s - loss: -3.6446e+00 - val_loss: -3.6549e+00
Epoch 9/200
 - 23s - loss: -3.6464e+00 - val_loss: -3.6726e+00
Epoch 10/200
 - 23s - loss: -3.6468e+00 - val_loss: -3.6711e+00
Epoch 11/200
 - 23s - loss: -3.6473e+00 - val_loss: -3.6697e+00
Epoch 12/200
 - 23s - loss: -3.6468e+00 - val_loss: -3.4290e+00
Epoch 13/200
 - 23s - loss: -3.6472e+00 - val_loss: -3.6735e+00
Epoch 14/200
 - 23s - loss: -3.6476e+00 - val_loss: -3.6738e+00
Epoch 15/200
 - 23s - loss: -3.6497e+00 - val_loss: -3.6730e+00
Epoch 16/200
 - 23s - loss: -3.6493e+00 - val_loss: -3.6663e+00
Epoch 17/200
 - 23s - loss: -3.6503e+00 - val_loss: -3.6749e+00
Epoch 18/200
 - 23s - loss: -3.6498e+00 - val_loss: -3.6718e+00
Epoch 19/200
 - 23s - loss: -3.6497e+00 - val_loss: -3.4647e+00
Epoch 20/200
 - 23s - loss: -3.6515e+00 - val_loss: -3.6741e+00
Epoch 21/200
 - 23s - loss: -3.6513e+00 - val_loss: -3.6749e+00
2020-01-08 10:31:27,359 [INFO] epoch = 20. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/ae_model_epoch_20.pickle
Epoch 22/200
 - 23s - loss: -3.6520e+00 - val_loss: -3.6737e+00
Epoch 23/200
 - 23s - loss: -3.6514e+00 - val_loss: -3.6746e+00
Epoch 24/200
 - 23s - loss: -3.6523e+00 - val_loss: -3.6741e+00
Epoch 25/200
 - 23s - loss: -3.6494e+00 - val_loss: -3.6743e+00
Epoch 26/200
 - 23s - loss: -3.6499e+00 - val_loss: -3.6718e+00
Epoch 27/200
 - 23s - loss: -3.6512e+00 - val_loss: -3.6744e+00
Epoch 28/200
 - 23s - loss: -3.6528e+00 - val_loss: -3.6731e+00
Epoch 29/200
 - 23s - loss: -3.6502e+00 - val_loss: -3.6740e+00
Epoch 30/200
 - 23s - loss: -3.6501e+00 - val_loss: -3.6607e+00
Epoch 31/200
 - 23s - loss: -3.6521e+00 - val_loss: -3.6747e+00
Epoch 32/200
 - 23s - loss: -3.6501e+00 - val_loss: -3.6748e+00
Epoch 33/200
 - 23s - loss: -3.6517e+00 - val_loss: -3.6688e+00
Epoch 34/200
 - 23s - loss: -3.6532e+00 - val_loss: -3.6745e+00
Epoch 35/200
 - 23s - loss: -3.6523e+00 - val_loss: -3.5814e+00
Epoch 36/200
 - 23s - loss: -3.6530e+00 - val_loss: -3.6758e+00
Epoch 37/200
 - 23s - loss: -3.6531e+00 - val_loss: -3.6747e+00
Epoch 38/200
 - 23s - loss: -3.6511e+00 - val_loss: -3.6755e+00
Epoch 39/200
 - 23s - loss: -3.6525e+00 - val_loss: -3.6645e+00
Epoch 40/200
 - 23s - loss: -3.6538e+00 - val_loss: -3.6749e+00
Epoch 41/200
 - 23s - loss: -3.6532e+00 - val_loss: -3.6756e+00
2020-01-08 10:39:12,312 [INFO] epoch = 40. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/ae_model_epoch_40.pickle
Epoch 42/200
 - 23s - loss: -3.6530e+00 - val_loss: -3.6728e+00
Epoch 43/200
 - 23s - loss: -3.6508e+00 - val_loss: -3.6140e+00
Epoch 44/200
 - 23s - loss: -3.6530e+00 - val_loss: -3.6743e+00
Epoch 45/200
 - 23s - loss: -3.6539e+00 - val_loss: -3.6746e+00
Epoch 46/200
 - 23s - loss: -3.6531e+00 - val_loss: -3.6763e+00
Epoch 47/200
 - 23s - loss: -3.6509e+00 - val_loss: -3.6756e+00
Epoch 48/200
 - 23s - loss: -3.6526e+00 - val_loss: -3.6764e+00
Epoch 49/200
 - 23s - loss: -3.6518e+00 - val_loss: -3.6748e+00
Epoch 50/200
 - 23s - loss: -3.6529e+00 - val_loss: -3.6767e+00
Epoch 51/200
 - 23s - loss: -3.6541e+00 - val_loss: -3.6761e+00
Epoch 52/200
 - 23s - loss: -3.6534e+00 - val_loss: -3.6766e+00
Epoch 53/200
 - 23s - loss: -3.6544e+00 - val_loss: -3.6755e+00
Epoch 54/200
 - 23s - loss: -3.6532e+00 - val_loss: -3.6768e+00
Epoch 55/200
 - 23s - loss: -3.6541e+00 - val_loss: -3.6753e+00
Epoch 56/200
 - 23s - loss: -3.6540e+00 - val_loss: -3.6751e+00
Epoch 57/200
 - 23s - loss: -3.6514e+00 - val_loss: -3.6751e+00
Epoch 58/200
 - 23s - loss: -3.6535e+00 - val_loss: -3.6760e+00
Epoch 59/200
 - 23s - loss: -3.6545e+00 - val_loss: -3.6762e+00
Epoch 60/200
 - 23s - loss: -3.6539e+00 - val_loss: -3.6755e+00
Epoch 61/200
 - 23s - loss: -3.6539e+00 - val_loss: -3.6760e+00
2020-01-08 10:46:57,348 [INFO] epoch = 60. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/ae_model_epoch_60.pickle
Epoch 62/200
 - 23s - loss: -3.6515e+00 - val_loss: -3.6755e+00
Epoch 63/200
 - 23s - loss: -3.6533e+00 - val_loss: -3.6761e+00
Epoch 64/200
 - 23s - loss: -3.6545e+00 - val_loss: -3.6750e+00
Epoch 65/200
 - 23s - loss: -3.6514e+00 - val_loss: -3.6375e+00
Epoch 66/200
 - 23s - loss: -3.6535e+00 - val_loss: -3.6758e+00
Epoch 67/200
 - 23s - loss: -3.6537e+00 - val_loss: -3.6664e+00
Epoch 68/200
 - 23s - loss: -3.6547e+00 - val_loss: -3.6771e+00
Epoch 69/200
 - 23s - loss: -3.6543e+00 - val_loss: -3.6764e+00
Epoch 70/200
 - 23s - loss: -3.6543e+00 - val_loss: -3.6756e+00
Epoch 71/200
 - 23s - loss: -3.6545e+00 - val_loss: -3.6770e+00
Epoch 72/200
 - 23s - loss: -3.6518e+00 - val_loss: -3.6643e+00
Epoch 73/200
 - 23s - loss: -3.6518e+00 - val_loss: -3.6755e+00
Epoch 74/200
 - 23s - loss: -3.6531e+00 - val_loss: -3.6692e+00
Epoch 75/200
 - 23s - loss: -3.6544e+00 - val_loss: -3.6771e+00
Epoch 76/200
 - 23s - loss: -3.6519e+00 - val_loss: -3.6276e+00
Epoch 77/200
 - 23s - loss: -3.6543e+00 - val_loss: -3.6769e+00
Epoch 78/200
 - 23s - loss: -3.6521e+00 - val_loss: -3.6557e+00
Epoch 79/200
 - 23s - loss: -3.6542e+00 - val_loss: -3.6771e+00
Epoch 80/200
 - 23s - loss: -3.6519e+00 - val_loss: -3.6753e+00
Epoch 81/200
 - 23s - loss: -3.6536e+00 - val_loss: -3.6637e+00
2020-01-08 10:54:43,872 [INFO] epoch = 80. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/ae_model_epoch_80.pickle
Epoch 82/200
 - 23s - loss: -3.6545e+00 - val_loss: -3.6746e+00
Epoch 83/200
 - 23s - loss: -3.6540e+00 - val_loss: -3.6771e+00
Epoch 84/200
 - 23s - loss: -3.6539e+00 - val_loss: -3.6768e+00
Epoch 85/200
 - 23s - loss: -3.6521e+00 - val_loss: -3.6125e+00
Epoch 86/200
 - 23s - loss: -3.6542e+00 - val_loss: -3.5995e+00
Epoch 87/200
 - 23s - loss: -3.6522e+00 - val_loss: -3.6773e+00
Epoch 88/200
 - 23s - loss: -3.6541e+00 - val_loss: -3.6772e+00
Epoch 89/200
 - 23s - loss: -3.6524e+00 - val_loss: -3.6758e+00
Epoch 90/200
 - 23s - loss: -3.6526e+00 - val_loss: -3.6747e+00
Epoch 91/200
 - 23s - loss: -3.6522e+00 - val_loss: -3.6752e+00
Epoch 92/200
 - 23s - loss: -3.6523e+00 - val_loss: -3.6758e+00
Epoch 93/200
 - 23s - loss: -3.6542e+00 - val_loss: -3.6771e+00
Epoch 94/200
 - 23s - loss: -3.6523e+00 - val_loss: -3.6747e+00
Epoch 95/200
 - 23s - loss: -3.6543e+00 - val_loss: -3.6771e+00
Epoch 96/200
 - 23s - loss: -3.6550e+00 - val_loss: -3.6747e+00
Epoch 97/200
 - 23s - loss: -3.6545e+00 - val_loss: -3.6760e+00
Epoch 98/200
 - 23s - loss: -3.6526e+00 - val_loss: -3.6760e+00
Epoch 99/200
 - 23s - loss: -3.6548e+00 - val_loss: -3.6761e+00
Epoch 100/200
 - 23s - loss: -3.6548e+00 - val_loss: -3.6774e+00
Epoch 101/200
 - 23s - loss: -3.6523e+00 - val_loss: -3.6754e+00
2020-01-08 11:02:30,613 [INFO] epoch = 100. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/ae_model_epoch_100.pickle
Epoch 102/200
 - 23s - loss: -3.6547e+00 - val_loss: -3.6773e+00
Epoch 103/200
 - 23s - loss: -3.6523e+00 - val_loss: -3.6767e+00
Epoch 104/200
 - 23s - loss: -3.6548e+00 - val_loss: -3.6769e+00
Epoch 105/200
 - 23s - loss: -3.6547e+00 - val_loss: -3.6773e+00
Epoch 106/200
 - 23s - loss: -3.6537e+00 - val_loss: -3.6773e+00
Epoch 107/200
 - 23s - loss: -3.6553e+00 - val_loss: -3.6767e+00
Epoch 108/200
 - 23s - loss: -3.6525e+00 - val_loss: -3.6759e+00
Epoch 109/200
 - 23s - loss: -3.6538e+00 - val_loss: -3.6750e+00
Epoch 110/200
 - 23s - loss: -3.6553e+00 - val_loss: -3.6770e+00
Epoch 111/200
 - 23s - loss: -3.6546e+00 - val_loss: -3.6763e+00
Epoch 112/200
 - 23s - loss: -3.6543e+00 - val_loss: -3.6756e+00
Epoch 113/200
 - 23s - loss: -3.6551e+00 - val_loss: -3.6771e+00
Epoch 114/200
 - 23s - loss: -3.6549e+00 - val_loss: -3.6766e+00
Epoch 115/200
 - 23s - loss: -3.6550e+00 - val_loss: -3.6755e+00
Epoch 116/200
 - 23s - loss: -3.6547e+00 - val_loss: -3.6766e+00
Epoch 117/200
 - 23s - loss: -3.6522e+00 - val_loss: -3.6554e+00
Epoch 118/200
 - 23s - loss: -3.6540e+00 - val_loss: -3.6774e+00
Epoch 119/200
 - 23s - loss: -3.6546e+00 - val_loss: -3.6157e+00
Epoch 120/200
 - 23s - loss: -3.6548e+00 - val_loss: -3.6772e+00
Epoch 121/200
 - 23s - loss: -3.6551e+00 - val_loss: -3.5724e+00
2020-01-08 11:10:17,169 [INFO] epoch = 120. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/ae_model_epoch_120.pickle
Epoch 122/200
 - 23s - loss: -3.6548e+00 - val_loss: -3.6774e+00
Epoch 123/200
 - 23s - loss: -3.6551e+00 - val_loss: -3.6759e+00
Epoch 124/200
 - 23s - loss: -3.6552e+00 - val_loss: -3.6775e+00
Epoch 125/200
 - 23s - loss: -3.6526e+00 - val_loss: -3.6670e+00
Epoch 126/200
 - 23s - loss: -3.6545e+00 - val_loss: -3.6773e+00
Epoch 127/200
 - 23s - loss: -3.6529e+00 - val_loss: -3.6771e+00
Epoch 128/200
 - 23s - loss: -3.6546e+00 - val_loss: -3.6770e+00
Epoch 129/200
 - 23s - loss: -3.6554e+00 - val_loss: -3.6774e+00
Epoch 130/200
 - 23s - loss: -3.6545e+00 - val_loss: -3.5525e+00
Epoch 131/200
 - 23s - loss: -3.6544e+00 - val_loss: -3.6699e+00
Epoch 132/200
 - 23s - loss: -3.6524e+00 - val_loss: -3.6742e+00
Epoch 133/200
 - 23s - loss: -3.6542e+00 - val_loss: -3.5946e+00
Epoch 134/200
 - 23s - loss: -3.6552e+00 - val_loss: -3.6770e+00
Epoch 135/200
 - 23s - loss: -3.6557e+00 - val_loss: -3.6767e+00
Epoch 136/200
 - 23s - loss: -3.6548e+00 - val_loss: -3.6772e+00
Epoch 137/200
 - 23s - loss: -3.6526e+00 - val_loss: -3.6769e+00
Epoch 138/200
 - 23s - loss: -3.6545e+00 - val_loss: -3.6775e+00
Epoch 139/200
 - 23s - loss: -3.6552e+00 - val_loss: -3.6775e+00
Epoch 140/200
 - 23s - loss: -3.6550e+00 - val_loss: -3.6756e+00
Epoch 141/200
 - 23s - loss: -3.6528e+00 - val_loss: -3.6773e+00
2020-01-08 11:18:03,822 [INFO] epoch = 140. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/ae_model_epoch_140.pickle
Epoch 142/200
 - 23s - loss: -3.6551e+00 - val_loss: -3.6622e+00
Epoch 143/200
 - 23s - loss: -3.6533e+00 - val_loss: -3.6773e+00
Epoch 144/200
 - 23s - loss: -3.6553e+00 - val_loss: -3.6772e+00
Epoch 145/200
 - 23s - loss: -3.6527e+00 - val_loss: -3.6764e+00
Epoch 146/200
 - 23s - loss: -3.6545e+00 - val_loss: -3.6768e+00
Epoch 147/200
 - 23s - loss: -3.6548e+00 - val_loss: -3.6738e+00
Epoch 148/200
 - 23s - loss: -3.6555e+00 - val_loss: -3.6775e+00
Epoch 149/200
 - 23s - loss: -3.6555e+00 - val_loss: -3.6773e+00
Epoch 150/200
 - 23s - loss: -3.6547e+00 - val_loss: -3.6775e+00
Epoch 151/200
 - 23s - loss: -3.6525e+00 - val_loss: -3.6774e+00
Epoch 152/200
 - 23s - loss: -3.6550e+00 - val_loss: -3.6767e+00
Epoch 153/200
 - 23s - loss: -3.6557e+00 - val_loss: -3.6775e+00
Epoch 154/200
 - 23s - loss: -3.6553e+00 - val_loss: -3.6776e+00
Epoch 155/200
 - 23s - loss: -3.6554e+00 - val_loss: -3.6772e+00
Epoch 156/200
 - 23s - loss: -3.6528e+00 - val_loss: -3.5472e+00
Epoch 157/200
 - 23s - loss: -3.6542e+00 - val_loss: -3.6751e+00
Epoch 158/200
 - 23s - loss: -3.6551e+00 - val_loss: -3.6773e+00
Epoch 159/200
 - 23s - loss: -3.6553e+00 - val_loss: -3.6709e+00
Epoch 160/200
 - 23s - loss: -3.6528e+00 - val_loss: -3.6728e+00
Epoch 161/200
 - 23s - loss: -3.6550e+00 - val_loss: -3.6765e+00
2020-01-08 11:25:50,271 [INFO] epoch = 160. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/ae_model_epoch_160.pickle
Epoch 162/200
 - 23s - loss: -3.6554e+00 - val_loss: -3.6777e+00
Epoch 163/200
 - 23s - loss: -3.6551e+00 - val_loss: -3.6740e+00
Epoch 164/200
 - 23s - loss: -3.6556e+00 - val_loss: -3.6770e+00
Epoch 165/200
 - 23s - loss: -3.6534e+00 - val_loss: -3.6552e+00
Epoch 166/200
 - 23s - loss: -3.6544e+00 - val_loss: -3.6770e+00
Epoch 167/200
 - 23s - loss: -3.6551e+00 - val_loss: -3.5666e+00
Epoch 168/200
 - 23s - loss: -3.6556e+00 - val_loss: -3.6775e+00
Epoch 169/200
 - 23s - loss: -3.6555e+00 - val_loss: -3.6349e+00
Epoch 170/200
 - 23s - loss: -3.6552e+00 - val_loss: -3.6779e+00
Epoch 171/200
 - 23s - loss: -3.6553e+00 - val_loss: -3.6765e+00
Epoch 172/200
 - 23s - loss: -3.6530e+00 - val_loss: -3.5722e+00
Epoch 173/200
 - 23s - loss: -3.6557e+00 - val_loss: -3.6778e+00
Epoch 174/200
 - 23s - loss: -3.6562e+00 - val_loss: -3.6776e+00
Epoch 175/200
 - 23s - loss: -3.6535e+00 - val_loss: -3.5725e+00
Epoch 176/200
 - 23s - loss: -3.6550e+00 - val_loss: -3.6779e+00
Epoch 177/200
 - 23s - loss: -3.6556e+00 - val_loss: -3.6779e+00
Epoch 178/200
 - 23s - loss: -3.6559e+00 - val_loss: -3.6773e+00
Epoch 179/200
 - 23s - loss: -3.6555e+00 - val_loss: -3.6777e+00
Epoch 180/200
 - 23s - loss: -3.6528e+00 - val_loss: -3.6774e+00
Epoch 181/200
 - 23s - loss: -3.6549e+00 - val_loss: -3.6777e+00
2020-01-08 11:33:36,439 [INFO] epoch = 180. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/ae_model_epoch_180.pickle
Epoch 182/200
 - 23s - loss: -3.6558e+00 - val_loss: -3.6770e+00
Epoch 183/200
 - 23s - loss: -3.6549e+00 - val_loss: -3.6779e+00
Epoch 184/200
 - 23s - loss: -3.6557e+00 - val_loss: -3.6774e+00
Epoch 185/200
 - 23s - loss: -3.6555e+00 - val_loss: -3.6778e+00
Epoch 186/200
 - 23s - loss: -3.6531e+00 - val_loss: -3.6717e+00
Epoch 187/200
 - 23s - loss: -3.6552e+00 - val_loss: -3.6774e+00
Epoch 188/200
 - 23s - loss: -3.6560e+00 - val_loss: -3.6776e+00
Epoch 189/200
 - 23s - loss: -3.6530e+00 - val_loss: -3.6745e+00
Epoch 190/200
 - 23s - loss: -3.6553e+00 - val_loss: -3.6778e+00
Epoch 191/200
 - 23s - loss: -3.6557e+00 - val_loss: -3.6775e+00
Epoch 192/200
 - 23s - loss: -3.6560e+00 - val_loss: -3.6775e+00
Epoch 193/200
 - 23s - loss: -3.6536e+00 - val_loss: -3.6772e+00
Epoch 194/200
 - 23s - loss: -3.6551e+00 - val_loss: -3.5658e+00
Epoch 195/200
 - 23s - loss: -3.6558e+00 - val_loss: -3.6159e+00
Epoch 196/200
 - 23s - loss: -3.6556e+00 - val_loss: -3.6451e+00
Epoch 197/200
 - 23s - loss: -3.6562e+00 - val_loss: -3.6776e+00
Epoch 198/200
 - 23s - loss: -3.6552e+00 - val_loss: -3.6776e+00
Epoch 199/200
 - 23s - loss: -3.6535e+00 - val_loss: -3.6429e+00
Epoch 200/200
 - 23s - loss: -3.6562e+00 - val_loss: -3.6776e+00
2020-01-08 11:40:59,261 [INFO] WeightRestorer::on_train_end(): restoring best weights
2020-01-08 11:41:27,305 [INFO] Last epoch loss evaluation: train_loss = -3.673298, val_loss = -3.677948
2020-01-08 11:41:27,305 [INFO] Training autoencoder complete
2020-01-08 11:41:27,305 [INFO] Encoding data for supervised training
2020-01-08 11:41:40,786 [INFO] Encoding complete
2020-01-08 11:41:40,786 [INFO] Training neural network layers (after autoencoder)
Train on 484116 samples, validate on 645487 samples
Epoch 1/200
 - 5s - loss: 0.0248 - val_loss: 0.0090
 - val_f1: 0.9763
Epoch 2/200
 - 5s - loss: 0.0093 - val_loss: 0.0086
 - val_f1: 0.9764
Epoch 3/200
 - 5s - loss: 0.0088 - val_loss: 0.0082
 - val_f1: 0.9775
Epoch 4/200
 - 5s - loss: 0.0086 - val_loss: 0.0082
 - val_f1: 0.9777
Epoch 5/200
 - 5s - loss: 0.0085 - val_loss: 0.0081
 - val_f1: 0.9776
Epoch 6/200
 - 5s - loss: 0.0084 - val_loss: 0.0081
 - val_f1: 0.9777
Epoch 7/200
 - 5s - loss: 0.0084 - val_loss: 0.0081
 - val_f1: 0.9778
Epoch 8/200
 - 5s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9764
Epoch 9/200
 - 5s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9776
Epoch 10/200
 - 5s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9778
Epoch 11/200
 - 5s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9784
Epoch 12/200
 - 5s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9780
Epoch 13/200
 - 5s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9777
Epoch 14/200
 - 5s - loss: 0.0081 - val_loss: 0.0081
 - val_f1: 0.9778
Epoch 15/200
 - 5s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9764
Epoch 16/200
 - 5s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 17/200
 - 5s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9763
Epoch 18/200
 - 5s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 19/200
 - 5s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9762
Epoch 20/200
 - 5s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 21/200
 - 5s - loss: 0.0081 - val_loss: 0.0079
/home/hasitha/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2020-01-08 11:46:14,559 [INFO] epoch = 20. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/ann_model_epoch_20.pickle
 - val_f1: 0.9778
Epoch 22/200
 - 5s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 23/200
 - 5s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9777
Epoch 24/200
 - 5s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 25/200
 - 5s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 26/200
 - 5s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 27/200
 - 5s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9782
Epoch 28/200
 - 5s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 29/200
 - 5s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 30/200
 - 5s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 31/200
 - 5s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9776
Epoch 32/200
 - 5s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9756
Epoch 33/200
 - 5s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 34/200
 - 5s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 35/200
 - 5s - loss: 0.0080 - val_loss: 0.0078
 - val_f1: 0.9780
Epoch 36/200
 - 5s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9763
Epoch 37/200
 - 5s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9776
Epoch 38/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 39/200
 - 5s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 40/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9784
Epoch 41/200
 - 5s - loss: 0.0080 - val_loss: 0.0079
2020-01-08 11:50:43,051 [INFO] epoch = 40. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/ann_model_epoch_40.pickle
 - val_f1: 0.9779
Epoch 42/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9778
Epoch 43/200
 - 5s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9778
Epoch 44/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9763
Epoch 45/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 46/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 47/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 48/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 49/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 50/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 51/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9786
Epoch 52/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 53/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 54/200
 - 5s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 55/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 56/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 57/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9785
Epoch 58/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 59/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 60/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 61/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
2020-01-08 11:55:11,558 [INFO] epoch = 60. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/ann_model_epoch_60.pickle
 - val_f1: 0.9781
Epoch 62/200
 - 5s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 63/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 64/200
 - 5s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 65/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9785
Epoch 66/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9763
Epoch 67/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 68/200
 - 5s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 69/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 70/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 71/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9784
Epoch 72/200
 - 5s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9780
Epoch 73/200
 - 5s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 74/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 75/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 76/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9786
Epoch 77/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 78/200
 - 5s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9780
Epoch 79/200
 - 5s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 80/200
 - 5s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 81/200
 - 5s - loss: 0.0079 - val_loss: 0.0078
2020-01-08 11:59:40,307 [INFO] epoch = 80. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/ann_model_epoch_80.pickle
 - val_f1: 0.9784
Epoch 82/200
 - 5s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9763
Epoch 83/200
 - 5s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 84/200
 - 5s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 85/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 86/200
 - 5s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 87/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 88/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 89/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 90/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 91/200
 - 5s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9786
Epoch 92/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 93/200
 - 5s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 94/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 95/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 96/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 97/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 98/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 99/200
 - 5s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9785
Epoch 100/200
 - 5s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 101/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
2020-01-08 12:04:07,246 [INFO] epoch = 100. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/ann_model_epoch_100.pickle
 - val_f1: 0.9786
Epoch 102/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 103/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 104/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 105/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 106/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9780
Epoch 107/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 108/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 109/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9767
Epoch 110/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 111/200
 - 5s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9787
Epoch 112/200
 - 5s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9786
Epoch 113/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 114/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9780
Epoch 115/200
 - 5s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 116/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 117/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9787
Epoch 118/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 119/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9769
Epoch 120/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9787
Epoch 121/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
2020-01-08 12:08:35,204 [INFO] epoch = 120. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/ann_model_epoch_120.pickle
 - val_f1: 0.9781
Epoch 122/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9780
Epoch 123/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 124/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9787
Epoch 125/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 126/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 127/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 128/200
 - 5s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 129/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 130/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 131/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9787
Epoch 132/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9788
Epoch 133/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 134/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9780
Epoch 135/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 136/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 137/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9788
Epoch 138/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 139/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 140/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9767
Epoch 141/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
2020-01-08 12:13:03,221 [INFO] epoch = 140. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/ann_model_epoch_140.pickle
 - val_f1: 0.9786
Epoch 142/200
 - 5s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9788
Epoch 143/200
 - 5s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9782
Epoch 144/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9789
Epoch 145/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 146/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9787
Epoch 147/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 148/200
 - 5s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9787
Epoch 149/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 150/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9787
Epoch 151/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9788
Epoch 152/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 153/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 154/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 155/200
 - 5s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 156/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9788
Epoch 157/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9780
Epoch 158/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 159/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 160/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 161/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
2020-01-08 12:17:32,481 [INFO] epoch = 160. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/ann_model_epoch_160.pickle
 - val_f1: 0.9785
Epoch 162/200
 - 5s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 163/200
 - 5s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9789
Epoch 164/200
 - 5s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 165/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 166/200
 - 5s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 167/200
 - 5s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 168/200
 - 5s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 169/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 170/200
 - 5s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9789
Epoch 171/200
 - 5s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9788
Epoch 172/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 173/200
 - 5s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 174/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 175/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 176/200
 - 5s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9787
Epoch 177/200
 - 5s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9768
Epoch 178/200
 - 5s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 179/200
 - 5s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 180/200
 - 5s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9780
Epoch 181/200
 - 5s - loss: 0.0077 - val_loss: 0.0078
2020-01-08 12:22:02,083 [INFO] epoch = 180. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/ann_model_epoch_180.pickle
 - val_f1: 0.9783
Epoch 182/200
 - 5s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 183/200
 - 5s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 184/200
 - 5s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9789
Epoch 185/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 186/200
 - 5s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 187/200
 - 5s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9788
Epoch 188/200
 - 5s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 189/200
 - 5s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 190/200
 - 5s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 191/200
 - 5s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 192/200
 - 5s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9788
Epoch 193/200
 - 5s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9784
Epoch 194/200
 - 5s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 195/200
 - 5s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 196/200
 - 5s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9788
Epoch 197/200
 - 5s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 198/200
 - 5s - loss: 0.0077 - val_loss: 0.0078
2020-01-08 12:25:59,419 [INFO] WeightRestorer::on_train_end(): restoring best weights
2020-01-08 12:26:16,750 [INFO] Last epoch loss evaluation: train_loss = 0.007548, val_loss = 0.007739
2020-01-08 12:26:16,793 [INFO] Training complete. time_to_train = 7401.28 sec, 123.35 min
2020-01-08 12:26:16,800 [INFO] Model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/best_model.pickle
2020-01-08 12:26:16,803 [INFO] Training history saved to: results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/training_error_history.csv
2020-01-08 12:26:16,962 [INFO] Plot saved to: results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/training_error_history.png
2020-01-08 12:26:17,108 [INFO] Plot saved to: results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/training_f1_history.png
2020-01-08 12:26:17,108 [INFO] Making predictions on training, validation, testing data
2020-01-08 12:27:34,221 [INFO] Evaluating predictions (results)
2020-01-08 12:27:46,398 [INFO] Dataset: Testing. Classification report below
2020-01-08 12:27:46,398 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99    535650
                     Bot       1.00      1.00      1.00     11465
        Brute Force -Web       1.00      0.12      0.22        24
        Brute Force -XSS       0.60      0.33      0.43         9
        DDOS attack-HOIC       1.00      1.00      1.00     27447
    DDOS attack-LOIC-UDP       0.76      0.97      0.86        67
  DDoS attacks-LOIC-HTTP       0.99      0.99      0.99     23010
   DoS attacks-GoldenEye       0.99      1.00      1.00      1651
        DoS attacks-Hulk       1.00      1.00      1.00     18478
DoS attacks-SlowHTTPTest       0.74      0.53      0.61      5596
   DoS attacks-Slowloris       0.86      0.97      0.91       440
          FTP-BruteForce       0.72      0.86      0.78      7718
           Infilteration       0.45      0.01      0.02      6404
           SQL Injection       1.00      0.25      0.40         4
          SSH-Bruteforce       1.00      1.00      1.00      7525

               micro avg       0.98      0.98      0.98    645488
               macro avg       0.87      0.74      0.75    645488
            weighted avg       0.98      0.98      0.98    645488

2020-01-08 12:27:46,398 [INFO] Overall accuracy (micro avg): 0.9836216939741714
2020-01-08 12:28:00,223 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9836         0.9836                       0.9836                0.0012                   0.0164  0.9836
1     Macro avg        0.9978         0.8737                       0.7356                0.0044                   0.2644  0.7475
2  Weighted avg        0.9910         0.9785                       0.9836                0.0494                   0.0164  0.9786
/home/hasitha/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2020-01-08 12:28:12,429 [INFO] Dataset: Validation. Classification report below
2020-01-08 12:28:12,429 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99    535650
                     Bot       1.00      1.00      1.00     11465
        Brute Force -Web       1.00      0.20      0.33        25
        Brute Force -XSS       0.60      0.67      0.63         9
        DDOS attack-HOIC       1.00      1.00      1.00     27447
    DDOS attack-LOIC-UDP       0.80      0.99      0.88        68
  DDoS attacks-LOIC-HTTP       0.99      0.99      0.99     23009
   DoS attacks-GoldenEye       0.99      1.00      1.00      1651
        DoS attacks-Hulk       1.00      1.00      1.00     18478
DoS attacks-SlowHTTPTest       0.74      0.52      0.61      5596
   DoS attacks-Slowloris       0.87      0.97      0.92       439
          FTP-BruteForce       0.71      0.87      0.79      7718
           Infilteration       0.45      0.01      0.02      6403
           SQL Injection       0.00      0.00      0.00         4
          SSH-Bruteforce       1.00      1.00      1.00      7525

               micro avg       0.98      0.98      0.98    645487
               macro avg       0.81      0.75      0.74    645487
            weighted avg       0.98      0.98      0.98    645487

2020-01-08 12:28:12,429 [INFO] Overall accuracy (micro avg): 0.9836743420084371
/home/hasitha/sunanda/ids_experiments/utility.py:328: RuntimeWarning: invalid value encountered in true_divide
  PPV = TP / (TP + FP)    # Precision or positive predictive value
/home/hasitha/sunanda/ids_experiments/utility.py:332: RuntimeWarning: invalid value encountered in true_divide
  FDR = FP / (TP + FP)    # False discovery rate
2020-01-08 12:28:26,317 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9837         0.9837                       0.9837                0.0012                   0.0163  0.9837
1     Macro avg        0.9978         0.8105                       0.7476                0.0044                   0.2524  0.7443
2  Weighted avg        0.9910         0.9786                       0.9837                0.0493                   0.0163  0.9787
2020-01-08 12:29:05,945 [INFO] Dataset: Training. Classification report below
2020-01-08 12:29:05,945 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99   1606949
                     Bot       1.00      1.00      1.00     34396
        Brute Force -Web       1.00      0.12      0.22        73
        Brute Force -XSS       0.62      0.50      0.55        26
        DDOS attack-HOIC       1.00      1.00      1.00     82341
    DDOS attack-LOIC-UDP       0.77      0.96      0.85       203
  DDoS attacks-LOIC-HTTP       0.99      0.99      0.99     69029
   DoS attacks-GoldenEye       1.00      1.00      1.00      4954
        DoS attacks-Hulk       1.00      1.00      1.00     55435
DoS attacks-SlowHTTPTest       0.74      0.52      0.61     16787
   DoS attacks-Slowloris       0.89      0.98      0.93      1318
          FTP-BruteForce       0.71      0.87      0.78     23153
           Infilteration       0.55      0.01      0.02     19210
           SQL Injection       0.80      0.33      0.47        12
          SSH-Bruteforce       1.00      1.00      1.00     22576

               micro avg       0.98      0.98      0.98   1936462
               macro avg       0.87      0.75      0.76   1936462
            weighted avg       0.98      0.98      0.98   1936462

2020-01-08 12:29:05,945 [INFO] Overall accuracy (micro avg): 0.9836728012220224
2020-01-08 12:29:50,959 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9837         0.9837                       0.9837                0.0012                   0.0163  0.9837
1     Macro avg        0.9978         0.8700                       0.7528                0.0044                   0.2472  0.7619
2  Weighted avg        0.9910         0.9795                       0.9837                0.0492                   0.0163  0.9787
2020-01-08 12:29:51,006 [INFO] Results saved to: results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/semi_sup_perf_ids18_subset_ae_ann_rep1_results.xlsx
2020-01-08 12:29:51,013 [INFO] ================= Finished running experiment no. 1 ================= 

2020-01-08 12:29:51,089 [INFO] Created directory: results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2
2020-01-08 12:29:51,090 [INFO] Initialized logging. log_filename = results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/run_log.log
2020-01-08 12:29:51,090 [INFO] ================= Running experiment no. 2  ================= 

2020-01-08 12:29:51,090 [INFO] Experiment parameters given below
2020-01-08 12:29:51,090 [INFO] 
{'experiment_num': 2, 'results_dir': 'results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2', 'model_type': 'classifier', 'model': 'ae_ann', 'normal_label': 'BENIGN', 'split_random_seed': 42, 'unsupervised_ratio': 0.75, 'ae_encoder_units': [64], 'ae_encoder_activations': ['relu'], 'ae_encoder_dropout_rates': [0.2], 'ae_encoder_l1_param': -1, 'ae_decoder_units': '', 'ae_decoder_activations': '', 'ae_decoder_dropout_rates': '', 'output_activation': 'sigmoid', 'loss_function': 'binary_crossentropy', 'ae_epochs': 200, 'ann_layer_units': [64], 'ann_layer_activations': ['relu'], 'ann_layer_dropout_rates': [0.2], 'ann_epochs': 200, 'early_stop_patience': 50, 'batch_size': 256, 'dataset_dir': '../Datasets/small_datasets/ids2018', 'description': 'semi_sup_perf_ids18_subset_ae_ann_rep2'}
2020-01-08 12:29:51,090 [INFO] Created tensorboard log directory: results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/tf_logs_run_2020_01_08-12_29_51
2020-01-08 12:29:51,090 [INFO] Loading datsets from: ../Datasets/small_datasets/ids2018
2020-01-08 12:29:51,090 [INFO] Reading X, y files
2020-01-08 12:29:51,090 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_train.h5
2020-01-08 12:29:55,541 [INFO] Reading complete. time_to_read=4.45 seconds
2020-01-08 12:29:55,541 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_val.h5
2020-01-08 12:29:57,073 [INFO] Reading complete. time_to_read=1.53 seconds
2020-01-08 12:29:57,073 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_test.h5
2020-01-08 12:29:58,599 [INFO] Reading complete. time_to_read=1.53 seconds
2020-01-08 12:29:58,599 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_train.h5
2020-01-08 12:29:58,854 [INFO] Reading complete. time_to_read=0.26 seconds
2020-01-08 12:29:58,854 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_val.h5
2020-01-08 12:29:58,939 [INFO] Reading complete. time_to_read=0.08 seconds
2020-01-08 12:29:58,939 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_test.h5
2020-01-08 12:29:59,023 [INFO] Reading complete. time_to_read=0.08 seconds
2020-01-08 12:30:02,895 [INFO] Initializing model
2020-01-08 12:30:03,017 [INFO] _________________________________________________________________
2020-01-08 12:30:03,017 [INFO] Layer (type)                 Output Shape              Param #   
2020-01-08 12:30:03,017 [INFO] =================================================================
2020-01-08 12:30:03,017 [INFO] dense_5 (Dense)              (None, 64)                4992      
2020-01-08 12:30:03,017 [INFO] _________________________________________________________________
2020-01-08 12:30:03,018 [INFO] batch_normalization_3 (Batch (None, 64)                256       
2020-01-08 12:30:03,018 [INFO] _________________________________________________________________
2020-01-08 12:30:03,018 [INFO] dropout_3 (Dropout)          (None, 64)                0         
2020-01-08 12:30:03,018 [INFO] _________________________________________________________________
2020-01-08 12:30:03,018 [INFO] dense_6 (Dense)              (None, 77)                5005      
2020-01-08 12:30:03,018 [INFO] =================================================================
2020-01-08 12:30:03,018 [INFO] Total params: 10,253
2020-01-08 12:30:03,018 [INFO] Trainable params: 10,125
2020-01-08 12:30:03,018 [INFO] Non-trainable params: 128
2020-01-08 12:30:03,018 [INFO] _________________________________________________________________
2020-01-08 12:30:03,130 [INFO] _________________________________________________________________
2020-01-08 12:30:03,130 [INFO] Layer (type)                 Output Shape              Param #   
2020-01-08 12:30:03,130 [INFO] =================================================================
2020-01-08 12:30:03,130 [INFO] dense_7 (Dense)              (None, 64)                4160      
2020-01-08 12:30:03,130 [INFO] _________________________________________________________________
2020-01-08 12:30:03,130 [INFO] batch_normalization_4 (Batch (None, 64)                256       
2020-01-08 12:30:03,130 [INFO] _________________________________________________________________
2020-01-08 12:30:03,130 [INFO] dropout_4 (Dropout)          (None, 64)                0         
2020-01-08 12:30:03,130 [INFO] _________________________________________________________________
2020-01-08 12:30:03,130 [INFO] dense_8 (Dense)              (None, 15)                975       
2020-01-08 12:30:03,130 [INFO] =================================================================
2020-01-08 12:30:03,130 [INFO] Total params: 5,391
2020-01-08 12:30:03,131 [INFO] Trainable params: 5,263
2020-01-08 12:30:03,131 [INFO] Non-trainable params: 128
2020-01-08 12:30:03,131 [INFO] _________________________________________________________________
2020-01-08 12:30:03,131 [INFO] Training model
2020-01-08 12:30:03,131 [INFO] Splitting train set into 2 sets (unsupervised, supervised), random_seed = 42
2020-01-08 12:30:26,051 [INFO] Split sizes (instances). total = 1936462, unsupervised = 1452346, supervised = 484116, unsupervised dataset hash = 451b88645977a5a563e4faa1956ec3ea034501bc
2020-01-08 12:30:26,051 [INFO] Training autoencoder
 - val_f1: 0.9787
Epoch 00198: early stopping
Train on 1452346 samples, validate on 645487 samples
Epoch 1/200
 - 24s - loss: -3.3792e+00 - val_loss: -3.6583e+00
Epoch 2/200
 - 25s - loss: -3.6277e+00 - val_loss: -3.6648e+00
Epoch 3/200
 - 24s - loss: -3.6365e+00 - val_loss: -3.6673e+00
Epoch 4/200
 - 24s - loss: -3.6396e+00 - val_loss: -3.6362e+00
Epoch 5/200
 - 24s - loss: -3.6422e+00 - val_loss: -3.6704e+00
Epoch 6/200
 - 24s - loss: -3.6443e+00 - val_loss: -3.6696e+00
Epoch 7/200
 - 24s - loss: -3.6451e+00 - val_loss: -3.6700e+00
Epoch 8/200
 - 24s - loss: -3.6464e+00 - val_loss: -3.6708e+00
Epoch 9/200
 - 25s - loss: -3.6468e+00 - val_loss: -3.6709e+00
Epoch 10/200
 - 24s - loss: -3.6477e+00 - val_loss: -3.6722e+00
Epoch 11/200
 - 25s - loss: -3.6479e+00 - val_loss: -3.6720e+00
Epoch 12/200
 - 25s - loss: -3.6485e+00 - val_loss: -3.6727e+00
Epoch 13/200
 - 24s - loss: -3.6487e+00 - val_loss: -3.6727e+00
Epoch 14/200
 - 25s - loss: -3.6492e+00 - val_loss: -3.6729e+00
Epoch 15/200
 - 24s - loss: -3.6494e+00 - val_loss: -3.6728e+00
Epoch 16/200
 - 24s - loss: -3.6497e+00 - val_loss: -3.6738e+00
Epoch 17/200
 - 25s - loss: -3.6498e+00 - val_loss: -3.6743e+00
Epoch 18/200
 - 24s - loss: -3.6497e+00 - val_loss: -3.6743e+00
Epoch 19/200
 - 25s - loss: -3.6510e+00 - val_loss: -3.6746e+00
Epoch 20/200
 - 25s - loss: -3.6516e+00 - val_loss: -3.6623e+00
Epoch 21/200
 - 25s - loss: -3.6511e+00 - val_loss: -3.6750e+00
2020-01-08 12:38:55,457 [INFO] epoch = 20. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/ae_model_epoch_20.pickle
Epoch 22/200
 - 25s - loss: -3.6515e+00 - val_loss: -3.6747e+00
Epoch 23/200
 - 25s - loss: -3.6509e+00 - val_loss: -3.6754e+00
Epoch 24/200
 - 25s - loss: -3.6518e+00 - val_loss: -3.6751e+00
Epoch 25/200
 - 25s - loss: -3.6515e+00 - val_loss: -3.6751e+00
Epoch 26/200
 - 25s - loss: -3.6522e+00 - val_loss: -3.6693e+00
Epoch 27/200
 - 25s - loss: -3.6525e+00 - val_loss: -3.6600e+00
Epoch 28/200
 - 25s - loss: -3.6535e+00 - val_loss: -3.6767e+00
Epoch 29/200
 - 25s - loss: -3.6523e+00 - val_loss: -3.6768e+00
Epoch 30/200
 - 25s - loss: -3.6534e+00 - val_loss: -3.6762e+00
Epoch 31/200
 - 25s - loss: -3.6523e+00 - val_loss: -3.6762e+00
Epoch 32/200
 - 25s - loss: -3.6529e+00 - val_loss: -3.6762e+00
Epoch 33/200
 - 25s - loss: -3.6531e+00 - val_loss: -3.6757e+00
Epoch 34/200
 - 25s - loss: -3.6526e+00 - val_loss: -3.6776e+00
Epoch 35/200
 - 25s - loss: -3.6528e+00 - val_loss: -3.6776e+00
Epoch 36/200
 - 25s - loss: -3.6534e+00 - val_loss: -3.6725e+00
Epoch 37/200
 - 25s - loss: -3.6535e+00 - val_loss: -3.6776e+00
Epoch 38/200
 - 25s - loss: -3.6519e+00 - val_loss: -3.6756e+00
Epoch 39/200
 - 25s - loss: -3.6539e+00 - val_loss: -3.6779e+00
Epoch 40/200
 - 25s - loss: -3.6533e+00 - val_loss: -3.6780e+00
Epoch 41/200
 - 25s - loss: -3.6536e+00 - val_loss: -3.6771e+00
2020-01-08 12:47:10,200 [INFO] epoch = 40. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/ae_model_epoch_40.pickle
Epoch 42/200
 - 25s - loss: -3.6541e+00 - val_loss: -3.6675e+00
Epoch 43/200
 - 25s - loss: -3.6541e+00 - val_loss: -3.6771e+00
Epoch 44/200
 - 25s - loss: -3.6535e+00 - val_loss: -3.6781e+00
Epoch 45/200
 - 25s - loss: -3.6532e+00 - val_loss: -3.6710e+00
Epoch 46/200
 - 25s - loss: -3.6541e+00 - val_loss: -3.6779e+00
Epoch 47/200
 - 25s - loss: -3.6543e+00 - val_loss: -3.6497e+00
Epoch 48/200
 - 25s - loss: -3.6537e+00 - val_loss: -3.6776e+00
Epoch 49/200
 - 25s - loss: -3.6545e+00 - val_loss: -3.6781e+00
Epoch 50/200
 - 25s - loss: -3.6538e+00 - val_loss: -3.6506e+00
Epoch 51/200
 - 24s - loss: -3.6541e+00 - val_loss: -3.6408e+00
Epoch 52/200
 - 24s - loss: -3.6547e+00 - val_loss: -3.6781e+00
Epoch 53/200
 - 24s - loss: -3.6550e+00 - val_loss: -3.6777e+00
Epoch 54/200
 - 25s - loss: -3.6540e+00 - val_loss: -3.6780e+00
Epoch 55/200
 - 25s - loss: -3.6548e+00 - val_loss: -3.6776e+00
Epoch 56/200
 - 25s - loss: -3.6535e+00 - val_loss: -3.6770e+00
Epoch 57/200
 - 25s - loss: -3.6542e+00 - val_loss: -3.6491e+00
Epoch 58/200
 - 25s - loss: -3.6550e+00 - val_loss: -3.6783e+00
Epoch 59/200
 - 25s - loss: -3.6548e+00 - val_loss: -3.6783e+00
Epoch 60/200
 - 25s - loss: -3.6534e+00 - val_loss: -3.6773e+00
Epoch 61/200
 - 25s - loss: -3.6551e+00 - val_loss: -3.6778e+00
2020-01-08 12:55:23,068 [INFO] epoch = 60. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/ae_model_epoch_60.pickle
Epoch 62/200
 - 25s - loss: -3.6541e+00 - val_loss: -3.6516e+00
Epoch 63/200
 - 25s - loss: -3.6548e+00 - val_loss: -3.6062e+00
Epoch 64/200
 - 25s - loss: -3.6550e+00 - val_loss: -3.6781e+00
Epoch 65/200
 - 25s - loss: -3.6551e+00 - val_loss: -3.6778e+00
Epoch 66/200
 - 25s - loss: -3.6542e+00 - val_loss: -3.6253e+00
Epoch 67/200
 - 25s - loss: -3.6553e+00 - val_loss: -3.6713e+00
Epoch 68/200
 - 25s - loss: -3.6551e+00 - val_loss: -3.6777e+00
Epoch 69/200
 - 25s - loss: -3.6548e+00 - val_loss: -3.6780e+00
Epoch 70/200
 - 25s - loss: -3.6549e+00 - val_loss: -3.6782e+00
Epoch 71/200
 - 25s - loss: -3.6553e+00 - val_loss: -3.6385e+00
Epoch 72/200
 - 25s - loss: -3.6550e+00 - val_loss: -3.6781e+00
Epoch 73/200
 - 25s - loss: -3.6546e+00 - val_loss: -3.5886e+00
Epoch 74/200
 - 25s - loss: -3.6548e+00 - val_loss: -3.6780e+00
Epoch 75/200
 - 25s - loss: -3.6554e+00 - val_loss: -3.6770e+00
Epoch 76/200
 - 25s - loss: -3.6545e+00 - val_loss: -3.6784e+00
Epoch 77/200
 - 25s - loss: -3.6550e+00 - val_loss: -3.6732e+00
Epoch 78/200
 - 25s - loss: -3.6547e+00 - val_loss: -3.6712e+00
Epoch 79/200
 - 25s - loss: -3.6548e+00 - val_loss: -3.6783e+00
Epoch 80/200
 - 25s - loss: -3.6552e+00 - val_loss: -3.6781e+00
Epoch 81/200
 - 25s - loss: -3.6547e+00 - val_loss: -3.6783e+00
2020-01-08 13:03:39,588 [INFO] epoch = 80. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/ae_model_epoch_80.pickle
Epoch 82/200
 - 25s - loss: -3.6555e+00 - val_loss: -3.6784e+00
Epoch 83/200
 - 25s - loss: -3.6549e+00 - val_loss: -3.6782e+00
Epoch 84/200
 - 25s - loss: -3.6555e+00 - val_loss: -3.6786e+00
Epoch 85/200
 - 25s - loss: -3.6556e+00 - val_loss: -3.6778e+00
Epoch 86/200
 - 25s - loss: -3.6549e+00 - val_loss: -3.6778e+00
Epoch 87/200
 - 25s - loss: -3.6556e+00 - val_loss: -3.6754e+00
Epoch 88/200
 - 24s - loss: -3.6549e+00 - val_loss: -3.6785e+00
Epoch 89/200
 - 24s - loss: -3.6546e+00 - val_loss: -3.6785e+00
Epoch 90/200
 - 24s - loss: -3.6558e+00 - val_loss: -3.6783e+00
Epoch 91/200
 - 24s - loss: -3.6546e+00 - val_loss: -3.6784e+00
Epoch 92/200
 - 24s - loss: -3.6553e+00 - val_loss: -3.6519e+00
Epoch 93/200
 - 25s - loss: -3.6558e+00 - val_loss: -3.6791e+00
Epoch 94/200
 - 25s - loss: -3.6553e+00 - val_loss: -3.6785e+00
Epoch 95/200
 - 25s - loss: -3.6554e+00 - val_loss: -3.6783e+00
Epoch 96/200
 - 25s - loss: -3.6557e+00 - val_loss: -3.5803e+00
Epoch 97/200
 - 25s - loss: -3.6554e+00 - val_loss: -3.6785e+00
Epoch 98/200
 - 25s - loss: -3.6553e+00 - val_loss: -3.5902e+00
Epoch 99/200
 - 25s - loss: -3.6556e+00 - val_loss: -3.6773e+00
Epoch 100/200
 - 25s - loss: -3.6557e+00 - val_loss: -3.6788e+00
Epoch 101/200
 - 25s - loss: -3.6554e+00 - val_loss: -3.6782e+00
2020-01-08 13:11:50,626 [INFO] epoch = 100. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/ae_model_epoch_100.pickle
Epoch 102/200
 - 25s - loss: -3.6557e+00 - val_loss: -3.6786e+00
Epoch 103/200
 - 25s - loss: -3.6555e+00 - val_loss: -3.6750e+00
Epoch 104/200
 - 25s - loss: -3.6558e+00 - val_loss: -3.6780e+00
Epoch 105/200
 - 25s - loss: -3.6548e+00 - val_loss: -3.6260e+00
Epoch 106/200
 - 25s - loss: -3.6558e+00 - val_loss: -3.6785e+00
Epoch 107/200
 - 25s - loss: -3.6554e+00 - val_loss: -3.5301e+00
Epoch 108/200
 - 25s - loss: -3.6558e+00 - val_loss: -3.6786e+00
Epoch 109/200
 - 25s - loss: -3.6555e+00 - val_loss: -3.6787e+00
Epoch 110/200
 - 25s - loss: -3.6556e+00 - val_loss: -3.6637e+00
Epoch 111/200
 - 25s - loss: -3.6552e+00 - val_loss: -3.6784e+00
Epoch 112/200
 - 25s - loss: -3.6560e+00 - val_loss: -3.6787e+00
Epoch 113/200
 - 25s - loss: -3.6553e+00 - val_loss: -3.6781e+00
Epoch 114/200
 - 25s - loss: -3.6558e+00 - val_loss: -3.6787e+00
Epoch 115/200
 - 25s - loss: -3.6558e+00 - val_loss: -3.6571e+00
Epoch 116/200
 - 25s - loss: -3.6552e+00 - val_loss: -3.6784e+00
Epoch 117/200
 - 25s - loss: -3.6556e+00 - val_loss: -3.6333e+00
Epoch 118/200
 - 25s - loss: -3.6557e+00 - val_loss: -3.6787e+00
Epoch 119/200
 - 25s - loss: -3.6559e+00 - val_loss: -3.6781e+00
Epoch 120/200
 - 25s - loss: -3.6554e+00 - val_loss: -3.6774e+00
Epoch 121/200
 - 25s - loss: -3.6558e+00 - val_loss: -3.6788e+00
2020-01-08 13:20:06,012 [INFO] epoch = 120. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/ae_model_epoch_120.pickle
Epoch 122/200
 - 25s - loss: -3.6549e+00 - val_loss: -3.5903e+00
Epoch 123/200
 - 25s - loss: -3.6558e+00 - val_loss: -3.6784e+00
Epoch 124/200
 - 25s - loss: -3.6558e+00 - val_loss: -3.6224e+00
Epoch 125/200
 - 25s - loss: -3.6561e+00 - val_loss: -3.6783e+00
Epoch 126/200
 - 25s - loss: -3.6559e+00 - val_loss: -3.6773e+00
Epoch 127/200
 - 25s - loss: -3.6554e+00 - val_loss: -3.6782e+00
Epoch 128/200
 - 25s - loss: -3.6551e+00 - val_loss: -3.6786e+00
Epoch 129/200
 - 25s - loss: -3.6559e+00 - val_loss: -3.6782e+00
Epoch 130/200
 - 25s - loss: -3.6551e+00 - val_loss: -3.6694e+00
Epoch 131/200
 - 25s - loss: -3.6555e+00 - val_loss: -3.6789e+00
Epoch 132/200
 - 25s - loss: -3.6560e+00 - val_loss: -3.6658e+00
Epoch 133/200
 - 25s - loss: -3.6555e+00 - val_loss: -3.6475e+00
Epoch 134/200
 - 25s - loss: -3.6561e+00 - val_loss: -3.6789e+00
Epoch 135/200
 - 25s - loss: -3.6557e+00 - val_loss: -3.6775e+00
Epoch 136/200
 - 25s - loss: -3.6557e+00 - val_loss: -3.6789e+00
Epoch 137/200
 - 25s - loss: -3.6556e+00 - val_loss: -3.6788e+00
Epoch 138/200
 - 25s - loss: -3.6555e+00 - val_loss: -3.6499e+00
Epoch 139/200
 - 25s - loss: -3.6559e+00 - val_loss: -3.6774e+00
Epoch 140/200
 - 25s - loss: -3.6556e+00 - val_loss: -3.6790e+00
Epoch 141/200
 - 25s - loss: -3.6557e+00 - val_loss: -3.6164e+00
2020-01-08 13:28:22,575 [INFO] epoch = 140. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/ae_model_epoch_140.pickle
Epoch 142/200
 - 25s - loss: -3.6560e+00 - val_loss: -3.6254e+00
Epoch 143/200
 - 26s - loss: -3.6556e+00 - val_loss: -3.6785e+00
2020-01-08 13:29:13,101 [INFO] WeightRestorer::on_train_end(): restoring best weights
2020-01-08 13:29:44,978 [INFO] Last epoch loss evaluation: train_loss = -3.673638, val_loss = -3.679061
2020-01-08 13:29:44,978 [INFO] Training autoencoder complete
2020-01-08 13:29:44,978 [INFO] Encoding data for supervised training
2020-01-08 13:29:58,756 [INFO] Encoding complete
2020-01-08 13:29:58,756 [INFO] Training neural network layers (after autoencoder)
Epoch 00143: early stopping
Train on 484116 samples, validate on 645487 samples
Epoch 1/200
 - 5s - loss: 0.0238 - val_loss: 0.0093
 - val_f1: 0.9738
Epoch 2/200
 - 5s - loss: 0.0092 - val_loss: 0.0085
 - val_f1: 0.9751
Epoch 3/200
 - 5s - loss: 0.0088 - val_loss: 0.0085
 - val_f1: 0.9753
Epoch 4/200
 - 5s - loss: 0.0087 - val_loss: 0.0083
 - val_f1: 0.9772
Epoch 5/200
 - 5s - loss: 0.0086 - val_loss: 0.0082
 - val_f1: 0.9773
Epoch 6/200
 - 5s - loss: 0.0085 - val_loss: 0.0083
 - val_f1: 0.9777
Epoch 7/200
 - 5s - loss: 0.0085 - val_loss: 0.0082
 - val_f1: 0.9774
Epoch 8/200
 - 5s - loss: 0.0084 - val_loss: 0.0084
 - val_f1: 0.9766
Epoch 9/200
 - 5s - loss: 0.0083 - val_loss: 0.0080
 - val_f1: 0.9777
Epoch 10/200
 - 5s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9775
Epoch 11/200
 - 5s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9777
Epoch 12/200
 - 5s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9778
Epoch 13/200
 - 5s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9779
Epoch 14/200
 - 5s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9776
Epoch 15/200
 - 5s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9777
Epoch 16/200
 - 5s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9776
Epoch 17/200
 - 5s - loss: 0.0081 - val_loss: 0.0081
 - val_f1: 0.9778
Epoch 18/200
 - 5s - loss: 0.0081 - val_loss: 0.0081
 - val_f1: 0.9780
Epoch 19/200
 - 5s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 20/200
 - 5s - loss: 0.0081 - val_loss: 0.0081
 - val_f1: 0.9777
Epoch 21/200
 - 5s - loss: 0.0081 - val_loss: 0.0079
/home/hasitha/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2020-01-08 13:34:49,667 [INFO] epoch = 20. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/ann_model_epoch_20.pickle
 - val_f1: 0.9782
Epoch 22/200
 - 5s - loss: 0.0081 - val_loss: 0.0082
 - val_f1: 0.9778
Epoch 23/200
 - 5s - loss: 0.0081 - val_loss: 0.0081
 - val_f1: 0.9762
Epoch 24/200
 - 5s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9778
Epoch 25/200
 - 5s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9781
Epoch 26/200
 - 5s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9764
Epoch 27/200
 - 5s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9784
Epoch 28/200
 - 5s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 29/200
 - 5s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9778
Epoch 30/200
 - 5s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 31/200
 - 5s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9782
Epoch 32/200
 - 5s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 33/200
 - 5s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9780
Epoch 34/200
 - 5s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9784
Epoch 35/200
 - 5s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9776
Epoch 36/200
 - 5s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9778
Epoch 37/200
 - 5s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9784
Epoch 38/200
 - 5s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9784
Epoch 39/200
 - 5s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9765
Epoch 40/200
 - 5s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9786
Epoch 41/200
 - 5s - loss: 0.0080 - val_loss: 0.0079
2020-01-08 13:39:34,805 [INFO] epoch = 40. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/ann_model_epoch_40.pickle
 - val_f1: 0.9780
Epoch 42/200
 - 5s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 43/200
 - 5s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9768
Epoch 44/200
 - 5s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 45/200
 - 5s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 46/200
 - 5s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9785
Epoch 47/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 48/200
 - 5s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 49/200
 - 5s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 50/200
 - 5s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 51/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 52/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 53/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 54/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9778
Epoch 55/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 56/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 57/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 58/200
 - 5s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9780
Epoch 59/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9787
Epoch 60/200
 - 5s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 61/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
2020-01-08 13:44:19,870 [INFO] epoch = 60. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/ann_model_epoch_60.pickle
 - val_f1: 0.9785
Epoch 62/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 63/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 64/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 65/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9759
Epoch 66/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 67/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 68/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9785
Epoch 69/200
 - 5s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9780
Epoch 70/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 71/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 72/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 73/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9785
Epoch 74/200
 - 5s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9766
Epoch 75/200
 - 5s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 76/200
 - 5s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 77/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9784
Epoch 78/200
 - 5s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 79/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 80/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 81/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
2020-01-08 13:49:05,974 [INFO] epoch = 80. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/ann_model_epoch_80.pickle
 - val_f1: 0.9784
Epoch 82/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9767
Epoch 83/200
 - 5s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 84/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 85/200
 - 5s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 86/200
 - 5s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9788
Epoch 87/200
 - 5s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 88/200
 - 5s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 89/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9766
Epoch 90/200
 - 5s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 91/200
 - 5s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 92/200
 - 5s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 93/200
 - 5s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 94/200
 - 5s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9763
Epoch 95/200
 - 5s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 96/200
 - 5s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 97/200
 - 5s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 98/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 99/200
 - 5s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 100/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 101/200
 - 5s - loss: 0.0079 - val_loss: 0.0078
2020-01-08 13:53:51,564 [INFO] epoch = 100. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/ann_model_epoch_100.pickle
 - val_f1: 0.9783
Epoch 102/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9789
Epoch 103/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 104/200
 - 5s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 105/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 106/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 107/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9788
Epoch 108/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 109/200
 - 5s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9784
Epoch 110/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9787
Epoch 111/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 112/200
 - 5s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9784
Epoch 113/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 114/200
 - 5s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 115/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9788
Epoch 116/200
 - 5s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9767
Epoch 117/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9768
Epoch 118/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 119/200
 - 5s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 120/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 121/200
 - 5s - loss: 0.0078 - val_loss: 0.0079
2020-01-08 13:58:16,435 [INFO] epoch = 120. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/ann_model_epoch_120.pickle
 - val_f1: 0.9781
Epoch 122/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 123/200
 - 5s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 124/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 125/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 126/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 127/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9780
Epoch 128/200
 - 5s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 129/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 130/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9788
Epoch 131/200
 - 5s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9784
Epoch 132/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 133/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 134/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 135/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 136/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 137/200
 - 5s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 138/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 139/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 140/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 141/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
2020-01-08 14:02:51,631 [INFO] epoch = 140. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/ann_model_epoch_140.pickle
 - val_f1: 0.9766
Epoch 142/200
 - 5s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9785
Epoch 143/200
 - 5s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 144/200
 - 5s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 145/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 146/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9780
Epoch 147/200
 - 5s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 148/200
 - 5s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 149/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 150/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9788
Epoch 151/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 152/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9780
Epoch 153/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 154/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 155/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 156/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 157/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 158/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 159/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 160/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9787
Epoch 161/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
2020-01-08 14:07:37,086 [INFO] epoch = 160. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/ann_model_epoch_160.pickle
 - val_f1: 0.9787
Epoch 162/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 163/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9780
Epoch 164/200
 - 5s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 165/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 166/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 167/200
 - 5s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 168/200
 - 5s - loss: 0.0078 - val_loss: 0.0079
2020-01-08 14:09:26,020 [INFO] WeightRestorer::on_train_end(): restoring best weights
2020-01-08 14:09:46,484 [INFO] Last epoch loss evaluation: train_loss = 0.007602, val_loss = 0.007778
2020-01-08 14:09:46,527 [INFO] Training complete. time_to_train = 5983.40 sec, 99.72 min
2020-01-08 14:09:46,535 [INFO] Model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/best_model.pickle
2020-01-08 14:09:46,537 [INFO] Training history saved to: results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/training_error_history.csv
2020-01-08 14:09:46,690 [INFO] Plot saved to: results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/training_error_history.png
2020-01-08 14:09:46,825 [INFO] Plot saved to: results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/training_f1_history.png
2020-01-08 14:09:46,825 [INFO] Making predictions on training, validation, testing data
2020-01-08 14:11:14,688 [INFO] Evaluating predictions (results)
2020-01-08 14:11:26,860 [INFO] Dataset: Testing. Classification report below
2020-01-08 14:11:26,860 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99    535650
                     Bot       1.00      1.00      1.00     11465
        Brute Force -Web       1.00      0.12      0.22        24
        Brute Force -XSS       1.00      0.33      0.50         9
        DDOS attack-HOIC       1.00      1.00      1.00     27447
    DDOS attack-LOIC-UDP       0.70      0.99      0.82        67
  DDoS attacks-LOIC-HTTP       0.99      0.99      0.99     23010
   DoS attacks-GoldenEye       0.99      1.00      0.99      1651
        DoS attacks-Hulk       1.00      1.00      1.00     18478
DoS attacks-SlowHTTPTest       0.75      0.49      0.59      5596
   DoS attacks-Slowloris       0.98      0.75      0.85       440
          FTP-BruteForce       0.70      0.88      0.78      7718
           Infilteration       0.33      0.01      0.02      6404
           SQL Injection       1.00      0.25      0.40         4
          SSH-Bruteforce       1.00      1.00      1.00      7525

               micro avg       0.98      0.98      0.98    645488
               macro avg       0.90      0.72      0.74    645488
            weighted avg       0.98      0.98      0.98    645488

2020-01-08 14:11:26,860 [INFO] Overall accuracy (micro avg): 0.9832777681382148
2020-01-08 14:11:40,688 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9833         0.9833                       0.9833                0.0012                   0.0167  0.9833
1     Macro avg        0.9978         0.8952                       0.7208                0.0045                   0.2792  0.7442
2  Weighted avg        0.9908         0.9771                       0.9833                0.0503                   0.0167  0.9783
2020-01-08 14:11:52,847 [INFO] Dataset: Validation. Classification report below
2020-01-08 14:11:52,847 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99    535650
                     Bot       1.00      1.00      1.00     11465
        Brute Force -Web       1.00      0.20      0.33        25
        Brute Force -XSS       1.00      0.67      0.80         9
        DDOS attack-HOIC       1.00      1.00      1.00     27447
    DDOS attack-LOIC-UDP       0.72      0.99      0.83        68
  DDoS attacks-LOIC-HTTP       0.99      0.99      0.99     23009
   DoS attacks-GoldenEye       0.99      1.00      1.00      1651
        DoS attacks-Hulk       1.00      1.00      1.00     18478
DoS attacks-SlowHTTPTest       0.76      0.48      0.59      5596
   DoS attacks-Slowloris       0.97      0.80      0.88       439
          FTP-BruteForce       0.70      0.89      0.78      7718
           Infilteration       0.43      0.01      0.03      6403
           SQL Injection       0.00      0.00      0.00         4
          SSH-Bruteforce       1.00      1.00      1.00      7525

               micro avg       0.98      0.98      0.98    645487
               macro avg       0.84      0.73      0.75    645487
            weighted avg       0.98      0.98      0.98    645487

2020-01-08 14:11:52,847 [INFO] Overall accuracy (micro avg): 0.9834125241871641
/home/hasitha/sunanda/ids_experiments/utility.py:334: RuntimeWarning: invalid value encountered in true_divide
  F1 = (2 * PPV * TPR) / (PPV + TPR)  # F1 score
2020-01-08 14:12:06,667 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9834         0.9834                       0.9834                0.0012                   0.0166  0.9834
1     Macro avg        0.9978         0.8371                       0.7347                0.0044                   0.2653  0.7481
2  Weighted avg        0.9909         0.9782                       0.9834                0.0499                   0.0166  0.9784
2020-01-08 14:12:46,356 [INFO] Dataset: Training. Classification report below
2020-01-08 14:12:46,356 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99   1606949
                     Bot       1.00      1.00      1.00     34396
        Brute Force -Web       1.00      0.12      0.22        73
        Brute Force -XSS       1.00      0.50      0.67        26
        DDOS attack-HOIC       1.00      1.00      1.00     82341
    DDOS attack-LOIC-UDP       0.72      0.99      0.84       203
  DDoS attacks-LOIC-HTTP       0.99      0.99      0.99     69029
   DoS attacks-GoldenEye       0.99      1.00      1.00      4954
        DoS attacks-Hulk       1.00      1.00      1.00     55435
DoS attacks-SlowHTTPTest       0.75      0.48      0.59     16787
   DoS attacks-Slowloris       0.98      0.78      0.87      1318
          FTP-BruteForce       0.70      0.88      0.78     23153
           Infilteration       0.45      0.01      0.03     19210
           SQL Injection       0.75      0.25      0.38        12
          SSH-Bruteforce       1.00      1.00      1.00     22576

               micro avg       0.98      0.98      0.98   1936462
               macro avg       0.89      0.73      0.76   1936462
            weighted avg       0.98      0.98      0.98   1936462

2020-01-08 14:12:46,356 [INFO] Overall accuracy (micro avg): 0.9833572773439396
2020-01-08 14:13:31,432 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9834         0.9834                       0.9834                0.0012                   0.0166  0.9834
1     Macro avg        0.9978         0.8882                       0.7336                0.0044                   0.2664  0.7559
2  Weighted avg        0.9909         0.9783                       0.9834                0.0500                   0.0166  0.9784
2020-01-08 14:13:31,458 [INFO] Results saved to: results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/semi_sup_perf_ids18_subset_ae_ann_rep2_results.xlsx
2020-01-08 14:13:31,463 [INFO] ================= Finished running experiment no. 2 ================= 

2020-01-08 14:13:31,540 [INFO] Created directory: results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3
2020-01-08 14:13:31,540 [INFO] Initialized logging. log_filename = results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/run_log.log
2020-01-08 14:13:31,540 [INFO] ================= Running experiment no. 3  ================= 

2020-01-08 14:13:31,540 [INFO] Experiment parameters given below
2020-01-08 14:13:31,540 [INFO] 
{'experiment_num': 3, 'results_dir': 'results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3', 'model_type': 'classifier', 'model': 'ae_ann', 'normal_label': 'BENIGN', 'split_random_seed': 42, 'unsupervised_ratio': 0.75, 'ae_encoder_units': [64], 'ae_encoder_activations': ['relu'], 'ae_encoder_dropout_rates': [0.2], 'ae_encoder_l1_param': -1, 'ae_decoder_units': '', 'ae_decoder_activations': '', 'ae_decoder_dropout_rates': '', 'output_activation': 'sigmoid', 'loss_function': 'binary_crossentropy', 'ae_epochs': 200, 'ann_layer_units': [64], 'ann_layer_activations': ['relu'], 'ann_layer_dropout_rates': [0.2], 'ann_epochs': 200, 'early_stop_patience': 50, 'batch_size': 256, 'dataset_dir': '../Datasets/small_datasets/ids2018', 'description': 'semi_sup_perf_ids18_subset_ae_ann_rep3'}
2020-01-08 14:13:31,540 [INFO] Created tensorboard log directory: results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/tf_logs_run_2020_01_08-14_13_31
2020-01-08 14:13:31,540 [INFO] Loading datsets from: ../Datasets/small_datasets/ids2018
2020-01-08 14:13:31,540 [INFO] Reading X, y files
2020-01-08 14:13:31,540 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_train.h5
2020-01-08 14:13:35,995 [INFO] Reading complete. time_to_read=4.45 seconds
2020-01-08 14:13:35,995 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_val.h5
2020-01-08 14:13:37,576 [INFO] Reading complete. time_to_read=1.58 seconds
2020-01-08 14:13:37,576 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_test.h5
2020-01-08 14:13:39,103 [INFO] Reading complete. time_to_read=1.53 seconds
2020-01-08 14:13:39,103 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_train.h5
2020-01-08 14:13:39,351 [INFO] Reading complete. time_to_read=0.25 seconds
2020-01-08 14:13:39,351 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_val.h5
2020-01-08 14:13:39,436 [INFO] Reading complete. time_to_read=0.08 seconds
2020-01-08 14:13:39,436 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_test.h5
2020-01-08 14:13:39,520 [INFO] Reading complete. time_to_read=0.08 seconds
2020-01-08 14:13:43,390 [INFO] Initializing model
2020-01-08 14:13:43,512 [INFO] _________________________________________________________________
2020-01-08 14:13:43,512 [INFO] Layer (type)                 Output Shape              Param #   
2020-01-08 14:13:43,512 [INFO] =================================================================
2020-01-08 14:13:43,512 [INFO] dense_9 (Dense)              (None, 64)                4992      
2020-01-08 14:13:43,512 [INFO] _________________________________________________________________
2020-01-08 14:13:43,513 [INFO] batch_normalization_5 (Batch (None, 64)                256       
2020-01-08 14:13:43,513 [INFO] _________________________________________________________________
2020-01-08 14:13:43,513 [INFO] dropout_5 (Dropout)          (None, 64)                0         
2020-01-08 14:13:43,513 [INFO] _________________________________________________________________
2020-01-08 14:13:43,513 [INFO] dense_10 (Dense)             (None, 77)                5005      
2020-01-08 14:13:43,513 [INFO] =================================================================
2020-01-08 14:13:43,513 [INFO] Total params: 10,253
2020-01-08 14:13:43,513 [INFO] Trainable params: 10,125
2020-01-08 14:13:43,513 [INFO] Non-trainable params: 128
2020-01-08 14:13:43,513 [INFO] _________________________________________________________________
2020-01-08 14:13:43,626 [INFO] _________________________________________________________________
2020-01-08 14:13:43,626 [INFO] Layer (type)                 Output Shape              Param #   
2020-01-08 14:13:43,626 [INFO] =================================================================
2020-01-08 14:13:43,626 [INFO] dense_11 (Dense)             (None, 64)                4160      
2020-01-08 14:13:43,626 [INFO] _________________________________________________________________
2020-01-08 14:13:43,626 [INFO] batch_normalization_6 (Batch (None, 64)                256       
2020-01-08 14:13:43,626 [INFO] _________________________________________________________________
2020-01-08 14:13:43,626 [INFO] dropout_6 (Dropout)          (None, 64)                0         
2020-01-08 14:13:43,626 [INFO] _________________________________________________________________
2020-01-08 14:13:43,626 [INFO] dense_12 (Dense)             (None, 15)                975       
2020-01-08 14:13:43,626 [INFO] =================================================================
2020-01-08 14:13:43,627 [INFO] Total params: 5,391
2020-01-08 14:13:43,627 [INFO] Trainable params: 5,263
2020-01-08 14:13:43,627 [INFO] Non-trainable params: 128
2020-01-08 14:13:43,627 [INFO] _________________________________________________________________
2020-01-08 14:13:43,627 [INFO] Training model
2020-01-08 14:13:43,627 [INFO] Splitting train set into 2 sets (unsupervised, supervised), random_seed = 42
2020-01-08 14:14:07,700 [INFO] Split sizes (instances). total = 1936462, unsupervised = 1452346, supervised = 484116, unsupervised dataset hash = 451b88645977a5a563e4faa1956ec3ea034501bc
2020-01-08 14:14:07,701 [INFO] Training autoencoder
 - val_f1: 0.9781
Epoch 00168: early stopping
Train on 1452346 samples, validate on 645487 samples
Epoch 1/200
 - 24s - loss: -3.3781e+00 - val_loss: -3.6565e+00
Epoch 2/200
 - 24s - loss: -3.6290e+00 - val_loss: -3.6648e+00
Epoch 3/200
 - 24s - loss: -3.6372e+00 - val_loss: -3.6134e+00
Epoch 4/200
 - 24s - loss: -3.6418e+00 - val_loss: -3.6700e+00
Epoch 5/200
 - 24s - loss: -3.6446e+00 - val_loss: -3.6682e+00
Epoch 6/200
 - 24s - loss: -3.6445e+00 - val_loss: -3.6664e+00
Epoch 7/200
 - 24s - loss: -3.6470e+00 - val_loss: -3.6728e+00
Epoch 8/200
 - 24s - loss: -3.6477e+00 - val_loss: -3.6715e+00
Epoch 9/200
 - 24s - loss: -3.6473e+00 - val_loss: -3.6734e+00
Epoch 10/200
 - 24s - loss: -3.6481e+00 - val_loss: -3.6742e+00
Epoch 11/200
 - 24s - loss: -3.6490e+00 - val_loss: -3.6743e+00
Epoch 12/200
 - 24s - loss: -3.6493e+00 - val_loss: -3.6734e+00
Epoch 13/200
 - 24s - loss: -3.6503e+00 - val_loss: -3.6736e+00
Epoch 14/200
 - 24s - loss: -3.6500e+00 - val_loss: -3.6740e+00
Epoch 15/200
 - 24s - loss: -3.6496e+00 - val_loss: -3.6737e+00
Epoch 16/200
 - 24s - loss: -3.6505e+00 - val_loss: -3.6754e+00
Epoch 17/200
 - 24s - loss: -3.6513e+00 - val_loss: -3.6751e+00
Epoch 18/200
 - 24s - loss: -3.6513e+00 - val_loss: -3.6740e+00
Epoch 19/200
 - 24s - loss: -3.6507e+00 - val_loss: -3.6744e+00
Epoch 20/200
 - 24s - loss: -3.6506e+00 - val_loss: -3.6738e+00
Epoch 21/200
 - 24s - loss: -3.6513e+00 - val_loss: -3.6745e+00
2020-01-08 14:22:29,377 [INFO] epoch = 20. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/ae_model_epoch_20.pickle
Epoch 22/200
 - 24s - loss: -3.6510e+00 - val_loss: -3.6756e+00
Epoch 23/200
 - 24s - loss: -3.6526e+00 - val_loss: -3.6748e+00
Epoch 24/200
 - 24s - loss: -3.6516e+00 - val_loss: -3.6740e+00
Epoch 25/200
 - 24s - loss: -3.6502e+00 - val_loss: -3.6741e+00
Epoch 26/200
 - 24s - loss: -3.6511e+00 - val_loss: -3.6752e+00
Epoch 27/200
 - 24s - loss: -3.6515e+00 - val_loss: -3.6749e+00
Epoch 28/200
 - 24s - loss: -3.6529e+00 - val_loss: -3.6745e+00
Epoch 29/200
 - 24s - loss: -3.6516e+00 - val_loss: -3.6748e+00
Epoch 30/200
 - 24s - loss: -3.6531e+00 - val_loss: -3.6676e+00
Epoch 31/200
 - 24s - loss: -3.6524e+00 - val_loss: -3.6754e+00
Epoch 32/200
 - 24s - loss: -3.6528e+00 - val_loss: -3.6761e+00
Epoch 33/200
 - 24s - loss: -3.6527e+00 - val_loss: -3.6765e+00
Epoch 34/200
 - 24s - loss: -3.6534e+00 - val_loss: -3.6754e+00
Epoch 35/200
 - 24s - loss: -3.6522e+00 - val_loss: -3.6766e+00
Epoch 36/200
 - 24s - loss: -3.6530e+00 - val_loss: -3.6761e+00
Epoch 37/200
 - 24s - loss: -3.6532e+00 - val_loss: -3.6767e+00
Epoch 38/200
 - 24s - loss: -3.6527e+00 - val_loss: -3.6768e+00
Epoch 39/200
 - 24s - loss: -3.6539e+00 - val_loss: -3.6764e+00
Epoch 40/200
 - 24s - loss: -3.6518e+00 - val_loss: -3.6764e+00
Epoch 41/200
 - 24s - loss: -3.6537e+00 - val_loss: -3.6709e+00
2020-01-08 14:30:25,609 [INFO] epoch = 40. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/ae_model_epoch_40.pickle
Epoch 42/200
 - 24s - loss: -3.6526e+00 - val_loss: -3.6762e+00
Epoch 43/200
 - 24s - loss: -3.6525e+00 - val_loss: -3.6766e+00
Epoch 44/200
 - 24s - loss: -3.6526e+00 - val_loss: -3.6765e+00
Epoch 45/200
 - 24s - loss: -3.6524e+00 - val_loss: -3.6686e+00
Epoch 46/200
 - 24s - loss: -3.6533e+00 - val_loss: -3.6767e+00
Epoch 47/200
 - 24s - loss: -3.6530e+00 - val_loss: -3.6515e+00
Epoch 48/200
 - 24s - loss: -3.6538e+00 - val_loss: -3.6268e+00
Epoch 49/200
 - 24s - loss: -3.6537e+00 - val_loss: -3.6764e+00
Epoch 50/200
 - 24s - loss: -3.6533e+00 - val_loss: -3.6768e+00
Epoch 51/200
 - 24s - loss: -3.6532e+00 - val_loss: -3.6770e+00
Epoch 52/200
 - 24s - loss: -3.6542e+00 - val_loss: -3.6761e+00
Epoch 53/200
 - 24s - loss: -3.6529e+00 - val_loss: -3.6767e+00
Epoch 54/200
 - 24s - loss: -3.6519e+00 - val_loss: -3.6755e+00
Epoch 55/200
 - 24s - loss: -3.6547e+00 - val_loss: -3.6769e+00
Epoch 56/200
 - 24s - loss: -3.6533e+00 - val_loss: -3.6764e+00
Epoch 57/200
 - 24s - loss: -3.6544e+00 - val_loss: -3.6769e+00
Epoch 58/200
 - 24s - loss: -3.6530e+00 - val_loss: -3.6772e+00
Epoch 59/200
 - 24s - loss: -3.6527e+00 - val_loss: -3.5535e+00
Epoch 60/200
 - 24s - loss: -3.6544e+00 - val_loss: -3.6773e+00
Epoch 61/200
 - 24s - loss: -3.6548e+00 - val_loss: -3.6775e+00
2020-01-08 14:38:22,038 [INFO] epoch = 60. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/ae_model_epoch_60.pickle
Epoch 62/200
 - 24s - loss: -3.6535e+00 - val_loss: -3.6771e+00
Epoch 63/200
 - 24s - loss: -3.6539e+00 - val_loss: -3.6477e+00
Epoch 64/200
 - 24s - loss: -3.6542e+00 - val_loss: -3.6769e+00
Epoch 65/200
 - 24s - loss: -3.6535e+00 - val_loss: -3.6772e+00
Epoch 66/200
 - 24s - loss: -3.6548e+00 - val_loss: -3.6773e+00
Epoch 67/200
 - 24s - loss: -3.6540e+00 - val_loss: -3.6772e+00
Epoch 68/200
 - 24s - loss: -3.6538e+00 - val_loss: -3.6773e+00
Epoch 69/200
 - 24s - loss: -3.6542e+00 - val_loss: -3.6774e+00
Epoch 70/200
 - 24s - loss: -3.6547e+00 - val_loss: -3.6768e+00
Epoch 71/200
 - 24s - loss: -3.6551e+00 - val_loss: -3.6771e+00
Epoch 72/200
 - 24s - loss: -3.6545e+00 - val_loss: -3.6766e+00
Epoch 73/200
 - 24s - loss: -3.6540e+00 - val_loss: -3.6758e+00
Epoch 74/200
 - 24s - loss: -3.6546e+00 - val_loss: -3.6682e+00
Epoch 75/200
 - 24s - loss: -3.6542e+00 - val_loss: -3.6769e+00
Epoch 76/200
 - 24s - loss: -3.6551e+00 - val_loss: -3.6767e+00
Epoch 77/200
 - 24s - loss: -3.6533e+00 - val_loss: -3.6751e+00
Epoch 78/200
 - 24s - loss: -3.6551e+00 - val_loss: -3.6756e+00
Epoch 79/200
 - 24s - loss: -3.6546e+00 - val_loss: -3.6774e+00
Epoch 80/200
 - 24s - loss: -3.6550e+00 - val_loss: -3.6771e+00
Epoch 81/200
 - 24s - loss: -3.6544e+00 - val_loss: -3.6252e+00
2020-01-08 14:46:20,574 [INFO] epoch = 80. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/ae_model_epoch_80.pickle
Epoch 82/200
 - 24s - loss: -3.6553e+00 - val_loss: -3.6777e+00
Epoch 83/200
 - 24s - loss: -3.6554e+00 - val_loss: -3.6763e+00
Epoch 84/200
 - 24s - loss: -3.6553e+00 - val_loss: -3.6768e+00
Epoch 85/200
 - 24s - loss: -3.6534e+00 - val_loss: -3.6686e+00
Epoch 86/200
 - 24s - loss: -3.6555e+00 - val_loss: -3.6777e+00
Epoch 87/200
 - 24s - loss: -3.6551e+00 - val_loss: -3.6760e+00
Epoch 88/200
 - 24s - loss: -3.6551e+00 - val_loss: -3.6776e+00
Epoch 89/200
 - 24s - loss: -3.6548e+00 - val_loss: -3.6700e+00
Epoch 90/200
 - 24s - loss: -3.6551e+00 - val_loss: -3.6118e+00
Epoch 91/200
 - 24s - loss: -3.6552e+00 - val_loss: -3.6077e+00
Epoch 92/200
 - 24s - loss: -3.6557e+00 - val_loss: -3.6768e+00
Epoch 93/200
 - 24s - loss: -3.6549e+00 - val_loss: -3.6775e+00
Epoch 94/200
 - 24s - loss: -3.6554e+00 - val_loss: -3.6764e+00
Epoch 95/200
 - 24s - loss: -3.6549e+00 - val_loss: -3.6772e+00
Epoch 96/200
 - 24s - loss: -3.6552e+00 - val_loss: -3.6684e+00
Epoch 97/200
 - 24s - loss: -3.6548e+00 - val_loss: -3.6777e+00
Epoch 98/200
 - 24s - loss: -3.6550e+00 - val_loss: -3.6762e+00
Epoch 99/200
 - 24s - loss: -3.6555e+00 - val_loss: -3.6774e+00
Epoch 100/200
 - 24s - loss: -3.6551e+00 - val_loss: -3.6739e+00
Epoch 101/200
 - 24s - loss: -3.6544e+00 - val_loss: -3.6699e+00
2020-01-08 14:54:20,712 [INFO] epoch = 100. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/ae_model_epoch_100.pickle
Epoch 102/200
 - 24s - loss: -3.6553e+00 - val_loss: -3.6737e+00
Epoch 103/200
 - 24s - loss: -3.6554e+00 - val_loss: -3.6777e+00
Epoch 104/200
 - 24s - loss: -3.6552e+00 - val_loss: -3.6764e+00
Epoch 105/200
 - 24s - loss: -3.6549e+00 - val_loss: -3.6771e+00
Epoch 106/200
 - 24s - loss: -3.6551e+00 - val_loss: -3.6776e+00
Epoch 107/200
 - 24s - loss: -3.6553e+00 - val_loss: -3.6742e+00
Epoch 108/200
 - 24s - loss: -3.6555e+00 - val_loss: -3.6740e+00
Epoch 109/200
 - 24s - loss: -3.6552e+00 - val_loss: -3.6756e+00
Epoch 110/200
 - 24s - loss: -3.6556e+00 - val_loss: -3.6765e+00
Epoch 111/200
 - 24s - loss: -3.6542e+00 - val_loss: -3.6775e+00
Epoch 112/200
 - 24s - loss: -3.6555e+00 - val_loss: -3.6777e+00
Epoch 113/200
 - 24s - loss: -3.6554e+00 - val_loss: -3.6530e+00
Epoch 114/200
 - 24s - loss: -3.6551e+00 - val_loss: -3.6771e+00
Epoch 115/200
 - 24s - loss: -3.6551e+00 - val_loss: -3.6733e+00
Epoch 116/200
 - 24s - loss: -3.6557e+00 - val_loss: -3.6779e+00
Epoch 117/200
 - 24s - loss: -3.6554e+00 - val_loss: -3.6694e+00
Epoch 118/200
 - 24s - loss: -3.6550e+00 - val_loss: -3.6682e+00
Epoch 119/200
 - 24s - loss: -3.6552e+00 - val_loss: -3.6777e+00
Epoch 120/200
 - 24s - loss: -3.6548e+00 - val_loss: -3.6766e+00
Epoch 121/200
 - 24s - loss: -3.6554e+00 - val_loss: -3.6773e+00
2020-01-08 15:02:20,693 [INFO] epoch = 120. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/ae_model_epoch_120.pickle
Epoch 122/200
 - 24s - loss: -3.6557e+00 - val_loss: -3.6776e+00
Epoch 123/200
 - 24s - loss: -3.6548e+00 - val_loss: -3.6774e+00
Epoch 124/200
 - 24s - loss: -3.6555e+00 - val_loss: -3.6765e+00
Epoch 125/200
 - 24s - loss: -3.6551e+00 - val_loss: -3.6774e+00
Epoch 126/200
 - 24s - loss: -3.6551e+00 - val_loss: -3.6772e+00
Epoch 127/200
 - 24s - loss: -3.6554e+00 - val_loss: -3.5809e+00
Epoch 128/200
 - 24s - loss: -3.6561e+00 - val_loss: -3.6776e+00
Epoch 129/200
 - 24s - loss: -3.6559e+00 - val_loss: -3.6758e+00
Epoch 130/200
 - 24s - loss: -3.6546e+00 - val_loss: -3.6776e+00
Epoch 131/200
 - 24s - loss: -3.6540e+00 - val_loss: -3.6772e+00
Epoch 132/200
 - 24s - loss: -3.6557e+00 - val_loss: -3.6779e+00
Epoch 133/200
 - 24s - loss: -3.6551e+00 - val_loss: -3.6332e+00
Epoch 134/200
 - 24s - loss: -3.6557e+00 - val_loss: -3.6273e+00
Epoch 135/200
 - 24s - loss: -3.6533e+00 - val_loss: -3.6759e+00
Epoch 136/200
 - 24s - loss: -3.6544e+00 - val_loss: -3.6778e+00
Epoch 137/200
 - 24s - loss: -3.6556e+00 - val_loss: -3.6770e+00
Epoch 138/200
 - 24s - loss: -3.6551e+00 - val_loss: -3.6775e+00
Epoch 139/200
 - 24s - loss: -3.6536e+00 - val_loss: -3.6761e+00
Epoch 140/200
 - 24s - loss: -3.6550e+00 - val_loss: -3.6779e+00
Epoch 141/200
 - 24s - loss: -3.6556e+00 - val_loss: -3.6773e+00
2020-01-08 15:10:20,935 [INFO] epoch = 140. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/ae_model_epoch_140.pickle
Epoch 142/200
 - 24s - loss: -3.6537e+00 - val_loss: -3.6763e+00
Epoch 143/200
 - 24s - loss: -3.6548e+00 - val_loss: -3.6779e+00
Epoch 144/200
 - 24s - loss: -3.6552e+00 - val_loss: -3.6768e+00
Epoch 145/200
 - 24s - loss: -3.6555e+00 - val_loss: -3.6777e+00
Epoch 146/200
 - 24s - loss: -3.6560e+00 - val_loss: -3.6770e+00
Epoch 147/200
 - 24s - loss: -3.6555e+00 - val_loss: -3.6779e+00
Epoch 148/200
 - 24s - loss: -3.6553e+00 - val_loss: -3.6774e+00
Epoch 149/200
 - 24s - loss: -3.6548e+00 - val_loss: -3.6771e+00
Epoch 150/200
 - 24s - loss: -3.6558e+00 - val_loss: -3.6762e+00
Epoch 151/200
 - 24s - loss: -3.6537e+00 - val_loss: -3.6732e+00
Epoch 152/200
 - 24s - loss: -3.6555e+00 - val_loss: -3.6460e+00
Epoch 153/200
 - 24s - loss: -3.6556e+00 - val_loss: -3.6361e+00
Epoch 154/200
 - 24s - loss: -3.6561e+00 - val_loss: -3.6776e+00
Epoch 155/200
 - 24s - loss: -3.6534e+00 - val_loss: -3.6739e+00
Epoch 156/200
 - 24s - loss: -3.6549e+00 - val_loss: -3.6747e+00
Epoch 157/200
 - 24s - loss: -3.6554e+00 - val_loss: -3.6780e+00
Epoch 158/200
 - 24s - loss: -3.6558e+00 - val_loss: -3.6625e+00
Epoch 159/200
 - 24s - loss: -3.6559e+00 - val_loss: -3.6503e+00
Epoch 160/200
 - 24s - loss: -3.6561e+00 - val_loss: -3.6766e+00
Epoch 161/200
 - 24s - loss: -3.6552e+00 - val_loss: -3.6779e+00
2020-01-08 15:18:20,738 [INFO] epoch = 160. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/ae_model_epoch_160.pickle
Epoch 162/200
 - 24s - loss: -3.6556e+00 - val_loss: -3.6754e+00
Epoch 163/200
 - 24s - loss: -3.6528e+00 - val_loss: -3.6716e+00
Epoch 164/200
 - 24s - loss: -3.6550e+00 - val_loss: -3.6779e+00
Epoch 165/200
 - 24s - loss: -3.6552e+00 - val_loss: -3.6656e+00
Epoch 166/200
 - 24s - loss: -3.6556e+00 - val_loss: -3.6773e+00
Epoch 167/200
 - 24s - loss: -3.6555e+00 - val_loss: -3.6166e+00
Epoch 168/200
 - 24s - loss: -3.6559e+00 - val_loss: -3.6779e+00
Epoch 169/200
 - 24s - loss: -3.6556e+00 - val_loss: -3.6779e+00
Epoch 170/200
 - 24s - loss: -3.6560e+00 - val_loss: -3.6770e+00
Epoch 171/200
 - 24s - loss: -3.6558e+00 - val_loss: -3.6776e+00
Epoch 172/200
 - 24s - loss: -3.6555e+00 - val_loss: -3.6186e+00
Epoch 173/200
 - 24s - loss: -3.6560e+00 - val_loss: -3.6661e+00
Epoch 174/200
 - 24s - loss: -3.6534e+00 - val_loss: -3.6485e+00
Epoch 175/200
 - 24s - loss: -3.6552e+00 - val_loss: -3.6773e+00
Epoch 176/200
 - 24s - loss: -3.6563e+00 - val_loss: -3.6728e+00
Epoch 177/200
 - 24s - loss: -3.6553e+00 - val_loss: -3.6776e+00
Epoch 178/200
 - 24s - loss: -3.6554e+00 - val_loss: -3.6714e+00
Epoch 179/200
 - 24s - loss: -3.6553e+00 - val_loss: -3.6306e+00
Epoch 180/200
 - 24s - loss: -3.6561e+00 - val_loss: -3.6738e+00
Epoch 181/200
 - 24s - loss: -3.6533e+00 - val_loss: -3.6048e+00
2020-01-08 15:26:20,155 [INFO] epoch = 180. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/ae_model_epoch_180.pickle
Epoch 182/200
 - 24s - loss: -3.6549e+00 - val_loss: -3.6772e+00
Epoch 183/200
 - 24s - loss: -3.6555e+00 - val_loss: -3.6687e+00
Epoch 184/200
 - 24s - loss: -3.6557e+00 - val_loss: -3.6779e+00
Epoch 185/200
 - 24s - loss: -3.6557e+00 - val_loss: -3.6761e+00
Epoch 186/200
 - 24s - loss: -3.6553e+00 - val_loss: -3.6505e+00
Epoch 187/200
 - 24s - loss: -3.6563e+00 - val_loss: -3.6770e+00
Epoch 188/200
 - 24s - loss: -3.6548e+00 - val_loss: -3.6756e+00
Epoch 189/200
 - 24s - loss: -3.6558e+00 - val_loss: -3.6781e+00
Epoch 190/200
 - 24s - loss: -3.6553e+00 - val_loss: -3.6509e+00
Epoch 191/200
 - 24s - loss: -3.6556e+00 - val_loss: -3.6421e+00
Epoch 192/200
 - 24s - loss: -3.6564e+00 - val_loss: -3.6775e+00
Epoch 193/200
 - 24s - loss: -3.6547e+00 - val_loss: -3.6423e+00
Epoch 194/200
 - 24s - loss: -3.6559e+00 - val_loss: -3.6729e+00
Epoch 195/200
 - 24s - loss: -3.6558e+00 - val_loss: -3.6741e+00
Epoch 196/200
 - 24s - loss: -3.6563e+00 - val_loss: -3.6742e+00
Epoch 197/200
 - 24s - loss: -3.6562e+00 - val_loss: -3.6770e+00
Epoch 198/200
 - 24s - loss: -3.6540e+00 - val_loss: -3.6306e+00
Epoch 199/200
 - 24s - loss: -3.6562e+00 - val_loss: -3.6779e+00
Epoch 200/200
 - 24s - loss: -3.6561e+00 - val_loss: -3.6770e+00
2020-01-08 15:33:56,211 [INFO] WeightRestorer::on_train_end(): restoring best weights
2020-01-08 15:34:30,781 [INFO] Last epoch loss evaluation: train_loss = -3.673368, val_loss = -3.678058
2020-01-08 15:34:30,781 [INFO] Training autoencoder complete
2020-01-08 15:34:30,781 [INFO] Encoding data for supervised training
2020-01-08 15:34:48,343 [INFO] Encoding complete
2020-01-08 15:34:48,343 [INFO] Training neural network layers (after autoencoder)
Train on 484116 samples, validate on 645487 samples
Epoch 1/200
 - 5s - loss: 0.0248 - val_loss: 0.0094
 - val_f1: 0.9761
Epoch 2/200
 - 5s - loss: 0.0093 - val_loss: 0.0087
 - val_f1: 0.9772
Epoch 3/200
 - 5s - loss: 0.0089 - val_loss: 0.0084
 - val_f1: 0.9774
Epoch 4/200
 - 5s - loss: 0.0086 - val_loss: 0.0083
 - val_f1: 0.9775
Epoch 5/200
 - 5s - loss: 0.0085 - val_loss: 0.0083
 - val_f1: 0.9774
Epoch 6/200
 - 5s - loss: 0.0084 - val_loss: 0.0081
 - val_f1: 0.9776
Epoch 7/200
 - 5s - loss: 0.0084 - val_loss: 0.0082
 - val_f1: 0.9774
Epoch 8/200
 - 5s - loss: 0.0084 - val_loss: 0.0082
 - val_f1: 0.9757
Epoch 9/200
 - 5s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9775
Epoch 10/200
 - 5s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9778
Epoch 11/200
 - 5s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9776
Epoch 12/200
 - 5s - loss: 0.0082 - val_loss: 0.0082
 - val_f1: 0.9775
Epoch 13/200
 - 5s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9778
Epoch 14/200
 - 5s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9775
Epoch 15/200
 - 5s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9778
Epoch 16/200
 - 5s - loss: 0.0081 - val_loss: 0.0081
 - val_f1: 0.9778
Epoch 17/200
 - 5s - loss: 0.0081 - val_loss: 0.0081
 - val_f1: 0.9777
Epoch 18/200
 - 5s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 19/200
 - 5s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 20/200
 - 5s - loss: 0.0081 - val_loss: 0.0081
 - val_f1: 0.9777
Epoch 21/200
 - 5s - loss: 0.0081 - val_loss: 0.0081
/home/hasitha/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2020-01-08 15:39:59,272 [INFO] epoch = 20. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/ann_model_epoch_20.pickle
 - val_f1: 0.9778
Epoch 22/200
 - 5s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 23/200
 - 5s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9778
Epoch 24/200
 - 5s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9778
Epoch 25/200
 - 5s - loss: 0.0080 - val_loss: 0.0081
 - val_f1: 0.9762
Epoch 26/200
 - 5s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9762
Epoch 27/200
 - 5s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9778
Epoch 28/200
 - 5s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9765
Epoch 29/200
 - 5s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 30/200
 - 5s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9780
Epoch 31/200
 - 5s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 32/200
 - 5s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 33/200
 - 5s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9785
Epoch 34/200
 - 5s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9782
Epoch 35/200
 - 5s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9763
Epoch 36/200
 - 5s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9783
Epoch 37/200
 - 5s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9780
Epoch 38/200
 - 5s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9778
Epoch 39/200
 - 5s - loss: 0.0080 - val_loss: 0.0081
 - val_f1: 0.9778
Epoch 40/200
 - 5s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 41/200
 - 5s - loss: 0.0080 - val_loss: 0.0080
2020-01-08 15:45:07,113 [INFO] epoch = 40. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/ann_model_epoch_40.pickle
 - val_f1: 0.9783
Epoch 42/200
 - 5s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 43/200
 - 5s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 44/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 45/200
 - 5s - loss: 0.0079 - val_loss: 0.0081
 - val_f1: 0.9776
Epoch 46/200
 - 5s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 47/200
 - 5s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9782
Epoch 48/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 49/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 50/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 51/200
 - 5s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9778
Epoch 52/200
 - 5s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 53/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 54/200
 - 5s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 55/200
 - 5s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 56/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 57/200
 - 5s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 58/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 59/200
 - 5s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 60/200
 - 5s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9786
Epoch 61/200
 - 5s - loss: 0.0079 - val_loss: 0.0080
2020-01-08 15:50:14,452 [INFO] epoch = 60. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/ann_model_epoch_60.pickle
 - val_f1: 0.9784
Epoch 62/200
 - 5s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9782
Epoch 63/200
 - 5s - loss: 0.0079 - val_loss: 0.0081
 - val_f1: 0.9779
Epoch 64/200
 - 5s - loss: 0.0079 - val_loss: 0.0081
 - val_f1: 0.9782
Epoch 65/200
 - 5s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9781
Epoch 66/200
 - 5s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9781
Epoch 67/200
 - 5s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9781
Epoch 68/200
 - 5s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9780
Epoch 69/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 70/200
 - 5s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9782
Epoch 71/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 72/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 73/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 74/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 75/200
 - 5s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9762
Epoch 76/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9784
Epoch 77/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 78/200
 - 5s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9784
Epoch 79/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 80/200
 - 5s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 81/200
 - 5s - loss: 0.0079 - val_loss: 0.0080
2020-01-08 15:55:20,930 [INFO] epoch = 80. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/ann_model_epoch_80.pickle
 - val_f1: 0.9765
Epoch 82/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 83/200
 - 5s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9783
Epoch 84/200
 - 5s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 85/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 86/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 87/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 88/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9777
Epoch 89/200
 - 5s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 90/200
 - 5s - loss: 0.0078 - val_loss: 0.0080
 - val_f1: 0.9778
Epoch 91/200
 - 5s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9764
Epoch 92/200
 - 5s - loss: 0.0079 - val_loss: 0.0079
2020-01-08 15:58:20,006 [INFO] WeightRestorer::on_train_end(): restoring best weights
2020-01-08 15:58:42,247 [INFO] Last epoch loss evaluation: train_loss = 0.007730, val_loss = 0.007849
2020-01-08 15:58:42,290 [INFO] Training complete. time_to_train = 6298.66 sec, 104.98 min
2020-01-08 15:58:42,298 [INFO] Model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/best_model.pickle
2020-01-08 15:58:42,300 [INFO] Training history saved to: results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/training_error_history.csv
2020-01-08 15:58:42,436 [INFO] Plot saved to: results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/training_error_history.png
2020-01-08 15:58:42,560 [INFO] Plot saved to: results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/training_f1_history.png
2020-01-08 15:58:42,560 [INFO] Making predictions on training, validation, testing data
2020-01-08 16:00:21,477 [INFO] Evaluating predictions (results)
2020-01-08 16:00:33,638 [INFO] Dataset: Testing. Classification report below
2020-01-08 16:00:33,638 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99    535650
                     Bot       1.00      1.00      1.00     11465
        Brute Force -Web       1.00      0.12      0.22        24
        Brute Force -XSS       1.00      0.33      0.50         9
        DDOS attack-HOIC       1.00      1.00      1.00     27447
    DDOS attack-LOIC-UDP       0.70      0.99      0.82        67
  DDoS attacks-LOIC-HTTP       0.99      0.99      0.99     23010
   DoS attacks-GoldenEye       0.99      0.99      0.99      1651
        DoS attacks-Hulk       1.00      1.00      1.00     18478
DoS attacks-SlowHTTPTest       0.72      0.47      0.57      5596
   DoS attacks-Slowloris       0.97      0.75      0.85       440
          FTP-BruteForce       0.69      0.87      0.77      7718
           Infilteration       0.53      0.02      0.04      6404
           SQL Injection       0.50      0.25      0.33         4
          SSH-Bruteforce       1.00      1.00      1.00      7525

               micro avg       0.98      0.98      0.98    645488
               macro avg       0.87      0.72      0.74    645488
            weighted avg       0.98      0.98      0.98    645488

2020-01-08 16:00:33,638 [INFO] Overall accuracy (micro avg): 0.9830469350320997
2020-01-08 16:00:47,466 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9830         0.9830                       0.9830                0.0012                   0.0170  0.9830
1     Macro avg        0.9977         0.8725                       0.7188                0.0045                   0.2812  0.7384
2  Weighted avg        0.9909         0.9788                       0.9830                0.0499                   0.0170  0.9782
2020-01-08 16:00:59,631 [INFO] Dataset: Validation. Classification report below
2020-01-08 16:00:59,631 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99    535650
                     Bot       1.00      1.00      1.00     11465
        Brute Force -Web       1.00      0.20      0.33        25
        Brute Force -XSS       0.75      0.67      0.71         9
        DDOS attack-HOIC       1.00      1.00      1.00     27447
    DDOS attack-LOIC-UDP       0.75      1.00      0.86        68
  DDoS attacks-LOIC-HTTP       0.99      0.99      0.99     23009
   DoS attacks-GoldenEye       0.99      0.99      0.99      1651
        DoS attacks-Hulk       1.00      1.00      1.00     18478
DoS attacks-SlowHTTPTest       0.73      0.46      0.57      5596
   DoS attacks-Slowloris       0.97      0.80      0.88       439
          FTP-BruteForce       0.69      0.88      0.77      7718
           Infilteration       0.43      0.02      0.03      6403
           SQL Injection       0.00      0.00      0.00         4
          SSH-Bruteforce       1.00      1.00      1.00      7525

               micro avg       0.98      0.98      0.98    645487
               macro avg       0.82      0.73      0.74    645487
            weighted avg       0.98      0.98      0.98    645487

2020-01-08 16:00:59,631 [INFO] Overall accuracy (micro avg): 0.9830283181535802
/home/hasitha/sunanda/ids_experiments/utility.py:334: RuntimeWarning: invalid value encountered in true_divide
  F1 = (2 * PPV * TPR) / (PPV + TPR)  # F1 score
2020-01-08 16:01:13,459 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9830         0.9830                       0.9830                0.0012                   0.0170  0.9830
1     Macro avg        0.9977         0.8192                       0.7334                0.0045                   0.2666  0.7412
2  Weighted avg        0.9908         0.9778                       0.9830                0.0498                   0.0170  0.9781
2020-01-08 16:01:53,185 [INFO] Dataset: Training. Classification report below
2020-01-08 16:01:53,185 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99   1606949
                     Bot       1.00      1.00      1.00     34396
        Brute Force -Web       1.00      0.12      0.22        73
        Brute Force -XSS       1.00      0.50      0.67        26
        DDOS attack-HOIC       1.00      1.00      1.00     82341
    DDOS attack-LOIC-UDP       0.72      0.99      0.83       203
  DDoS attacks-LOIC-HTTP       0.99      0.99      0.99     69029
   DoS attacks-GoldenEye       0.99      0.99      0.99      4954
        DoS attacks-Hulk       1.00      1.00      1.00     55435
DoS attacks-SlowHTTPTest       0.72      0.46      0.56     16787
   DoS attacks-Slowloris       0.97      0.78      0.87      1318
          FTP-BruteForce       0.69      0.87      0.77     23153
           Infilteration       0.47      0.02      0.04     19210
           SQL Injection       1.00      0.25      0.40        12
          SSH-Bruteforce       1.00      1.00      1.00     22576

               micro avg       0.98      0.98      0.98   1936462
               macro avg       0.90      0.73      0.76   1936462
            weighted avg       0.98      0.98      0.98   1936462

2020-01-08 16:01:53,185 [INFO] Overall accuracy (micro avg): 0.9830484667398586
2020-01-08 16:02:38,304 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9830         0.9830                       0.9830                0.0012                   0.0170  0.9830
1     Macro avg        0.9977         0.9035                       0.7316                0.0044                   0.2684  0.7554
2  Weighted avg        0.9909         0.9783                       0.9830                0.0498                   0.0170  0.9781
2020-01-08 16:02:38,330 [INFO] Results saved to: results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/semi_sup_perf_ids18_subset_ae_ann_rep3_results.xlsx
2020-01-08 16:02:38,335 [INFO] ================= Finished running experiment no. 3 ================= 

2020-01-08 16:02:38,412 [INFO] ================= Finished running 3 experiments ================= 

 - val_f1: 0.9781
Epoch 00092: early stopping
