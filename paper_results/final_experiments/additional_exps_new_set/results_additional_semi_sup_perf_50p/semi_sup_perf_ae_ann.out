Using TensorFlow backend.
2019-12-23 15:43:40,978 [INFO] Read 6 experiments from file: experiment_specs/additional_exps/semi_sup_perf_ae_ann.csv
2019-12-23 15:43:40,978 [INFO] ================= Started running experiments ================= 

2019-12-23 15:43:40,978 [INFO] Created directory: results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1
2019-12-23 15:43:40,978 [INFO] Initialized logging. log_filename = results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/run_log.log
2019-12-23 15:43:40,978 [INFO] ================= Running experiment no. 1  ================= 

2019-12-23 15:43:40,978 [INFO] Experiment parameters given below
2019-12-23 15:43:40,979 [INFO] 
{'experiment_num': 1, 'results_dir': 'results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1', 'model_type': 'classifier', 'model': 'ae_ann', 'normal_label': 'normal', 'split_random_seed': 42, 'unsupervised_ratio': 0.5, 'ae_encoder_units': [64], 'ae_encoder_activations': ['relu'], 'ae_encoder_dropout_rates': [0.2], 'ae_encoder_l1_param': -1, 'ae_decoder_units': '', 'ae_decoder_activations': '', 'ae_decoder_dropout_rates': '', 'output_activation': 'sigmoid', 'loss_function': 'binary_crossentropy', 'ae_epochs': 200, 'ann_layer_units': [64], 'ann_layer_activations': ['relu'], 'ann_layer_dropout_rates': [0.2], 'ann_epochs': 300, 'early_stop_patience': 50, 'batch_size': 256, 'dataset_dir': '../Datasets/small_datasets/nsl_kdd_five_classes', 'description': 'semi_sup_perf_nsl_ae_ann_rep1'}
2019-12-23 15:43:40,979 [INFO] Created tensorboard log directory: results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/tf_logs_run_2019_12_23-15_43_40
2019-12-23 15:43:40,979 [INFO] Loading datsets from: ../Datasets/small_datasets/nsl_kdd_five_classes
2019-12-23 15:43:40,979 [INFO] Reading X, y files
2019-12-23 15:43:40,979 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_train.h5
2019-12-23 15:43:41,244 [INFO] Reading complete. time_to_read=0.26 seconds
2019-12-23 15:43:41,244 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_val.h5
2019-12-23 15:43:41,309 [INFO] Reading complete. time_to_read=0.07 seconds
2019-12-23 15:43:41,309 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_test.h5
2019-12-23 15:43:41,369 [INFO] Reading complete. time_to_read=0.06 seconds
2019-12-23 15:43:41,369 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_train.h5
2019-12-23 15:43:41,376 [INFO] Reading complete. time_to_read=0.01 seconds
2019-12-23 15:43:41,376 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_val.h5
2019-12-23 15:43:41,380 [INFO] Reading complete. time_to_read=0.00 seconds
2019-12-23 15:43:41,380 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_test.h5
2019-12-23 15:43:41,383 [INFO] Reading complete. time_to_read=0.00 seconds
2019-12-23 15:43:41,577 [INFO] Initializing model
WARNING:tensorflow:From /home/sunanda/test/ml_env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
2019-12-23 15:43:41,590 [WARNING] From /home/sunanda/test/ml_env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From /home/sunanda/test/ml_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
2019-12-23 15:43:41,653 [WARNING] From /home/sunanda/test/ml_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
2019-12-23 15:43:41,692 [INFO] _________________________________________________________________
2019-12-23 15:43:41,692 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-23 15:43:41,692 [INFO] =================================================================
2019-12-23 15:43:41,692 [INFO] dense_1 (Dense)              (None, 64)                7872      
2019-12-23 15:43:41,693 [INFO] _________________________________________________________________
2019-12-23 15:43:41,693 [INFO] batch_normalization_1 (Batch (None, 64)                256       
2019-12-23 15:43:41,693 [INFO] _________________________________________________________________
2019-12-23 15:43:41,693 [INFO] dropout_1 (Dropout)          (None, 64)                0         
2019-12-23 15:43:41,693 [INFO] _________________________________________________________________
2019-12-23 15:43:41,693 [INFO] dense_2 (Dense)              (None, 122)               7930      
2019-12-23 15:43:41,693 [INFO] =================================================================
2019-12-23 15:43:41,693 [INFO] Total params: 16,058
2019-12-23 15:43:41,693 [INFO] Trainable params: 15,930
2019-12-23 15:43:41,693 [INFO] Non-trainable params: 128
2019-12-23 15:43:41,693 [INFO] _________________________________________________________________
2019-12-23 15:43:41,796 [INFO] _________________________________________________________________
2019-12-23 15:43:41,796 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-23 15:43:41,796 [INFO] =================================================================
2019-12-23 15:43:41,796 [INFO] dense_3 (Dense)              (None, 64)                4160      
2019-12-23 15:43:41,796 [INFO] _________________________________________________________________
2019-12-23 15:43:41,796 [INFO] batch_normalization_2 (Batch (None, 64)                256       
2019-12-23 15:43:41,796 [INFO] _________________________________________________________________
2019-12-23 15:43:41,796 [INFO] dropout_2 (Dropout)          (None, 64)                0         
2019-12-23 15:43:41,796 [INFO] _________________________________________________________________
2019-12-23 15:43:41,796 [INFO] dense_4 (Dense)              (None, 5)                 325       
2019-12-23 15:43:41,796 [INFO] =================================================================
2019-12-23 15:43:41,796 [INFO] Total params: 4,741
2019-12-23 15:43:41,797 [INFO] Trainable params: 4,613
2019-12-23 15:43:41,797 [INFO] Non-trainable params: 128
2019-12-23 15:43:41,797 [INFO] _________________________________________________________________
2019-12-23 15:43:41,797 [INFO] Training model
2019-12-23 15:43:41,797 [INFO] Splitting train set into 2 sets (unsupervised, supervised), random_seed = 42
2019-12-23 15:43:42,481 [INFO] Split sizes (instances). total = 100778, unsupervised = 50389, supervised = 50389, unsupervised dataset hash = e354a362b75401773a1959143869099e0046a21e
2019-12-23 15:43:42,482 [INFO] Training autoencoder
WARNING:tensorflow:From /home/sunanda/test/ml_env/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
2019-12-23 15:43:42,854 [WARNING] From /home/sunanda/test/ml_env/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
2019-12-23 15:43:43.126017: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-12-23 15:43:43.144745: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3299620000 Hz
2019-12-23 15:43:43.145149: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x56f6eb0 executing computations on platform Host. Devices:
2019-12-23 15:43:43.145186: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
Train on 50389 samples, validate on 25195 samples
Epoch 1/200
 - 1s - loss: 0.0436 - val_loss: -9.2496e-01
Epoch 2/200
 - 1s - loss: -1.5033e+00 - val_loss: -2.1370e+00
Epoch 3/200
 - 1s - loss: -2.4204e+00 - val_loss: -2.7495e+00
Epoch 4/200
 - 1s - loss: -2.8396e+00 - val_loss: -3.0119e+00
Epoch 5/200
 - 1s - loss: -3.0166e+00 - val_loss: -3.1105e+00
Epoch 6/200
 - 1s - loss: -3.0926e+00 - val_loss: -3.1576e+00
Epoch 7/200
 - 1s - loss: -3.1321e+00 - val_loss: -3.1815e+00
Epoch 8/200
 - 1s - loss: -3.1561e+00 - val_loss: -3.1962e+00
Epoch 9/200
 - 1s - loss: -3.1718e+00 - val_loss: -3.2044e+00
Epoch 10/200
 - 1s - loss: -3.1825e+00 - val_loss: -3.2107e+00
Epoch 11/200
 - 1s - loss: -3.1913e+00 - val_loss: -3.2152e+00
Epoch 12/200
 - 1s - loss: -3.1984e+00 - val_loss: -3.2185e+00
Epoch 13/200
 - 1s - loss: -3.2027e+00 - val_loss: -3.2209e+00
Epoch 14/200
 - 1s - loss: -3.2068e+00 - val_loss: -3.2240e+00
Epoch 15/200
 - 1s - loss: -3.2105e+00 - val_loss: -3.2253e+00
Epoch 16/200
 - 1s - loss: -3.2134e+00 - val_loss: -3.2267e+00
Epoch 17/200
 - 1s - loss: -3.2165e+00 - val_loss: -3.2285e+00
Epoch 18/200
 - 1s - loss: -3.2189e+00 - val_loss: -3.2298e+00
Epoch 19/200
 - 1s - loss: -3.2213e+00 - val_loss: -3.2307e+00
Epoch 20/200
 - 1s - loss: -3.2240e+00 - val_loss: -3.2318e+00
Epoch 21/200
 - 1s - loss: -3.2259e+00 - val_loss: -3.2325e+00
2019-12-23 15:44:00,040 [INFO] epoch = 20. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/ae_model_epoch_20.pickle
Epoch 22/200
 - 1s - loss: -3.2270e+00 - val_loss: -3.2330e+00
Epoch 23/200
 - 1s - loss: -3.2286e+00 - val_loss: -3.2339e+00
Epoch 24/200
 - 1s - loss: -3.2295e+00 - val_loss: -3.2342e+00
Epoch 25/200
 - 1s - loss: -3.2309e+00 - val_loss: -3.2352e+00
Epoch 26/200
 - 1s - loss: -3.2325e+00 - val_loss: -3.2353e+00
Epoch 27/200
 - 1s - loss: -3.2330e+00 - val_loss: -3.2360e+00
Epoch 28/200
 - 1s - loss: -3.2345e+00 - val_loss: -3.2362e+00
Epoch 29/200
 - 1s - loss: -3.2358e+00 - val_loss: -3.2369e+00
Epoch 30/200
 - 1s - loss: -3.2364e+00 - val_loss: -3.2371e+00
Epoch 31/200
 - 1s - loss: -3.2378e+00 - val_loss: -3.2378e+00
Epoch 32/200
 - 1s - loss: -3.2381e+00 - val_loss: -3.2375e+00
Epoch 33/200
 - 1s - loss: -3.2386e+00 - val_loss: -3.2384e+00
Epoch 34/200
 - 1s - loss: -3.2398e+00 - val_loss: -3.2387e+00
Epoch 35/200
 - 1s - loss: -3.2402e+00 - val_loss: -3.2383e+00
Epoch 36/200
 - 1s - loss: -3.2408e+00 - val_loss: -3.2393e+00
Epoch 37/200
 - 1s - loss: -3.2417e+00 - val_loss: -3.2396e+00
Epoch 38/200
 - 1s - loss: -3.2421e+00 - val_loss: -3.2398e+00
Epoch 39/200
 - 1s - loss: -3.2428e+00 - val_loss: -3.2400e+00
Epoch 40/200
 - 1s - loss: -3.2434e+00 - val_loss: -3.2400e+00
Epoch 41/200
 - 1s - loss: -3.2437e+00 - val_loss: -3.2402e+00
2019-12-23 15:44:15,702 [INFO] epoch = 40. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/ae_model_epoch_40.pickle
Epoch 42/200
 - 1s - loss: -3.2441e+00 - val_loss: -3.2402e+00
Epoch 43/200
 - 1s - loss: -3.2450e+00 - val_loss: -3.2405e+00
Epoch 44/200
 - 1s - loss: -3.2448e+00 - val_loss: -3.2406e+00
Epoch 45/200
 - 1s - loss: -3.2456e+00 - val_loss: -3.2409e+00
Epoch 46/200
 - 1s - loss: -3.2461e+00 - val_loss: -3.2407e+00
Epoch 47/200
 - 1s - loss: -3.2462e+00 - val_loss: -3.2409e+00
Epoch 48/200
 - 1s - loss: -3.2471e+00 - val_loss: -3.2407e+00
Epoch 49/200
 - 1s - loss: -3.2480e+00 - val_loss: -3.2412e+00
Epoch 50/200
 - 1s - loss: -3.2482e+00 - val_loss: -3.2415e+00
Epoch 51/200
 - 1s - loss: -3.2477e+00 - val_loss: -3.2412e+00
Epoch 52/200
 - 1s - loss: -3.2482e+00 - val_loss: -3.2420e+00
Epoch 53/200
 - 1s - loss: -3.2487e+00 - val_loss: -3.2418e+00
Epoch 54/200
 - 1s - loss: -3.2494e+00 - val_loss: -3.2421e+00
Epoch 55/200
 - 1s - loss: -3.2477e+00 - val_loss: -3.2421e+00
Epoch 56/200
 - 1s - loss: -3.2501e+00 - val_loss: -3.2424e+00
Epoch 57/200
 - 1s - loss: -3.2504e+00 - val_loss: -3.2423e+00
Epoch 58/200
 - 1s - loss: -3.2502e+00 - val_loss: -3.2423e+00
Epoch 59/200
 - 1s - loss: -3.2505e+00 - val_loss: -3.2423e+00
Epoch 60/200
 - 1s - loss: -3.2504e+00 - val_loss: -3.2427e+00
Epoch 61/200
 - 1s - loss: -3.2515e+00 - val_loss: -3.2425e+00
2019-12-23 15:44:31,384 [INFO] epoch = 60. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/ae_model_epoch_60.pickle
Epoch 62/200
 - 1s - loss: -3.2507e+00 - val_loss: -3.2422e+00
Epoch 63/200
 - 1s - loss: -3.2513e+00 - val_loss: -3.2427e+00
Epoch 64/200
 - 1s - loss: -3.2512e+00 - val_loss: -3.2428e+00
Epoch 65/200
 - 1s - loss: -3.2519e+00 - val_loss: -3.2431e+00
Epoch 66/200
 - 1s - loss: -3.2514e+00 - val_loss: -3.2431e+00
Epoch 67/200
 - 1s - loss: -3.2524e+00 - val_loss: -3.2433e+00
Epoch 68/200
 - 1s - loss: -3.2526e+00 - val_loss: -3.2433e+00
Epoch 69/200
 - 1s - loss: -3.2528e+00 - val_loss: -3.2435e+00
Epoch 70/200
 - 1s - loss: -3.2528e+00 - val_loss: -3.2435e+00
Epoch 71/200
 - 1s - loss: -3.2530e+00 - val_loss: -3.2436e+00
Epoch 72/200
 - 1s - loss: -3.2533e+00 - val_loss: -3.2437e+00
Epoch 73/200
 - 1s - loss: -3.2535e+00 - val_loss: -3.2437e+00
Epoch 74/200
 - 1s - loss: -3.2537e+00 - val_loss: -3.2437e+00
Epoch 75/200
 - 1s - loss: -3.2540e+00 - val_loss: -3.2438e+00
Epoch 76/200
 - 1s - loss: -3.2541e+00 - val_loss: -3.2439e+00
Epoch 77/200
 - 1s - loss: -3.2543e+00 - val_loss: -3.2440e+00
Epoch 78/200
 - 1s - loss: -3.2543e+00 - val_loss: -3.2440e+00
Epoch 79/200
 - 1s - loss: -3.2542e+00 - val_loss: -3.2440e+00
Epoch 80/200
 - 1s - loss: -3.2544e+00 - val_loss: -3.2442e+00
Epoch 81/200
 - 1s - loss: -3.2547e+00 - val_loss: -3.2439e+00
2019-12-23 15:44:47,109 [INFO] epoch = 80. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/ae_model_epoch_80.pickle
Epoch 82/200
 - 1s - loss: -3.2543e+00 - val_loss: -3.2440e+00
Epoch 83/200
 - 1s - loss: -3.2546e+00 - val_loss: -3.2440e+00
Epoch 84/200
 - 1s - loss: -3.2543e+00 - val_loss: -3.2444e+00
Epoch 85/200
 - 1s - loss: -3.2545e+00 - val_loss: -3.2439e+00
Epoch 86/200
 - 1s - loss: -3.2553e+00 - val_loss: -3.2443e+00
Epoch 87/200
 - 1s - loss: -3.2552e+00 - val_loss: -3.2440e+00
Epoch 88/200
 - 1s - loss: -3.2557e+00 - val_loss: -3.2444e+00
Epoch 89/200
 - 1s - loss: -3.2553e+00 - val_loss: -3.2445e+00
Epoch 90/200
 - 1s - loss: -3.2558e+00 - val_loss: -3.2444e+00
Epoch 91/200
 - 1s - loss: -3.2557e+00 - val_loss: -3.2447e+00
Epoch 92/200
 - 1s - loss: -3.2559e+00 - val_loss: -3.2446e+00
Epoch 93/200
 - 1s - loss: -3.2558e+00 - val_loss: -3.2449e+00
Epoch 94/200
 - 1s - loss: -3.2557e+00 - val_loss: -3.2442e+00
Epoch 95/200
 - 1s - loss: -3.2559e+00 - val_loss: -3.2446e+00
Epoch 96/200
 - 1s - loss: -3.2560e+00 - val_loss: -3.2445e+00
Epoch 97/200
 - 1s - loss: -3.2560e+00 - val_loss: -3.2448e+00
Epoch 98/200
 - 1s - loss: -3.2541e+00 - val_loss: -3.2441e+00
Epoch 99/200
 - 1s - loss: -3.2562e+00 - val_loss: -3.2445e+00
Epoch 100/200
 - 1s - loss: -3.2566e+00 - val_loss: -3.2449e+00
Epoch 101/200
 - 1s - loss: -3.2563e+00 - val_loss: -3.2447e+00
2019-12-23 15:45:02,819 [INFO] epoch = 100. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/ae_model_epoch_100.pickle
Epoch 102/200
 - 1s - loss: -3.2562e+00 - val_loss: -3.2447e+00
Epoch 103/200
 - 1s - loss: -3.2566e+00 - val_loss: -3.2447e+00
Epoch 104/200
 - 1s - loss: -3.2567e+00 - val_loss: -3.2448e+00
Epoch 105/200
 - 1s - loss: -3.2563e+00 - val_loss: -3.2449e+00
Epoch 106/200
 - 1s - loss: -3.2567e+00 - val_loss: -3.2447e+00
Epoch 107/200
 - 1s - loss: -3.2570e+00 - val_loss: -3.2447e+00
Epoch 108/200
 - 1s - loss: -3.2551e+00 - val_loss: -3.2448e+00
Epoch 109/200
 - 1s - loss: -3.2557e+00 - val_loss: -3.2451e+00
Epoch 110/200
 - 1s - loss: -3.2571e+00 - val_loss: -3.2450e+00
Epoch 111/200
 - 1s - loss: -3.2571e+00 - val_loss: -3.2447e+00
Epoch 112/200
 - 1s - loss: -3.2573e+00 - val_loss: -3.2451e+00
Epoch 113/200
 - 1s - loss: -3.2575e+00 - val_loss: -3.2448e+00
Epoch 114/200
 - 1s - loss: -3.2573e+00 - val_loss: -3.2449e+00
Epoch 115/200
 - 1s - loss: -3.2573e+00 - val_loss: -3.2449e+00
Epoch 116/200
 - 1s - loss: -3.2573e+00 - val_loss: -3.2450e+00
Epoch 117/200
 - 1s - loss: -3.2570e+00 - val_loss: -3.2453e+00
Epoch 118/200
 - 1s - loss: -3.2576e+00 - val_loss: -3.2454e+00
Epoch 119/200
 - 1s - loss: -3.2573e+00 - val_loss: -3.2451e+00
Epoch 120/200
 - 1s - loss: -3.2576e+00 - val_loss: -3.2448e+00
Epoch 121/200
 - 1s - loss: -3.2576e+00 - val_loss: -3.2450e+00
2019-12-23 15:45:18,495 [INFO] epoch = 120. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/ae_model_epoch_120.pickle
Epoch 122/200
 - 1s - loss: -3.2575e+00 - val_loss: -3.2451e+00
Epoch 123/200
 - 1s - loss: -3.2569e+00 - val_loss: -3.2452e+00
Epoch 124/200
 - 1s - loss: -3.2578e+00 - val_loss: -3.2450e+00
Epoch 125/200
 - 1s - loss: -3.2577e+00 - val_loss: -3.2453e+00
Epoch 126/200
 - 1s - loss: -3.2580e+00 - val_loss: -3.2450e+00
Epoch 127/200
 - 1s - loss: -3.2583e+00 - val_loss: -3.2451e+00
Epoch 128/200
 - 1s - loss: -3.2582e+00 - val_loss: -3.2453e+00
Epoch 129/200
 - 1s - loss: -3.2582e+00 - val_loss: -3.2450e+00
Epoch 130/200
 - 1s - loss: -3.2582e+00 - val_loss: -3.2454e+00
Epoch 131/200
 - 1s - loss: -3.2582e+00 - val_loss: -3.2451e+00
Epoch 132/200
 - 1s - loss: -3.2583e+00 - val_loss: -3.2456e+00
Epoch 133/200
 - 1s - loss: -3.2581e+00 - val_loss: -3.2458e+00
Epoch 134/200
 - 1s - loss: -3.2581e+00 - val_loss: -3.2454e+00
Epoch 135/200
 - 1s - loss: -3.2582e+00 - val_loss: -3.2458e+00
Epoch 136/200
 - 1s - loss: -3.2577e+00 - val_loss: -3.2455e+00
Epoch 137/200
 - 1s - loss: -3.2586e+00 - val_loss: -3.2454e+00
Epoch 138/200
 - 1s - loss: -3.2583e+00 - val_loss: -3.2456e+00
Epoch 139/200
 - 1s - loss: -3.2586e+00 - val_loss: -3.2456e+00
Epoch 140/200
 - 1s - loss: -3.2581e+00 - val_loss: -3.2454e+00
Epoch 141/200
 - 1s - loss: -3.2575e+00 - val_loss: -3.2458e+00
2019-12-23 15:45:34,080 [INFO] epoch = 140. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/ae_model_epoch_140.pickle
Epoch 142/200
 - 1s - loss: -3.2588e+00 - val_loss: -3.2456e+00
Epoch 143/200
 - 1s - loss: -3.2587e+00 - val_loss: -3.2454e+00
Epoch 144/200
 - 1s - loss: -3.2586e+00 - val_loss: -3.2455e+00
Epoch 145/200
 - 1s - loss: -3.2593e+00 - val_loss: -3.2458e+00
Epoch 146/200
 - 1s - loss: -3.2590e+00 - val_loss: -3.2459e+00
Epoch 147/200
 - 1s - loss: -3.2590e+00 - val_loss: -3.2459e+00
Epoch 148/200
 - 1s - loss: -3.2589e+00 - val_loss: -3.2457e+00
Epoch 149/200
 - 1s - loss: -3.2584e+00 - val_loss: -3.2457e+00
Epoch 150/200
 - 1s - loss: -3.2583e+00 - val_loss: -3.2459e+00
Epoch 151/200
 - 1s - loss: -3.2589e+00 - val_loss: -3.2461e+00
Epoch 152/200
 - 1s - loss: -3.2591e+00 - val_loss: -3.2459e+00
Epoch 153/200
 - 1s - loss: -3.2595e+00 - val_loss: -3.2457e+00
Epoch 154/200
 - 1s - loss: -3.2586e+00 - val_loss: -3.2457e+00
Epoch 155/200
 - 1s - loss: -3.2597e+00 - val_loss: -3.2458e+00
Epoch 156/200
 - 1s - loss: -3.2587e+00 - val_loss: -3.2459e+00
Epoch 157/200
 - 1s - loss: -3.2589e+00 - val_loss: -3.2456e+00
Epoch 158/200
 - 1s - loss: -3.2592e+00 - val_loss: -3.2459e+00
Epoch 159/200
 - 1s - loss: -3.2590e+00 - val_loss: -3.2459e+00
Epoch 160/200
 - 1s - loss: -3.2594e+00 - val_loss: -3.2457e+00
Epoch 161/200
 - 1s - loss: -3.2594e+00 - val_loss: -3.2454e+00
2019-12-23 15:45:49,852 [INFO] epoch = 160. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/ae_model_epoch_160.pickle
Epoch 162/200
 - 1s - loss: -3.2592e+00 - val_loss: -3.2454e+00
Epoch 163/200
 - 1s - loss: -3.2592e+00 - val_loss: -3.2456e+00
Epoch 164/200
 - 1s - loss: -3.2593e+00 - val_loss: -3.2456e+00
Epoch 165/200
 - 1s - loss: -3.2596e+00 - val_loss: -3.2455e+00
Epoch 166/200
 - 1s - loss: -3.2593e+00 - val_loss: -3.2461e+00
Epoch 167/200
 - 1s - loss: -3.2596e+00 - val_loss: -3.2460e+00
Epoch 168/200
 - 1s - loss: -3.2596e+00 - val_loss: -3.2460e+00
Epoch 169/200
 - 1s - loss: -3.2590e+00 - val_loss: -3.2459e+00
Epoch 170/200
 - 1s - loss: -3.2599e+00 - val_loss: -3.2457e+00
Epoch 171/200
 - 1s - loss: -3.2593e+00 - val_loss: -3.2461e+00
Epoch 172/200
 - 1s - loss: -3.2595e+00 - val_loss: -3.2463e+00
Epoch 173/200
 - 1s - loss: -3.2600e+00 - val_loss: -3.2462e+00
Epoch 174/200
 - 1s - loss: -3.2594e+00 - val_loss: -3.2458e+00
Epoch 175/200
 - 1s - loss: -3.2600e+00 - val_loss: -3.2461e+00
Epoch 176/200
 - 1s - loss: -3.2597e+00 - val_loss: -3.2462e+00
Epoch 177/200
 - 1s - loss: -3.2601e+00 - val_loss: -3.2463e+00
Epoch 178/200
 - 1s - loss: -3.2600e+00 - val_loss: -3.2461e+00
Epoch 179/200
 - 1s - loss: -3.2576e+00 - val_loss: -3.2465e+00
Epoch 180/200
 - 1s - loss: -3.2598e+00 - val_loss: -3.2466e+00
Epoch 181/200
 - 1s - loss: -3.2599e+00 - val_loss: -3.2466e+00
2019-12-23 15:46:05,523 [INFO] epoch = 180. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/ae_model_epoch_180.pickle
Epoch 182/200
 - 1s - loss: -3.2598e+00 - val_loss: -3.2463e+00
Epoch 183/200
 - 1s - loss: -3.2597e+00 - val_loss: -3.2463e+00
Epoch 184/200
 - 1s - loss: -3.2597e+00 - val_loss: -3.2463e+00
Epoch 185/200
 - 1s - loss: -3.2593e+00 - val_loss: -3.2463e+00
Epoch 186/200
 - 1s - loss: -3.2603e+00 - val_loss: -3.2461e+00
Epoch 187/200
 - 1s - loss: -3.2605e+00 - val_loss: -3.2460e+00
Epoch 188/200
 - 1s - loss: -3.2604e+00 - val_loss: -3.2460e+00
Epoch 189/200
 - 1s - loss: -3.2601e+00 - val_loss: -3.2459e+00
Epoch 190/200
 - 1s - loss: -3.2602e+00 - val_loss: -3.2461e+00
Epoch 191/200
 - 1s - loss: -3.2598e+00 - val_loss: -3.2466e+00
Epoch 192/200
 - 1s - loss: -3.2600e+00 - val_loss: -3.2469e+00
Epoch 193/200
 - 1s - loss: -3.2605e+00 - val_loss: -3.2466e+00
Epoch 194/200
 - 1s - loss: -3.2582e+00 - val_loss: -3.2464e+00
Epoch 195/200
 - 1s - loss: -3.2599e+00 - val_loss: -3.2464e+00
Epoch 196/200
 - 1s - loss: -3.2602e+00 - val_loss: -3.2460e+00
Epoch 197/200
 - 1s - loss: -3.2605e+00 - val_loss: -3.2461e+00
Epoch 198/200
 - 1s - loss: -3.2598e+00 - val_loss: -3.2461e+00
Epoch 199/200
 - 1s - loss: -3.2604e+00 - val_loss: -3.2465e+00
Epoch 200/200
 - 1s - loss: -3.2605e+00 - val_loss: -3.2465e+00
2019-12-23 15:46:20,432 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-23 15:46:21,453 [INFO] Last epoch loss evaluation: train_loss = -3.274230, val_loss = -3.246856
2019-12-23 15:46:21,453 [INFO] Training autoencoder complete
2019-12-23 15:46:21,453 [INFO] Encoding data for supervised training
2019-12-23 15:46:22,093 [INFO] Encoding complete
2019-12-23 15:46:22,093 [INFO] Training neural network layers (after autoencoder)
Train on 50389 samples, validate on 25195 samples
Epoch 1/300
 - 1s - loss: 0.1388 - val_loss: 0.0393
 - val_f1: 0.9713
Epoch 2/300
 - 0s - loss: 0.0343 - val_loss: 0.0206
 - val_f1: 0.9833
Epoch 3/300
 - 0s - loss: 0.0220 - val_loss: 0.0146
 - val_f1: 0.9887
Epoch 4/300
 - 0s - loss: 0.0176 - val_loss: 0.0120
 - val_f1: 0.9916
Epoch 5/300
 - 0s - loss: 0.0156 - val_loss: 0.0111
 - val_f1: 0.9909
Epoch 6/300
 - 0s - loss: 0.0138 - val_loss: 0.0101
 - val_f1: 0.9929
Epoch 7/300
 - 0s - loss: 0.0127 - val_loss: 0.0094
 - val_f1: 0.9930
Epoch 8/300
 - 0s - loss: 0.0121 - val_loss: 0.0093
 - val_f1: 0.9931
Epoch 9/300
 - 0s - loss: 0.0111 - val_loss: 0.0091
 - val_f1: 0.9926
Epoch 10/300
 - 0s - loss: 0.0108 - val_loss: 0.0085
 - val_f1: 0.9940
Epoch 11/300
 - 0s - loss: 0.0103 - val_loss: 0.0088
 - val_f1: 0.9927
Epoch 12/300
 - 0s - loss: 0.0096 - val_loss: 0.0083
 - val_f1: 0.9934
Epoch 13/300
 - 0s - loss: 0.0097 - val_loss: 0.0082
 - val_f1: 0.9940
Epoch 14/300
 - 0s - loss: 0.0094 - val_loss: 0.0077
 - val_f1: 0.9938
Epoch 15/300
 - 0s - loss: 0.0088 - val_loss: 0.0073
 - val_f1: 0.9946
Epoch 16/300
 - 0s - loss: 0.0084 - val_loss: 0.0077
 - val_f1: 0.9945
Epoch 17/300
 - 0s - loss: 0.0080 - val_loss: 0.0076
 - val_f1: 0.9942
Epoch 18/300
 - 0s - loss: 0.0080 - val_loss: 0.0074
 - val_f1: 0.9943
Epoch 19/300
 - 0s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9945
Epoch 20/300
 - 0s - loss: 0.0076 - val_loss: 0.0075
 - val_f1: 0.9940
Epoch 21/300
 - 0s - loss: 0.0075 - val_loss: 0.0073
 - val_f1: 0.9948
Epoch 22/300
 - 0s - loss: 0.0076 - val_loss: 0.0076
 - val_f1: 0.9937
Epoch 23/300
 - 0s - loss: 0.0071 - val_loss: 0.0074
 - val_f1: 0.9949
Epoch 24/300
 - 0s - loss: 0.0072 - val_loss: 0.0073
 - val_f1: 0.9945
Epoch 25/300
 - 0s - loss: 0.0072 - val_loss: 0.0072
 - val_f1: 0.9946
Epoch 26/300
 - 0s - loss: 0.0073 - val_loss: 0.0074
 - val_f1: 0.9943
Epoch 27/300
 - 0s - loss: 0.0068 - val_loss: 0.0073
 - val_f1: 0.9948
Epoch 28/300
 - 0s - loss: 0.0063 - val_loss: 0.0075
 - val_f1: 0.9949
Epoch 29/300
 - 0s - loss: 0.0063 - val_loss: 0.0071
 - val_f1: 0.9950
Epoch 30/300
 - 0s - loss: 0.0069 - val_loss: 0.0074
 - val_f1: 0.9950
Epoch 31/300
 - 0s - loss: 0.0064 - val_loss: 0.0074
2019-12-23 15:46:39,132 [INFO] epoch = 30. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/ann_model_epoch_30.pickle
 - val_f1: 0.9948
Epoch 32/300
 - 0s - loss: 0.0062 - val_loss: 0.0077
 - val_f1: 0.9942
Epoch 33/300
 - 0s - loss: 0.0064 - val_loss: 0.0080
 - val_f1: 0.9940
Epoch 34/300
 - 0s - loss: 0.0060 - val_loss: 0.0076
 - val_f1: 0.9942
Epoch 35/300
 - 0s - loss: 0.0063 - val_loss: 0.0076
 - val_f1: 0.9948
Epoch 36/300
 - 0s - loss: 0.0063 - val_loss: 0.0077
 - val_f1: 0.9949
Epoch 37/300
 - 0s - loss: 0.0058 - val_loss: 0.0070
 - val_f1: 0.9955
Epoch 38/300
 - 0s - loss: 0.0059 - val_loss: 0.0076
 - val_f1: 0.9946
Epoch 39/300
 - 0s - loss: 0.0056 - val_loss: 0.0070
 - val_f1: 0.9953
Epoch 40/300
 - 0s - loss: 0.0060 - val_loss: 0.0077
 - val_f1: 0.9942
Epoch 41/300
 - 0s - loss: 0.0058 - val_loss: 0.0075
 - val_f1: 0.9949
Epoch 42/300
 - 0s - loss: 0.0057 - val_loss: 0.0074
 - val_f1: 0.9954
Epoch 43/300
 - 0s - loss: 0.0057 - val_loss: 0.0070
 - val_f1: 0.9957
Epoch 44/300
 - 0s - loss: 0.0053 - val_loss: 0.0079
 - val_f1: 0.9950
Epoch 45/300
 - 0s - loss: 0.0058 - val_loss: 0.0069
 - val_f1: 0.9958
Epoch 46/300
 - 0s - loss: 0.0054 - val_loss: 0.0065
 - val_f1: 0.9955
Epoch 47/300
 - 0s - loss: 0.0052 - val_loss: 0.0068
 - val_f1: 0.9952
Epoch 48/300
 - 0s - loss: 0.0051 - val_loss: 0.0070
 - val_f1: 0.9951
Epoch 49/300
 - 0s - loss: 0.0055 - val_loss: 0.0068
 - val_f1: 0.9950
Epoch 50/300
 - 0s - loss: 0.0050 - val_loss: 0.0068
 - val_f1: 0.9953
Epoch 51/300
 - 0s - loss: 0.0051 - val_loss: 0.0067
 - val_f1: 0.9958
Epoch 52/300
 - 0s - loss: 0.0052 - val_loss: 0.0075
 - val_f1: 0.9947
Epoch 53/300
 - 0s - loss: 0.0051 - val_loss: 0.0066
 - val_f1: 0.9957
Epoch 54/300
 - 0s - loss: 0.0052 - val_loss: 0.0068
 - val_f1: 0.9953
Epoch 55/300
 - 0s - loss: 0.0051 - val_loss: 0.0068
 - val_f1: 0.9951
Epoch 56/300
 - 0s - loss: 0.0052 - val_loss: 0.0070
 - val_f1: 0.9953
Epoch 57/300
 - 0s - loss: 0.0052 - val_loss: 0.0076
 - val_f1: 0.9947
Epoch 58/300
 - 0s - loss: 0.0052 - val_loss: 0.0071
 - val_f1: 0.9950
Epoch 59/300
 - 0s - loss: 0.0051 - val_loss: 0.0069
 - val_f1: 0.9951
Epoch 60/300
 - 0s - loss: 0.0051 - val_loss: 0.0069
 - val_f1: 0.9954
Epoch 61/300
 - 0s - loss: 0.0049 - val_loss: 0.0066
2019-12-23 15:46:54,985 [INFO] epoch = 60. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/ann_model_epoch_60.pickle
 - val_f1: 0.9955
Epoch 62/300
 - 0s - loss: 0.0047 - val_loss: 0.0070
 - val_f1: 0.9957
Epoch 63/300
 - 0s - loss: 0.0047 - val_loss: 0.0067
 - val_f1: 0.9957
Epoch 64/300
 - 0s - loss: 0.0049 - val_loss: 0.0066
 - val_f1: 0.9960
Epoch 65/300
 - 0s - loss: 0.0047 - val_loss: 0.0075
 - val_f1: 0.9951
Epoch 66/300
 - 0s - loss: 0.0051 - val_loss: 0.0071
 - val_f1: 0.9950
Epoch 67/300
 - 0s - loss: 0.0049 - val_loss: 0.0066
 - val_f1: 0.9951
Epoch 68/300
 - 0s - loss: 0.0046 - val_loss: 0.0069
 - val_f1: 0.9959
Epoch 69/300
 - 0s - loss: 0.0046 - val_loss: 0.0068
 - val_f1: 0.9956
Epoch 70/300
 - 0s - loss: 0.0045 - val_loss: 0.0071
 - val_f1: 0.9953
Epoch 71/300
 - 0s - loss: 0.0047 - val_loss: 0.0069
 - val_f1: 0.9951
Epoch 72/300
 - 0s - loss: 0.0047 - val_loss: 0.0065
 - val_f1: 0.9956
Epoch 73/300
 - 0s - loss: 0.0047 - val_loss: 0.0066
 - val_f1: 0.9957
Epoch 74/300
 - 0s - loss: 0.0045 - val_loss: 0.0074
 - val_f1: 0.9949
Epoch 75/300
 - 0s - loss: 0.0045 - val_loss: 0.0067
 - val_f1: 0.9957
Epoch 76/300
 - 0s - loss: 0.0044 - val_loss: 0.0064
 - val_f1: 0.9956
Epoch 77/300
 - 0s - loss: 0.0044 - val_loss: 0.0065
 - val_f1: 0.9952
Epoch 78/300
 - 0s - loss: 0.0045 - val_loss: 0.0060
 - val_f1: 0.9961
Epoch 79/300
 - 0s - loss: 0.0046 - val_loss: 0.0065
 - val_f1: 0.9955
Epoch 80/300
 - 0s - loss: 0.0045 - val_loss: 0.0066
 - val_f1: 0.9956
Epoch 81/300
 - 0s - loss: 0.0045 - val_loss: 0.0072
 - val_f1: 0.9953
Epoch 82/300
 - 0s - loss: 0.0043 - val_loss: 0.0067
 - val_f1: 0.9959
Epoch 83/300
 - 0s - loss: 0.0044 - val_loss: 0.0071
 - val_f1: 0.9953
Epoch 84/300
 - 0s - loss: 0.0043 - val_loss: 0.0072
 - val_f1: 0.9953
Epoch 85/300
 - 0s - loss: 0.0044 - val_loss: 0.0067
 - val_f1: 0.9951
Epoch 86/300
 - 0s - loss: 0.0046 - val_loss: 0.0063
 - val_f1: 0.9958
Epoch 87/300
 - 0s - loss: 0.0048 - val_loss: 0.0070
 - val_f1: 0.9952
Epoch 88/300
 - 0s - loss: 0.0044 - val_loss: 0.0068
 - val_f1: 0.9962
Epoch 89/300
 - 0s - loss: 0.0045 - val_loss: 0.0066
 - val_f1: 0.9954
Epoch 90/300
 - 0s - loss: 0.0042 - val_loss: 0.0068
 - val_f1: 0.9957
Epoch 91/300
 - 0s - loss: 0.0041 - val_loss: 0.0072
2019-12-23 15:47:10,802 [INFO] epoch = 90. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/ann_model_epoch_90.pickle
 - val_f1: 0.9953
Epoch 92/300
 - 0s - loss: 0.0044 - val_loss: 0.0069
 - val_f1: 0.9954
Epoch 93/300
 - 0s - loss: 0.0043 - val_loss: 0.0077
 - val_f1: 0.9948
Epoch 94/300
 - 0s - loss: 0.0041 - val_loss: 0.0064
 - val_f1: 0.9961
Epoch 95/300
 - 0s - loss: 0.0044 - val_loss: 0.0066
 - val_f1: 0.9960
Epoch 96/300
 - 0s - loss: 0.0041 - val_loss: 0.0067
 - val_f1: 0.9961
Epoch 97/300
 - 0s - loss: 0.0043 - val_loss: 0.0069
 - val_f1: 0.9954
Epoch 98/300
 - 0s - loss: 0.0042 - val_loss: 0.0070
 - val_f1: 0.9959
Epoch 99/300
 - 0s - loss: 0.0039 - val_loss: 0.0071
 - val_f1: 0.9949
Epoch 100/300
 - 0s - loss: 0.0041 - val_loss: 0.0067
 - val_f1: 0.9963
Epoch 101/300
 - 0s - loss: 0.0043 - val_loss: 0.0066
 - val_f1: 0.9961
Epoch 102/300
 - 0s - loss: 0.0041 - val_loss: 0.0065
 - val_f1: 0.9956
Epoch 103/300
 - 0s - loss: 0.0041 - val_loss: 0.0061
 - val_f1: 0.9962
Epoch 104/300
 - 0s - loss: 0.0038 - val_loss: 0.0065
 - val_f1: 0.9962
Epoch 105/300
 - 0s - loss: 0.0040 - val_loss: 0.0071
 - val_f1: 0.9955
Epoch 106/300
 - 0s - loss: 0.0040 - val_loss: 0.0066
 - val_f1: 0.9957
Epoch 107/300
 - 0s - loss: 0.0041 - val_loss: 0.0074
 - val_f1: 0.9953
Epoch 108/300
 - 0s - loss: 0.0041 - val_loss: 0.0064
 - val_f1: 0.9957
Epoch 109/300
 - 0s - loss: 0.0039 - val_loss: 0.0064
 - val_f1: 0.9959
Epoch 110/300
 - 0s - loss: 0.0043 - val_loss: 0.0066
 - val_f1: 0.9961
Epoch 111/300
 - 0s - loss: 0.0039 - val_loss: 0.0063
 - val_f1: 0.9962
Epoch 112/300
 - 0s - loss: 0.0041 - val_loss: 0.0065
 - val_f1: 0.9956
Epoch 113/300
 - 0s - loss: 0.0038 - val_loss: 0.0081
 - val_f1: 0.9951
Epoch 114/300
 - 0s - loss: 0.0039 - val_loss: 0.0068
 - val_f1: 0.9957
Epoch 115/300
 - 0s - loss: 0.0042 - val_loss: 0.0067
 - val_f1: 0.9960
Epoch 116/300
 - 0s - loss: 0.0040 - val_loss: 0.0067
 - val_f1: 0.9961
Epoch 117/300
 - 0s - loss: 0.0038 - val_loss: 0.0066
 - val_f1: 0.9961
Epoch 118/300
 - 0s - loss: 0.0039 - val_loss: 0.0062
 - val_f1: 0.9962
Epoch 119/300
 - 0s - loss: 0.0040 - val_loss: 0.0070
 - val_f1: 0.9955
Epoch 120/300
 - 0s - loss: 0.0038 - val_loss: 0.0063
 - val_f1: 0.9964
Epoch 121/300
 - 0s - loss: 0.0037 - val_loss: 0.0066
2019-12-23 15:47:26,638 [INFO] epoch = 120. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/ann_model_epoch_120.pickle
 - val_f1: 0.9960
Epoch 122/300
 - 0s - loss: 0.0036 - val_loss: 0.0067
 - val_f1: 0.9957
Epoch 123/300
 - 0s - loss: 0.0038 - val_loss: 0.0068
 - val_f1: 0.9957
Epoch 124/300
 - 0s - loss: 0.0042 - val_loss: 0.0064
 - val_f1: 0.9961
Epoch 125/300
 - 0s - loss: 0.0037 - val_loss: 0.0065
 - val_f1: 0.9961
Epoch 126/300
 - 0s - loss: 0.0038 - val_loss: 0.0073
 - val_f1: 0.9953
Epoch 127/300
 - 0s - loss: 0.0038 - val_loss: 0.0067
 - val_f1: 0.9961
Epoch 128/300
 - 0s - loss: 0.0037 - val_loss: 0.0067
2019-12-23 15:47:30,516 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-23 15:47:31,352 [INFO] Last epoch loss evaluation: train_loss = 0.002860, val_loss = 0.006023
2019-12-23 15:47:31,355 [INFO] Training complete. time_to_train = 229.56 sec, 3.83 min
2019-12-23 15:47:31,362 [INFO] Model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/best_model.pickle
2019-12-23 15:47:31,552 [INFO] Plot saved to: results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/training_error_history.png
2019-12-23 15:47:31,716 [INFO] Plot saved to: results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/training_f1_history.png
2019-12-23 15:47:31,716 [INFO] Making predictions on training, validation, testing data
2019-12-23 15:47:34,137 [INFO] Evaluating predictions (results)
2019-12-23 15:47:34,491 [INFO] Dataset: Testing. Classification report below
2019-12-23 15:47:34,491 [INFO] 
              precision    recall  f1-score   support

         dos       0.94      0.85      0.89      7458
      normal       0.70      0.97      0.81      9711
       probe       0.82      0.70      0.75      2421
         r2l       0.92      0.11      0.20      2421
         u2r       0.55      0.01      0.02       533

   micro avg       0.79      0.79      0.79     22544
   macro avg       0.78      0.53      0.54     22544
weighted avg       0.81      0.79      0.75     22544

2019-12-23 15:47:34,491 [INFO] Overall accuracy (micro avg): 0.7856192334989354
2019-12-23 15:47:34,790 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.7856         0.7856                       0.7856                0.0536                   0.2144  0.7856
1     Macro avg        0.9142         0.7837                       0.5276                0.0720                   0.4724  0.5361
2  Weighted avg        0.8790         0.8117                       0.7856                0.1458                   0.2144  0.7484
2019-12-23 15:47:35,125 [INFO] Dataset: Validation. Classification report below
2019-12-23 15:47:35,125 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00      9186
      normal       1.00      1.00      1.00     13469
       probe       0.99      0.99      0.99      2331
         r2l       0.91      0.87      0.89       199
         u2r       0.57      0.40      0.47        10

   micro avg       1.00      1.00      1.00     25195
   macro avg       0.89      0.85      0.87     25195
weighted avg       1.00      1.00      1.00     25195

2019-12-23 15:47:35,125 [INFO] Overall accuracy (micro avg): 0.9961103393530463
2019-12-23 15:47:35,481 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9961         0.9961                       0.9961                0.0010                   0.0039  0.9961
1     Macro avg        0.9984         0.8941                       0.8510                0.0013                   0.1490  0.8694
2  Weighted avg        0.9978         0.9960                       0.9961                0.0028                   0.0039  0.9961
2019-12-23 15:47:36,925 [INFO] Dataset: Training. Classification report below
2019-12-23 15:47:36,925 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00     36741
      normal       1.00      1.00      1.00     53874
       probe       1.00      0.99      0.99      9325
         r2l       0.90      0.89      0.90       796
         u2r       0.92      0.55      0.69        42

   micro avg       1.00      1.00      1.00    100778
   macro avg       0.96      0.89      0.91    100778
weighted avg       1.00      1.00      1.00    100778

2019-12-23 15:47:36,925 [INFO] Overall accuracy (micro avg): 0.9964972513842307
2019-12-23 15:47:38,546 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9965         0.9965                       0.9965                0.0009                   0.0035  0.9965
1     Macro avg        0.9986         0.9631                       0.8857                0.0012                   0.1143  0.9150
2  Weighted avg        0.9979         0.9965                       0.9965                0.0026                   0.0035  0.9965
2019-12-23 15:47:38,583 [INFO] Results saved to: results_additional_exps/semi_sup_perf_nsl_ae_ann_rep1/semi_sup_perf_nsl_ae_ann_rep1_results.xlsx
2019-12-23 15:47:38,583 [INFO] ================= Finished running experiment no. 1 ================= 

2019-12-23 15:47:38,587 [INFO] Created directory: results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2
2019-12-23 15:47:38,587 [INFO] Initialized logging. log_filename = results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2/run_log.log
2019-12-23 15:47:38,587 [INFO] ================= Running experiment no. 2  ================= 

2019-12-23 15:47:38,587 [INFO] Experiment parameters given below
2019-12-23 15:47:38,587 [INFO] 
{'experiment_num': 2, 'results_dir': 'results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2', 'model_type': 'classifier', 'model': 'ae_ann', 'normal_label': 'normal', 'split_random_seed': 42, 'unsupervised_ratio': 0.5, 'ae_encoder_units': [64], 'ae_encoder_activations': ['relu'], 'ae_encoder_dropout_rates': [0.2], 'ae_encoder_l1_param': -1, 'ae_decoder_units': '', 'ae_decoder_activations': '', 'ae_decoder_dropout_rates': '', 'output_activation': 'sigmoid', 'loss_function': 'binary_crossentropy', 'ae_epochs': 200, 'ann_layer_units': [64], 'ann_layer_activations': ['relu'], 'ann_layer_dropout_rates': [0.2], 'ann_epochs': 300, 'early_stop_patience': 50, 'batch_size': 256, 'dataset_dir': '../Datasets/small_datasets/nsl_kdd_five_classes', 'description': 'semi_sup_perf_nsl_ae_ann_rep2'}
2019-12-23 15:47:38,587 [INFO] Created tensorboard log directory: results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2/tf_logs_run_2019_12_23-15_47_38
2019-12-23 15:47:38,587 [INFO] Loading datsets from: ../Datasets/small_datasets/nsl_kdd_five_classes
2019-12-23 15:47:38,588 [INFO] Reading X, y files
2019-12-23 15:47:38,588 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_train.h5
2019-12-23 15:47:38,834 [INFO] Reading complete. time_to_read=0.25 seconds
2019-12-23 15:47:38,834 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_val.h5
2019-12-23 15:47:38,900 [INFO] Reading complete. time_to_read=0.07 seconds
2019-12-23 15:47:38,900 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_test.h5
2019-12-23 15:47:38,959 [INFO] Reading complete. time_to_read=0.06 seconds
2019-12-23 15:47:38,959 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_train.h5
2019-12-23 15:47:38,966 [INFO] Reading complete. time_to_read=0.01 seconds
2019-12-23 15:47:38,966 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_val.h5
2019-12-23 15:47:38,970 [INFO] Reading complete. time_to_read=0.00 seconds
2019-12-23 15:47:38,970 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_test.h5
2019-12-23 15:47:38,974 [INFO] Reading complete. time_to_read=0.00 seconds
2019-12-23 15:47:39,161 [INFO] Initializing model
2019-12-23 15:47:39,266 [INFO] _________________________________________________________________
2019-12-23 15:47:39,266 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-23 15:47:39,266 [INFO] =================================================================
2019-12-23 15:47:39,266 [INFO] dense_5 (Dense)              (None, 64)                7872      
2019-12-23 15:47:39,266 [INFO] _________________________________________________________________
2019-12-23 15:47:39,266 [INFO] batch_normalization_3 (Batch (None, 64)                256       
2019-12-23 15:47:39,266 [INFO] _________________________________________________________________
2019-12-23 15:47:39,267 [INFO] dropout_3 (Dropout)          (None, 64)                0         
2019-12-23 15:47:39,267 [INFO] _________________________________________________________________
2019-12-23 15:47:39,267 [INFO] dense_6 (Dense)              (None, 122)               7930      
2019-12-23 15:47:39,267 [INFO] =================================================================
2019-12-23 15:47:39,267 [INFO] Total params: 16,058
2019-12-23 15:47:39,267 [INFO] Trainable params: 15,930
2019-12-23 15:47:39,267 [INFO] Non-trainable params: 128
2019-12-23 15:47:39,267 [INFO] _________________________________________________________________
2019-12-23 15:47:39,371 [INFO] _________________________________________________________________
2019-12-23 15:47:39,371 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-23 15:47:39,371 [INFO] =================================================================
2019-12-23 15:47:39,371 [INFO] dense_7 (Dense)              (None, 64)                4160      
2019-12-23 15:47:39,371 [INFO] _________________________________________________________________
2019-12-23 15:47:39,372 [INFO] batch_normalization_4 (Batch (None, 64)                256       
2019-12-23 15:47:39,372 [INFO] _________________________________________________________________
2019-12-23 15:47:39,372 [INFO] dropout_4 (Dropout)          (None, 64)                0         
2019-12-23 15:47:39,372 [INFO] _________________________________________________________________
2019-12-23 15:47:39,372 [INFO] dense_8 (Dense)              (None, 5)                 325       
2019-12-23 15:47:39,372 [INFO] =================================================================
2019-12-23 15:47:39,372 [INFO] Total params: 4,741
2019-12-23 15:47:39,372 [INFO] Trainable params: 4,613
2019-12-23 15:47:39,372 [INFO] Non-trainable params: 128
2019-12-23 15:47:39,372 [INFO] _________________________________________________________________
2019-12-23 15:47:39,372 [INFO] Training model
2019-12-23 15:47:39,372 [INFO] Splitting train set into 2 sets (unsupervised, supervised), random_seed = 42
2019-12-23 15:47:40,050 [INFO] Split sizes (instances). total = 100778, unsupervised = 50389, supervised = 50389, unsupervised dataset hash = 06e3c0571318a4e80f9239b32a18cc0a10c17120
2019-12-23 15:47:40,050 [INFO] Training autoencoder
 - val_f1: 0.9961
Epoch 00128: early stopping
Train on 50389 samples, validate on 25195 samples
Epoch 1/200
 - 1s - loss: 0.0536 - val_loss: -9.1078e-01
Epoch 2/200
 - 1s - loss: -1.4970e+00 - val_loss: -2.1598e+00
Epoch 3/200
 - 1s - loss: -2.4276e+00 - val_loss: -2.7656e+00
Epoch 4/200
 - 1s - loss: -2.8469e+00 - val_loss: -3.0204e+00
Epoch 5/200
 - 1s - loss: -3.0187e+00 - val_loss: -3.1164e+00
Epoch 6/200
 - 1s - loss: -3.0912e+00 - val_loss: -3.1591e+00
Epoch 7/200
 - 1s - loss: -3.1304e+00 - val_loss: -3.1817e+00
Epoch 8/200
 - 1s - loss: -3.1550e+00 - val_loss: -3.1960e+00
Epoch 9/200
 - 1s - loss: -3.1699e+00 - val_loss: -3.2049e+00
Epoch 10/200
 - 1s - loss: -3.1816e+00 - val_loss: -3.2104e+00
Epoch 11/200
 - 1s - loss: -3.1891e+00 - val_loss: -3.2154e+00
Epoch 12/200
 - 1s - loss: -3.1966e+00 - val_loss: -3.2190e+00
Epoch 13/200
 - 1s - loss: -3.2012e+00 - val_loss: -3.2221e+00
Epoch 14/200
 - 1s - loss: -3.2060e+00 - val_loss: -3.2242e+00
Epoch 15/200
 - 1s - loss: -3.2100e+00 - val_loss: -3.2256e+00
Epoch 16/200
 - 1s - loss: -3.2133e+00 - val_loss: -3.2274e+00
Epoch 17/200
 - 1s - loss: -3.2167e+00 - val_loss: -3.2287e+00
Epoch 18/200
 - 1s - loss: -3.2182e+00 - val_loss: -3.2300e+00
Epoch 19/200
 - 1s - loss: -3.2213e+00 - val_loss: -3.2313e+00
Epoch 20/200
 - 1s - loss: -3.2230e+00 - val_loss: -3.2319e+00
Epoch 21/200
 - 1s - loss: -3.2246e+00 - val_loss: -3.2330e+00
2019-12-23 15:47:57,693 [INFO] epoch = 20. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2/ae_model_epoch_20.pickle
Epoch 22/200
 - 1s - loss: -3.2266e+00 - val_loss: -3.2337e+00
Epoch 23/200
 - 1s - loss: -3.2269e+00 - val_loss: -3.2343e+00
Epoch 24/200
 - 1s - loss: -3.2300e+00 - val_loss: -3.2352e+00
Epoch 25/200
 - 1s - loss: -3.2311e+00 - val_loss: -3.2360e+00
Epoch 26/200
 - 1s - loss: -3.2321e+00 - val_loss: -3.2358e+00
Epoch 27/200
 - 1s - loss: -3.2333e+00 - val_loss: -3.2365e+00
Epoch 28/200
 - 1s - loss: -3.2346e+00 - val_loss: -3.2369e+00
Epoch 29/200
 - 1s - loss: -3.2349e+00 - val_loss: -3.2374e+00
Epoch 30/200
 - 1s - loss: -3.2363e+00 - val_loss: -3.2378e+00
Epoch 31/200
 - 1s - loss: -3.2370e+00 - val_loss: -3.2383e+00
Epoch 32/200
 - 1s - loss: -3.2379e+00 - val_loss: -3.2384e+00
Epoch 33/200
 - 1s - loss: -3.2386e+00 - val_loss: -3.2385e+00
Epoch 34/200
 - 1s - loss: -3.2399e+00 - val_loss: -3.2389e+00
Epoch 35/200
 - 1s - loss: -3.2406e+00 - val_loss: -3.2393e+00
Epoch 36/200
 - 1s - loss: -3.2410e+00 - val_loss: -3.2391e+00
Epoch 37/200
 - 1s - loss: -3.2412e+00 - val_loss: -3.2396e+00
Epoch 38/200
 - 1s - loss: -3.2419e+00 - val_loss: -3.2398e+00
Epoch 39/200
 - 1s - loss: -3.2427e+00 - val_loss: -3.2401e+00
Epoch 40/200
 - 1s - loss: -3.2429e+00 - val_loss: -3.2401e+00
Epoch 41/200
 - 1s - loss: -3.2436e+00 - val_loss: -3.2402e+00
2019-12-23 15:48:13,317 [INFO] epoch = 40. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2/ae_model_epoch_40.pickle
Epoch 42/200
 - 1s - loss: -3.2443e+00 - val_loss: -3.2399e+00
Epoch 43/200
 - 1s - loss: -3.2451e+00 - val_loss: -3.2404e+00
Epoch 44/200
 - 1s - loss: -3.2454e+00 - val_loss: -3.2406e+00
Epoch 45/200
 - 1s - loss: -3.2452e+00 - val_loss: -3.2409e+00
Epoch 46/200
 - 1s - loss: -3.2454e+00 - val_loss: -3.2411e+00
Epoch 47/200
 - 1s - loss: -3.2462e+00 - val_loss: -3.2412e+00
Epoch 48/200
 - 1s - loss: -3.2460e+00 - val_loss: -3.2418e+00
Epoch 49/200
 - 1s - loss: -3.2469e+00 - val_loss: -3.2416e+00
Epoch 50/200
 - 1s - loss: -3.2475e+00 - val_loss: -3.2415e+00
Epoch 51/200
 - 1s - loss: -3.2478e+00 - val_loss: -3.2413e+00
Epoch 52/200
 - 1s - loss: -3.2479e+00 - val_loss: -3.2420e+00
Epoch 53/200
 - 1s - loss: -3.2477e+00 - val_loss: -3.2418e+00
Epoch 54/200
 - 1s - loss: -3.2480e+00 - val_loss: -3.2419e+00
Epoch 55/200
 - 1s - loss: -3.2486e+00 - val_loss: -3.2422e+00
Epoch 56/200
 - 1s - loss: -3.2490e+00 - val_loss: -3.2422e+00
Epoch 57/200
 - 1s - loss: -3.2493e+00 - val_loss: -3.2420e+00
Epoch 58/200
 - 1s - loss: -3.2496e+00 - val_loss: -3.2424e+00
Epoch 59/200
 - 1s - loss: -3.2492e+00 - val_loss: -3.2423e+00
Epoch 60/200
 - 1s - loss: -3.2500e+00 - val_loss: -3.2428e+00
Epoch 61/200
 - 1s - loss: -3.2497e+00 - val_loss: -3.2426e+00
2019-12-23 15:48:28,939 [INFO] epoch = 60. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2/ae_model_epoch_60.pickle
Epoch 62/200
 - 1s - loss: -3.2501e+00 - val_loss: -3.2429e+00
Epoch 63/200
 - 1s - loss: -3.2491e+00 - val_loss: -3.2428e+00
Epoch 64/200
 - 1s - loss: -3.2500e+00 - val_loss: -3.2430e+00
Epoch 65/200
 - 1s - loss: -3.2507e+00 - val_loss: -3.2430e+00
Epoch 66/200
 - 1s - loss: -3.2504e+00 - val_loss: -3.2429e+00
Epoch 67/200
 - 1s - loss: -3.2507e+00 - val_loss: -3.2432e+00
Epoch 68/200
 - 1s - loss: -3.2514e+00 - val_loss: -3.2434e+00
Epoch 69/200
 - 1s - loss: -3.2514e+00 - val_loss: -3.2430e+00
Epoch 70/200
 - 1s - loss: -3.2520e+00 - val_loss: -3.2435e+00
Epoch 71/200
 - 1s - loss: -3.2518e+00 - val_loss: -3.2435e+00
Epoch 72/200
 - 1s - loss: -3.2518e+00 - val_loss: -3.2437e+00
Epoch 73/200
 - 1s - loss: -3.2522e+00 - val_loss: -3.2437e+00
Epoch 74/200
 - 1s - loss: -3.2521e+00 - val_loss: -3.2436e+00
Epoch 75/200
 - 1s - loss: -3.2521e+00 - val_loss: -3.2437e+00
Epoch 76/200
 - 1s - loss: -3.2525e+00 - val_loss: -3.2437e+00
Epoch 77/200
 - 1s - loss: -3.2522e+00 - val_loss: -3.2439e+00
Epoch 78/200
 - 1s - loss: -3.2526e+00 - val_loss: -3.2439e+00
Epoch 79/200
 - 1s - loss: -3.2531e+00 - val_loss: -3.2440e+00
Epoch 80/200
 - 1s - loss: -3.2532e+00 - val_loss: -3.2439e+00
Epoch 81/200
 - 1s - loss: -3.2530e+00 - val_loss: -3.2438e+00
2019-12-23 15:48:44,619 [INFO] epoch = 80. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2/ae_model_epoch_80.pickle
Epoch 82/200
 - 1s - loss: -3.2518e+00 - val_loss: -3.2439e+00
Epoch 83/200
 - 1s - loss: -3.2532e+00 - val_loss: -3.2441e+00
Epoch 84/200
 - 1s - loss: -3.2539e+00 - val_loss: -3.2441e+00
Epoch 85/200
 - 1s - loss: -3.2533e+00 - val_loss: -3.2443e+00
Epoch 86/200
 - 1s - loss: -3.2538e+00 - val_loss: -3.2442e+00
Epoch 87/200
 - 1s - loss: -3.2540e+00 - val_loss: -3.2441e+00
Epoch 88/200
 - 1s - loss: -3.2540e+00 - val_loss: -3.2444e+00
Epoch 89/200
 - 1s - loss: -3.2541e+00 - val_loss: -3.2443e+00
Epoch 90/200
 - 1s - loss: -3.2542e+00 - val_loss: -3.2444e+00
Epoch 91/200
 - 1s - loss: -3.2543e+00 - val_loss: -3.2444e+00
Epoch 92/200
 - 1s - loss: -3.2543e+00 - val_loss: -3.2444e+00
Epoch 93/200
 - 1s - loss: -3.2535e+00 - val_loss: -3.2445e+00
Epoch 94/200
 - 1s - loss: -3.2543e+00 - val_loss: -3.2442e+00
Epoch 95/200
 - 1s - loss: -3.2544e+00 - val_loss: -3.2444e+00
Epoch 96/200
 - 1s - loss: -3.2543e+00 - val_loss: -3.2443e+00
Epoch 97/200
 - 1s - loss: -3.2541e+00 - val_loss: -3.2448e+00
Epoch 98/200
 - 1s - loss: -3.2547e+00 - val_loss: -3.2445e+00
Epoch 99/200
 - 1s - loss: -3.2553e+00 - val_loss: -3.2447e+00
Epoch 100/200
 - 1s - loss: -3.2552e+00 - val_loss: -3.2447e+00
Epoch 101/200
 - 1s - loss: -3.2541e+00 - val_loss: -3.2447e+00
2019-12-23 15:49:00,247 [INFO] epoch = 100. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2/ae_model_epoch_100.pickle
Epoch 102/200
 - 1s - loss: -3.2550e+00 - val_loss: -3.2450e+00
Epoch 103/200
 - 1s - loss: -3.2548e+00 - val_loss: -3.2450e+00
Epoch 104/200
 - 1s - loss: -3.2552e+00 - val_loss: -3.2451e+00
Epoch 105/200
 - 1s - loss: -3.2553e+00 - val_loss: -3.2450e+00
Epoch 106/200
 - 1s - loss: -3.2549e+00 - val_loss: -3.2451e+00
Epoch 107/200
 - 1s - loss: -3.2554e+00 - val_loss: -3.2450e+00
Epoch 108/200
 - 1s - loss: -3.2559e+00 - val_loss: -3.2449e+00
Epoch 109/200
 - 1s - loss: -3.2554e+00 - val_loss: -3.2448e+00
Epoch 110/200
 - 1s - loss: -3.2559e+00 - val_loss: -3.2451e+00
Epoch 111/200
 - 1s - loss: -3.2556e+00 - val_loss: -3.2453e+00
Epoch 112/200
 - 1s - loss: -3.2557e+00 - val_loss: -3.2450e+00
Epoch 113/200
 - 1s - loss: -3.2557e+00 - val_loss: -3.2451e+00
Epoch 114/200
 - 1s - loss: -3.2557e+00 - val_loss: -3.2451e+00
Epoch 115/200
 - 1s - loss: -3.2563e+00 - val_loss: -3.2453e+00
Epoch 116/200
 - 1s - loss: -3.2559e+00 - val_loss: -3.2453e+00
Epoch 117/200
 - 1s - loss: -3.2561e+00 - val_loss: -3.2454e+00
Epoch 118/200
 - 1s - loss: -3.2554e+00 - val_loss: -3.2453e+00
Epoch 119/200
 - 1s - loss: -3.2565e+00 - val_loss: -3.2452e+00
Epoch 120/200
 - 1s - loss: -3.2564e+00 - val_loss: -3.2455e+00
Epoch 121/200
 - 1s - loss: -3.2562e+00 - val_loss: -3.2453e+00
2019-12-23 15:49:15,930 [INFO] epoch = 120. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2/ae_model_epoch_120.pickle
Epoch 122/200
 - 1s - loss: -3.2565e+00 - val_loss: -3.2456e+00
Epoch 123/200
 - 1s - loss: -3.2563e+00 - val_loss: -3.2454e+00
Epoch 124/200
 - 1s - loss: -3.2567e+00 - val_loss: -3.2455e+00
Epoch 125/200
 - 1s - loss: -3.2565e+00 - val_loss: -3.2453e+00
Epoch 126/200
 - 1s - loss: -3.2567e+00 - val_loss: -3.2456e+00
Epoch 127/200
 - 1s - loss: -3.2565e+00 - val_loss: -3.2455e+00
Epoch 128/200
 - 1s - loss: -3.2569e+00 - val_loss: -3.2454e+00
Epoch 129/200
 - 1s - loss: -3.2561e+00 - val_loss: -3.2451e+00
Epoch 130/200
 - 1s - loss: -3.2564e+00 - val_loss: -3.2452e+00
Epoch 131/200
 - 1s - loss: -3.2568e+00 - val_loss: -3.2451e+00
Epoch 132/200
 - 1s - loss: -3.2566e+00 - val_loss: -3.2455e+00
Epoch 133/200
 - 1s - loss: -3.2569e+00 - val_loss: -3.2452e+00
Epoch 134/200
 - 1s - loss: -3.2570e+00 - val_loss: -3.2450e+00
Epoch 135/200
 - 1s - loss: -3.2571e+00 - val_loss: -3.2453e+00
Epoch 136/200
 - 1s - loss: -3.2568e+00 - val_loss: -3.2454e+00
Epoch 137/200
 - 1s - loss: -3.2567e+00 - val_loss: -3.2456e+00
Epoch 138/200
 - 1s - loss: -3.2568e+00 - val_loss: -3.2452e+00
Epoch 139/200
 - 1s - loss: -3.2571e+00 - val_loss: -3.2455e+00
Epoch 140/200
 - 1s - loss: -3.2571e+00 - val_loss: -3.2457e+00
Epoch 141/200
 - 1s - loss: -3.2571e+00 - val_loss: -3.2458e+00
2019-12-23 15:49:31,587 [INFO] epoch = 140. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2/ae_model_epoch_140.pickle
Epoch 142/200
 - 1s - loss: -3.2575e+00 - val_loss: -3.2453e+00
Epoch 143/200
 - 1s - loss: -3.2571e+00 - val_loss: -3.2456e+00
Epoch 144/200
 - 1s - loss: -3.2577e+00 - val_loss: -3.2448e+00
Epoch 145/200
 - 1s - loss: -3.2577e+00 - val_loss: -3.2455e+00
Epoch 146/200
 - 1s - loss: -3.2576e+00 - val_loss: -3.2457e+00
Epoch 147/200
 - 1s - loss: -3.2573e+00 - val_loss: -3.2459e+00
Epoch 148/200
 - 1s - loss: -3.2572e+00 - val_loss: -3.2455e+00
Epoch 149/200
 - 1s - loss: -3.2572e+00 - val_loss: -3.2458e+00
Epoch 150/200
 - 1s - loss: -3.2575e+00 - val_loss: -3.2457e+00
Epoch 151/200
 - 1s - loss: -3.2578e+00 - val_loss: -3.2454e+00
Epoch 152/200
 - 1s - loss: -3.2571e+00 - val_loss: -3.2456e+00
Epoch 153/200
 - 1s - loss: -3.2571e+00 - val_loss: -3.2460e+00
Epoch 154/200
 - 1s - loss: -3.2577e+00 - val_loss: -3.2458e+00
Epoch 155/200
 - 1s - loss: -3.2576e+00 - val_loss: -3.2457e+00
Epoch 156/200
 - 1s - loss: -3.2579e+00 - val_loss: -3.2456e+00
Epoch 157/200
 - 1s - loss: -3.2577e+00 - val_loss: -3.2460e+00
Epoch 158/200
 - 1s - loss: -3.2583e+00 - val_loss: -3.2458e+00
Epoch 159/200
 - 1s - loss: -3.2576e+00 - val_loss: -3.2458e+00
Epoch 160/200
 - 1s - loss: -3.2575e+00 - val_loss: -3.2459e+00
Epoch 161/200
 - 1s - loss: -3.2578e+00 - val_loss: -3.2459e+00
2019-12-23 15:49:47,207 [INFO] epoch = 160. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2/ae_model_epoch_160.pickle
Epoch 162/200
 - 1s - loss: -3.2583e+00 - val_loss: -3.2458e+00
Epoch 163/200
 - 1s - loss: -3.2584e+00 - val_loss: -3.2459e+00
Epoch 164/200
 - 1s - loss: -3.2579e+00 - val_loss: -3.2459e+00
Epoch 165/200
 - 1s - loss: -3.2581e+00 - val_loss: -3.2457e+00
Epoch 166/200
 - 1s - loss: -3.2579e+00 - val_loss: -3.2460e+00
Epoch 167/200
 - 1s - loss: -3.2580e+00 - val_loss: -3.2456e+00
Epoch 168/200
 - 1s - loss: -3.2582e+00 - val_loss: -3.2458e+00
Epoch 169/200
 - 1s - loss: -3.2584e+00 - val_loss: -3.2460e+00
Epoch 170/200
 - 1s - loss: -3.2588e+00 - val_loss: -3.2459e+00
Epoch 171/200
 - 1s - loss: -3.2579e+00 - val_loss: -3.2457e+00
Epoch 172/200
 - 1s - loss: -3.2581e+00 - val_loss: -3.2460e+00
Epoch 173/200
 - 1s - loss: -3.2578e+00 - val_loss: -3.2459e+00
Epoch 174/200
 - 1s - loss: -3.2585e+00 - val_loss: -3.2459e+00
Epoch 175/200
 - 1s - loss: -3.2581e+00 - val_loss: -3.2463e+00
Epoch 176/200
 - 1s - loss: -3.2583e+00 - val_loss: -3.2462e+00
Epoch 177/200
 - 1s - loss: -3.2585e+00 - val_loss: -3.2462e+00
Epoch 178/200
 - 1s - loss: -3.2581e+00 - val_loss: -3.2461e+00
Epoch 179/200
 - 1s - loss: -3.2587e+00 - val_loss: -3.2460e+00
Epoch 180/200
 - 1s - loss: -3.2586e+00 - val_loss: -3.2461e+00
Epoch 181/200
 - 1s - loss: -3.2582e+00 - val_loss: -3.2461e+00
2019-12-23 15:50:02,822 [INFO] epoch = 180. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2/ae_model_epoch_180.pickle
Epoch 182/200
 - 1s - loss: -3.2580e+00 - val_loss: -3.2459e+00
Epoch 183/200
 - 1s - loss: -3.2587e+00 - val_loss: -3.2460e+00
Epoch 184/200
 - 1s - loss: -3.2580e+00 - val_loss: -3.2461e+00
Epoch 185/200
 - 1s - loss: -3.2585e+00 - val_loss: -3.2463e+00
Epoch 186/200
 - 1s - loss: -3.2587e+00 - val_loss: -3.2460e+00
Epoch 187/200
 - 1s - loss: -3.2589e+00 - val_loss: -3.2460e+00
Epoch 188/200
 - 1s - loss: -3.2591e+00 - val_loss: -3.2461e+00
Epoch 189/200
 - 1s - loss: -3.2587e+00 - val_loss: -3.2461e+00
Epoch 190/200
 - 1s - loss: -3.2587e+00 - val_loss: -3.2459e+00
Epoch 191/200
 - 1s - loss: -3.2589e+00 - val_loss: -3.2461e+00
Epoch 192/200
 - 1s - loss: -3.2586e+00 - val_loss: -3.2462e+00
Epoch 193/200
 - 1s - loss: -3.2589e+00 - val_loss: -3.2463e+00
Epoch 194/200
 - 1s - loss: -3.2582e+00 - val_loss: -3.2461e+00
Epoch 195/200
 - 1s - loss: -3.2587e+00 - val_loss: -3.2462e+00
Epoch 196/200
 - 1s - loss: -3.2592e+00 - val_loss: -3.2464e+00
Epoch 197/200
 - 1s - loss: -3.2588e+00 - val_loss: -3.2465e+00
Epoch 198/200
 - 1s - loss: -3.2590e+00 - val_loss: -3.2464e+00
Epoch 199/200
 - 1s - loss: -3.2592e+00 - val_loss: -3.2463e+00
Epoch 200/200
 - 1s - loss: -3.2589e+00 - val_loss: -3.2462e+00
2019-12-23 15:50:17,681 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-23 15:50:18,826 [INFO] Last epoch loss evaluation: train_loss = -3.272545, val_loss = -3.246453
2019-12-23 15:50:18,826 [INFO] Training autoencoder complete
2019-12-23 15:50:18,826 [INFO] Encoding data for supervised training
2019-12-23 15:50:19,565 [INFO] Encoding complete
2019-12-23 15:50:19,567 [INFO] Training neural network layers (after autoencoder)
Train on 50389 samples, validate on 25195 samples
Epoch 1/300
 - 1s - loss: 0.1358 - val_loss: 0.0377
 - val_f1: 0.9734
Epoch 2/300
 - 0s - loss: 0.0343 - val_loss: 0.0220
 - val_f1: 0.9818
Epoch 3/300
 - 0s - loss: 0.0215 - val_loss: 0.0148
 - val_f1: 0.9894
Epoch 4/300
 - 0s - loss: 0.0172 - val_loss: 0.0128
 - val_f1: 0.9901
Epoch 5/300
 - 0s - loss: 0.0151 - val_loss: 0.0116
 - val_f1: 0.9903
Epoch 6/300
 - 0s - loss: 0.0133 - val_loss: 0.0107
 - val_f1: 0.9915
Epoch 7/300
 - 0s - loss: 0.0127 - val_loss: 0.0100
 - val_f1: 0.9928
Epoch 8/300
 - 0s - loss: 0.0114 - val_loss: 0.0099
 - val_f1: 0.9924
Epoch 9/300
 - 0s - loss: 0.0108 - val_loss: 0.0093
 - val_f1: 0.9931
Epoch 10/300
 - 0s - loss: 0.0104 - val_loss: 0.0093
 - val_f1: 0.9931
Epoch 11/300
 - 0s - loss: 0.0100 - val_loss: 0.0094
 - val_f1: 0.9930
Epoch 12/300
 - 0s - loss: 0.0095 - val_loss: 0.0090
 - val_f1: 0.9929
Epoch 13/300
 - 0s - loss: 0.0089 - val_loss: 0.0086
 - val_f1: 0.9937
Epoch 14/300
 - 0s - loss: 0.0088 - val_loss: 0.0093
 - val_f1: 0.9929
Epoch 15/300
 - 0s - loss: 0.0082 - val_loss: 0.0085
 - val_f1: 0.9938
Epoch 16/300
 - 0s - loss: 0.0079 - val_loss: 0.0086
 - val_f1: 0.9939
Epoch 17/300
 - 0s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9940
Epoch 18/300
 - 0s - loss: 0.0077 - val_loss: 0.0082
 - val_f1: 0.9942
Epoch 19/300
 - 0s - loss: 0.0075 - val_loss: 0.0082
 - val_f1: 0.9946
Epoch 20/300
 - 0s - loss: 0.0075 - val_loss: 0.0081
 - val_f1: 0.9946
Epoch 21/300
 - 0s - loss: 0.0074 - val_loss: 0.0080
 - val_f1: 0.9945
Epoch 22/300
 - 0s - loss: 0.0072 - val_loss: 0.0078
 - val_f1: 0.9945
Epoch 23/300
 - 0s - loss: 0.0069 - val_loss: 0.0080
 - val_f1: 0.9940
Epoch 24/300
 - 0s - loss: 0.0069 - val_loss: 0.0076
 - val_f1: 0.9951
Epoch 25/300
 - 0s - loss: 0.0067 - val_loss: 0.0079
 - val_f1: 0.9946
Epoch 26/300
 - 0s - loss: 0.0067 - val_loss: 0.0076
 - val_f1: 0.9950
Epoch 27/300
 - 0s - loss: 0.0064 - val_loss: 0.0076
 - val_f1: 0.9948
Epoch 28/300
 - 0s - loss: 0.0065 - val_loss: 0.0080
 - val_f1: 0.9949
Epoch 29/300
 - 0s - loss: 0.0066 - val_loss: 0.0077
 - val_f1: 0.9944
Epoch 30/300
 - 0s - loss: 0.0059 - val_loss: 0.0080
 - val_f1: 0.9939
Epoch 31/300
 - 0s - loss: 0.0062 - val_loss: 0.0077
2019-12-23 15:50:38,394 [INFO] epoch = 30. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2/ann_model_epoch_30.pickle
 - val_f1: 0.9946
Epoch 32/300
 - 0s - loss: 0.0057 - val_loss: 0.0074
 - val_f1: 0.9952
Epoch 33/300
 - 0s - loss: 0.0058 - val_loss: 0.0076
 - val_f1: 0.9950
Epoch 34/300
 - 0s - loss: 0.0061 - val_loss: 0.0073
 - val_f1: 0.9949
Epoch 35/300
 - 0s - loss: 0.0055 - val_loss: 0.0076
 - val_f1: 0.9947
Epoch 36/300
 - 0s - loss: 0.0058 - val_loss: 0.0080
 - val_f1: 0.9945
Epoch 37/300
 - 0s - loss: 0.0057 - val_loss: 0.0071
 - val_f1: 0.9945
Epoch 38/300
 - 0s - loss: 0.0054 - val_loss: 0.0074
 - val_f1: 0.9948
Epoch 39/300
 - 0s - loss: 0.0058 - val_loss: 0.0072
 - val_f1: 0.9952
Epoch 40/300
 - 0s - loss: 0.0054 - val_loss: 0.0073
 - val_f1: 0.9951
Epoch 41/300
 - 0s - loss: 0.0053 - val_loss: 0.0072
 - val_f1: 0.9955
Epoch 42/300
 - 0s - loss: 0.0052 - val_loss: 0.0073
 - val_f1: 0.9955
Epoch 43/300
 - 0s - loss: 0.0053 - val_loss: 0.0072
 - val_f1: 0.9949
Epoch 44/300
 - 0s - loss: 0.0051 - val_loss: 0.0071
 - val_f1: 0.9957
Epoch 45/300
 - 0s - loss: 0.0054 - val_loss: 0.0073
 - val_f1: 0.9950
Epoch 46/300
 - 0s - loss: 0.0050 - val_loss: 0.0075
 - val_f1: 0.9952
Epoch 47/300
 - 0s - loss: 0.0051 - val_loss: 0.0075
 - val_f1: 0.9953
Epoch 48/300
 - 0s - loss: 0.0050 - val_loss: 0.0077
 - val_f1: 0.9946
Epoch 49/300
 - 0s - loss: 0.0052 - val_loss: 0.0072
 - val_f1: 0.9952
Epoch 50/300
 - 0s - loss: 0.0049 - val_loss: 0.0071
 - val_f1: 0.9957
Epoch 51/300
 - 0s - loss: 0.0048 - val_loss: 0.0075
 - val_f1: 0.9948
Epoch 52/300
 - 0s - loss: 0.0049 - val_loss: 0.0075
 - val_f1: 0.9945
Epoch 53/300
 - 0s - loss: 0.0050 - val_loss: 0.0073
 - val_f1: 0.9949
Epoch 54/300
 - 0s - loss: 0.0047 - val_loss: 0.0078
 - val_f1: 0.9943
Epoch 55/300
 - 0s - loss: 0.0049 - val_loss: 0.0076
 - val_f1: 0.9952
Epoch 56/300
 - 0s - loss: 0.0052 - val_loss: 0.0078
 - val_f1: 0.9943
Epoch 57/300
 - 0s - loss: 0.0046 - val_loss: 0.0075
 - val_f1: 0.9954
Epoch 58/300
 - 0s - loss: 0.0046 - val_loss: 0.0074
 - val_f1: 0.9951
Epoch 59/300
 - 0s - loss: 0.0049 - val_loss: 0.0071
 - val_f1: 0.9954
Epoch 60/300
 - 0s - loss: 0.0048 - val_loss: 0.0075
 - val_f1: 0.9955
Epoch 61/300
 - 0s - loss: 0.0047 - val_loss: 0.0077
2019-12-23 15:50:55,557 [INFO] epoch = 60. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2/ann_model_epoch_60.pickle
 - val_f1: 0.9943
Epoch 62/300
 - 0s - loss: 0.0048 - val_loss: 0.0074
 - val_f1: 0.9945
Epoch 63/300
 - 0s - loss: 0.0045 - val_loss: 0.0072
 - val_f1: 0.9955
Epoch 64/300
 - 0s - loss: 0.0042 - val_loss: 0.0083
 - val_f1: 0.9942
Epoch 65/300
 - 0s - loss: 0.0045 - val_loss: 0.0079
 - val_f1: 0.9945
Epoch 66/300
 - 0s - loss: 0.0045 - val_loss: 0.0075
 - val_f1: 0.9948
Epoch 67/300
 - 0s - loss: 0.0046 - val_loss: 0.0074
 - val_f1: 0.9950
Epoch 68/300
 - 0s - loss: 0.0049 - val_loss: 0.0077
 - val_f1: 0.9945
Epoch 69/300
 - 0s - loss: 0.0048 - val_loss: 0.0073
 - val_f1: 0.9956
Epoch 70/300
 - 0s - loss: 0.0045 - val_loss: 0.0072
 - val_f1: 0.9954
Epoch 71/300
 - 0s - loss: 0.0046 - val_loss: 0.0072
 - val_f1: 0.9950
Epoch 72/300
 - 0s - loss: 0.0044 - val_loss: 0.0076
 - val_f1: 0.9945
Epoch 73/300
 - 0s - loss: 0.0042 - val_loss: 0.0070
 - val_f1: 0.9956
Epoch 74/300
 - 0s - loss: 0.0044 - val_loss: 0.0077
 - val_f1: 0.9951
Epoch 75/300
 - 0s - loss: 0.0043 - val_loss: 0.0076
 - val_f1: 0.9948
Epoch 76/300
 - 0s - loss: 0.0043 - val_loss: 0.0073
 - val_f1: 0.9957
Epoch 77/300
 - 0s - loss: 0.0045 - val_loss: 0.0069
 - val_f1: 0.9957
Epoch 78/300
 - 0s - loss: 0.0042 - val_loss: 0.0074
 - val_f1: 0.9948
Epoch 79/300
 - 0s - loss: 0.0044 - val_loss: 0.0077
 - val_f1: 0.9952
Epoch 80/300
 - 0s - loss: 0.0042 - val_loss: 0.0080
 - val_f1: 0.9946
Epoch 81/300
 - 0s - loss: 0.0042 - val_loss: 0.0076
 - val_f1: 0.9952
Epoch 82/300
 - 0s - loss: 0.0042 - val_loss: 0.0075
 - val_f1: 0.9949
Epoch 83/300
 - 0s - loss: 0.0039 - val_loss: 0.0073
 - val_f1: 0.9954
Epoch 84/300
 - 0s - loss: 0.0041 - val_loss: 0.0072
 - val_f1: 0.9957
Epoch 85/300
 - 0s - loss: 0.0043 - val_loss: 0.0072
 - val_f1: 0.9949
Epoch 86/300
 - 0s - loss: 0.0042 - val_loss: 0.0075
 - val_f1: 0.9950
Epoch 87/300
 - 0s - loss: 0.0041 - val_loss: 0.0072
 - val_f1: 0.9954
Epoch 88/300
 - 0s - loss: 0.0042 - val_loss: 0.0070
 - val_f1: 0.9955
Epoch 89/300
 - 0s - loss: 0.0040 - val_loss: 0.0073
 - val_f1: 0.9950
Epoch 90/300
 - 0s - loss: 0.0040 - val_loss: 0.0073
 - val_f1: 0.9952
Epoch 91/300
 - 0s - loss: 0.0041 - val_loss: 0.0071
2019-12-23 15:51:12,620 [INFO] epoch = 90. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2/ann_model_epoch_90.pickle
 - val_f1: 0.9954
Epoch 92/300
 - 0s - loss: 0.0044 - val_loss: 0.0077
 - val_f1: 0.9947
Epoch 93/300
 - 0s - loss: 0.0041 - val_loss: 0.0074
 - val_f1: 0.9949
Epoch 94/300
 - 0s - loss: 0.0041 - val_loss: 0.0073
 - val_f1: 0.9956
Epoch 95/300
 - 0s - loss: 0.0041 - val_loss: 0.0068
 - val_f1: 0.9958
Epoch 96/300
 - 0s - loss: 0.0037 - val_loss: 0.0070
 - val_f1: 0.9955
Epoch 97/300
 - 0s - loss: 0.0037 - val_loss: 0.0073
 - val_f1: 0.9955
Epoch 98/300
 - 0s - loss: 0.0042 - val_loss: 0.0074
 - val_f1: 0.9953
Epoch 99/300
 - 0s - loss: 0.0042 - val_loss: 0.0079
 - val_f1: 0.9950
Epoch 100/300
 - 0s - loss: 0.0042 - val_loss: 0.0074
 - val_f1: 0.9949
Epoch 101/300
 - 0s - loss: 0.0043 - val_loss: 0.0069
 - val_f1: 0.9958
Epoch 102/300
 - 0s - loss: 0.0040 - val_loss: 0.0071
 - val_f1: 0.9959
Epoch 103/300
 - 0s - loss: 0.0038 - val_loss: 0.0070
 - val_f1: 0.9957
Epoch 104/300
 - 0s - loss: 0.0039 - val_loss: 0.0071
 - val_f1: 0.9952
Epoch 105/300
 - 0s - loss: 0.0039 - val_loss: 0.0069
 - val_f1: 0.9955
Epoch 106/300
 - 0s - loss: 0.0037 - val_loss: 0.0068
 - val_f1: 0.9957
Epoch 107/300
 - 0s - loss: 0.0038 - val_loss: 0.0070
 - val_f1: 0.9955
Epoch 108/300
 - 0s - loss: 0.0038 - val_loss: 0.0071
 - val_f1: 0.9954
Epoch 109/300
 - 0s - loss: 0.0038 - val_loss: 0.0072
 - val_f1: 0.9952
Epoch 110/300
 - 0s - loss: 0.0037 - val_loss: 0.0070
 - val_f1: 0.9955
Epoch 111/300
 - 0s - loss: 0.0039 - val_loss: 0.0074
 - val_f1: 0.9954
Epoch 112/300
 - 0s - loss: 0.0039 - val_loss: 0.0072
 - val_f1: 0.9952
Epoch 113/300
 - 0s - loss: 0.0040 - val_loss: 0.0070
 - val_f1: 0.9958
Epoch 114/300
 - 0s - loss: 0.0036 - val_loss: 0.0072
 - val_f1: 0.9959
Epoch 115/300
 - 0s - loss: 0.0041 - val_loss: 0.0072
 - val_f1: 0.9958
Epoch 116/300
 - 0s - loss: 0.0036 - val_loss: 0.0077
 - val_f1: 0.9956
Epoch 117/300
 - 0s - loss: 0.0037 - val_loss: 0.0073
 - val_f1: 0.9959
Epoch 118/300
 - 0s - loss: 0.0036 - val_loss: 0.0072
 - val_f1: 0.9959
Epoch 119/300
 - 0s - loss: 0.0038 - val_loss: 0.0071
 - val_f1: 0.9955
Epoch 120/300
 - 0s - loss: 0.0037 - val_loss: 0.0076
 - val_f1: 0.9956
Epoch 121/300
 - 0s - loss: 0.0037 - val_loss: 0.0073
2019-12-23 15:51:29,674 [INFO] epoch = 120. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2/ann_model_epoch_120.pickle
 - val_f1: 0.9957
Epoch 122/300
 - 0s - loss: 0.0038 - val_loss: 0.0076
 - val_f1: 0.9959
Epoch 123/300
 - 0s - loss: 0.0040 - val_loss: 0.0075
 - val_f1: 0.9958
Epoch 124/300
 - 0s - loss: 0.0035 - val_loss: 0.0071
 - val_f1: 0.9956
Epoch 125/300
 - 0s - loss: 0.0035 - val_loss: 0.0072
 - val_f1: 0.9956
Epoch 126/300
 - 0s - loss: 0.0036 - val_loss: 0.0078
 - val_f1: 0.9956
Epoch 127/300
 - 0s - loss: 0.0036 - val_loss: 0.0072
 - val_f1: 0.9952
Epoch 128/300
 - 0s - loss: 0.0035 - val_loss: 0.0072
 - val_f1: 0.9959
Epoch 129/300
 - 0s - loss: 0.0035 - val_loss: 0.0073
 - val_f1: 0.9959
Epoch 130/300
 - 0s - loss: 0.0037 - val_loss: 0.0078
 - val_f1: 0.9956
Epoch 131/300
 - 0s - loss: 0.0037 - val_loss: 0.0082
 - val_f1: 0.9951
Epoch 132/300
 - 0s - loss: 0.0039 - val_loss: 0.0078
 - val_f1: 0.9949
Epoch 133/300
 - 0s - loss: 0.0041 - val_loss: 0.0075
 - val_f1: 0.9954
Epoch 134/300
 - 0s - loss: 0.0039 - val_loss: 0.0072
 - val_f1: 0.9955
Epoch 135/300
 - 0s - loss: 0.0038 - val_loss: 0.0070
 - val_f1: 0.9952
Epoch 136/300
 - 0s - loss: 0.0037 - val_loss: 0.0072
 - val_f1: 0.9957
Epoch 137/300
 - 0s - loss: 0.0037 - val_loss: 0.0071
 - val_f1: 0.9957
Epoch 138/300
 - 0s - loss: 0.0037 - val_loss: 0.0071
 - val_f1: 0.9958
Epoch 139/300
 - 0s - loss: 0.0035 - val_loss: 0.0076
 - val_f1: 0.9955
Epoch 140/300
 - 0s - loss: 0.0038 - val_loss: 0.0073
 - val_f1: 0.9959
Epoch 141/300
 - 0s - loss: 0.0035 - val_loss: 0.0073
 - val_f1: 0.9962
Epoch 142/300
 - 0s - loss: 0.0036 - val_loss: 0.0072
 - val_f1: 0.9959
Epoch 143/300
 - 0s - loss: 0.0035 - val_loss: 0.0071
 - val_f1: 0.9960
Epoch 144/300
 - 0s - loss: 0.0038 - val_loss: 0.0074
 - val_f1: 0.9958
Epoch 145/300
 - 0s - loss: 0.0036 - val_loss: 0.0075
2019-12-23 15:51:43,512 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-23 15:51:44,456 [INFO] Last epoch loss evaluation: train_loss = 0.002593, val_loss = 0.006757
2019-12-23 15:51:44,460 [INFO] Training complete. time_to_train = 245.09 sec, 4.08 min
2019-12-23 15:51:44,468 [INFO] Model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2/best_model.pickle
2019-12-23 15:51:44,651 [INFO] Plot saved to: results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2/training_error_history.png
2019-12-23 15:51:44,819 [INFO] Plot saved to: results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2/training_f1_history.png
2019-12-23 15:51:44,819 [INFO] Making predictions on training, validation, testing data
2019-12-23 15:51:47,507 [INFO] Evaluating predictions (results)
2019-12-23 15:51:47,769 [INFO] Dataset: Testing. Classification report below
2019-12-23 15:51:47,769 [INFO] 
              precision    recall  f1-score   support

         dos       0.96      0.82      0.88      7458
      normal       0.68      0.97      0.80      9711
       probe       0.81      0.70      0.75      2421
         r2l       0.98      0.10      0.18      2421
         u2r       0.79      0.02      0.04       533

   micro avg       0.78      0.78      0.78     22544
   macro avg       0.84      0.52      0.53     22544
weighted avg       0.82      0.78      0.74     22544

2019-12-23 15:51:47,769 [INFO] Overall accuracy (micro avg): 0.7768807665010646
2019-12-23 15:51:48,066 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.7769         0.7769                       0.7769                0.0558                   0.2231  0.7769
1     Macro avg        0.9108         0.8421                       0.5224                0.0754                   0.4776  0.5318
2  Weighted avg        0.8720         0.8219                       0.7769                0.1540                   0.2231  0.7397
2019-12-23 15:51:48,400 [INFO] Dataset: Validation. Classification report below
2019-12-23 15:51:48,400 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00      9186
      normal       1.00      1.00      1.00     13469
       probe       0.99      0.99      0.99      2331
         r2l       0.90      0.86      0.88       199
         u2r       1.00      0.40      0.57        10

   micro avg       1.00      1.00      1.00     25195
   macro avg       0.98      0.85      0.89     25195
weighted avg       1.00      1.00      1.00     25195

2019-12-23 15:51:48,400 [INFO] Overall accuracy (micro avg): 0.9957928160349275
2019-12-23 15:51:48,758 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9958         0.9958                       0.9958                0.0011                   0.0042  0.9958
1     Macro avg        0.9983         0.9762                       0.8493                0.0014                   0.1507  0.8870
2  Weighted avg        0.9975         0.9958                       0.9958                0.0027                   0.0042  0.9957
2019-12-23 15:51:50,200 [INFO] Dataset: Training. Classification report below
2019-12-23 15:51:50,200 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00     36741
      normal       1.00      1.00      1.00     53874
       probe       1.00      0.99      0.99      9325
         r2l       0.90      0.89      0.89       796
         u2r       0.83      0.71      0.77        42

   micro avg       1.00      1.00      1.00    100778
   macro avg       0.94      0.92      0.93    100778
weighted avg       1.00      1.00      1.00    100778

2019-12-23 15:51:50,200 [INFO] Overall accuracy (micro avg): 0.9967353985989006
2019-12-23 15:51:51,821 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9967         0.9967                       0.9967                0.0008                   0.0033  0.9967
1     Macro avg        0.9987         0.9448                       0.9180                0.0011                   0.0820  0.9305
2  Weighted avg        0.9980         0.9967                       0.9967                0.0022                   0.0033  0.9967
2019-12-23 15:51:51,859 [INFO] Results saved to: results_additional_exps/semi_sup_perf_nsl_ae_ann_rep2/semi_sup_perf_nsl_ae_ann_rep2_results.xlsx
2019-12-23 15:51:51,860 [INFO] ================= Finished running experiment no. 2 ================= 

2019-12-23 15:51:51,863 [INFO] Created directory: results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3
2019-12-23 15:51:51,863 [INFO] Initialized logging. log_filename = results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/run_log.log
2019-12-23 15:51:51,863 [INFO] ================= Running experiment no. 3  ================= 

2019-12-23 15:51:51,863 [INFO] Experiment parameters given below
2019-12-23 15:51:51,863 [INFO] 
{'experiment_num': 3, 'results_dir': 'results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3', 'model_type': 'classifier', 'model': 'ae_ann', 'normal_label': 'normal', 'split_random_seed': 42, 'unsupervised_ratio': 0.5, 'ae_encoder_units': [64], 'ae_encoder_activations': ['relu'], 'ae_encoder_dropout_rates': [0.2], 'ae_encoder_l1_param': -1, 'ae_decoder_units': '', 'ae_decoder_activations': '', 'ae_decoder_dropout_rates': '', 'output_activation': 'sigmoid', 'loss_function': 'binary_crossentropy', 'ae_epochs': 200, 'ann_layer_units': [64], 'ann_layer_activations': ['relu'], 'ann_layer_dropout_rates': [0.2], 'ann_epochs': 300, 'early_stop_patience': 50, 'batch_size': 256, 'dataset_dir': '../Datasets/small_datasets/nsl_kdd_five_classes', 'description': 'semi_sup_perf_nsl_ae_ann_rep3'}
2019-12-23 15:51:51,863 [INFO] Created tensorboard log directory: results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/tf_logs_run_2019_12_23-15_51_51
2019-12-23 15:51:51,864 [INFO] Loading datsets from: ../Datasets/small_datasets/nsl_kdd_five_classes
2019-12-23 15:51:51,864 [INFO] Reading X, y files
2019-12-23 15:51:51,864 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_train.h5
2019-12-23 15:51:52,113 [INFO] Reading complete. time_to_read=0.25 seconds
2019-12-23 15:51:52,113 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_val.h5
2019-12-23 15:51:52,175 [INFO] Reading complete. time_to_read=0.06 seconds
2019-12-23 15:51:52,175 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/X_test.h5
2019-12-23 15:51:52,232 [INFO] Reading complete. time_to_read=0.06 seconds
2019-12-23 15:51:52,232 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_train.h5
2019-12-23 15:51:52,239 [INFO] Reading complete. time_to_read=0.01 seconds
2019-12-23 15:51:52,239 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_val.h5
2019-12-23 15:51:52,243 [INFO] Reading complete. time_to_read=0.00 seconds
2019-12-23 15:51:52,243 [INFO] Reading HDF dataset ../Datasets/small_datasets/nsl_kdd_five_classes/y_test.h5
2019-12-23 15:51:52,247 [INFO] Reading complete. time_to_read=0.00 seconds
2019-12-23 15:51:52,435 [INFO] Initializing model
2019-12-23 15:51:52,543 [INFO] _________________________________________________________________
2019-12-23 15:51:52,543 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-23 15:51:52,543 [INFO] =================================================================
2019-12-23 15:51:52,543 [INFO] dense_9 (Dense)              (None, 64)                7872      
2019-12-23 15:51:52,543 [INFO] _________________________________________________________________
2019-12-23 15:51:52,543 [INFO] batch_normalization_5 (Batch (None, 64)                256       
2019-12-23 15:51:52,543 [INFO] _________________________________________________________________
2019-12-23 15:51:52,544 [INFO] dropout_5 (Dropout)          (None, 64)                0         
2019-12-23 15:51:52,544 [INFO] _________________________________________________________________
2019-12-23 15:51:52,544 [INFO] dense_10 (Dense)             (None, 122)               7930      
2019-12-23 15:51:52,544 [INFO] =================================================================
2019-12-23 15:51:52,544 [INFO] Total params: 16,058
2019-12-23 15:51:52,544 [INFO] Trainable params: 15,930
2019-12-23 15:51:52,544 [INFO] Non-trainable params: 128
2019-12-23 15:51:52,544 [INFO] _________________________________________________________________
2019-12-23 15:51:52,650 [INFO] _________________________________________________________________
2019-12-23 15:51:52,650 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-23 15:51:52,650 [INFO] =================================================================
2019-12-23 15:51:52,650 [INFO] dense_11 (Dense)             (None, 64)                4160      
2019-12-23 15:51:52,650 [INFO] _________________________________________________________________
2019-12-23 15:51:52,651 [INFO] batch_normalization_6 (Batch (None, 64)                256       
2019-12-23 15:51:52,651 [INFO] _________________________________________________________________
2019-12-23 15:51:52,651 [INFO] dropout_6 (Dropout)          (None, 64)                0         
2019-12-23 15:51:52,651 [INFO] _________________________________________________________________
2019-12-23 15:51:52,651 [INFO] dense_12 (Dense)             (None, 5)                 325       
2019-12-23 15:51:52,651 [INFO] =================================================================
2019-12-23 15:51:52,651 [INFO] Total params: 4,741
2019-12-23 15:51:52,651 [INFO] Trainable params: 4,613
2019-12-23 15:51:52,651 [INFO] Non-trainable params: 128
2019-12-23 15:51:52,651 [INFO] _________________________________________________________________
2019-12-23 15:51:52,651 [INFO] Training model
2019-12-23 15:51:52,651 [INFO] Splitting train set into 2 sets (unsupervised, supervised), random_seed = 42
2019-12-23 15:51:53,353 [INFO] Split sizes (instances). total = 100778, unsupervised = 50389, supervised = 50389, unsupervised dataset hash = 06e3c0571318a4e80f9239b32a18cc0a10c17120
2019-12-23 15:51:53,353 [INFO] Training autoencoder
 - val_f1: 0.9957
Epoch 00145: early stopping
Train on 50389 samples, validate on 25195 samples
Epoch 1/200
 - 1s - loss: 0.0408 - val_loss: -9.0201e-01
Epoch 2/200
 - 1s - loss: -1.5020e+00 - val_loss: -2.1336e+00
Epoch 3/200
 - 1s - loss: -2.4101e+00 - val_loss: -2.7383e+00
Epoch 4/200
 - 1s - loss: -2.8361e+00 - val_loss: -3.0128e+00
Epoch 5/200
 - 1s - loss: -3.0160e+00 - val_loss: -3.1140e+00
Epoch 6/200
 - 1s - loss: -3.0922e+00 - val_loss: -3.1580e+00
Epoch 7/200
 - 1s - loss: -3.1299e+00 - val_loss: -3.1817e+00
Epoch 8/200
 - 1s - loss: -3.1545e+00 - val_loss: -3.1952e+00
Epoch 9/200
 - 1s - loss: -3.1703e+00 - val_loss: -3.2040e+00
Epoch 10/200
 - 1s - loss: -3.1789e+00 - val_loss: -3.2108e+00
Epoch 11/200
 - 1s - loss: -3.1905e+00 - val_loss: -3.2154e+00
Epoch 12/200
 - 1s - loss: -3.1962e+00 - val_loss: -3.2183e+00
Epoch 13/200
 - 1s - loss: -3.2027e+00 - val_loss: -3.2216e+00
Epoch 14/200
 - 1s - loss: -3.2070e+00 - val_loss: -3.2241e+00
Epoch 15/200
 - 1s - loss: -3.2108e+00 - val_loss: -3.2257e+00
Epoch 16/200
 - 1s - loss: -3.2128e+00 - val_loss: -3.2273e+00
Epoch 17/200
 - 1s - loss: -3.2167e+00 - val_loss: -3.2285e+00
Epoch 18/200
 - 1s - loss: -3.2190e+00 - val_loss: -3.2301e+00
Epoch 19/200
 - 1s - loss: -3.2211e+00 - val_loss: -3.2310e+00
Epoch 20/200
 - 1s - loss: -3.2237e+00 - val_loss: -3.2319e+00
Epoch 21/200
 - 1s - loss: -3.2259e+00 - val_loss: -3.2327e+00
2019-12-23 15:52:11,869 [INFO] epoch = 20. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/ae_model_epoch_20.pickle
Epoch 22/200
 - 1s - loss: -3.2267e+00 - val_loss: -3.2331e+00
Epoch 23/200
 - 1s - loss: -3.2273e+00 - val_loss: -3.2339e+00
Epoch 24/200
 - 1s - loss: -3.2299e+00 - val_loss: -3.2347e+00
Epoch 25/200
 - 1s - loss: -3.2304e+00 - val_loss: -3.2351e+00
Epoch 26/200
 - 1s - loss: -3.2322e+00 - val_loss: -3.2353e+00
Epoch 27/200
 - 1s - loss: -3.2322e+00 - val_loss: -3.2361e+00
Epoch 28/200
 - 1s - loss: -3.2346e+00 - val_loss: -3.2366e+00
Epoch 29/200
 - 1s - loss: -3.2353e+00 - val_loss: -3.2367e+00
Epoch 30/200
 - 1s - loss: -3.2355e+00 - val_loss: -3.2375e+00
Epoch 31/200
 - 1s - loss: -3.2363e+00 - val_loss: -3.2378e+00
Epoch 32/200
 - 1s - loss: -3.2373e+00 - val_loss: -3.2380e+00
Epoch 33/200
 - 1s - loss: -3.2384e+00 - val_loss: -3.2385e+00
Epoch 34/200
 - 1s - loss: -3.2390e+00 - val_loss: -3.2386e+00
Epoch 35/200
 - 1s - loss: -3.2389e+00 - val_loss: -3.2385e+00
Epoch 36/200
 - 1s - loss: -3.2399e+00 - val_loss: -3.2391e+00
Epoch 37/200
 - 1s - loss: -3.2405e+00 - val_loss: -3.2393e+00
Epoch 38/200
 - 1s - loss: -3.2409e+00 - val_loss: -3.2396e+00
Epoch 39/200
 - 1s - loss: -3.2416e+00 - val_loss: -3.2397e+00
Epoch 40/200
 - 1s - loss: -3.2424e+00 - val_loss: -3.2399e+00
Epoch 41/200
 - 1s - loss: -3.2416e+00 - val_loss: -3.2403e+00
2019-12-23 15:52:28,029 [INFO] epoch = 40. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/ae_model_epoch_40.pickle
Epoch 42/200
 - 1s - loss: -3.2430e+00 - val_loss: -3.2407e+00
Epoch 43/200
 - 1s - loss: -3.2433e+00 - val_loss: -3.2404e+00
Epoch 44/200
 - 1s - loss: -3.2443e+00 - val_loss: -3.2406e+00
Epoch 45/200
 - 1s - loss: -3.2449e+00 - val_loss: -3.2411e+00
Epoch 46/200
 - 1s - loss: -3.2452e+00 - val_loss: -3.2413e+00
Epoch 47/200
 - 1s - loss: -3.2452e+00 - val_loss: -3.2414e+00
Epoch 48/200
 - 1s - loss: -3.2451e+00 - val_loss: -3.2411e+00
Epoch 49/200
 - 1s - loss: -3.2457e+00 - val_loss: -3.2415e+00
Epoch 50/200
 - 1s - loss: -3.2458e+00 - val_loss: -3.2414e+00
Epoch 51/200
 - 1s - loss: -3.2462e+00 - val_loss: -3.2415e+00
Epoch 52/200
 - 1s - loss: -3.2466e+00 - val_loss: -3.2418e+00
Epoch 53/200
 - 1s - loss: -3.2463e+00 - val_loss: -3.2418e+00
Epoch 54/200
 - 1s - loss: -3.2468e+00 - val_loss: -3.2419e+00
Epoch 55/200
 - 1s - loss: -3.2476e+00 - val_loss: -3.2422e+00
Epoch 56/200
 - 1s - loss: -3.2473e+00 - val_loss: -3.2422e+00
Epoch 57/200
 - 1s - loss: -3.2483e+00 - val_loss: -3.2425e+00
Epoch 58/200
 - 1s - loss: -3.2483e+00 - val_loss: -3.2424e+00
Epoch 59/200
 - 1s - loss: -3.2491e+00 - val_loss: -3.2423e+00
Epoch 60/200
 - 1s - loss: -3.2487e+00 - val_loss: -3.2425e+00
Epoch 61/200
 - 1s - loss: -3.2497e+00 - val_loss: -3.2429e+00
2019-12-23 15:52:44,251 [INFO] epoch = 60. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/ae_model_epoch_60.pickle
Epoch 62/200
 - 1s - loss: -3.2468e+00 - val_loss: -3.2424e+00
Epoch 63/200
 - 1s - loss: -3.2492e+00 - val_loss: -3.2429e+00
Epoch 64/200
 - 1s - loss: -3.2497e+00 - val_loss: -3.2429e+00
Epoch 65/200
 - 1s - loss: -3.2497e+00 - val_loss: -3.2432e+00
Epoch 66/200
 - 1s - loss: -3.2499e+00 - val_loss: -3.2431e+00
Epoch 67/200
 - 1s - loss: -3.2504e+00 - val_loss: -3.2432e+00
Epoch 68/200
 - 1s - loss: -3.2501e+00 - val_loss: -3.2433e+00
Epoch 69/200
 - 1s - loss: -3.2503e+00 - val_loss: -3.2433e+00
Epoch 70/200
 - 1s - loss: -3.2505e+00 - val_loss: -3.2433e+00
Epoch 71/200
 - 1s - loss: -3.2513e+00 - val_loss: -3.2436e+00
Epoch 72/200
 - 1s - loss: -3.2509e+00 - val_loss: -3.2437e+00
Epoch 73/200
 - 1s - loss: -3.2515e+00 - val_loss: -3.2436e+00
Epoch 74/200
 - 1s - loss: -3.2520e+00 - val_loss: -3.2436e+00
Epoch 75/200
 - 1s - loss: -3.2512e+00 - val_loss: -3.2435e+00
Epoch 76/200
 - 1s - loss: -3.2524e+00 - val_loss: -3.2438e+00
Epoch 77/200
 - 1s - loss: -3.2518e+00 - val_loss: -3.2441e+00
Epoch 78/200
 - 1s - loss: -3.2523e+00 - val_loss: -3.2437e+00
Epoch 79/200
 - 1s - loss: -3.2525e+00 - val_loss: -3.2438e+00
Epoch 80/200
 - 1s - loss: -3.2530e+00 - val_loss: -3.2440e+00
Epoch 81/200
 - 1s - loss: -3.2529e+00 - val_loss: -3.2440e+00
2019-12-23 15:53:00,428 [INFO] epoch = 80. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/ae_model_epoch_80.pickle
Epoch 82/200
 - 1s - loss: -3.2530e+00 - val_loss: -3.2441e+00
Epoch 83/200
 - 1s - loss: -3.2532e+00 - val_loss: -3.2438e+00
Epoch 84/200
 - 1s - loss: -3.2536e+00 - val_loss: -3.2441e+00
Epoch 85/200
 - 1s - loss: -3.2534e+00 - val_loss: -3.2444e+00
Epoch 86/200
 - 1s - loss: -3.2517e+00 - val_loss: -3.2440e+00
Epoch 87/200
 - 1s - loss: -3.2537e+00 - val_loss: -3.2444e+00
Epoch 88/200
 - 1s - loss: -3.2538e+00 - val_loss: -3.2443e+00
Epoch 89/200
 - 1s - loss: -3.2544e+00 - val_loss: -3.2442e+00
Epoch 90/200
 - 1s - loss: -3.2537e+00 - val_loss: -3.2441e+00
Epoch 91/200
 - 1s - loss: -3.2541e+00 - val_loss: -3.2444e+00
Epoch 92/200
 - 1s - loss: -3.2546e+00 - val_loss: -3.2441e+00
Epoch 93/200
 - 1s - loss: -3.2520e+00 - val_loss: -3.2447e+00
Epoch 94/200
 - 1s - loss: -3.2547e+00 - val_loss: -3.2446e+00
Epoch 95/200
 - 1s - loss: -3.2545e+00 - val_loss: -3.2447e+00
Epoch 96/200
 - 1s - loss: -3.2541e+00 - val_loss: -3.2445e+00
Epoch 97/200
 - 1s - loss: -3.2550e+00 - val_loss: -3.2448e+00
Epoch 98/200
 - 1s - loss: -3.2554e+00 - val_loss: -3.2447e+00
Epoch 99/200
 - 1s - loss: -3.2553e+00 - val_loss: -3.2446e+00
Epoch 100/200
 - 1s - loss: -3.2546e+00 - val_loss: -3.2446e+00
Epoch 101/200
 - 1s - loss: -3.2544e+00 - val_loss: -3.2448e+00
2019-12-23 15:53:16,536 [INFO] epoch = 100. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/ae_model_epoch_100.pickle
Epoch 102/200
 - 1s - loss: -3.2548e+00 - val_loss: -3.2446e+00
Epoch 103/200
 - 1s - loss: -3.2551e+00 - val_loss: -3.2449e+00
Epoch 104/200
 - 1s - loss: -3.2551e+00 - val_loss: -3.2447e+00
Epoch 105/200
 - 1s - loss: -3.2554e+00 - val_loss: -3.2450e+00
Epoch 106/200
 - 1s - loss: -3.2559e+00 - val_loss: -3.2450e+00
Epoch 107/200
 - 1s - loss: -3.2554e+00 - val_loss: -3.2451e+00
Epoch 108/200
 - 1s - loss: -3.2559e+00 - val_loss: -3.2451e+00
Epoch 109/200
 - 1s - loss: -3.2559e+00 - val_loss: -3.2448e+00
Epoch 110/200
 - 1s - loss: -3.2563e+00 - val_loss: -3.2449e+00
Epoch 111/200
 - 1s - loss: -3.2559e+00 - val_loss: -3.2448e+00
Epoch 112/200
 - 1s - loss: -3.2559e+00 - val_loss: -3.2451e+00
Epoch 113/200
 - 1s - loss: -3.2559e+00 - val_loss: -3.2449e+00
Epoch 114/200
 - 1s - loss: -3.2555e+00 - val_loss: -3.2451e+00
Epoch 115/200
 - 1s - loss: -3.2567e+00 - val_loss: -3.2452e+00
Epoch 116/200
 - 1s - loss: -3.2556e+00 - val_loss: -3.2454e+00
Epoch 117/200
 - 1s - loss: -3.2566e+00 - val_loss: -3.2454e+00
Epoch 118/200
 - 1s - loss: -3.2566e+00 - val_loss: -3.2454e+00
Epoch 119/200
 - 1s - loss: -3.2567e+00 - val_loss: -3.2454e+00
Epoch 120/200
 - 1s - loss: -3.2571e+00 - val_loss: -3.2455e+00
Epoch 121/200
 - 1s - loss: -3.2567e+00 - val_loss: -3.2452e+00
2019-12-23 15:53:32,746 [INFO] epoch = 120. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/ae_model_epoch_120.pickle
Epoch 122/200
 - 1s - loss: -3.2568e+00 - val_loss: -3.2455e+00
Epoch 123/200
 - 1s - loss: -3.2559e+00 - val_loss: -3.2455e+00
Epoch 124/200
 - 1s - loss: -3.2570e+00 - val_loss: -3.2453e+00
Epoch 125/200
 - 1s - loss: -3.2569e+00 - val_loss: -3.2456e+00
Epoch 126/200
 - 1s - loss: -3.2569e+00 - val_loss: -3.2456e+00
Epoch 127/200
 - 1s - loss: -3.2574e+00 - val_loss: -3.2455e+00
Epoch 128/200
 - 1s - loss: -3.2573e+00 - val_loss: -3.2458e+00
Epoch 129/200
 - 1s - loss: -3.2574e+00 - val_loss: -3.2456e+00
Epoch 130/200
 - 1s - loss: -3.2566e+00 - val_loss: -3.2455e+00
Epoch 131/200
 - 1s - loss: -3.2569e+00 - val_loss: -3.2455e+00
Epoch 132/200
 - 1s - loss: -3.2576e+00 - val_loss: -3.2455e+00
Epoch 133/200
 - 1s - loss: -3.2539e+00 - val_loss: -3.2455e+00
Epoch 134/200
 - 1s - loss: -3.2573e+00 - val_loss: -3.2455e+00
Epoch 135/200
 - 1s - loss: -3.2548e+00 - val_loss: -3.2454e+00
Epoch 136/200
 - 1s - loss: -3.2577e+00 - val_loss: -3.2457e+00
Epoch 137/200
 - 1s - loss: -3.2578e+00 - val_loss: -3.2457e+00
Epoch 138/200
 - 1s - loss: -3.2579e+00 - val_loss: -3.2455e+00
Epoch 139/200
 - 1s - loss: -3.2575e+00 - val_loss: -3.2454e+00
Epoch 140/200
 - 1s - loss: -3.2578e+00 - val_loss: -3.2455e+00
Epoch 141/200
 - 1s - loss: -3.2578e+00 - val_loss: -3.2460e+00
2019-12-23 15:53:48,966 [INFO] epoch = 140. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/ae_model_epoch_140.pickle
Epoch 142/200
 - 1s - loss: -3.2576e+00 - val_loss: -3.2456e+00
Epoch 143/200
 - 1s - loss: -3.2575e+00 - val_loss: -3.2454e+00
Epoch 144/200
 - 1s - loss: -3.2577e+00 - val_loss: -3.2455e+00
Epoch 145/200
 - 1s - loss: -3.2577e+00 - val_loss: -3.2457e+00
Epoch 146/200
 - 1s - loss: -3.2577e+00 - val_loss: -3.2457e+00
Epoch 147/200
 - 1s - loss: -3.2577e+00 - val_loss: -3.2458e+00
Epoch 148/200
 - 1s - loss: -3.2577e+00 - val_loss: -3.2459e+00
Epoch 149/200
 - 1s - loss: -3.2585e+00 - val_loss: -3.2460e+00
Epoch 150/200
 - 1s - loss: -3.2581e+00 - val_loss: -3.2460e+00
Epoch 151/200
 - 1s - loss: -3.2579e+00 - val_loss: -3.2461e+00
Epoch 152/200
 - 1s - loss: -3.2580e+00 - val_loss: -3.2461e+00
Epoch 153/200
 - 1s - loss: -3.2583e+00 - val_loss: -3.2459e+00
Epoch 154/200
 - 1s - loss: -3.2582e+00 - val_loss: -3.2460e+00
Epoch 155/200
 - 1s - loss: -3.2585e+00 - val_loss: -3.2460e+00
Epoch 156/200
 - 1s - loss: -3.2591e+00 - val_loss: -3.2459e+00
Epoch 157/200
 - 1s - loss: -3.2587e+00 - val_loss: -3.2457e+00
Epoch 158/200
 - 1s - loss: -3.2585e+00 - val_loss: -3.2459e+00
Epoch 159/200
 - 1s - loss: -3.2586e+00 - val_loss: -3.2461e+00
Epoch 160/200
 - 1s - loss: -3.2584e+00 - val_loss: -3.2460e+00
Epoch 161/200
 - 1s - loss: -3.2589e+00 - val_loss: -3.2461e+00
2019-12-23 15:54:05,091 [INFO] epoch = 160. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/ae_model_epoch_160.pickle
Epoch 162/200
 - 1s - loss: -3.2588e+00 - val_loss: -3.2459e+00
Epoch 163/200
 - 1s - loss: -3.2590e+00 - val_loss: -3.2459e+00
Epoch 164/200
 - 1s - loss: -3.2589e+00 - val_loss: -3.2462e+00
Epoch 165/200
 - 1s - loss: -3.2580e+00 - val_loss: -3.2462e+00
Epoch 166/200
 - 1s - loss: -3.2589e+00 - val_loss: -3.2463e+00
Epoch 167/200
 - 1s - loss: -3.2589e+00 - val_loss: -3.2462e+00
Epoch 168/200
 - 1s - loss: -3.2590e+00 - val_loss: -3.2462e+00
Epoch 169/200
 - 1s - loss: -3.2592e+00 - val_loss: -3.2461e+00
Epoch 170/200
 - 1s - loss: -3.2587e+00 - val_loss: -3.2462e+00
Epoch 171/200
 - 1s - loss: -3.2588e+00 - val_loss: -3.2461e+00
Epoch 172/200
 - 1s - loss: -3.2589e+00 - val_loss: -3.2461e+00
Epoch 173/200
 - 1s - loss: -3.2587e+00 - val_loss: -3.2464e+00
Epoch 174/200
 - 1s - loss: -3.2593e+00 - val_loss: -3.2463e+00
Epoch 175/200
 - 1s - loss: -3.2591e+00 - val_loss: -3.2463e+00
Epoch 176/200
 - 1s - loss: -3.2590e+00 - val_loss: -3.2461e+00
Epoch 177/200
 - 1s - loss: -3.2595e+00 - val_loss: -3.2464e+00
Epoch 178/200
 - 1s - loss: -3.2593e+00 - val_loss: -3.2463e+00
Epoch 179/200
 - 1s - loss: -3.2593e+00 - val_loss: -3.2468e+00
Epoch 180/200
 - 1s - loss: -3.2595e+00 - val_loss: -3.2461e+00
Epoch 181/200
 - 1s - loss: -3.2594e+00 - val_loss: -3.2464e+00
2019-12-23 15:54:21,254 [INFO] epoch = 180. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/ae_model_epoch_180.pickle
Epoch 182/200
 - 1s - loss: -3.2595e+00 - val_loss: -3.2462e+00
Epoch 183/200
 - 1s - loss: -3.2590e+00 - val_loss: -3.2461e+00
Epoch 184/200
 - 1s - loss: -3.2592e+00 - val_loss: -3.2462e+00
Epoch 185/200
 - 1s - loss: -3.2594e+00 - val_loss: -3.2463e+00
Epoch 186/200
 - 1s - loss: -3.2593e+00 - val_loss: -3.2463e+00
Epoch 187/200
 - 1s - loss: -3.2596e+00 - val_loss: -3.2466e+00
Epoch 188/200
 - 1s - loss: -3.2590e+00 - val_loss: -3.2464e+00
Epoch 189/200
 - 1s - loss: -3.2597e+00 - val_loss: -3.2464e+00
Epoch 190/200
 - 1s - loss: -3.2592e+00 - val_loss: -3.2466e+00
Epoch 191/200
 - 1s - loss: -3.2596e+00 - val_loss: -3.2465e+00
Epoch 192/200
 - 1s - loss: -3.2596e+00 - val_loss: -3.2465e+00
Epoch 193/200
 - 1s - loss: -3.2602e+00 - val_loss: -3.2465e+00
Epoch 194/200
 - 1s - loss: -3.2596e+00 - val_loss: -3.2464e+00
Epoch 195/200
 - 1s - loss: -3.2602e+00 - val_loss: -3.2465e+00
Epoch 196/200
 - 1s - loss: -3.2597e+00 - val_loss: -3.2462e+00
Epoch 197/200
 - 1s - loss: -3.2597e+00 - val_loss: -3.2464e+00
Epoch 198/200
 - 1s - loss: -3.2595e+00 - val_loss: -3.2467e+00
Epoch 199/200
 - 1s - loss: -3.2598e+00 - val_loss: -3.2466e+00
Epoch 200/200
 - 1s - loss: -3.2594e+00 - val_loss: -3.2463e+00
2019-12-23 15:54:36,547 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-23 15:54:37,800 [INFO] Last epoch loss evaluation: train_loss = -3.274021, val_loss = -3.246799
2019-12-23 15:54:37,800 [INFO] Training autoencoder complete
2019-12-23 15:54:37,800 [INFO] Encoding data for supervised training
2019-12-23 15:54:38,784 [INFO] Encoding complete
2019-12-23 15:54:38,784 [INFO] Training neural network layers (after autoencoder)
Train on 50389 samples, validate on 25195 samples
Epoch 1/300
 - 1s - loss: 0.1362 - val_loss: 0.0355
 - val_f1: 0.9730
Epoch 2/300
 - 0s - loss: 0.0325 - val_loss: 0.0189
 - val_f1: 0.9846
Epoch 3/300
 - 0s - loss: 0.0217 - val_loss: 0.0139
 - val_f1: 0.9892
Epoch 4/300
 - 0s - loss: 0.0172 - val_loss: 0.0118
 - val_f1: 0.9905
Epoch 5/300
 - 0s - loss: 0.0151 - val_loss: 0.0104
 - val_f1: 0.9914
Epoch 6/300
 - 0s - loss: 0.0136 - val_loss: 0.0100
 - val_f1: 0.9912
Epoch 7/300
 - 0s - loss: 0.0125 - val_loss: 0.0095
 - val_f1: 0.9921
Epoch 8/300
 - 0s - loss: 0.0118 - val_loss: 0.0088
 - val_f1: 0.9928
Epoch 9/300
 - 0s - loss: 0.0108 - val_loss: 0.0084
 - val_f1: 0.9933
Epoch 10/300
 - 0s - loss: 0.0103 - val_loss: 0.0082
 - val_f1: 0.9928
Epoch 11/300
 - 0s - loss: 0.0097 - val_loss: 0.0079
 - val_f1: 0.9941
Epoch 12/300
 - 0s - loss: 0.0098 - val_loss: 0.0079
 - val_f1: 0.9944
Epoch 13/300
 - 0s - loss: 0.0091 - val_loss: 0.0078
 - val_f1: 0.9941
Epoch 14/300
 - 0s - loss: 0.0088 - val_loss: 0.0080
 - val_f1: 0.9927
Epoch 15/300
 - 0s - loss: 0.0090 - val_loss: 0.0079
 - val_f1: 0.9936
Epoch 16/300
 - 0s - loss: 0.0082 - val_loss: 0.0073
 - val_f1: 0.9945
Epoch 17/300
 - 0s - loss: 0.0079 - val_loss: 0.0071
 - val_f1: 0.9944
Epoch 18/300
 - 0s - loss: 0.0079 - val_loss: 0.0069
 - val_f1: 0.9944
Epoch 19/300
 - 0s - loss: 0.0076 - val_loss: 0.0068
 - val_f1: 0.9945
Epoch 20/300
 - 0s - loss: 0.0071 - val_loss: 0.0067
 - val_f1: 0.9949
Epoch 21/300
 - 0s - loss: 0.0075 - val_loss: 0.0070
 - val_f1: 0.9950
Epoch 22/300
 - 0s - loss: 0.0075 - val_loss: 0.0072
 - val_f1: 0.9944
Epoch 23/300
 - 0s - loss: 0.0072 - val_loss: 0.0066
 - val_f1: 0.9951
Epoch 24/300
 - 0s - loss: 0.0072 - val_loss: 0.0068
 - val_f1: 0.9946
Epoch 25/300
 - 0s - loss: 0.0068 - val_loss: 0.0067
 - val_f1: 0.9949
Epoch 26/300
 - 0s - loss: 0.0066 - val_loss: 0.0063
 - val_f1: 0.9949
Epoch 27/300
 - 0s - loss: 0.0061 - val_loss: 0.0072
 - val_f1: 0.9943
Epoch 28/300
 - 0s - loss: 0.0067 - val_loss: 0.0063
 - val_f1: 0.9953
Epoch 29/300
 - 0s - loss: 0.0065 - val_loss: 0.0066
 - val_f1: 0.9945
Epoch 30/300
 - 0s - loss: 0.0061 - val_loss: 0.0066
 - val_f1: 0.9953
Epoch 31/300
 - 0s - loss: 0.0061 - val_loss: 0.0067
2019-12-23 15:54:59,423 [INFO] epoch = 30. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/ann_model_epoch_30.pickle
 - val_f1: 0.9954
Epoch 32/300
 - 0s - loss: 0.0064 - val_loss: 0.0065
 - val_f1: 0.9951
Epoch 33/300
 - 0s - loss: 0.0061 - val_loss: 0.0063
 - val_f1: 0.9954
Epoch 34/300
 - 0s - loss: 0.0061 - val_loss: 0.0072
 - val_f1: 0.9940
Epoch 35/300
 - 0s - loss: 0.0059 - val_loss: 0.0066
 - val_f1: 0.9953
Epoch 36/300
 - 0s - loss: 0.0059 - val_loss: 0.0064
 - val_f1: 0.9953
Epoch 37/300
 - 0s - loss: 0.0057 - val_loss: 0.0066
 - val_f1: 0.9937
Epoch 38/300
 - 0s - loss: 0.0059 - val_loss: 0.0061
 - val_f1: 0.9950
Epoch 39/300
 - 0s - loss: 0.0058 - val_loss: 0.0064
 - val_f1: 0.9952
Epoch 40/300
 - 0s - loss: 0.0057 - val_loss: 0.0062
 - val_f1: 0.9952
Epoch 41/300
 - 0s - loss: 0.0053 - val_loss: 0.0061
 - val_f1: 0.9958
Epoch 42/300
 - 0s - loss: 0.0057 - val_loss: 0.0063
 - val_f1: 0.9952
Epoch 43/300
 - 0s - loss: 0.0056 - val_loss: 0.0069
 - val_f1: 0.9949
Epoch 44/300
 - 0s - loss: 0.0056 - val_loss: 0.0060
 - val_f1: 0.9952
Epoch 45/300
 - 0s - loss: 0.0055 - val_loss: 0.0057
 - val_f1: 0.9959
Epoch 46/300
 - 0s - loss: 0.0054 - val_loss: 0.0060
 - val_f1: 0.9957
Epoch 47/300
 - 0s - loss: 0.0055 - val_loss: 0.0061
 - val_f1: 0.9952
Epoch 48/300
 - 0s - loss: 0.0053 - val_loss: 0.0059
 - val_f1: 0.9959
Epoch 49/300
 - 0s - loss: 0.0053 - val_loss: 0.0057
 - val_f1: 0.9961
Epoch 50/300
 - 0s - loss: 0.0051 - val_loss: 0.0057
 - val_f1: 0.9961
Epoch 51/300
 - 0s - loss: 0.0053 - val_loss: 0.0066
 - val_f1: 0.9954
Epoch 52/300
 - 0s - loss: 0.0052 - val_loss: 0.0063
 - val_f1: 0.9955
Epoch 53/300
 - 0s - loss: 0.0051 - val_loss: 0.0058
 - val_f1: 0.9957
Epoch 54/300
 - 0s - loss: 0.0049 - val_loss: 0.0065
 - val_f1: 0.9954
Epoch 55/300
 - 0s - loss: 0.0048 - val_loss: 0.0059
 - val_f1: 0.9956
Epoch 56/300
 - 0s - loss: 0.0048 - val_loss: 0.0061
 - val_f1: 0.9956
Epoch 57/300
 - 0s - loss: 0.0051 - val_loss: 0.0064
 - val_f1: 0.9955
Epoch 58/300
 - 0s - loss: 0.0049 - val_loss: 0.0070
 - val_f1: 0.9947
Epoch 59/300
 - 0s - loss: 0.0052 - val_loss: 0.0064
 - val_f1: 0.9951
Epoch 60/300
 - 0s - loss: 0.0051 - val_loss: 0.0063
 - val_f1: 0.9953
Epoch 61/300
 - 0s - loss: 0.0048 - val_loss: 0.0062
2019-12-23 15:55:17,765 [INFO] epoch = 60. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/ann_model_epoch_60.pickle
 - val_f1: 0.9955
Epoch 62/300
 - 0s - loss: 0.0048 - val_loss: 0.0061
 - val_f1: 0.9953
Epoch 63/300
 - 0s - loss: 0.0048 - val_loss: 0.0061
 - val_f1: 0.9953
Epoch 64/300
 - 0s - loss: 0.0047 - val_loss: 0.0061
 - val_f1: 0.9959
Epoch 65/300
 - 0s - loss: 0.0046 - val_loss: 0.0060
 - val_f1: 0.9956
Epoch 66/300
 - 0s - loss: 0.0046 - val_loss: 0.0059
 - val_f1: 0.9959
Epoch 67/300
 - 0s - loss: 0.0047 - val_loss: 0.0061
 - val_f1: 0.9957
Epoch 68/300
 - 0s - loss: 0.0047 - val_loss: 0.0059
 - val_f1: 0.9959
Epoch 69/300
 - 0s - loss: 0.0047 - val_loss: 0.0060
 - val_f1: 0.9959
Epoch 70/300
 - 0s - loss: 0.0049 - val_loss: 0.0061
 - val_f1: 0.9956
Epoch 71/300
 - 0s - loss: 0.0045 - val_loss: 0.0061
 - val_f1: 0.9952
Epoch 72/300
 - 0s - loss: 0.0045 - val_loss: 0.0059
 - val_f1: 0.9956
Epoch 73/300
 - 0s - loss: 0.0046 - val_loss: 0.0059
 - val_f1: 0.9961
Epoch 74/300
 - 0s - loss: 0.0045 - val_loss: 0.0060
 - val_f1: 0.9957
Epoch 75/300
 - 0s - loss: 0.0042 - val_loss: 0.0058
 - val_f1: 0.9959
Epoch 76/300
 - 0s - loss: 0.0043 - val_loss: 0.0058
 - val_f1: 0.9960
Epoch 77/300
 - 0s - loss: 0.0043 - val_loss: 0.0063
 - val_f1: 0.9954
Epoch 78/300
 - 0s - loss: 0.0045 - val_loss: 0.0061
 - val_f1: 0.9961
Epoch 79/300
 - 0s - loss: 0.0045 - val_loss: 0.0066
 - val_f1: 0.9954
Epoch 80/300
 - 0s - loss: 0.0044 - val_loss: 0.0062
 - val_f1: 0.9958
Epoch 81/300
 - 0s - loss: 0.0041 - val_loss: 0.0061
 - val_f1: 0.9957
Epoch 82/300
 - 0s - loss: 0.0044 - val_loss: 0.0064
 - val_f1: 0.9952
Epoch 83/300
 - 0s - loss: 0.0044 - val_loss: 0.0062
 - val_f1: 0.9954
Epoch 84/300
 - 0s - loss: 0.0042 - val_loss: 0.0059
 - val_f1: 0.9961
Epoch 85/300
 - 0s - loss: 0.0042 - val_loss: 0.0061
 - val_f1: 0.9956
Epoch 86/300
 - 0s - loss: 0.0043 - val_loss: 0.0064
 - val_f1: 0.9961
Epoch 87/300
 - 0s - loss: 0.0043 - val_loss: 0.0058
 - val_f1: 0.9959
Epoch 88/300
 - 0s - loss: 0.0041 - val_loss: 0.0062
 - val_f1: 0.9959
Epoch 89/300
 - 0s - loss: 0.0044 - val_loss: 0.0059
 - val_f1: 0.9961
Epoch 90/300
 - 0s - loss: 0.0041 - val_loss: 0.0064
 - val_f1: 0.9955
Epoch 91/300
 - 0s - loss: 0.0041 - val_loss: 0.0065
2019-12-23 15:55:36,219 [INFO] epoch = 90. Intermediate model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/ann_model_epoch_90.pickle
 - val_f1: 0.9949
Epoch 92/300
 - 0s - loss: 0.0040 - val_loss: 0.0063
 - val_f1: 0.9958
Epoch 93/300
 - 0s - loss: 0.0042 - val_loss: 0.0064
 - val_f1: 0.9958
Epoch 94/300
 - 0s - loss: 0.0039 - val_loss: 0.0060
 - val_f1: 0.9955
Epoch 95/300
 - 0s - loss: 0.0040 - val_loss: 0.0062
 - val_f1: 0.9961
Epoch 96/300
 - 0s - loss: 0.0041 - val_loss: 0.0064
 - val_f1: 0.9954
Epoch 97/300
 - 0s - loss: 0.0040 - val_loss: 0.0061
 - val_f1: 0.9961
Epoch 98/300
 - 0s - loss: 0.0040 - val_loss: 0.0064
 - val_f1: 0.9956
Epoch 99/300
 - 0s - loss: 0.0040 - val_loss: 0.0063
2019-12-23 15:55:41,415 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-23 15:55:42,523 [INFO] Last epoch loss evaluation: train_loss = 0.003554, val_loss = 0.005670
2019-12-23 15:55:42,527 [INFO] Training complete. time_to_train = 229.88 sec, 3.83 min
2019-12-23 15:55:42,535 [INFO] Model saved to results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/best_model.pickle
2019-12-23 15:55:42,712 [INFO] Plot saved to: results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/training_error_history.png
2019-12-23 15:55:42,872 [INFO] Plot saved to: results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/training_f1_history.png
2019-12-23 15:55:42,872 [INFO] Making predictions on training, validation, testing data
2019-12-23 15:55:46,178 [INFO] Evaluating predictions (results)
2019-12-23 15:55:46,440 [INFO] Dataset: Testing. Classification report below
2019-12-23 15:55:46,440 [INFO] 
              precision    recall  f1-score   support

         dos       0.97      0.81      0.88      7458
      normal       0.67      0.93      0.78      9711
       probe       0.70      0.73      0.71      2421
         r2l       0.96      0.12      0.21      2421
         u2r       0.83      0.04      0.07       533

   micro avg       0.76      0.76      0.76     22544
   macro avg       0.83      0.52      0.53     22544
weighted avg       0.81      0.76      0.73     22544

2019-12-23 15:55:46,440 [INFO] Overall accuracy (micro avg): 0.7590046132008517
2019-12-23 15:55:46,737 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.7590         0.7590                       0.7590                0.0602                   0.2410  0.7590
1     Macro avg        0.9036         0.8257                       0.5241                0.0799                   0.4759  0.5313
2  Weighted avg        0.8599         0.8060                       0.7590                0.1585                   0.2410  0.7274
2019-12-23 15:55:47,070 [INFO] Dataset: Validation. Classification report below
2019-12-23 15:55:47,070 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00      9186
      normal       1.00      1.00      1.00     13469
       probe       0.99      0.99      0.99      2331
         r2l       0.90      0.86      0.88       199
         u2r       0.75      0.30      0.43        10

   micro avg       1.00      1.00      1.00     25195
   macro avg       0.93      0.83      0.86     25195
weighted avg       1.00      1.00      1.00     25195

2019-12-23 15:55:47,070 [INFO] Overall accuracy (micro avg): 0.9960706489382815
2019-12-23 15:55:47,426 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9961         0.9961                       0.9961                0.0010                   0.0039  0.9961
1     Macro avg        0.9984         0.9277                       0.8297                0.0013                   0.1703  0.8594
2  Weighted avg        0.9976         0.9960                       0.9961                0.0027                   0.0039  0.9960
2019-12-23 15:55:48,873 [INFO] Dataset: Training. Classification report below
2019-12-23 15:55:48,873 [INFO] 
              precision    recall  f1-score   support

         dos       1.00      1.00      1.00     36741
      normal       1.00      1.00      1.00     53874
       probe       1.00      0.99      0.99      9325
         r2l       0.90      0.85      0.87       796
         u2r       0.87      0.62      0.72        42

   micro avg       1.00      1.00      1.00    100778
   macro avg       0.95      0.89      0.92    100778
weighted avg       1.00      1.00      1.00    100778

2019-12-23 15:55:48,873 [INFO] Overall accuracy (micro avg): 0.9962392585683383
2019-12-23 15:55:50,501 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9962         0.9962                       0.9962                0.0009                   0.0038  0.9962
1     Macro avg        0.9985         0.9514                       0.8914                0.0013                   0.1086  0.9171
2  Weighted avg        0.9977         0.9962                       0.9962                0.0028                   0.0038  0.9962
2019-12-23 15:55:50,540 [INFO] Results saved to: results_additional_exps/semi_sup_perf_nsl_ae_ann_rep3/semi_sup_perf_nsl_ae_ann_rep3_results.xlsx
2019-12-23 15:55:50,540 [INFO] ================= Finished running experiment no. 3 ================= 

2019-12-23 15:55:50,543 [INFO] Created directory: results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1
2019-12-23 15:55:50,544 [INFO] Initialized logging. log_filename = results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/run_log.log
2019-12-23 15:55:50,544 [INFO] ================= Running experiment no. 1  ================= 

2019-12-23 15:55:50,544 [INFO] Experiment parameters given below
2019-12-23 15:55:50,544 [INFO] 
{'experiment_num': 1, 'results_dir': 'results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1', 'model_type': 'classifier', 'model': 'ae_ann', 'normal_label': 'BENIGN', 'split_random_seed': 42, 'unsupervised_ratio': 0.5, 'ae_encoder_units': [64], 'ae_encoder_activations': ['relu'], 'ae_encoder_dropout_rates': [0.2], 'ae_encoder_l1_param': -1, 'ae_decoder_units': '', 'ae_decoder_activations': '', 'ae_decoder_dropout_rates': '', 'output_activation': 'sigmoid', 'loss_function': 'binary_crossentropy', 'ae_epochs': 200, 'ann_layer_units': [64], 'ann_layer_activations': ['relu'], 'ann_layer_dropout_rates': [0.2], 'ann_epochs': 300, 'early_stop_patience': 50, 'batch_size': 256, 'dataset_dir': '../Datasets/full_datasets/ids2017', 'description': 'semi_sup_perf_ids17_ae_ann_rep1'}
2019-12-23 15:55:50,544 [INFO] Created tensorboard log directory: results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/tf_logs_run_2019_12_23-15_55_50
2019-12-23 15:55:50,544 [INFO] Loading datsets from: ../Datasets/full_datasets/ids2017
2019-12-23 15:55:50,544 [INFO] Reading X, y files
2019-12-23 15:55:50,544 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_train.h5
2019-12-23 15:55:54,578 [INFO] Reading complete. time_to_read=4.03 seconds
2019-12-23 15:55:54,578 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_val.h5
2019-12-23 15:55:55,959 [INFO] Reading complete. time_to_read=1.38 seconds
2019-12-23 15:55:55,960 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_test.h5
2019-12-23 15:55:57,343 [INFO] Reading complete. time_to_read=1.38 seconds
2019-12-23 15:55:57,343 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_train.h5
2019-12-23 15:55:57,558 [INFO] Reading complete. time_to_read=0.22 seconds
2019-12-23 15:55:57,558 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_val.h5
2019-12-23 15:55:57,634 [INFO] Reading complete. time_to_read=0.08 seconds
2019-12-23 15:55:57,634 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_test.h5
2019-12-23 15:55:57,704 [INFO] Reading complete. time_to_read=0.07 seconds
2019-12-23 15:56:00,865 [INFO] Initializing model
2019-12-23 15:56:00,974 [INFO] _________________________________________________________________
2019-12-23 15:56:00,974 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-23 15:56:00,974 [INFO] =================================================================
2019-12-23 15:56:00,974 [INFO] dense_13 (Dense)             (None, 64)                5056      
2019-12-23 15:56:00,974 [INFO] _________________________________________________________________
2019-12-23 15:56:00,975 [INFO] batch_normalization_7 (Batch (None, 64)                256       
2019-12-23 15:56:00,975 [INFO] _________________________________________________________________
2019-12-23 15:56:00,975 [INFO] dropout_7 (Dropout)          (None, 64)                0         
2019-12-23 15:56:00,975 [INFO] _________________________________________________________________
2019-12-23 15:56:00,975 [INFO] dense_14 (Dense)             (None, 78)                5070      
2019-12-23 15:56:00,975 [INFO] =================================================================
2019-12-23 15:56:00,975 [INFO] Total params: 10,382
2019-12-23 15:56:00,975 [INFO] Trainable params: 10,254
2019-12-23 15:56:00,975 [INFO] Non-trainable params: 128
2019-12-23 15:56:00,975 [INFO] _________________________________________________________________
2019-12-23 15:56:01,082 [INFO] _________________________________________________________________
2019-12-23 15:56:01,082 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-23 15:56:01,082 [INFO] =================================================================
2019-12-23 15:56:01,082 [INFO] dense_15 (Dense)             (None, 64)                4160      
2019-12-23 15:56:01,082 [INFO] _________________________________________________________________
2019-12-23 15:56:01,083 [INFO] batch_normalization_8 (Batch (None, 64)                256       
2019-12-23 15:56:01,083 [INFO] _________________________________________________________________
2019-12-23 15:56:01,083 [INFO] dropout_8 (Dropout)          (None, 64)                0         
2019-12-23 15:56:01,083 [INFO] _________________________________________________________________
2019-12-23 15:56:01,083 [INFO] dense_16 (Dense)             (None, 12)                780       
2019-12-23 15:56:01,083 [INFO] =================================================================
2019-12-23 15:56:01,083 [INFO] Total params: 5,196
2019-12-23 15:56:01,083 [INFO] Trainable params: 5,068
2019-12-23 15:56:01,083 [INFO] Non-trainable params: 128
2019-12-23 15:56:01,083 [INFO] _________________________________________________________________
2019-12-23 15:56:01,083 [INFO] Training model
2019-12-23 15:56:01,083 [INFO] Splitting train set into 2 sets (unsupervised, supervised), random_seed = 42
2019-12-23 15:56:18,203 [INFO] Split sizes (instances). total = 1696684, unsupervised = 848342, supervised = 848342, unsupervised dataset hash = b75e78959164c90d19a336ef2d2a5a10f094d2bb
2019-12-23 15:56:18,203 [INFO] Training autoencoder
 - val_f1: 0.9958
Epoch 00099: early stopping
Train on 848342 samples, validate on 565562 samples
Epoch 1/200
 - 14s - loss: -3.6537e+00 - val_loss: -4.1379e+00
Epoch 2/200
 - 14s - loss: -4.1025e+00 - val_loss: -4.1479e+00
Epoch 3/200
 - 14s - loss: -4.1165e+00 - val_loss: -4.1528e+00
Epoch 4/200
 - 14s - loss: -4.1240e+00 - val_loss: -4.1563e+00
Epoch 5/200
 - 14s - loss: -4.1279e+00 - val_loss: -4.1585e+00
Epoch 6/200
 - 14s - loss: -4.1305e+00 - val_loss: -4.1599e+00
Epoch 7/200
 - 14s - loss: -4.1322e+00 - val_loss: -4.1602e+00
Epoch 8/200
 - 14s - loss: -4.1335e+00 - val_loss: -4.1606e+00
Epoch 9/200
 - 14s - loss: -4.1347e+00 - val_loss: -4.1616e+00
Epoch 10/200
 - 14s - loss: -4.1355e+00 - val_loss: -4.1596e+00
Epoch 11/200
 - 14s - loss: -4.1363e+00 - val_loss: -4.1619e+00
Epoch 12/200
 - 14s - loss: -4.1367e+00 - val_loss: -4.1616e+00
Epoch 13/200
 - 14s - loss: -4.1376e+00 - val_loss: -4.1629e+00
Epoch 14/200
 - 14s - loss: -4.1379e+00 - val_loss: -4.1628e+00
Epoch 15/200
 - 14s - loss: -4.1384e+00 - val_loss: -4.1619e+00
Epoch 16/200
 - 14s - loss: -4.1390e+00 - val_loss: -4.1638e+00
Epoch 17/200
 - 14s - loss: -4.1395e+00 - val_loss: -4.1611e+00
Epoch 18/200
 - 14s - loss: -4.1398e+00 - val_loss: -4.1609e+00
Epoch 19/200
 - 14s - loss: -4.1396e+00 - val_loss: -4.1616e+00
Epoch 20/200
 - 14s - loss: -4.1404e+00 - val_loss: -4.1638e+00
Epoch 21/200
 - 14s - loss: -4.1406e+00 - val_loss: -4.1616e+00
2019-12-23 16:01:13,121 [INFO] epoch = 20. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/ae_model_epoch_20.pickle
Epoch 22/200
 - 14s - loss: -4.1405e+00 - val_loss: -4.1627e+00
Epoch 23/200
 - 14s - loss: -4.1409e+00 - val_loss: -4.1646e+00
Epoch 24/200
 - 14s - loss: -4.1414e+00 - val_loss: -4.1620e+00
Epoch 25/200
 - 14s - loss: -4.1412e+00 - val_loss: -4.1639e+00
Epoch 26/200
 - 14s - loss: -4.1415e+00 - val_loss: -4.1623e+00
Epoch 27/200
 - 14s - loss: -4.1416e+00 - val_loss: -4.1638e+00
Epoch 28/200
 - 14s - loss: -4.1419e+00 - val_loss: -4.1614e+00
Epoch 29/200
 - 14s - loss: -4.1419e+00 - val_loss: -4.1620e+00
Epoch 30/200
 - 14s - loss: -4.1421e+00 - val_loss: -4.1651e+00
Epoch 31/200
 - 14s - loss: -4.1422e+00 - val_loss: -4.1641e+00
Epoch 32/200
 - 14s - loss: -4.1423e+00 - val_loss: -4.1639e+00
Epoch 33/200
 - 14s - loss: -4.1425e+00 - val_loss: -4.1646e+00
Epoch 34/200
 - 14s - loss: -4.1426e+00 - val_loss: -4.1649e+00
Epoch 35/200
 - 14s - loss: -4.1427e+00 - val_loss: -4.1639e+00
Epoch 36/200
 - 14s - loss: -4.1428e+00 - val_loss: -4.1641e+00
Epoch 37/200
 - 14s - loss: -4.1425e+00 - val_loss: -4.1627e+00
Epoch 38/200
 - 14s - loss: -4.1430e+00 - val_loss: -4.1649e+00
Epoch 39/200
 - 14s - loss: -4.1431e+00 - val_loss: -4.1651e+00
Epoch 40/200
 - 14s - loss: -4.1428e+00 - val_loss: -4.1618e+00
Epoch 41/200
 - 14s - loss: -4.1428e+00 - val_loss: -4.1623e+00
2019-12-23 16:05:52,544 [INFO] epoch = 40. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/ae_model_epoch_40.pickle
Epoch 42/200
 - 14s - loss: -4.1427e+00 - val_loss: -4.1644e+00
Epoch 43/200
 - 14s - loss: -4.1432e+00 - val_loss: -4.1642e+00
Epoch 44/200
 - 14s - loss: -4.1432e+00 - val_loss: -4.1649e+00
Epoch 45/200
 - 14s - loss: -4.1429e+00 - val_loss: -4.1660e+00
Epoch 46/200
 - 14s - loss: -4.1434e+00 - val_loss: -4.1655e+00
Epoch 47/200
 - 14s - loss: -4.1437e+00 - val_loss: -4.1637e+00
Epoch 48/200
 - 14s - loss: -4.1434e+00 - val_loss: -4.1651e+00
Epoch 49/200
 - 14s - loss: -4.1437e+00 - val_loss: -4.1646e+00
Epoch 50/200
 - 14s - loss: -4.1434e+00 - val_loss: -4.1644e+00
Epoch 51/200
 - 14s - loss: -4.1438e+00 - val_loss: -4.1655e+00
Epoch 52/200
 - 14s - loss: -4.1440e+00 - val_loss: -4.1634e+00
Epoch 53/200
 - 14s - loss: -4.1439e+00 - val_loss: -4.1646e+00
Epoch 54/200
 - 14s - loss: -4.1439e+00 - val_loss: -4.1577e+00
Epoch 55/200
 - 14s - loss: -4.1439e+00 - val_loss: -4.1582e+00
Epoch 56/200
 - 14s - loss: -4.1439e+00 - val_loss: -4.1590e+00
Epoch 57/200
 - 14s - loss: -4.1437e+00 - val_loss: -4.1550e+00
Epoch 58/200
 - 14s - loss: -4.1440e+00 - val_loss: -4.1639e+00
Epoch 59/200
 - 14s - loss: -4.1442e+00 - val_loss: -4.1624e+00
Epoch 60/200
 - 14s - loss: -4.1439e+00 - val_loss: -4.1610e+00
Epoch 61/200
 - 14s - loss: -4.1439e+00 - val_loss: -4.1598e+00
2019-12-23 16:10:31,474 [INFO] epoch = 60. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/ae_model_epoch_60.pickle
Epoch 62/200
 - 14s - loss: -4.1442e+00 - val_loss: -4.1615e+00
Epoch 63/200
 - 14s - loss: -4.1441e+00 - val_loss: -4.1571e+00
Epoch 64/200
 - 14s - loss: -4.1443e+00 - val_loss: -4.1649e+00
Epoch 65/200
 - 14s - loss: -4.1442e+00 - val_loss: -4.1641e+00
Epoch 66/200
 - 14s - loss: -4.1442e+00 - val_loss: -4.1641e+00
Epoch 67/200
 - 14s - loss: -4.1443e+00 - val_loss: -4.1663e+00
Epoch 68/200
 - 14s - loss: -4.1444e+00 - val_loss: -4.1634e+00
Epoch 69/200
 - 14s - loss: -4.1438e+00 - val_loss: -4.1603e+00
Epoch 70/200
 - 14s - loss: -4.1445e+00 - val_loss: -4.1659e+00
Epoch 71/200
 - 14s - loss: -4.1439e+00 - val_loss: -4.1616e+00
Epoch 72/200
 - 14s - loss: -4.1444e+00 - val_loss: -4.1637e+00
Epoch 73/200
 - 14s - loss: -4.1445e+00 - val_loss: -4.1660e+00
Epoch 74/200
 - 14s - loss: -4.1444e+00 - val_loss: -4.1663e+00
Epoch 75/200
 - 14s - loss: -4.1445e+00 - val_loss: -4.1667e+00
Epoch 76/200
 - 14s - loss: -4.1446e+00 - val_loss: -4.1649e+00
Epoch 77/200
 - 14s - loss: -4.1445e+00 - val_loss: -4.1645e+00
Epoch 78/200
 - 14s - loss: -4.1444e+00 - val_loss: -4.1653e+00
Epoch 79/200
 - 14s - loss: -4.1445e+00 - val_loss: -4.1647e+00
Epoch 80/200
 - 14s - loss: -4.1447e+00 - val_loss: -4.1646e+00
Epoch 81/200
 - 14s - loss: -4.1447e+00 - val_loss: -4.1620e+00
2019-12-23 16:15:11,191 [INFO] epoch = 80. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/ae_model_epoch_80.pickle
Epoch 82/200
 - 14s - loss: -4.1446e+00 - val_loss: -4.1637e+00
Epoch 83/200
 - 14s - loss: -4.1448e+00 - val_loss: -4.1667e+00
Epoch 84/200
 - 14s - loss: -4.1448e+00 - val_loss: -4.1643e+00
Epoch 85/200
 - 14s - loss: -4.1448e+00 - val_loss: -4.1618e+00
Epoch 86/200
 - 14s - loss: -4.1448e+00 - val_loss: -4.1616e+00
Epoch 87/200
 - 14s - loss: -4.1448e+00 - val_loss: -4.1602e+00
Epoch 88/200
 - 14s - loss: -4.1449e+00 - val_loss: -4.1591e+00
Epoch 89/200
 - 14s - loss: -4.1449e+00 - val_loss: -4.1663e+00
Epoch 90/200
 - 14s - loss: -4.1448e+00 - val_loss: -4.1621e+00
Epoch 91/200
 - 14s - loss: -4.1445e+00 - val_loss: -4.1646e+00
Epoch 92/200
 - 14s - loss: -4.1449e+00 - val_loss: -4.1659e+00
Epoch 93/200
 - 14s - loss: -4.1450e+00 - val_loss: -4.1652e+00
Epoch 94/200
 - 14s - loss: -4.1448e+00 - val_loss: -4.1661e+00
Epoch 95/200
 - 14s - loss: -4.1449e+00 - val_loss: -4.1625e+00
Epoch 96/200
 - 14s - loss: -4.1447e+00 - val_loss: -4.1620e+00
Epoch 97/200
 - 14s - loss: -4.1451e+00 - val_loss: -4.1602e+00
Epoch 98/200
 - 14s - loss: -4.1450e+00 - val_loss: -4.1651e+00
Epoch 99/200
 - 14s - loss: -4.1448e+00 - val_loss: -4.1661e+00
Epoch 100/200
 - 14s - loss: -4.1449e+00 - val_loss: -4.1647e+00
Epoch 101/200
 - 14s - loss: -4.1449e+00 - val_loss: -4.1646e+00
2019-12-23 16:19:50,889 [INFO] epoch = 100. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/ae_model_epoch_100.pickle
Epoch 102/200
 - 14s - loss: -4.1450e+00 - val_loss: -4.1644e+00
Epoch 103/200
 - 14s - loss: -4.1451e+00 - val_loss: -4.1602e+00
Epoch 104/200
 - 14s - loss: -4.1453e+00 - val_loss: -4.1630e+00
Epoch 105/200
 - 14s - loss: -4.1445e+00 - val_loss: -4.1588e+00
Epoch 106/200
 - 14s - loss: -4.1448e+00 - val_loss: -4.1651e+00
Epoch 107/200
 - 14s - loss: -4.1451e+00 - val_loss: -4.1633e+00
Epoch 108/200
 - 14s - loss: -4.1452e+00 - val_loss: -4.1619e+00
Epoch 109/200
 - 14s - loss: -4.1452e+00 - val_loss: -4.1619e+00
Epoch 110/200
 - 14s - loss: -4.1452e+00 - val_loss: -4.1636e+00
Epoch 111/200
 - 14s - loss: -4.1448e+00 - val_loss: -4.1659e+00
Epoch 112/200
 - 14s - loss: -4.1453e+00 - val_loss: -4.1658e+00
Epoch 113/200
 - 14s - loss: -4.1451e+00 - val_loss: -4.1613e+00
Epoch 114/200
 - 14s - loss: -4.1452e+00 - val_loss: -4.1636e+00
Epoch 115/200
 - 14s - loss: -4.1453e+00 - val_loss: -4.1672e+00
Epoch 116/200
 - 14s - loss: -4.1453e+00 - val_loss: -4.1667e+00
Epoch 117/200
 - 14s - loss: -4.1453e+00 - val_loss: -4.1610e+00
Epoch 118/200
 - 14s - loss: -4.1450e+00 - val_loss: -4.1649e+00
Epoch 119/200
 - 14s - loss: -4.1453e+00 - val_loss: -4.1606e+00
Epoch 120/200
 - 14s - loss: -4.1453e+00 - val_loss: -4.1577e+00
Epoch 121/200
 - 14s - loss: -4.1452e+00 - val_loss: -4.1622e+00
2019-12-23 16:24:30,175 [INFO] epoch = 120. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/ae_model_epoch_120.pickle
Epoch 122/200
 - 14s - loss: -4.1454e+00 - val_loss: -4.1661e+00
Epoch 123/200
 - 14s - loss: -4.1454e+00 - val_loss: -4.1627e+00
Epoch 124/200
 - 14s - loss: -4.1450e+00 - val_loss: -4.1660e+00
Epoch 125/200
 - 14s - loss: -4.1455e+00 - val_loss: -4.1624e+00
Epoch 126/200
 - 14s - loss: -4.1453e+00 - val_loss: -4.1630e+00
Epoch 127/200
 - 14s - loss: -4.1453e+00 - val_loss: -4.1595e+00
Epoch 128/200
 - 14s - loss: -4.1452e+00 - val_loss: -4.1640e+00
Epoch 129/200
 - 14s - loss: -4.1453e+00 - val_loss: -4.1652e+00
Epoch 130/200
 - 14s - loss: -4.1455e+00 - val_loss: -4.1623e+00
Epoch 131/200
 - 14s - loss: -4.1453e+00 - val_loss: -4.1655e+00
Epoch 132/200
 - 14s - loss: -4.1451e+00 - val_loss: -4.1670e+00
Epoch 133/200
 - 14s - loss: -4.1457e+00 - val_loss: -4.1664e+00
Epoch 134/200
 - 14s - loss: -4.1455e+00 - val_loss: -4.1662e+00
Epoch 135/200
 - 14s - loss: -4.1455e+00 - val_loss: -4.1664e+00
Epoch 136/200
 - 14s - loss: -4.1452e+00 - val_loss: -4.1653e+00
Epoch 137/200
 - 14s - loss: -4.1454e+00 - val_loss: -4.1641e+00
Epoch 138/200
 - 14s - loss: -4.1455e+00 - val_loss: -4.1664e+00
Epoch 139/200
 - 14s - loss: -4.1451e+00 - val_loss: -4.1587e+00
Epoch 140/200
 - 14s - loss: -4.1456e+00 - val_loss: -4.1658e+00
Epoch 141/200
 - 14s - loss: -4.1454e+00 - val_loss: -4.1672e+00
2019-12-23 16:29:09,439 [INFO] epoch = 140. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/ae_model_epoch_140.pickle
Epoch 142/200
 - 14s - loss: -4.1451e+00 - val_loss: -4.1673e+00
Epoch 143/200
 - 14s - loss: -4.1455e+00 - val_loss: -4.1589e+00
Epoch 144/200
 - 14s - loss: -4.1457e+00 - val_loss: -4.1573e+00
Epoch 145/200
 - 14s - loss: -4.1456e+00 - val_loss: -4.1593e+00
Epoch 146/200
 - 14s - loss: -4.1456e+00 - val_loss: -4.1646e+00
Epoch 147/200
 - 14s - loss: -4.1457e+00 - val_loss: -4.1656e+00
Epoch 148/200
 - 14s - loss: -4.1455e+00 - val_loss: -4.1639e+00
Epoch 149/200
 - 14s - loss: -4.1452e+00 - val_loss: -4.1581e+00
Epoch 150/200
 - 14s - loss: -4.1456e+00 - val_loss: -4.1665e+00
Epoch 151/200
 - 14s - loss: -4.1456e+00 - val_loss: -4.1656e+00
Epoch 152/200
 - 14s - loss: -4.1454e+00 - val_loss: -4.1591e+00
Epoch 153/200
 - 14s - loss: -4.1456e+00 - val_loss: -4.1656e+00
Epoch 154/200
 - 14s - loss: -4.1457e+00 - val_loss: -4.1587e+00
Epoch 155/200
 - 14s - loss: -4.1454e+00 - val_loss: -4.1643e+00
Epoch 156/200
 - 14s - loss: -4.1456e+00 - val_loss: -4.1631e+00
Epoch 157/200
 - 14s - loss: -4.1456e+00 - val_loss: -4.1642e+00
Epoch 158/200
 - 14s - loss: -4.1455e+00 - val_loss: -4.1658e+00
Epoch 159/200
 - 14s - loss: -4.1456e+00 - val_loss: -4.1624e+00
Epoch 160/200
 - 14s - loss: -4.1458e+00 - val_loss: -4.1587e+00
Epoch 161/200
 - 14s - loss: -4.1454e+00 - val_loss: -4.1657e+00
2019-12-23 16:33:49,184 [INFO] epoch = 160. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/ae_model_epoch_160.pickle
Epoch 162/200
 - 14s - loss: -4.1457e+00 - val_loss: -4.1657e+00
Epoch 163/200
 - 14s - loss: -4.1457e+00 - val_loss: -4.1668e+00
Epoch 164/200
 - 14s - loss: -4.1459e+00 - val_loss: -4.1650e+00
Epoch 165/200
 - 14s - loss: -4.1457e+00 - val_loss: -4.1664e+00
Epoch 166/200
 - 14s - loss: -4.1457e+00 - val_loss: -4.1674e+00
Epoch 167/200
 - 14s - loss: -4.1457e+00 - val_loss: -4.1670e+00
Epoch 168/200
 - 14s - loss: -4.1458e+00 - val_loss: -4.1644e+00
Epoch 169/200
 - 14s - loss: -4.1453e+00 - val_loss: -4.1663e+00
Epoch 170/200
 - 14s - loss: -4.1458e+00 - val_loss: -4.1665e+00
Epoch 171/200
 - 14s - loss: -4.1461e+00 - val_loss: -4.1671e+00
Epoch 172/200
 - 14s - loss: -4.1457e+00 - val_loss: -4.1650e+00
Epoch 173/200
 - 14s - loss: -4.1457e+00 - val_loss: -4.1665e+00
Epoch 174/200
 - 14s - loss: -4.1459e+00 - val_loss: -4.1667e+00
Epoch 175/200
 - 14s - loss: -4.1457e+00 - val_loss: -4.1673e+00
Epoch 176/200
 - 14s - loss: -4.1455e+00 - val_loss: -4.1671e+00
Epoch 177/200
 - 14s - loss: -4.1456e+00 - val_loss: -4.1660e+00
Epoch 178/200
 - 14s - loss: -4.1458e+00 - val_loss: -4.1580e+00
Epoch 179/200
 - 14s - loss: -4.1457e+00 - val_loss: -4.1653e+00
Epoch 180/200
 - 14s - loss: -4.1459e+00 - val_loss: -4.1663e+00
Epoch 181/200
 - 14s - loss: -4.1459e+00 - val_loss: -4.1653e+00
2019-12-23 16:38:28,240 [INFO] epoch = 180. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/ae_model_epoch_180.pickle
Epoch 182/200
 - 14s - loss: -4.1459e+00 - val_loss: -4.1663e+00
Epoch 183/200
 - 14s - loss: -4.1454e+00 - val_loss: -4.1655e+00
Epoch 184/200
 - 14s - loss: -4.1458e+00 - val_loss: -4.1673e+00
Epoch 185/200
 - 14s - loss: -4.1459e+00 - val_loss: -4.1606e+00
Epoch 186/200
 - 14s - loss: -4.1460e+00 - val_loss: -4.1652e+00
Epoch 187/200
 - 14s - loss: -4.1460e+00 - val_loss: -4.1631e+00
Epoch 188/200
 - 14s - loss: -4.1457e+00 - val_loss: -4.1647e+00
Epoch 189/200
 - 14s - loss: -4.1459e+00 - val_loss: -4.1668e+00
Epoch 190/200
 - 14s - loss: -4.1460e+00 - val_loss: -4.1619e+00
Epoch 191/200
 - 14s - loss: -4.1459e+00 - val_loss: -4.1595e+00
Epoch 192/200
 - 14s - loss: -4.1459e+00 - val_loss: -4.1576e+00
Epoch 193/200
 - 14s - loss: -4.1460e+00 - val_loss: -4.1669e+00
Epoch 194/200
 - 14s - loss: -4.1456e+00 - val_loss: -4.1661e+00
Epoch 195/200
 - 14s - loss: -4.1458e+00 - val_loss: -4.1649e+00
Epoch 196/200
 - 14s - loss: -4.1461e+00 - val_loss: -4.1639e+00
Epoch 197/200
 - 14s - loss: -4.1459e+00 - val_loss: -4.1660e+00
Epoch 198/200
 - 14s - loss: -4.1458e+00 - val_loss: -4.1670e+00
Epoch 199/200
 - 14s - loss: -4.1456e+00 - val_loss: -4.1650e+00
Epoch 200/200
 - 14s - loss: -4.1458e+00 - val_loss: -4.1653e+00
2019-12-23 16:42:54,210 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-23 16:43:15,098 [INFO] Last epoch loss evaluation: train_loss = -4.166053, val_loss = -4.167425
2019-12-23 16:43:15,099 [INFO] Training autoencoder complete
2019-12-23 16:43:15,099 [INFO] Encoding data for supervised training
2019-12-23 16:43:32,860 [INFO] Encoding complete
2019-12-23 16:43:32,861 [INFO] Training neural network layers (after autoencoder)
Train on 848342 samples, validate on 565562 samples
Epoch 1/300
 - 7s - loss: 0.0221 - val_loss: 0.0102
 - val_f1: 0.9748
Epoch 2/300
 - 7s - loss: 0.0103 - val_loss: 0.0093
 - val_f1: 0.9766
Epoch 3/300
 - 7s - loss: 0.0091 - val_loss: 0.0083
 - val_f1: 0.9777
Epoch 4/300
 - 7s - loss: 0.0086 - val_loss: 0.0078
 - val_f1: 0.9796
Epoch 5/300
 - 7s - loss: 0.0082 - val_loss: 0.0079
 - val_f1: 0.9785
Epoch 6/300
 - 7s - loss: 0.0079 - val_loss: 0.0087
 - val_f1: 0.9723
Epoch 7/300
 - 7s - loss: 0.0079 - val_loss: 0.0070
 - val_f1: 0.9795
Epoch 8/300
 - 7s - loss: 0.0077 - val_loss: 0.0067
 - val_f1: 0.9846
Epoch 9/300
 - 7s - loss: 0.0075 - val_loss: 0.0063
 - val_f1: 0.9853
Epoch 10/300
 - 7s - loss: 0.0074 - val_loss: 0.0062
 - val_f1: 0.9841
Epoch 11/300
 - 7s - loss: 0.0074 - val_loss: 0.0065
 - val_f1: 0.9840
Epoch 12/300
 - 7s - loss: 0.0072 - val_loss: 0.0063
 - val_f1: 0.9827
Epoch 13/300
 - 7s - loss: 0.0072 - val_loss: 0.0060
 - val_f1: 0.9858
Epoch 14/300
 - 7s - loss: 0.0071 - val_loss: 0.0068
 - val_f1: 0.9826
Epoch 15/300
 - 7s - loss: 0.0071 - val_loss: 0.0072
 - val_f1: 0.9811
Epoch 16/300
 - 7s - loss: 0.0068 - val_loss: 0.0060
 - val_f1: 0.9886
Epoch 17/300
 - 7s - loss: 0.0068 - val_loss: 0.0061
 - val_f1: 0.9870
Epoch 18/300
 - 7s - loss: 0.0071 - val_loss: 0.0062
 - val_f1: 0.9839
Epoch 19/300
 - 7s - loss: 0.0070 - val_loss: 0.0064
 - val_f1: 0.9877
Epoch 20/300
 - 7s - loss: 0.0070 - val_loss: 0.0058
 - val_f1: 0.9853
Epoch 21/300
 - 7s - loss: 0.0068 - val_loss: 0.0057
 - val_f1: 0.9871
Epoch 22/300
 - 7s - loss: 0.0061 - val_loss: 0.0044
 - val_f1: 0.9910
Epoch 23/300
 - 7s - loss: 0.0070 - val_loss: 0.0061
 - val_f1: 0.9871
Epoch 24/300
 - 7s - loss: 0.0069 - val_loss: 0.0062
 - val_f1: 0.9848
Epoch 25/300
 - 7s - loss: 0.0064 - val_loss: 0.0048
 - val_f1: 0.9915
Epoch 26/300
 - 7s - loss: 0.0070 - val_loss: 0.0057
 - val_f1: 0.9871
Epoch 27/300
 - 7s - loss: 0.0069 - val_loss: 0.0071
 - val_f1: 0.9805
Epoch 28/300
 - 7s - loss: 0.0064 - val_loss: 0.0061
 - val_f1: 0.9865
Epoch 29/300
 - 7s - loss: 0.0068 - val_loss: 0.0062
 - val_f1: 0.9852
Epoch 30/300
 - 6s - loss: 0.0064 - val_loss: 0.0066
 - val_f1: 0.9845
Epoch 31/300
 - 7s - loss: 0.0063 - val_loss: 0.0048
2019-12-23 16:50:35,566 [INFO] epoch = 30. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/ann_model_epoch_30.pickle
 - val_f1: 0.9892
Epoch 32/300
 - 7s - loss: 0.0061 - val_loss: 0.0059
 - val_f1: 0.9845
Epoch 33/300
 - 7s - loss: 0.0070 - val_loss: 0.0060
 - val_f1: 0.9856
Epoch 34/300
 - 7s - loss: 0.0066 - val_loss: 0.0059
 - val_f1: 0.9852
Epoch 35/300
 - 7s - loss: 0.0068 - val_loss: 0.0052
 - val_f1: 0.9891
Epoch 36/300
 - 7s - loss: 0.0065 - val_loss: 0.0055
 - val_f1: 0.9886
Epoch 37/300
 - 7s - loss: 0.0062 - val_loss: 0.0053
 - val_f1: 0.9874
Epoch 38/300
 - 7s - loss: 0.0061 - val_loss: 0.0054
 - val_f1: 0.9853
Epoch 39/300
 - 7s - loss: 0.0060 - val_loss: 0.0042
 - val_f1: 0.9933
Epoch 40/300
 - 7s - loss: 0.0057 - val_loss: 0.0056
 - val_f1: 0.9897
Epoch 41/300
 - 7s - loss: 0.0055 - val_loss: 0.0045
 - val_f1: 0.9906
Epoch 42/300
 - 7s - loss: 0.0056 - val_loss: 0.0045
 - val_f1: 0.9922
Epoch 43/300
 - 7s - loss: 0.0056 - val_loss: 0.0044
 - val_f1: 0.9901
Epoch 44/300
 - 7s - loss: 0.0056 - val_loss: 0.0041
 - val_f1: 0.9926
Epoch 45/300
 - 7s - loss: 0.0055 - val_loss: 0.0050
 - val_f1: 0.9896
Epoch 46/300
 - 7s - loss: 0.0057 - val_loss: 0.0041
 - val_f1: 0.9906
Epoch 47/300
 - 7s - loss: 0.0055 - val_loss: 0.0040
 - val_f1: 0.9932
Epoch 48/300
 - 7s - loss: 0.0056 - val_loss: 0.0042
 - val_f1: 0.9918
Epoch 49/300
 - 7s - loss: 0.0052 - val_loss: 0.0039
 - val_f1: 0.9924
Epoch 50/300
 - 7s - loss: 0.0053 - val_loss: 0.0038
 - val_f1: 0.9914
Epoch 51/300
 - 7s - loss: 0.0052 - val_loss: 0.0043
 - val_f1: 0.9902
Epoch 52/300
 - 7s - loss: 0.0053 - val_loss: 0.0040
 - val_f1: 0.9904
Epoch 53/300
 - 7s - loss: 0.0053 - val_loss: 0.0042
 - val_f1: 0.9920
Epoch 54/300
 - 7s - loss: 0.0052 - val_loss: 0.0036
 - val_f1: 0.9930
Epoch 55/300
 - 6s - loss: 0.0053 - val_loss: 0.0047
 - val_f1: 0.9885
Epoch 56/300
 - 7s - loss: 0.0051 - val_loss: 0.0035
 - val_f1: 0.9932
Epoch 57/300
 - 7s - loss: 0.0052 - val_loss: 0.0037
 - val_f1: 0.9922
Epoch 58/300
 - 7s - loss: 0.0052 - val_loss: 0.0038
 - val_f1: 0.9928
Epoch 59/300
 - 7s - loss: 0.0051 - val_loss: 0.0035
 - val_f1: 0.9935
Epoch 60/300
 - 7s - loss: 0.0050 - val_loss: 0.0036
 - val_f1: 0.9934
Epoch 61/300
 - 7s - loss: 0.0049 - val_loss: 0.0041
2019-12-23 16:57:27,826 [INFO] epoch = 60. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/ann_model_epoch_60.pickle
 - val_f1: 0.9907
Epoch 62/300
 - 7s - loss: 0.0050 - val_loss: 0.0036
 - val_f1: 0.9929
Epoch 63/300
 - 7s - loss: 0.0049 - val_loss: 0.0035
 - val_f1: 0.9934
Epoch 64/300
 - 7s - loss: 0.0048 - val_loss: 0.0036
 - val_f1: 0.9916
Epoch 65/300
 - 7s - loss: 0.0048 - val_loss: 0.0095
 - val_f1: 0.9700
Epoch 66/300
 - 7s - loss: 0.0048 - val_loss: 0.0038
 - val_f1: 0.9928
Epoch 67/300
 - 7s - loss: 0.0048 - val_loss: 0.0034
 - val_f1: 0.9936
Epoch 68/300
 - 7s - loss: 0.0047 - val_loss: 0.0039
 - val_f1: 0.9945
Epoch 69/300
 - 7s - loss: 0.0048 - val_loss: 0.0035
 - val_f1: 0.9933
Epoch 70/300
 - 7s - loss: 0.0046 - val_loss: 0.0033
 - val_f1: 0.9941
Epoch 71/300
 - 7s - loss: 0.0045 - val_loss: 0.0038
 - val_f1: 0.9917
Epoch 72/300
 - 7s - loss: 0.0045 - val_loss: 0.0038
 - val_f1: 0.9911
Epoch 73/300
 - 7s - loss: 0.0050 - val_loss: 0.0037
 - val_f1: 0.9933
Epoch 74/300
 - 7s - loss: 0.0049 - val_loss: 0.0041
 - val_f1: 0.9912
Epoch 75/300
 - 7s - loss: 0.0049 - val_loss: 0.0032
 - val_f1: 0.9945
Epoch 76/300
 - 7s - loss: 0.0048 - val_loss: 0.0039
 - val_f1: 0.9914
Epoch 77/300
 - 7s - loss: 0.0057 - val_loss: 0.0037
 - val_f1: 0.9939
Epoch 78/300
 - 7s - loss: 0.0051 - val_loss: 0.0061
 - val_f1: 0.9882
Epoch 79/300
 - 7s - loss: 0.0057 - val_loss: 0.0070
 - val_f1: 0.9811
Epoch 80/300
 - 7s - loss: 0.0060 - val_loss: 0.0057
 - val_f1: 0.9858
Epoch 81/300
 - 7s - loss: 0.0064 - val_loss: 0.0055
 - val_f1: 0.9901
Epoch 82/300
 - 7s - loss: 0.0061 - val_loss: 0.0043
 - val_f1: 0.9922
Epoch 83/300
 - 7s - loss: 0.0059 - val_loss: 0.0055
 - val_f1: 0.9862
Epoch 84/300
 - 7s - loss: 0.0062 - val_loss: 0.0055
 - val_f1: 0.9902
Epoch 85/300
 - 7s - loss: 0.0056 - val_loss: 0.0048
 - val_f1: 0.9906
Epoch 86/300
 - 7s - loss: 0.0053 - val_loss: 0.0043
 - val_f1: 0.9947
Epoch 87/300
 - 7s - loss: 0.0053 - val_loss: 0.0042
 - val_f1: 0.9929
Epoch 88/300
 - 7s - loss: 0.0050 - val_loss: 0.0050
 - val_f1: 0.9911
Epoch 89/300
 - 7s - loss: 0.0048 - val_loss: 0.0037
 - val_f1: 0.9916
Epoch 90/300
 - 7s - loss: 0.0051 - val_loss: 0.0046
 - val_f1: 0.9919
Epoch 91/300
 - 7s - loss: 0.0049 - val_loss: 0.0037
2019-12-23 17:04:21,154 [INFO] epoch = 90. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/ann_model_epoch_90.pickle
 - val_f1: 0.9939
Epoch 92/300
 - 7s - loss: 0.0049 - val_loss: 0.0036
 - val_f1: 0.9915
Epoch 93/300
 - 7s - loss: 0.0048 - val_loss: 0.0036
 - val_f1: 0.9931
Epoch 94/300
 - 7s - loss: 0.0045 - val_loss: 0.0031
 - val_f1: 0.9946
Epoch 95/300
 - 7s - loss: 0.0047 - val_loss: 0.0040
 - val_f1: 0.9924
Epoch 96/300
 - 7s - loss: 0.0048 - val_loss: 0.0036
 - val_f1: 0.9912
Epoch 97/300
 - 7s - loss: 0.0048 - val_loss: 0.0033
 - val_f1: 0.9941
Epoch 98/300
 - 7s - loss: 0.0049 - val_loss: 0.0039
 - val_f1: 0.9917
Epoch 99/300
 - 7s - loss: 0.0051 - val_loss: 0.0035
 - val_f1: 0.9936
Epoch 100/300
 - 7s - loss: 0.0049 - val_loss: 0.0037
 - val_f1: 0.9921
Epoch 101/300
 - 7s - loss: 0.0049 - val_loss: 0.0078
 - val_f1: 0.9789
Epoch 102/300
 - 7s - loss: 0.0047 - val_loss: 0.0038
 - val_f1: 0.9936
Epoch 103/300
 - 7s - loss: 0.0046 - val_loss: 0.0034
 - val_f1: 0.9937
Epoch 104/300
 - 7s - loss: 0.0046 - val_loss: 0.0040
 - val_f1: 0.9896
Epoch 105/300
 - 7s - loss: 0.0047 - val_loss: 0.0062
 - val_f1: 0.9854
Epoch 106/300
 - 7s - loss: 0.0047 - val_loss: 0.0034
 - val_f1: 0.9947
Epoch 107/300
 - 7s - loss: 0.0045 - val_loss: 0.0035
 - val_f1: 0.9932
Epoch 108/300
 - 7s - loss: 0.0045 - val_loss: 0.0033
 - val_f1: 0.9934
Epoch 109/300
 - 7s - loss: 0.0046 - val_loss: 0.0034
 - val_f1: 0.9939
Epoch 110/300
 - 7s - loss: 0.0048 - val_loss: 0.0049
 - val_f1: 0.9889
Epoch 111/300
 - 7s - loss: 0.0044 - val_loss: 0.0036
 - val_f1: 0.9933
Epoch 112/300
 - 7s - loss: 0.0047 - val_loss: 0.0033
 - val_f1: 0.9935
Epoch 113/300
 - 7s - loss: 0.0046 - val_loss: 0.0036
 - val_f1: 0.9923
Epoch 114/300
 - 7s - loss: 0.0044 - val_loss: 0.0033
 - val_f1: 0.9934
Epoch 115/300
 - 7s - loss: 0.0044 - val_loss: 0.0037
 - val_f1: 0.9924
Epoch 116/300
 - 7s - loss: 0.0043 - val_loss: 0.0038
 - val_f1: 0.9928
Epoch 117/300
 - 7s - loss: 0.0044 - val_loss: 0.0031
 - val_f1: 0.9941
Epoch 118/300
 - 7s - loss: 0.0047 - val_loss: 0.0032
 - val_f1: 0.9945
Epoch 119/300
 - 7s - loss: 0.0046 - val_loss: 0.0036
 - val_f1: 0.9921
Epoch 120/300
 - 7s - loss: 0.0045 - val_loss: 0.0038
 - val_f1: 0.9908
Epoch 121/300
 - 7s - loss: 0.0055 - val_loss: 0.0065
2019-12-23 17:11:15,181 [INFO] epoch = 120. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/ann_model_epoch_120.pickle
 - val_f1: 0.9819
Epoch 122/300
 - 7s - loss: 0.0067 - val_loss: 0.0059
 - val_f1: 0.9856
Epoch 123/300
 - 7s - loss: 0.0059 - val_loss: 0.0038
 - val_f1: 0.9944
Epoch 124/300
 - 7s - loss: 0.0053 - val_loss: 0.0039
 - val_f1: 0.9917
Epoch 125/300
 - 7s - loss: 0.0048 - val_loss: 0.0032
 - val_f1: 0.9941
Epoch 126/300
 - 7s - loss: 0.0064 - val_loss: 0.0058
 - val_f1: 0.9884
Epoch 127/300
 - 7s - loss: 0.0066 - val_loss: 0.0055
 - val_f1: 0.9855
Epoch 128/300
 - 7s - loss: 0.0064 - val_loss: 0.0055
 - val_f1: 0.9864
Epoch 129/300
 - 7s - loss: 0.0061 - val_loss: 0.0051
 - val_f1: 0.9880
Epoch 130/300
 - 7s - loss: 0.0057 - val_loss: 0.0050
 - val_f1: 0.9940
Epoch 131/300
 - 7s - loss: 0.0052 - val_loss: 0.0037
 - val_f1: 0.9931
Epoch 132/300
 - 7s - loss: 0.0053 - val_loss: 0.0065
 - val_f1: 0.9857
Epoch 133/300
 - 7s - loss: 0.0052 - val_loss: 0.0037
 - val_f1: 0.9927
Epoch 134/300
 - 7s - loss: 0.0049 - val_loss: 0.0037
 - val_f1: 0.9930
Epoch 135/300
 - 7s - loss: 0.0048 - val_loss: 0.0039
 - val_f1: 0.9928
Epoch 136/300
 - 7s - loss: 0.0048 - val_loss: 0.0037
 - val_f1: 0.9914
Epoch 137/300
 - 7s - loss: 0.0044 - val_loss: 0.0034
 - val_f1: 0.9935
Epoch 138/300
 - 7s - loss: 0.0046 - val_loss: 0.0046
 - val_f1: 0.9923
Epoch 139/300
 - 7s - loss: 0.0043 - val_loss: 0.0036
 - val_f1: 0.9937
Epoch 140/300
 - 7s - loss: 0.0046 - val_loss: 0.0034
 - val_f1: 0.9934
Epoch 141/300
 - 7s - loss: 0.0046 - val_loss: 0.0034
 - val_f1: 0.9923
Epoch 142/300
 - 7s - loss: 0.0046 - val_loss: 0.0032
 - val_f1: 0.9939
Epoch 143/300
 - 7s - loss: 0.0044 - val_loss: 0.0032
 - val_f1: 0.9934
Epoch 144/300
 - 7s - loss: 0.0044 - val_loss: 0.0032
2019-12-23 17:16:39,149 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-23 17:16:59,111 [INFO] Last epoch loss evaluation: train_loss = 0.002988, val_loss = 0.003100
2019-12-23 17:16:59,146 [INFO] Training complete. time_to_train = 4858.06 sec, 80.97 min
2019-12-23 17:16:59,154 [INFO] Model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/best_model.pickle
2019-12-23 17:16:59,352 [INFO] Plot saved to: results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/training_error_history.png
2019-12-23 17:16:59,528 [INFO] Plot saved to: results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/training_f1_history.png
2019-12-23 17:16:59,528 [INFO] Making predictions on training, validation, testing data
2019-12-23 17:18:08,557 [INFO] Evaluating predictions (results)
/home/sunanda/test/ml_env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2019-12-23 17:18:18,707 [INFO] Dataset: Testing. Classification report below
2019-12-23 17:18:18,708 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       1.00      1.00      1.00    454265
                   Bot       1.00      0.36      0.53       391
                  DDoS       1.00      1.00      1.00     25605
         DoS GoldenEye       0.99      0.98      0.99      2058
              DoS Hulk       0.98      0.99      0.98     46025
      DoS Slowhttptest       0.89      0.99      0.93      1100
         DoS slowloris       0.99      0.98      0.98      1159
           FTP-Patator       0.99      0.98      0.99      1587
              PortScan       0.99      1.00      0.99     31761
           SSH-Patator       0.96      0.98      0.97      1179
Web Attack Brute Force       1.00      0.10      0.19       302
        Web Attack XSS       0.00      0.00      0.00       130

             micro avg       0.99      0.99      0.99    565562
             macro avg       0.90      0.78      0.80    565562
          weighted avg       0.99      0.99      0.99    565562

2019-12-23 17:18:18,708 [INFO] Overall accuracy (micro avg): 0.9944656819234673
/home/sunanda/test/ids_experiments/utility.py:307: RuntimeWarning: invalid value encountered in true_divide
  PPV = TP / (TP + FP)    # Precision or positive predictive value
/home/sunanda/test/ids_experiments/utility.py:311: RuntimeWarning: invalid value encountered in true_divide
  FDR = FP / (TP + FP)    # False discovery rate
2019-12-23 17:18:30,260 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9945         0.9945                       0.9945                0.0005                   0.0055  0.9945
1     Macro avg        0.9991         0.8985                       0.7784                0.0014                   0.2216  0.7953
2  Weighted avg        0.9954         0.9943                       0.9945                0.0115                   0.0055  0.9941
2019-12-23 17:18:40,574 [INFO] Dataset: Validation. Classification report below
2019-12-23 17:18:40,574 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       1.00      1.00      1.00    454264
                   Bot       1.00      0.33      0.50       391
                  DDoS       1.00      1.00      1.00     25605
         DoS GoldenEye       0.99      0.98      0.98      2059
              DoS Hulk       0.98      0.99      0.98     46025
      DoS Slowhttptest       0.89      0.98      0.93      1099
         DoS slowloris       0.99      0.97      0.98      1159
           FTP-Patator       0.99      0.98      0.99      1587
              PortScan       0.99      1.00      0.99     31761
           SSH-Patator       0.97      0.97      0.97      1180
Web Attack Brute Force       0.95      0.07      0.12       301
        Web Attack XSS       1.00      0.03      0.06       131

             micro avg       0.99      0.99      0.99    565562
             macro avg       0.98      0.77      0.79    565562
          weighted avg       0.99      0.99      0.99    565562

2019-12-23 17:18:40,574 [INFO] Overall accuracy (micro avg): 0.9946089022954159
2019-12-23 17:18:52,270 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9946         0.9946                       0.9946                0.0005                   0.0054  0.9946
1     Macro avg        0.9991         0.9785                       0.7744                0.0014                   0.2256  0.7921
2  Weighted avg        0.9955         0.9946                       0.9946                0.0113                   0.0054  0.9942
2019-12-23 17:19:26,308 [INFO] Dataset: Training. Classification report below
2019-12-23 17:19:26,308 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       1.00      1.00      1.00   1362791
                   Bot       1.00      0.36      0.53      1174
                  DDoS       1.00      1.00      1.00     76815
         DoS GoldenEye       0.99      0.98      0.99      6176
              DoS Hulk       0.98      0.99      0.98    138074
      DoS Slowhttptest       0.90      0.98      0.94      3300
         DoS slowloris       0.98      0.98      0.98      3478
           FTP-Patator       0.99      0.98      0.99      4761
              PortScan       0.99      1.00      0.99     95282
           SSH-Patator       0.97      0.97      0.97      3538
Web Attack Brute Force       0.99      0.09      0.16       904
        Web Attack XSS       1.00      0.03      0.06       391

             micro avg       0.99      0.99      0.99   1696684
             macro avg       0.98      0.78      0.80   1696684
          weighted avg       0.99      0.99      0.99   1696684

2019-12-23 17:19:26,308 [INFO] Overall accuracy (micro avg): 0.9946607618154
2019-12-23 17:20:04,989 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9947         0.9947                       0.9947                0.0005                   0.0053  0.9947
1     Macro avg        0.9991         0.9831                       0.7790                0.0014                   0.2210  0.7987
2  Weighted avg        0.9955         0.9947                       0.9947                0.0113                   0.0053  0.9943
2019-12-23 17:20:05,041 [INFO] Results saved to: results_additional_exps/semi_sup_perf_ids17_ae_ann_rep1/semi_sup_perf_ids17_ae_ann_rep1_results.xlsx
2019-12-23 17:20:05,045 [INFO] ================= Finished running experiment no. 1 ================= 

2019-12-23 17:20:05,112 [INFO] Created directory: results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2
2019-12-23 17:20:05,112 [INFO] Initialized logging. log_filename = results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/run_log.log
2019-12-23 17:20:05,112 [INFO] ================= Running experiment no. 2  ================= 

2019-12-23 17:20:05,112 [INFO] Experiment parameters given below
2019-12-23 17:20:05,112 [INFO] 
{'experiment_num': 2, 'results_dir': 'results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2', 'model_type': 'classifier', 'model': 'ae_ann', 'normal_label': 'BENIGN', 'split_random_seed': 42, 'unsupervised_ratio': 0.5, 'ae_encoder_units': [64], 'ae_encoder_activations': ['relu'], 'ae_encoder_dropout_rates': [0.2], 'ae_encoder_l1_param': -1, 'ae_decoder_units': '', 'ae_decoder_activations': '', 'ae_decoder_dropout_rates': '', 'output_activation': 'sigmoid', 'loss_function': 'binary_crossentropy', 'ae_epochs': 200, 'ann_layer_units': [64], 'ann_layer_activations': ['relu'], 'ann_layer_dropout_rates': [0.2], 'ann_epochs': 300, 'early_stop_patience': 50, 'batch_size': 256, 'dataset_dir': '../Datasets/full_datasets/ids2017', 'description': 'semi_sup_perf_ids17_ae_ann_rep2'}
2019-12-23 17:20:05,113 [INFO] Created tensorboard log directory: results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/tf_logs_run_2019_12_23-17_20_05
2019-12-23 17:20:05,113 [INFO] Loading datsets from: ../Datasets/full_datasets/ids2017
2019-12-23 17:20:05,113 [INFO] Reading X, y files
2019-12-23 17:20:05,113 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_train.h5
2019-12-23 17:20:09,138 [INFO] Reading complete. time_to_read=4.03 seconds
2019-12-23 17:20:09,138 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_val.h5
2019-12-23 17:20:10,524 [INFO] Reading complete. time_to_read=1.39 seconds
2019-12-23 17:20:10,524 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_test.h5
2019-12-23 17:20:11,911 [INFO] Reading complete. time_to_read=1.39 seconds
2019-12-23 17:20:11,911 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_train.h5
2019-12-23 17:20:12,115 [INFO] Reading complete. time_to_read=0.20 seconds
2019-12-23 17:20:12,115 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_val.h5
2019-12-23 17:20:12,184 [INFO] Reading complete. time_to_read=0.07 seconds
2019-12-23 17:20:12,184 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_test.h5
2019-12-23 17:20:12,251 [INFO] Reading complete. time_to_read=0.07 seconds
2019-12-23 17:20:15,396 [INFO] Initializing model
2019-12-23 17:20:15,508 [INFO] _________________________________________________________________
2019-12-23 17:20:15,508 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-23 17:20:15,508 [INFO] =================================================================
2019-12-23 17:20:15,508 [INFO] dense_17 (Dense)             (None, 64)                5056      
2019-12-23 17:20:15,509 [INFO] _________________________________________________________________
2019-12-23 17:20:15,509 [INFO] batch_normalization_9 (Batch (None, 64)                256       
2019-12-23 17:20:15,509 [INFO] _________________________________________________________________
2019-12-23 17:20:15,509 [INFO] dropout_9 (Dropout)          (None, 64)                0         
2019-12-23 17:20:15,509 [INFO] _________________________________________________________________
2019-12-23 17:20:15,509 [INFO] dense_18 (Dense)             (None, 78)                5070      
2019-12-23 17:20:15,509 [INFO] =================================================================
2019-12-23 17:20:15,509 [INFO] Total params: 10,382
2019-12-23 17:20:15,509 [INFO] Trainable params: 10,254
2019-12-23 17:20:15,509 [INFO] Non-trainable params: 128
2019-12-23 17:20:15,509 [INFO] _________________________________________________________________
2019-12-23 17:20:15,615 [INFO] _________________________________________________________________
2019-12-23 17:20:15,615 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-23 17:20:15,615 [INFO] =================================================================
2019-12-23 17:20:15,616 [INFO] dense_19 (Dense)             (None, 64)                4160      
2019-12-23 17:20:15,616 [INFO] _________________________________________________________________
2019-12-23 17:20:15,616 [INFO] batch_normalization_10 (Batc (None, 64)                256       
2019-12-23 17:20:15,616 [INFO] _________________________________________________________________
2019-12-23 17:20:15,616 [INFO] dropout_10 (Dropout)         (None, 64)                0         
2019-12-23 17:20:15,616 [INFO] _________________________________________________________________
2019-12-23 17:20:15,616 [INFO] dense_20 (Dense)             (None, 12)                780       
2019-12-23 17:20:15,616 [INFO] =================================================================
2019-12-23 17:20:15,616 [INFO] Total params: 5,196
2019-12-23 17:20:15,617 [INFO] Trainable params: 5,068
2019-12-23 17:20:15,617 [INFO] Non-trainable params: 128
2019-12-23 17:20:15,617 [INFO] _________________________________________________________________
2019-12-23 17:20:15,617 [INFO] Training model
2019-12-23 17:20:15,617 [INFO] Splitting train set into 2 sets (unsupervised, supervised), random_seed = 42
2019-12-23 17:20:32,329 [INFO] Split sizes (instances). total = 1696684, unsupervised = 848342, supervised = 848342, unsupervised dataset hash = b75e78959164c90d19a336ef2d2a5a10f094d2bb
2019-12-23 17:20:32,329 [INFO] Training autoencoder
 - val_f1: 0.9920
Epoch 00144: early stopping
Train on 848342 samples, validate on 565562 samples
Epoch 1/200
 - 15s - loss: -3.6516e+00 - val_loss: -4.1386e+00
Epoch 2/200
 - 14s - loss: -4.1040e+00 - val_loss: -4.1493e+00
Epoch 3/200
 - 14s - loss: -4.1182e+00 - val_loss: -4.1539e+00
Epoch 4/200
 - 14s - loss: -4.1254e+00 - val_loss: -4.1574e+00
Epoch 5/200
 - 14s - loss: -4.1293e+00 - val_loss: -4.1590e+00
Epoch 6/200
 - 14s - loss: -4.1317e+00 - val_loss: -4.1607e+00
Epoch 7/200
 - 14s - loss: -4.1334e+00 - val_loss: -4.1609e+00
Epoch 8/200
 - 14s - loss: -4.1351e+00 - val_loss: -4.1608e+00
Epoch 9/200
 - 14s - loss: -4.1361e+00 - val_loss: -4.1583e+00
Epoch 10/200
 - 14s - loss: -4.1376e+00 - val_loss: -4.1623e+00
Epoch 11/200
 - 14s - loss: -4.1376e+00 - val_loss: -4.1628e+00
Epoch 12/200
 - 14s - loss: -4.1385e+00 - val_loss: -4.1630e+00
Epoch 13/200
 - 14s - loss: -4.1391e+00 - val_loss: -4.1626e+00
Epoch 14/200
 - 14s - loss: -4.1391e+00 - val_loss: -4.1640e+00
Epoch 15/200
 - 14s - loss: -4.1399e+00 - val_loss: -4.1637e+00
Epoch 16/200
 - 14s - loss: -4.1398e+00 - val_loss: -4.1641e+00
Epoch 17/200
 - 14s - loss: -4.1405e+00 - val_loss: -4.1648e+00
Epoch 18/200
 - 14s - loss: -4.1409e+00 - val_loss: -4.1647e+00
Epoch 19/200
 - 14s - loss: -4.1408e+00 - val_loss: -4.1644e+00
Epoch 20/200
 - 14s - loss: -4.1412e+00 - val_loss: -4.1648e+00
Epoch 21/200
 - 14s - loss: -4.1415e+00 - val_loss: -4.1642e+00
2019-12-23 17:25:33,949 [INFO] epoch = 20. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/ae_model_epoch_20.pickle
Epoch 22/200
 - 14s - loss: -4.1418e+00 - val_loss: -4.1640e+00
Epoch 23/200
 - 14s - loss: -4.1416e+00 - val_loss: -4.1619e+00
Epoch 24/200
 - 14s - loss: -4.1419e+00 - val_loss: -4.1647e+00
Epoch 25/200
 - 14s - loss: -4.1423e+00 - val_loss: -4.1636e+00
Epoch 26/200
 - 14s - loss: -4.1422e+00 - val_loss: -4.1651e+00
Epoch 27/200
 - 14s - loss: -4.1425e+00 - val_loss: -4.1654e+00
Epoch 28/200
 - 14s - loss: -4.1425e+00 - val_loss: -4.1635e+00
Epoch 29/200
 - 14s - loss: -4.1425e+00 - val_loss: -4.1654e+00
Epoch 30/200
 - 14s - loss: -4.1428e+00 - val_loss: -4.1660e+00
Epoch 31/200
 - 14s - loss: -4.1431e+00 - val_loss: -4.1653e+00
Epoch 32/200
 - 14s - loss: -4.1430e+00 - val_loss: -4.1640e+00
Epoch 33/200
 - 14s - loss: -4.1430e+00 - val_loss: -4.1655e+00
Epoch 34/200
 - 14s - loss: -4.1431e+00 - val_loss: -4.1635e+00
Epoch 35/200
 - 14s - loss: -4.1430e+00 - val_loss: -4.1647e+00
Epoch 36/200
 - 14s - loss: -4.1433e+00 - val_loss: -4.1658e+00
Epoch 37/200
 - 14s - loss: -4.1434e+00 - val_loss: -4.1661e+00
Epoch 38/200
 - 14s - loss: -4.1432e+00 - val_loss: -4.1648e+00
Epoch 39/200
 - 14s - loss: -4.1437e+00 - val_loss: -4.1654e+00
Epoch 40/200
 - 14s - loss: -4.1436e+00 - val_loss: -4.1646e+00
Epoch 41/200
 - 14s - loss: -4.1436e+00 - val_loss: -4.1654e+00
2019-12-23 17:30:18,790 [INFO] epoch = 40. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/ae_model_epoch_40.pickle
Epoch 42/200
 - 14s - loss: -4.1434e+00 - val_loss: -4.1658e+00
Epoch 43/200
 - 14s - loss: -4.1437e+00 - val_loss: -4.1655e+00
Epoch 44/200
 - 14s - loss: -4.1434e+00 - val_loss: -4.1663e+00
Epoch 45/200
 - 14s - loss: -4.1438e+00 - val_loss: -4.1656e+00
Epoch 46/200
 - 14s - loss: -4.1438e+00 - val_loss: -4.1652e+00
Epoch 47/200
 - 14s - loss: -4.1440e+00 - val_loss: -4.1651e+00
Epoch 48/200
 - 14s - loss: -4.1441e+00 - val_loss: -4.1658e+00
Epoch 49/200
 - 14s - loss: -4.1442e+00 - val_loss: -4.1653e+00
Epoch 50/200
 - 14s - loss: -4.1443e+00 - val_loss: -4.1662e+00
Epoch 51/200
 - 14s - loss: -4.1442e+00 - val_loss: -4.1660e+00
Epoch 52/200
 - 14s - loss: -4.1442e+00 - val_loss: -4.1650e+00
Epoch 53/200
 - 14s - loss: -4.1442e+00 - val_loss: -4.1653e+00
Epoch 54/200
 - 14s - loss: -4.1443e+00 - val_loss: -4.1657e+00
Epoch 55/200
 - 14s - loss: -4.1442e+00 - val_loss: -4.1661e+00
Epoch 56/200
 - 14s - loss: -4.1443e+00 - val_loss: -4.1649e+00
Epoch 57/200
 - 14s - loss: -4.1446e+00 - val_loss: -4.1664e+00
Epoch 58/200
 - 14s - loss: -4.1446e+00 - val_loss: -4.1663e+00
Epoch 59/200
 - 14s - loss: -4.1448e+00 - val_loss: -4.1661e+00
Epoch 60/200
 - 14s - loss: -4.1444e+00 - val_loss: -4.1643e+00
Epoch 61/200
 - 14s - loss: -4.1446e+00 - val_loss: -4.1664e+00
2019-12-23 17:35:03,955 [INFO] epoch = 60. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/ae_model_epoch_60.pickle
Epoch 62/200
 - 14s - loss: -4.1445e+00 - val_loss: -4.1632e+00
Epoch 63/200
 - 14s - loss: -4.1446e+00 - val_loss: -4.1645e+00
Epoch 64/200
 - 14s - loss: -4.1446e+00 - val_loss: -4.1657e+00
Epoch 65/200
 - 14s - loss: -4.1449e+00 - val_loss: -4.1665e+00
Epoch 66/200
 - 14s - loss: -4.1448e+00 - val_loss: -4.1662e+00
Epoch 67/200
 - 14s - loss: -4.1446e+00 - val_loss: -4.1665e+00
Epoch 68/200
 - 14s - loss: -4.1445e+00 - val_loss: -4.1667e+00
Epoch 69/200
 - 14s - loss: -4.1447e+00 - val_loss: -4.1668e+00
Epoch 70/200
 - 14s - loss: -4.1446e+00 - val_loss: -4.1668e+00
Epoch 71/200
 - 14s - loss: -4.1450e+00 - val_loss: -4.1664e+00
Epoch 72/200
 - 14s - loss: -4.1450e+00 - val_loss: -4.1657e+00
Epoch 73/200
 - 14s - loss: -4.1448e+00 - val_loss: -4.1668e+00
Epoch 74/200
 - 14s - loss: -4.1451e+00 - val_loss: -4.1667e+00
Epoch 75/200
 - 14s - loss: -4.1449e+00 - val_loss: -4.1651e+00
Epoch 76/200
 - 14s - loss: -4.1450e+00 - val_loss: -4.1661e+00
Epoch 77/200
 - 14s - loss: -4.1449e+00 - val_loss: -4.1669e+00
Epoch 78/200
 - 14s - loss: -4.1449e+00 - val_loss: -4.1667e+00
Epoch 79/200
 - 14s - loss: -4.1450e+00 - val_loss: -4.1666e+00
Epoch 80/200
 - 14s - loss: -4.1452e+00 - val_loss: -4.1656e+00
Epoch 81/200
 - 14s - loss: -4.1453e+00 - val_loss: -4.1651e+00
2019-12-23 17:39:49,244 [INFO] epoch = 80. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/ae_model_epoch_80.pickle
Epoch 82/200
 - 14s - loss: -4.1450e+00 - val_loss: -4.1642e+00
Epoch 83/200
 - 14s - loss: -4.1451e+00 - val_loss: -4.1651e+00
Epoch 84/200
 - 14s - loss: -4.1451e+00 - val_loss: -4.1607e+00
Epoch 85/200
 - 14s - loss: -4.1449e+00 - val_loss: -4.1613e+00
Epoch 86/200
 - 14s - loss: -4.1454e+00 - val_loss: -4.1660e+00
Epoch 87/200
 - 14s - loss: -4.1446e+00 - val_loss: -4.1641e+00
Epoch 88/200
 - 14s - loss: -4.1451e+00 - val_loss: -4.1638e+00
Epoch 89/200
 - 14s - loss: -4.1452e+00 - val_loss: -4.1657e+00
Epoch 90/200
 - 14s - loss: -4.1446e+00 - val_loss: -4.1659e+00
Epoch 91/200
 - 14s - loss: -4.1452e+00 - val_loss: -4.1667e+00
Epoch 92/200
 - 14s - loss: -4.1454e+00 - val_loss: -4.1665e+00
Epoch 93/200
 - 14s - loss: -4.1453e+00 - val_loss: -4.1605e+00
Epoch 94/200
 - 14s - loss: -4.1453e+00 - val_loss: -4.1623e+00
Epoch 95/200
 - 14s - loss: -4.1454e+00 - val_loss: -4.1661e+00
Epoch 96/200
 - 14s - loss: -4.1454e+00 - val_loss: -4.1651e+00
Epoch 97/200
 - 14s - loss: -4.1455e+00 - val_loss: -4.1670e+00
Epoch 98/200
 - 14s - loss: -4.1453e+00 - val_loss: -4.1661e+00
Epoch 99/200
 - 14s - loss: -4.1453e+00 - val_loss: -4.1656e+00
Epoch 100/200
 - 14s - loss: -4.1454e+00 - val_loss: -4.1666e+00
Epoch 101/200
 - 14s - loss: -4.1454e+00 - val_loss: -4.1659e+00
2019-12-23 17:44:33,608 [INFO] epoch = 100. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/ae_model_epoch_100.pickle
Epoch 102/200
 - 14s - loss: -4.1455e+00 - val_loss: -4.1670e+00
Epoch 103/200
 - 14s - loss: -4.1455e+00 - val_loss: -4.1608e+00
Epoch 104/200
 - 14s - loss: -4.1453e+00 - val_loss: -4.1665e+00
Epoch 105/200
 - 14s - loss: -4.1452e+00 - val_loss: -4.1658e+00
Epoch 106/200
 - 14s - loss: -4.1455e+00 - val_loss: -4.1668e+00
Epoch 107/200
 - 14s - loss: -4.1448e+00 - val_loss: -4.1655e+00
Epoch 108/200
 - 14s - loss: -4.1455e+00 - val_loss: -4.1672e+00
Epoch 109/200
 - 14s - loss: -4.1454e+00 - val_loss: -4.1670e+00
Epoch 110/200
 - 14s - loss: -4.1454e+00 - val_loss: -4.1602e+00
Epoch 111/200
 - 14s - loss: -4.1454e+00 - val_loss: -4.1647e+00
Epoch 112/200
 - 14s - loss: -4.1451e+00 - val_loss: -4.1666e+00
Epoch 113/200
 - 14s - loss: -4.1458e+00 - val_loss: -4.1617e+00
Epoch 114/200
 - 14s - loss: -4.1456e+00 - val_loss: -4.1657e+00
Epoch 115/200
 - 14s - loss: -4.1451e+00 - val_loss: -4.1665e+00
Epoch 116/200
 - 14s - loss: -4.1452e+00 - val_loss: -4.1660e+00
Epoch 117/200
 - 14s - loss: -4.1456e+00 - val_loss: -4.1661e+00
Epoch 118/200
 - 14s - loss: -4.1456e+00 - val_loss: -4.1638e+00
Epoch 119/200
 - 14s - loss: -4.1456e+00 - val_loss: -4.1666e+00
Epoch 120/200
 - 14s - loss: -4.1457e+00 - val_loss: -4.1662e+00
Epoch 121/200
 - 14s - loss: -4.1452e+00 - val_loss: -4.1660e+00
2019-12-23 17:49:18,043 [INFO] epoch = 120. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/ae_model_epoch_120.pickle
Epoch 122/200
 - 14s - loss: -4.1456e+00 - val_loss: -4.1659e+00
Epoch 123/200
 - 14s - loss: -4.1452e+00 - val_loss: -4.1660e+00
Epoch 124/200
 - 14s - loss: -4.1457e+00 - val_loss: -4.1667e+00
Epoch 125/200
 - 14s - loss: -4.1458e+00 - val_loss: -4.1649e+00
Epoch 126/200
 - 14s - loss: -4.1457e+00 - val_loss: -4.1666e+00
Epoch 127/200
 - 14s - loss: -4.1453e+00 - val_loss: -4.1664e+00
Epoch 128/200
 - 14s - loss: -4.1456e+00 - val_loss: -4.1647e+00
Epoch 129/200
 - 14s - loss: -4.1458e+00 - val_loss: -4.1664e+00
Epoch 130/200
 - 14s - loss: -4.1455e+00 - val_loss: -4.1658e+00
Epoch 131/200
 - 14s - loss: -4.1457e+00 - val_loss: -4.1664e+00
Epoch 132/200
 - 14s - loss: -4.1457e+00 - val_loss: -4.1666e+00
Epoch 133/200
 - 14s - loss: -4.1457e+00 - val_loss: -4.1663e+00
Epoch 134/200
 - 14s - loss: -4.1456e+00 - val_loss: -4.1655e+00
Epoch 135/200
 - 14s - loss: -4.1458e+00 - val_loss: -4.1665e+00
Epoch 136/200
 - 14s - loss: -4.1456e+00 - val_loss: -4.1658e+00
Epoch 137/200
 - 14s - loss: -4.1458e+00 - val_loss: -4.1669e+00
Epoch 138/200
 - 14s - loss: -4.1457e+00 - val_loss: -4.1662e+00
Epoch 139/200
 - 14s - loss: -4.1459e+00 - val_loss: -4.1667e+00
Epoch 140/200
 - 14s - loss: -4.1457e+00 - val_loss: -4.1648e+00
Epoch 141/200
 - 14s - loss: -4.1458e+00 - val_loss: -4.1606e+00
2019-12-23 17:54:02,642 [INFO] epoch = 140. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/ae_model_epoch_140.pickle
Epoch 142/200
 - 14s - loss: -4.1459e+00 - val_loss: -4.1672e+00
Epoch 143/200
 - 14s - loss: -4.1456e+00 - val_loss: -4.1652e+00
Epoch 144/200
 - 14s - loss: -4.1459e+00 - val_loss: -4.1655e+00
Epoch 145/200
 - 14s - loss: -4.1457e+00 - val_loss: -4.1666e+00
Epoch 146/200
 - 14s - loss: -4.1458e+00 - val_loss: -4.1662e+00
Epoch 147/200
 - 14s - loss: -4.1458e+00 - val_loss: -4.1659e+00
Epoch 148/200
 - 14s - loss: -4.1457e+00 - val_loss: -4.1644e+00
Epoch 149/200
 - 14s - loss: -4.1457e+00 - val_loss: -4.1644e+00
Epoch 150/200
 - 14s - loss: -4.1455e+00 - val_loss: -4.1652e+00
Epoch 151/200
 - 14s - loss: -4.1458e+00 - val_loss: -4.1662e+00
Epoch 152/200
 - 14s - loss: -4.1459e+00 - val_loss: -4.1658e+00
Epoch 153/200
 - 14s - loss: -4.1459e+00 - val_loss: -4.1661e+00
Epoch 154/200
 - 14s - loss: -4.1458e+00 - val_loss: -4.1667e+00
Epoch 155/200
 - 14s - loss: -4.1458e+00 - val_loss: -4.1667e+00
Epoch 156/200
 - 14s - loss: -4.1459e+00 - val_loss: -4.1669e+00
Epoch 157/200
 - 14s - loss: -4.1459e+00 - val_loss: -4.1662e+00
Epoch 158/200
 - 14s - loss: -4.1455e+00 - val_loss: -4.1670e+00
Epoch 159/200
 - 14s - loss: -4.1459e+00 - val_loss: -4.1664e+00
Epoch 160/200
 - 14s - loss: -4.1461e+00 - val_loss: -4.1619e+00
Epoch 161/200
 - 14s - loss: -4.1459e+00 - val_loss: -4.1647e+00
2019-12-23 17:58:47,786 [INFO] epoch = 160. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/ae_model_epoch_160.pickle
Epoch 162/200
 - 14s - loss: -4.1461e+00 - val_loss: -4.1667e+00
Epoch 163/200
 - 14s - loss: -4.1457e+00 - val_loss: -4.1667e+00
Epoch 164/200
 - 14s - loss: -4.1460e+00 - val_loss: -4.1669e+00
Epoch 165/200
 - 14s - loss: -4.1462e+00 - val_loss: -4.1664e+00
Epoch 166/200
 - 14s - loss: -4.1458e+00 - val_loss: -4.1631e+00
Epoch 167/200
 - 14s - loss: -4.1461e+00 - val_loss: -4.1665e+00
Epoch 168/200
 - 14s - loss: -4.1459e+00 - val_loss: -4.1668e+00
Epoch 169/200
 - 14s - loss: -4.1459e+00 - val_loss: -4.1664e+00
Epoch 170/200
 - 14s - loss: -4.1458e+00 - val_loss: -4.1662e+00
Epoch 171/200
 - 14s - loss: -4.1461e+00 - val_loss: -4.1659e+00
Epoch 172/200
 - 14s - loss: -4.1461e+00 - val_loss: -4.1660e+00
Epoch 173/200
 - 14s - loss: -4.1460e+00 - val_loss: -4.1668e+00
Epoch 174/200
 - 14s - loss: -4.1461e+00 - val_loss: -4.1627e+00
Epoch 175/200
 - 14s - loss: -4.1461e+00 - val_loss: -4.1646e+00
Epoch 176/200
 - 14s - loss: -4.1461e+00 - val_loss: -4.1631e+00
Epoch 177/200
 - 14s - loss: -4.1460e+00 - val_loss: -4.1632e+00
Epoch 178/200
 - 14s - loss: -4.1463e+00 - val_loss: -4.1601e+00
Epoch 179/200
 - 14s - loss: -4.1460e+00 - val_loss: -4.1634e+00
Epoch 180/200
 - 14s - loss: -4.1460e+00 - val_loss: -4.1673e+00
Epoch 181/200
 - 14s - loss: -4.1459e+00 - val_loss: -4.1645e+00
2019-12-23 18:03:33,213 [INFO] epoch = 180. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/ae_model_epoch_180.pickle
Epoch 182/200
 - 14s - loss: -4.1462e+00 - val_loss: -4.1635e+00
Epoch 183/200
 - 14s - loss: -4.1462e+00 - val_loss: -4.1644e+00
Epoch 184/200
 - 14s - loss: -4.1461e+00 - val_loss: -4.1667e+00
Epoch 185/200
 - 14s - loss: -4.1461e+00 - val_loss: -4.1654e+00
Epoch 186/200
 - 14s - loss: -4.1462e+00 - val_loss: -4.1671e+00
Epoch 187/200
 - 14s - loss: -4.1460e+00 - val_loss: -4.1670e+00
Epoch 188/200
 - 14s - loss: -4.1461e+00 - val_loss: -4.1664e+00
Epoch 189/200
 - 14s - loss: -4.1457e+00 - val_loss: -4.1674e+00
Epoch 190/200
 - 14s - loss: -4.1460e+00 - val_loss: -4.1664e+00
Epoch 191/200
 - 14s - loss: -4.1460e+00 - val_loss: -4.1658e+00
Epoch 192/200
 - 14s - loss: -4.1459e+00 - val_loss: -4.1670e+00
Epoch 193/200
 - 14s - loss: -4.1460e+00 - val_loss: -4.1673e+00
Epoch 194/200
 - 14s - loss: -4.1459e+00 - val_loss: -4.1675e+00
Epoch 195/200
 - 14s - loss: -4.1462e+00 - val_loss: -4.1623e+00
Epoch 196/200
 - 14s - loss: -4.1462e+00 - val_loss: -4.1624e+00
Epoch 197/200
 - 14s - loss: -4.1463e+00 - val_loss: -4.1636e+00
Epoch 198/200
 - 14s - loss: -4.1462e+00 - val_loss: -4.1666e+00
Epoch 199/200
 - 14s - loss: -4.1461e+00 - val_loss: -4.1662e+00
Epoch 200/200
 - 14s - loss: -4.1461e+00 - val_loss: -4.1676e+00
2019-12-23 18:08:04,228 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-23 18:08:26,415 [INFO] Last epoch loss evaluation: train_loss = -4.166182, val_loss = -4.167598
2019-12-23 18:08:26,415 [INFO] Training autoencoder complete
2019-12-23 18:08:26,415 [INFO] Encoding data for supervised training
2019-12-23 18:08:45,355 [INFO] Encoding complete
2019-12-23 18:08:45,355 [INFO] Training neural network layers (after autoencoder)
Train on 848342 samples, validate on 565562 samples
Epoch 1/300
 - 7s - loss: 0.0219 - val_loss: 0.0094
 - val_f1: 0.9775
Epoch 2/300
 - 7s - loss: 0.0101 - val_loss: 0.0091
 - val_f1: 0.9777
Epoch 3/300
 - 7s - loss: 0.0087 - val_loss: 0.0085
 - val_f1: 0.9766
Epoch 4/300
 - 7s - loss: 0.0081 - val_loss: 0.0083
 - val_f1: 0.9779
Epoch 5/300
 - 7s - loss: 0.0076 - val_loss: 0.0082
 - val_f1: 0.9793
Epoch 6/300
 - 7s - loss: 0.0074 - val_loss: 0.0092
 - val_f1: 0.9713
Epoch 7/300
 - 7s - loss: 0.0074 - val_loss: 0.0082
 - val_f1: 0.9791
Epoch 8/300
 - 7s - loss: 0.0071 - val_loss: 0.0064
 - val_f1: 0.9822
Epoch 9/300
 - 7s - loss: 0.0068 - val_loss: 0.0064
 - val_f1: 0.9848
Epoch 10/300
 - 7s - loss: 0.0069 - val_loss: 0.0095
 - val_f1: 0.9742
Epoch 11/300
 - 7s - loss: 0.0069 - val_loss: 0.0068
 - val_f1: 0.9840
Epoch 12/300
 - 7s - loss: 0.0060 - val_loss: 0.0069
 - val_f1: 0.9840
Epoch 13/300
 - 7s - loss: 0.0055 - val_loss: 0.0040
 - val_f1: 0.9931
Epoch 14/300
 - 7s - loss: 0.0052 - val_loss: 0.0069
 - val_f1: 0.9832
Epoch 15/300
 - 7s - loss: 0.0051 - val_loss: 0.0046
 - val_f1: 0.9904
Epoch 16/300
 - 7s - loss: 0.0052 - val_loss: 0.0039
 - val_f1: 0.9928
Epoch 17/300
 - 7s - loss: 0.0052 - val_loss: 0.0049
 - val_f1: 0.9894
Epoch 18/300
 - 7s - loss: 0.0049 - val_loss: 0.0050
 - val_f1: 0.9898
Epoch 19/300
 - 7s - loss: 0.0049 - val_loss: 0.0048
 - val_f1: 0.9896
Epoch 20/300
 - 7s - loss: 0.0049 - val_loss: 0.0054
 - val_f1: 0.9858
Epoch 21/300
 - 7s - loss: 0.0047 - val_loss: 0.0048
 - val_f1: 0.9897
Epoch 22/300
 - 7s - loss: 0.0047 - val_loss: 0.0041
 - val_f1: 0.9914
Epoch 23/300
 - 7s - loss: 0.0048 - val_loss: 0.0036
 - val_f1: 0.9931
Epoch 24/300
 - 7s - loss: 0.0046 - val_loss: 0.0047
 - val_f1: 0.9885
Epoch 25/300
 - 7s - loss: 0.0047 - val_loss: 0.0036
 - val_f1: 0.9932
Epoch 26/300
 - 7s - loss: 0.0046 - val_loss: 0.0040
 - val_f1: 0.9897
Epoch 27/300
 - 7s - loss: 0.0046 - val_loss: 0.0050
 - val_f1: 0.9897
Epoch 28/300
 - 7s - loss: 0.0046 - val_loss: 0.0036
 - val_f1: 0.9936
Epoch 29/300
 - 7s - loss: 0.0045 - val_loss: 0.0038
 - val_f1: 0.9927
Epoch 30/300
 - 7s - loss: 0.0044 - val_loss: 0.0037
 - val_f1: 0.9939
Epoch 31/300
 - 7s - loss: 0.0044 - val_loss: 0.0062
2019-12-23 18:16:12,368 [INFO] epoch = 30. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/ann_model_epoch_30.pickle
 - val_f1: 0.9843
Epoch 32/300
 - 7s - loss: 0.0045 - val_loss: 0.0035
 - val_f1: 0.9906
Epoch 33/300
 - 7s - loss: 0.0044 - val_loss: 0.0036
 - val_f1: 0.9924
Epoch 34/300
 - 7s - loss: 0.0043 - val_loss: 0.0040
 - val_f1: 0.9913
Epoch 35/300
 - 7s - loss: 0.0045 - val_loss: 0.0035
 - val_f1: 0.9924
Epoch 36/300
 - 7s - loss: 0.0044 - val_loss: 0.0043
 - val_f1: 0.9897
Epoch 37/300
 - 7s - loss: 0.0043 - val_loss: 0.0044
 - val_f1: 0.9908
Epoch 38/300
 - 7s - loss: 0.0042 - val_loss: 0.0071
 - val_f1: 0.9736
Epoch 39/300
 - 7s - loss: 0.0042 - val_loss: 0.0038
 - val_f1: 0.9922
Epoch 40/300
 - 7s - loss: 0.0043 - val_loss: 0.0033
 - val_f1: 0.9939
Epoch 41/300
 - 7s - loss: 0.0043 - val_loss: 0.0047
 - val_f1: 0.9869
Epoch 42/300
 - 7s - loss: 0.0042 - val_loss: 0.0033
 - val_f1: 0.9937
Epoch 43/300
 - 7s - loss: 0.0041 - val_loss: 0.0032
 - val_f1: 0.9918
Epoch 44/300
 - 7s - loss: 0.0041 - val_loss: 0.0046
 - val_f1: 0.9893
Epoch 45/300
 - 7s - loss: 0.0042 - val_loss: 0.0033
 - val_f1: 0.9937
Epoch 46/300
 - 7s - loss: 0.0043 - val_loss: 0.0053
 - val_f1: 0.9873
Epoch 47/300
 - 7s - loss: 0.0041 - val_loss: 0.0040
 - val_f1: 0.9934
Epoch 48/300
 - 7s - loss: 0.0041 - val_loss: 0.0043
 - val_f1: 0.9906
Epoch 49/300
 - 7s - loss: 0.0041 - val_loss: 0.0033
 - val_f1: 0.9944
Epoch 50/300
 - 7s - loss: 0.0040 - val_loss: 0.0048
 - val_f1: 0.9890
Epoch 51/300
 - 7s - loss: 0.0040 - val_loss: 0.0032
 - val_f1: 0.9945
Epoch 52/300
 - 7s - loss: 0.0040 - val_loss: 0.0032
 - val_f1: 0.9935
Epoch 53/300
 - 7s - loss: 0.0041 - val_loss: 0.0046
 - val_f1: 0.9894
Epoch 54/300
 - 7s - loss: 0.0040 - val_loss: 0.0035
 - val_f1: 0.9922
Epoch 55/300
 - 7s - loss: 0.0041 - val_loss: 0.0035
 - val_f1: 0.9921
Epoch 56/300
 - 7s - loss: 0.0042 - val_loss: 0.0031
 - val_f1: 0.9936
Epoch 57/300
 - 7s - loss: 0.0041 - val_loss: 0.0034
 - val_f1: 0.9943
Epoch 58/300
 - 7s - loss: 0.0041 - val_loss: 0.0034
 - val_f1: 0.9929
Epoch 59/300
 - 7s - loss: 0.0039 - val_loss: 0.0052
 - val_f1: 0.9898
Epoch 60/300
 - 7s - loss: 0.0040 - val_loss: 0.0033
 - val_f1: 0.9941
Epoch 61/300
 - 7s - loss: 0.0042 - val_loss: 0.0035
2019-12-23 18:23:29,829 [INFO] epoch = 60. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/ann_model_epoch_60.pickle
 - val_f1: 0.9932
Epoch 62/300
 - 7s - loss: 0.0042 - val_loss: 0.0035
 - val_f1: 0.9939
Epoch 63/300
 - 7s - loss: 0.0040 - val_loss: 0.0032
 - val_f1: 0.9936
Epoch 64/300
 - 7s - loss: 0.0040 - val_loss: 0.0032
 - val_f1: 0.9926
Epoch 65/300
 - 7s - loss: 0.0040 - val_loss: 0.0035
 - val_f1: 0.9920
Epoch 66/300
 - 7s - loss: 0.0040 - val_loss: 0.0037
 - val_f1: 0.9918
Epoch 67/300
 - 7s - loss: 0.0040 - val_loss: 0.0033
 - val_f1: 0.9927
Epoch 68/300
 - 7s - loss: 0.0041 - val_loss: 0.0035
 - val_f1: 0.9924
Epoch 69/300
 - 7s - loss: 0.0039 - val_loss: 0.0028
 - val_f1: 0.9952
Epoch 70/300
 - 7s - loss: 0.0040 - val_loss: 0.0036
 - val_f1: 0.9939
Epoch 71/300
 - 7s - loss: 0.0040 - val_loss: 0.0049
 - val_f1: 0.9884
Epoch 72/300
 - 7s - loss: 0.0039 - val_loss: 0.0050
 - val_f1: 0.9881
Epoch 73/300
 - 7s - loss: 0.0040 - val_loss: 0.0033
 - val_f1: 0.9934
Epoch 74/300
 - 7s - loss: 0.0040 - val_loss: 0.0038
 - val_f1: 0.9916
Epoch 75/300
 - 7s - loss: 0.0040 - val_loss: 0.0043
 - val_f1: 0.9902
Epoch 76/300
 - 7s - loss: 0.0039 - val_loss: 0.0036
 - val_f1: 0.9921
Epoch 77/300
 - 7s - loss: 0.0039 - val_loss: 0.0029
 - val_f1: 0.9930
Epoch 78/300
 - 7s - loss: 0.0038 - val_loss: 0.0033
 - val_f1: 0.9931
Epoch 79/300
 - 7s - loss: 0.0038 - val_loss: 0.0043
 - val_f1: 0.9910
Epoch 80/300
 - 7s - loss: 0.0038 - val_loss: 0.0032
 - val_f1: 0.9922
Epoch 81/300
 - 7s - loss: 0.0037 - val_loss: 0.0032
 - val_f1: 0.9930
Epoch 82/300
 - 7s - loss: 0.0037 - val_loss: 0.0034
 - val_f1: 0.9932
Epoch 83/300
 - 7s - loss: 0.0038 - val_loss: 0.0044
 - val_f1: 0.9906
Epoch 84/300
 - 7s - loss: 0.0043 - val_loss: 0.0045
 - val_f1: 0.9909
Epoch 85/300
 - 7s - loss: 0.0053 - val_loss: 0.0059
 - val_f1: 0.9863
Epoch 86/300
 - 7s - loss: 0.0052 - val_loss: 0.0067
 - val_f1: 0.9778
Epoch 87/300
 - 7s - loss: 0.0046 - val_loss: 0.0045
 - val_f1: 0.9905
Epoch 88/300
 - 7s - loss: 0.0043 - val_loss: 0.0037
 - val_f1: 0.9914
Epoch 89/300
 - 7s - loss: 0.0042 - val_loss: 0.0086
 - val_f1: 0.9723
Epoch 90/300
 - 7s - loss: 0.0042 - val_loss: 0.0034
 - val_f1: 0.9918
Epoch 91/300
 - 7s - loss: 0.0041 - val_loss: 0.0039
2019-12-23 18:30:47,392 [INFO] epoch = 90. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/ann_model_epoch_90.pickle
 - val_f1: 0.9927
Epoch 92/300
 - 7s - loss: 0.0041 - val_loss: 0.0033
 - val_f1: 0.9919
Epoch 93/300
 - 7s - loss: 0.0041 - val_loss: 0.0035
 - val_f1: 0.9927
Epoch 94/300
 - 7s - loss: 0.0042 - val_loss: 0.0036
 - val_f1: 0.9916
Epoch 95/300
 - 7s - loss: 0.0043 - val_loss: 0.0035
 - val_f1: 0.9926
Epoch 96/300
 - 7s - loss: 0.0043 - val_loss: 0.0044
 - val_f1: 0.9911
Epoch 97/300
 - 7s - loss: 0.0042 - val_loss: 0.0034
 - val_f1: 0.9926
Epoch 98/300
 - 7s - loss: 0.0041 - val_loss: 0.0035
 - val_f1: 0.9932
Epoch 99/300
 - 7s - loss: 0.0041 - val_loss: 0.0034
 - val_f1: 0.9929
Epoch 100/300
 - 7s - loss: 0.0042 - val_loss: 0.0036
 - val_f1: 0.9913
Epoch 101/300
 - 7s - loss: 0.0042 - val_loss: 0.0032
 - val_f1: 0.9935
Epoch 102/300
 - 7s - loss: 0.0040 - val_loss: 0.0037
 - val_f1: 0.9904
Epoch 103/300
 - 7s - loss: 0.0042 - val_loss: 0.0036
 - val_f1: 0.9913
Epoch 104/300
 - 7s - loss: 0.0042 - val_loss: 0.0041
 - val_f1: 0.9935
Epoch 105/300
 - 7s - loss: 0.0042 - val_loss: 0.0036
 - val_f1: 0.9917
Epoch 106/300
 - 7s - loss: 0.0041 - val_loss: 0.0034
 - val_f1: 0.9925
Epoch 107/300
 - 7s - loss: 0.0040 - val_loss: 0.0042
 - val_f1: 0.9920
Epoch 108/300
 - 7s - loss: 0.0038 - val_loss: 0.0030
 - val_f1: 0.9947
Epoch 109/300
 - 7s - loss: 0.0037 - val_loss: 0.0033
 - val_f1: 0.9931
Epoch 110/300
 - 7s - loss: 0.0039 - val_loss: 0.0029
 - val_f1: 0.9942
Epoch 111/300
 - 7s - loss: 0.0042 - val_loss: 0.0032
 - val_f1: 0.9937
Epoch 112/300
 - 7s - loss: 0.0041 - val_loss: 0.0032
 - val_f1: 0.9933
Epoch 113/300
 - 7s - loss: 0.0040 - val_loss: 0.0043
 - val_f1: 0.9904
Epoch 114/300
 - 7s - loss: 0.0042 - val_loss: 0.0035
 - val_f1: 0.9932
Epoch 115/300
 - 7s - loss: 0.0042 - val_loss: 0.0032
 - val_f1: 0.9946
Epoch 116/300
 - 7s - loss: 0.0039 - val_loss: 0.0031
 - val_f1: 0.9947
Epoch 117/300
 - 7s - loss: 0.0041 - val_loss: 0.0033
 - val_f1: 0.9916
Epoch 118/300
 - 7s - loss: 0.0039 - val_loss: 0.0033
 - val_f1: 0.9920
Epoch 119/300
 - 7s - loss: 0.0040 - val_loss: 0.0032
2019-12-23 18:37:44,257 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-23 18:38:06,042 [INFO] Last epoch loss evaluation: train_loss = 0.002713, val_loss = 0.002836
2019-12-23 18:38:06,079 [INFO] Training complete. time_to_train = 4670.46 sec, 77.84 min
2019-12-23 18:38:06,087 [INFO] Model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/best_model.pickle
2019-12-23 18:38:06,277 [INFO] Plot saved to: results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/training_error_history.png
2019-12-23 18:38:06,445 [INFO] Plot saved to: results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/training_f1_history.png
2019-12-23 18:38:06,445 [INFO] Making predictions on training, validation, testing data
2019-12-23 18:39:25,393 [INFO] Evaluating predictions (results)
2019-12-23 18:39:35,534 [INFO] Dataset: Testing. Classification report below
2019-12-23 18:39:35,534 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       1.00      1.00      1.00    454265
                   Bot       0.97      0.38      0.54       391
                  DDoS       1.00      1.00      1.00     25605
         DoS GoldenEye       0.99      0.98      0.99      2058
              DoS Hulk       0.98      0.98      0.98     46025
      DoS Slowhttptest       0.89      0.99      0.94      1100
         DoS slowloris       0.98      0.98      0.98      1159
           FTP-Patator       0.98      1.00      0.99      1587
              PortScan       0.99      1.00      0.99     31761
           SSH-Patator       0.94      0.98      0.96      1179
Web Attack Brute Force       1.00      0.10      0.19       302
        Web Attack XSS       0.00      0.00      0.00       130

             micro avg       1.00      1.00      1.00    565562
             macro avg       0.89      0.78      0.80    565562
          weighted avg       0.99      1.00      0.99    565562

2019-12-23 18:39:35,534 [INFO] Overall accuracy (micro avg): 0.9950474041749623
/home/sunanda/test/ids_experiments/utility.py:313: RuntimeWarning: invalid value encountered in true_divide
  F1 = (2 * PPV * TPR) / (PPV + TPR)  # F1 score
2019-12-23 18:39:47,072 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9950         0.9950                       0.9950                0.0005                   0.0050  0.9950
1     Macro avg        0.9992         0.8944                       0.7823                0.0013                   0.2177  0.7966
2  Weighted avg        0.9959         0.9948                       0.9950                0.0106                   0.0050  0.9947
2019-12-23 18:39:57,388 [INFO] Dataset: Validation. Classification report below
2019-12-23 18:39:57,388 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       1.00      1.00      1.00    454264
                   Bot       0.99      0.34      0.51       391
                  DDoS       1.00      1.00      1.00     25605
         DoS GoldenEye       0.99      0.98      0.99      2059
              DoS Hulk       0.98      0.99      0.98     46025
      DoS Slowhttptest       0.89      0.98      0.94      1099
         DoS slowloris       0.98      0.98      0.98      1159
           FTP-Patator       0.98      0.99      0.98      1587
              PortScan       0.99      1.00      1.00     31761
           SSH-Patator       0.94      0.97      0.96      1180
Web Attack Brute Force       0.95      0.07      0.13       301
        Web Attack XSS       1.00      0.03      0.06       131

             micro avg       1.00      1.00      1.00    565562
             macro avg       0.97      0.78      0.79    565562
          weighted avg       1.00      1.00      0.99    565562

2019-12-23 18:39:57,388 [INFO] Overall accuracy (micro avg): 0.995158797797589
2019-12-23 18:40:09,095 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9952         0.9952                       0.9952                0.0004                   0.0048  0.9952
1     Macro avg        0.9992         0.9741                       0.7782                0.0013                   0.2218  0.7930
2  Weighted avg        0.9960         0.9952                       0.9952                0.0103                   0.0048  0.9947
2019-12-23 18:40:43,063 [INFO] Dataset: Training. Classification report below
2019-12-23 18:40:43,063 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       1.00      1.00      1.00   1362791
                   Bot       0.98      0.36      0.53      1174
                  DDoS       1.00      1.00      1.00     76815
         DoS GoldenEye       0.99      0.98      0.99      6176
              DoS Hulk       0.98      0.99      0.98    138074
      DoS Slowhttptest       0.90      0.99      0.94      3300
         DoS slowloris       0.98      0.99      0.98      3478
           FTP-Patator       0.98      0.99      0.99      4761
              PortScan       0.99      1.00      0.99     95282
           SSH-Patator       0.95      0.98      0.97      3538
Web Attack Brute Force       0.99      0.09      0.16       904
        Web Attack XSS       1.00      0.03      0.06       391

             micro avg       1.00      1.00      1.00   1696684
             macro avg       0.98      0.78      0.80   1696684
          weighted avg       1.00      1.00      0.99   1696684

2019-12-23 18:40:43,063 [INFO] Overall accuracy (micro avg): 0.995219498739895
2019-12-23 18:41:21,673 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9952         0.9952                       0.9952                0.0004                   0.0048  0.9952
1     Macro avg        0.9992         0.9788                       0.7823                0.0013                   0.2177  0.7989
2  Weighted avg        0.9960         0.9952                       0.9952                0.0104                   0.0048  0.9948
2019-12-23 18:41:21,725 [INFO] Results saved to: results_additional_exps/semi_sup_perf_ids17_ae_ann_rep2/semi_sup_perf_ids17_ae_ann_rep2_results.xlsx
2019-12-23 18:41:21,729 [INFO] ================= Finished running experiment no. 2 ================= 

2019-12-23 18:41:21,797 [INFO] Created directory: results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3
2019-12-23 18:41:21,797 [INFO] Initialized logging. log_filename = results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/run_log.log
2019-12-23 18:41:21,797 [INFO] ================= Running experiment no. 3  ================= 

2019-12-23 18:41:21,797 [INFO] Experiment parameters given below
2019-12-23 18:41:21,797 [INFO] 
{'experiment_num': 3, 'results_dir': 'results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3', 'model_type': 'classifier', 'model': 'ae_ann', 'normal_label': 'BENIGN', 'split_random_seed': 42, 'unsupervised_ratio': 0.5, 'ae_encoder_units': [64], 'ae_encoder_activations': ['relu'], 'ae_encoder_dropout_rates': [0.2], 'ae_encoder_l1_param': -1, 'ae_decoder_units': '', 'ae_decoder_activations': '', 'ae_decoder_dropout_rates': '', 'output_activation': 'sigmoid', 'loss_function': 'binary_crossentropy', 'ae_epochs': 200, 'ann_layer_units': [64], 'ann_layer_activations': ['relu'], 'ann_layer_dropout_rates': [0.2], 'ann_epochs': 300, 'early_stop_patience': 50, 'batch_size': 256, 'dataset_dir': '../Datasets/full_datasets/ids2017', 'description': 'semi_sup_perf_ids17_ae_ann_rep3'}
2019-12-23 18:41:21,797 [INFO] Created tensorboard log directory: results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/tf_logs_run_2019_12_23-18_41_21
2019-12-23 18:41:21,797 [INFO] Loading datsets from: ../Datasets/full_datasets/ids2017
2019-12-23 18:41:21,797 [INFO] Reading X, y files
2019-12-23 18:41:21,797 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_train.h5
2019-12-23 18:41:25,872 [INFO] Reading complete. time_to_read=4.07 seconds
2019-12-23 18:41:25,872 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_val.h5
2019-12-23 18:41:27,256 [INFO] Reading complete. time_to_read=1.38 seconds
2019-12-23 18:41:27,256 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/X_test.h5
2019-12-23 18:41:28,642 [INFO] Reading complete. time_to_read=1.39 seconds
2019-12-23 18:41:28,643 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_train.h5
2019-12-23 18:41:28,850 [INFO] Reading complete. time_to_read=0.21 seconds
2019-12-23 18:41:28,850 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_val.h5
2019-12-23 18:41:28,919 [INFO] Reading complete. time_to_read=0.07 seconds
2019-12-23 18:41:28,919 [INFO] Reading HDF dataset ../Datasets/full_datasets/ids2017/y_test.h5
2019-12-23 18:41:28,987 [INFO] Reading complete. time_to_read=0.07 seconds
2019-12-23 18:41:32,163 [INFO] Initializing model
2019-12-23 18:41:32,274 [INFO] _________________________________________________________________
2019-12-23 18:41:32,274 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-23 18:41:32,275 [INFO] =================================================================
2019-12-23 18:41:32,275 [INFO] dense_21 (Dense)             (None, 64)                5056      
2019-12-23 18:41:32,275 [INFO] _________________________________________________________________
2019-12-23 18:41:32,275 [INFO] batch_normalization_11 (Batc (None, 64)                256       
2019-12-23 18:41:32,275 [INFO] _________________________________________________________________
2019-12-23 18:41:32,275 [INFO] dropout_11 (Dropout)         (None, 64)                0         
2019-12-23 18:41:32,275 [INFO] _________________________________________________________________
2019-12-23 18:41:32,275 [INFO] dense_22 (Dense)             (None, 78)                5070      
2019-12-23 18:41:32,275 [INFO] =================================================================
2019-12-23 18:41:32,275 [INFO] Total params: 10,382
2019-12-23 18:41:32,275 [INFO] Trainable params: 10,254
2019-12-23 18:41:32,275 [INFO] Non-trainable params: 128
2019-12-23 18:41:32,275 [INFO] _________________________________________________________________
2019-12-23 18:41:32,382 [INFO] _________________________________________________________________
2019-12-23 18:41:32,382 [INFO] Layer (type)                 Output Shape              Param #   
2019-12-23 18:41:32,382 [INFO] =================================================================
2019-12-23 18:41:32,382 [INFO] dense_23 (Dense)             (None, 64)                4160      
2019-12-23 18:41:32,382 [INFO] _________________________________________________________________
2019-12-23 18:41:32,383 [INFO] batch_normalization_12 (Batc (None, 64)                256       
2019-12-23 18:41:32,383 [INFO] _________________________________________________________________
2019-12-23 18:41:32,383 [INFO] dropout_12 (Dropout)         (None, 64)                0         
2019-12-23 18:41:32,383 [INFO] _________________________________________________________________
2019-12-23 18:41:32,383 [INFO] dense_24 (Dense)             (None, 12)                780       
2019-12-23 18:41:32,383 [INFO] =================================================================
2019-12-23 18:41:32,383 [INFO] Total params: 5,196
2019-12-23 18:41:32,383 [INFO] Trainable params: 5,068
2019-12-23 18:41:32,383 [INFO] Non-trainable params: 128
2019-12-23 18:41:32,383 [INFO] _________________________________________________________________
2019-12-23 18:41:32,383 [INFO] Training model
2019-12-23 18:41:32,383 [INFO] Splitting train set into 2 sets (unsupervised, supervised), random_seed = 42
2019-12-23 18:41:49,173 [INFO] Split sizes (instances). total = 1696684, unsupervised = 848342, supervised = 848342, unsupervised dataset hash = b75e78959164c90d19a336ef2d2a5a10f094d2bb
2019-12-23 18:41:49,173 [INFO] Training autoencoder
 - val_f1: 0.9934
Epoch 00119: early stopping
Train on 848342 samples, validate on 565562 samples
Epoch 1/200
 - 15s - loss: -3.6473e+00 - val_loss: -4.1347e+00
Epoch 2/200
 - 15s - loss: -4.1022e+00 - val_loss: -4.1449e+00
Epoch 3/200
 - 14s - loss: -4.1175e+00 - val_loss: -4.1551e+00
Epoch 4/200
 - 14s - loss: -4.1245e+00 - val_loss: -4.1492e+00
Epoch 5/200
 - 14s - loss: -4.1282e+00 - val_loss: -4.1587e+00
Epoch 6/200
 - 14s - loss: -4.1308e+00 - val_loss: -4.1600e+00
Epoch 7/200
 - 14s - loss: -4.1322e+00 - val_loss: -4.1578e+00
Epoch 8/200
 - 14s - loss: -4.1335e+00 - val_loss: -4.1598e+00
Epoch 9/200
 - 14s - loss: -4.1346e+00 - val_loss: -4.1517e+00
Epoch 10/200
 - 14s - loss: -4.1355e+00 - val_loss: -4.1617e+00
Epoch 11/200
 - 14s - loss: -4.1362e+00 - val_loss: -4.1620e+00
Epoch 12/200
 - 14s - loss: -4.1370e+00 - val_loss: -4.1617e+00
Epoch 13/200
 - 14s - loss: -4.1372e+00 - val_loss: -4.1622e+00
Epoch 14/200
 - 14s - loss: -4.1379e+00 - val_loss: -4.1627e+00
Epoch 15/200
 - 14s - loss: -4.1379e+00 - val_loss: -4.1634e+00
Epoch 16/200
 - 14s - loss: -4.1382e+00 - val_loss: -4.1620e+00
Epoch 17/200
 - 14s - loss: -4.1385e+00 - val_loss: -4.1631e+00
Epoch 18/200
 - 14s - loss: -4.1390e+00 - val_loss: -4.1630e+00
Epoch 19/200
 - 14s - loss: -4.1392e+00 - val_loss: -4.1634e+00
Epoch 20/200
 - 14s - loss: -4.1394e+00 - val_loss: -4.1627e+00
Epoch 21/200
 - 14s - loss: -4.1394e+00 - val_loss: -4.1621e+00
2019-12-23 18:46:55,529 [INFO] epoch = 20. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/ae_model_epoch_20.pickle
Epoch 22/200
 - 15s - loss: -4.1397e+00 - val_loss: -4.1638e+00
Epoch 23/200
 - 15s - loss: -4.1398e+00 - val_loss: -4.1622e+00
Epoch 24/200
 - 15s - loss: -4.1398e+00 - val_loss: -4.1637e+00
Epoch 25/200
 - 15s - loss: -4.1400e+00 - val_loss: -4.1638e+00
Epoch 26/200
 - 15s - loss: -4.1404e+00 - val_loss: -4.1637e+00
Epoch 27/200
 - 15s - loss: -4.1397e+00 - val_loss: -4.1641e+00
Epoch 28/200
 - 15s - loss: -4.1408e+00 - val_loss: -4.1631e+00
Epoch 29/200
 - 15s - loss: -4.1408e+00 - val_loss: -4.1629e+00
Epoch 30/200
 - 15s - loss: -4.1407e+00 - val_loss: -4.1634e+00
Epoch 31/200
 - 15s - loss: -4.1412e+00 - val_loss: -4.1639e+00
Epoch 32/200
 - 14s - loss: -4.1411e+00 - val_loss: -4.1644e+00
Epoch 33/200
 - 15s - loss: -4.1412e+00 - val_loss: -4.1640e+00
Epoch 34/200
 - 15s - loss: -4.1413e+00 - val_loss: -4.1646e+00
Epoch 35/200
 - 15s - loss: -4.1414e+00 - val_loss: -4.1642e+00
Epoch 36/200
 - 15s - loss: -4.1417e+00 - val_loss: -4.1646e+00
Epoch 37/200
 - 15s - loss: -4.1417e+00 - val_loss: -4.1642e+00
Epoch 38/200
 - 15s - loss: -4.1420e+00 - val_loss: -4.1630e+00
Epoch 39/200
 - 15s - loss: -4.1422e+00 - val_loss: -4.1646e+00
Epoch 40/200
 - 14s - loss: -4.1421e+00 - val_loss: -4.1637e+00
Epoch 41/200
 - 15s - loss: -4.1420e+00 - val_loss: -4.1647e+00
2019-12-23 18:51:46,832 [INFO] epoch = 40. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/ae_model_epoch_40.pickle
Epoch 42/200
 - 15s - loss: -4.1423e+00 - val_loss: -4.1644e+00
Epoch 43/200
 - 15s - loss: -4.1425e+00 - val_loss: -4.1645e+00
Epoch 44/200
 - 15s - loss: -4.1420e+00 - val_loss: -4.1653e+00
Epoch 45/200
 - 15s - loss: -4.1426e+00 - val_loss: -4.1652e+00
Epoch 46/200
 - 15s - loss: -4.1426e+00 - val_loss: -4.1648e+00
Epoch 47/200
 - 15s - loss: -4.1425e+00 - val_loss: -4.1656e+00
Epoch 48/200
 - 15s - loss: -4.1427e+00 - val_loss: -4.1655e+00
Epoch 49/200
 - 15s - loss: -4.1428e+00 - val_loss: -4.1643e+00
Epoch 50/200
 - 15s - loss: -4.1429e+00 - val_loss: -4.1653e+00
Epoch 51/200
 - 15s - loss: -4.1431e+00 - val_loss: -4.1650e+00
Epoch 52/200
 - 15s - loss: -4.1432e+00 - val_loss: -4.1638e+00
Epoch 53/200
 - 15s - loss: -4.1430e+00 - val_loss: -4.1652e+00
Epoch 54/200
 - 15s - loss: -4.1432e+00 - val_loss: -4.1657e+00
Epoch 55/200
 - 15s - loss: -4.1432e+00 - val_loss: -4.1654e+00
Epoch 56/200
 - 15s - loss: -4.1430e+00 - val_loss: -4.1654e+00
Epoch 57/200
 - 15s - loss: -4.1433e+00 - val_loss: -4.1655e+00
Epoch 58/200
 - 15s - loss: -4.1435e+00 - val_loss: -4.1654e+00
Epoch 59/200
 - 15s - loss: -4.1434e+00 - val_loss: -4.1646e+00
Epoch 60/200
 - 15s - loss: -4.1434e+00 - val_loss: -4.1655e+00
Epoch 61/200
 - 15s - loss: -4.1435e+00 - val_loss: -4.1659e+00
2019-12-23 18:56:38,110 [INFO] epoch = 60. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/ae_model_epoch_60.pickle
Epoch 62/200
 - 15s - loss: -4.1437e+00 - val_loss: -4.1656e+00
Epoch 63/200
 - 15s - loss: -4.1432e+00 - val_loss: -4.1653e+00
Epoch 64/200
 - 15s - loss: -4.1434e+00 - val_loss: -4.1659e+00
Epoch 65/200
 - 15s - loss: -4.1436e+00 - val_loss: -4.1655e+00
Epoch 66/200
 - 15s - loss: -4.1436e+00 - val_loss: -4.1661e+00
Epoch 67/200
 - 15s - loss: -4.1436e+00 - val_loss: -4.1657e+00
Epoch 68/200
 - 14s - loss: -4.1438e+00 - val_loss: -4.1662e+00
Epoch 69/200
 - 15s - loss: -4.1438e+00 - val_loss: -4.1659e+00
Epoch 70/200
 - 15s - loss: -4.1436e+00 - val_loss: -4.1650e+00
Epoch 71/200
 - 15s - loss: -4.1438e+00 - val_loss: -4.1644e+00
Epoch 72/200
 - 14s - loss: -4.1438e+00 - val_loss: -4.1656e+00
Epoch 73/200
 - 15s - loss: -4.1438e+00 - val_loss: -4.1654e+00
Epoch 74/200
 - 14s - loss: -4.1439e+00 - val_loss: -4.1655e+00
Epoch 75/200
 - 15s - loss: -4.1438e+00 - val_loss: -4.1646e+00
Epoch 76/200
 - 15s - loss: -4.1440e+00 - val_loss: -4.1653e+00
Epoch 77/200
 - 14s - loss: -4.1435e+00 - val_loss: -4.1659e+00
Epoch 78/200
 - 15s - loss: -4.1441e+00 - val_loss: -4.1653e+00
Epoch 79/200
 - 15s - loss: -4.1440e+00 - val_loss: -4.1663e+00
Epoch 80/200
 - 15s - loss: -4.1437e+00 - val_loss: -4.1656e+00
Epoch 81/200
 - 15s - loss: -4.1442e+00 - val_loss: -4.1659e+00
2019-12-23 19:01:29,248 [INFO] epoch = 80. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/ae_model_epoch_80.pickle
Epoch 82/200
 - 15s - loss: -4.1443e+00 - val_loss: -4.1646e+00
Epoch 83/200
 - 15s - loss: -4.1440e+00 - val_loss: -4.1646e+00
Epoch 84/200
 - 15s - loss: -4.1440e+00 - val_loss: -4.1662e+00
Epoch 85/200
 - 15s - loss: -4.1442e+00 - val_loss: -4.1663e+00
Epoch 86/200
 - 15s - loss: -4.1443e+00 - val_loss: -4.1658e+00
Epoch 87/200
 - 15s - loss: -4.1441e+00 - val_loss: -4.1649e+00
Epoch 88/200
 - 15s - loss: -4.1442e+00 - val_loss: -4.1653e+00
Epoch 89/200
 - 15s - loss: -4.1442e+00 - val_loss: -4.1657e+00
Epoch 90/200
 - 14s - loss: -4.1443e+00 - val_loss: -4.1652e+00
Epoch 91/200
 - 15s - loss: -4.1444e+00 - val_loss: -4.1659e+00
Epoch 92/200
 - 14s - loss: -4.1444e+00 - val_loss: -4.1652e+00
Epoch 93/200
 - 15s - loss: -4.1446e+00 - val_loss: -4.1657e+00
Epoch 94/200
 - 15s - loss: -4.1444e+00 - val_loss: -4.1653e+00
Epoch 95/200
 - 15s - loss: -4.1444e+00 - val_loss: -4.1661e+00
Epoch 96/200
 - 15s - loss: -4.1444e+00 - val_loss: -4.1662e+00
Epoch 97/200
 - 15s - loss: -4.1443e+00 - val_loss: -4.1645e+00
Epoch 98/200
 - 15s - loss: -4.1445e+00 - val_loss: -4.1661e+00
Epoch 99/200
 - 15s - loss: -4.1444e+00 - val_loss: -4.1655e+00
Epoch 100/200
 - 15s - loss: -4.1448e+00 - val_loss: -4.1658e+00
Epoch 101/200
 - 15s - loss: -4.1444e+00 - val_loss: -4.1661e+00
2019-12-23 19:06:20,392 [INFO] epoch = 100. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/ae_model_epoch_100.pickle
Epoch 102/200
 - 15s - loss: -4.1446e+00 - val_loss: -4.1649e+00
Epoch 103/200
 - 15s - loss: -4.1445e+00 - val_loss: -4.1656e+00
Epoch 104/200
 - 15s - loss: -4.1447e+00 - val_loss: -4.1662e+00
Epoch 105/200
 - 15s - loss: -4.1444e+00 - val_loss: -4.1582e+00
Epoch 106/200
 - 15s - loss: -4.1446e+00 - val_loss: -4.1658e+00
Epoch 107/200
 - 15s - loss: -4.1446e+00 - val_loss: -4.1665e+00
Epoch 108/200
 - 15s - loss: -4.1447e+00 - val_loss: -4.1665e+00
Epoch 109/200
 - 15s - loss: -4.1449e+00 - val_loss: -4.1658e+00
Epoch 110/200
 - 15s - loss: -4.1445e+00 - val_loss: -4.1652e+00
Epoch 111/200
 - 15s - loss: -4.1447e+00 - val_loss: -4.1657e+00
Epoch 112/200
 - 15s - loss: -4.1446e+00 - val_loss: -4.1650e+00
Epoch 113/200
 - 15s - loss: -4.1448e+00 - val_loss: -4.1662e+00
Epoch 114/200
 - 15s - loss: -4.1445e+00 - val_loss: -4.1638e+00
Epoch 115/200
 - 15s - loss: -4.1447e+00 - val_loss: -4.1661e+00
Epoch 116/200
 - 15s - loss: -4.1449e+00 - val_loss: -4.1661e+00
Epoch 117/200
 - 15s - loss: -4.1448e+00 - val_loss: -4.1663e+00
Epoch 118/200
 - 15s - loss: -4.1449e+00 - val_loss: -4.1651e+00
Epoch 119/200
 - 15s - loss: -4.1449e+00 - val_loss: -4.1661e+00
Epoch 120/200
 - 15s - loss: -4.1448e+00 - val_loss: -4.1636e+00
Epoch 121/200
 - 15s - loss: -4.1449e+00 - val_loss: -4.1628e+00
2019-12-23 19:11:11,684 [INFO] epoch = 120. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/ae_model_epoch_120.pickle
Epoch 122/200
 - 15s - loss: -4.1448e+00 - val_loss: -4.1667e+00
Epoch 123/200
 - 15s - loss: -4.1449e+00 - val_loss: -4.1663e+00
Epoch 124/200
 - 15s - loss: -4.1447e+00 - val_loss: -4.1625e+00
Epoch 125/200
 - 15s - loss: -4.1446e+00 - val_loss: -4.1628e+00
Epoch 126/200
 - 15s - loss: -4.1449e+00 - val_loss: -4.1664e+00
Epoch 127/200
 - 14s - loss: -4.1450e+00 - val_loss: -4.1654e+00
Epoch 128/200
 - 15s - loss: -4.1448e+00 - val_loss: -4.1656e+00
Epoch 129/200
 - 15s - loss: -4.1450e+00 - val_loss: -4.1633e+00
Epoch 130/200
 - 15s - loss: -4.1449e+00 - val_loss: -4.1650e+00
Epoch 131/200
 - 15s - loss: -4.1452e+00 - val_loss: -4.1655e+00
Epoch 132/200
 - 15s - loss: -4.1452e+00 - val_loss: -4.1619e+00
Epoch 133/200
 - 15s - loss: -4.1450e+00 - val_loss: -4.1666e+00
Epoch 134/200
 - 15s - loss: -4.1449e+00 - val_loss: -4.1625e+00
Epoch 135/200
 - 15s - loss: -4.1452e+00 - val_loss: -4.1628e+00
Epoch 136/200
 - 15s - loss: -4.1448e+00 - val_loss: -4.1662e+00
Epoch 137/200
 - 15s - loss: -4.1447e+00 - val_loss: -4.1637e+00
Epoch 138/200
 - 15s - loss: -4.1450e+00 - val_loss: -4.1643e+00
Epoch 139/200
 - 15s - loss: -4.1449e+00 - val_loss: -4.1628e+00
Epoch 140/200
 - 15s - loss: -4.1451e+00 - val_loss: -4.1626e+00
Epoch 141/200
 - 15s - loss: -4.1449e+00 - val_loss: -4.1659e+00
2019-12-23 19:16:02,473 [INFO] epoch = 140. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/ae_model_epoch_140.pickle
Epoch 142/200
 - 15s - loss: -4.1451e+00 - val_loss: -4.1666e+00
Epoch 143/200
 - 15s - loss: -4.1450e+00 - val_loss: -4.1653e+00
Epoch 144/200
 - 14s - loss: -4.1452e+00 - val_loss: -4.1662e+00
Epoch 145/200
 - 15s - loss: -4.1449e+00 - val_loss: -4.1629e+00
Epoch 146/200
 - 15s - loss: -4.1453e+00 - val_loss: -4.1656e+00
Epoch 147/200
 - 15s - loss: -4.1449e+00 - val_loss: -4.1664e+00
Epoch 148/200
 - 15s - loss: -4.1449e+00 - val_loss: -4.1653e+00
Epoch 149/200
 - 15s - loss: -4.1452e+00 - val_loss: -4.1654e+00
Epoch 150/200
 - 15s - loss: -4.1452e+00 - val_loss: -4.1666e+00
Epoch 151/200
 - 15s - loss: -4.1450e+00 - val_loss: -4.1660e+00
Epoch 152/200
 - 15s - loss: -4.1448e+00 - val_loss: -4.1658e+00
Epoch 153/200
 - 15s - loss: -4.1451e+00 - val_loss: -4.1653e+00
Epoch 154/200
 - 15s - loss: -4.1450e+00 - val_loss: -4.1657e+00
Epoch 155/200
 - 15s - loss: -4.1449e+00 - val_loss: -4.1658e+00
Epoch 156/200
 - 15s - loss: -4.1452e+00 - val_loss: -4.1666e+00
Epoch 157/200
 - 14s - loss: -4.1452e+00 - val_loss: -4.1665e+00
Epoch 158/200
 - 15s - loss: -4.1453e+00 - val_loss: -4.1667e+00
Epoch 159/200
 - 15s - loss: -4.1451e+00 - val_loss: -4.1649e+00
Epoch 160/200
 - 15s - loss: -4.1452e+00 - val_loss: -4.1665e+00
Epoch 161/200
 - 15s - loss: -4.1452e+00 - val_loss: -4.1659e+00
2019-12-23 19:20:53,550 [INFO] epoch = 160. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/ae_model_epoch_160.pickle
Epoch 162/200
 - 15s - loss: -4.1451e+00 - val_loss: -4.1660e+00
Epoch 163/200
 - 15s - loss: -4.1449e+00 - val_loss: -4.1662e+00
Epoch 164/200
 - 15s - loss: -4.1452e+00 - val_loss: -4.1667e+00
Epoch 165/200
 - 15s - loss: -4.1451e+00 - val_loss: -4.1668e+00
Epoch 166/200
 - 15s - loss: -4.1452e+00 - val_loss: -4.1664e+00
Epoch 167/200
 - 15s - loss: -4.1454e+00 - val_loss: -4.1619e+00
Epoch 168/200
 - 15s - loss: -4.1451e+00 - val_loss: -4.1653e+00
Epoch 169/200
 - 15s - loss: -4.1452e+00 - val_loss: -4.1666e+00
Epoch 170/200
 - 15s - loss: -4.1451e+00 - val_loss: -4.1662e+00
Epoch 171/200
 - 14s - loss: -4.1452e+00 - val_loss: -4.1666e+00
Epoch 172/200
 - 15s - loss: -4.1451e+00 - val_loss: -4.1666e+00
Epoch 173/200
 - 15s - loss: -4.1452e+00 - val_loss: -4.1657e+00
Epoch 174/200
 - 15s - loss: -4.1454e+00 - val_loss: -4.1656e+00
Epoch 175/200
 - 15s - loss: -4.1452e+00 - val_loss: -4.1662e+00
Epoch 176/200
 - 14s - loss: -4.1450e+00 - val_loss: -4.1651e+00
Epoch 177/200
 - 15s - loss: -4.1451e+00 - val_loss: -4.1649e+00
Epoch 178/200
 - 15s - loss: -4.1453e+00 - val_loss: -4.1659e+00
Epoch 179/200
 - 15s - loss: -4.1454e+00 - val_loss: -4.1659e+00
Epoch 180/200
 - 15s - loss: -4.1453e+00 - val_loss: -4.1658e+00
Epoch 181/200
 - 15s - loss: -4.1448e+00 - val_loss: -4.1663e+00
2019-12-23 19:25:44,479 [INFO] epoch = 180. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/ae_model_epoch_180.pickle
Epoch 182/200
 - 15s - loss: -4.1451e+00 - val_loss: -4.1663e+00
Epoch 183/200
 - 15s - loss: -4.1455e+00 - val_loss: -4.1663e+00
Epoch 184/200
 - 15s - loss: -4.1453e+00 - val_loss: -4.1649e+00
Epoch 185/200
 - 15s - loss: -4.1455e+00 - val_loss: -4.1666e+00
Epoch 186/200
 - 15s - loss: -4.1452e+00 - val_loss: -4.1654e+00
Epoch 187/200
 - 15s - loss: -4.1455e+00 - val_loss: -4.1663e+00
Epoch 188/200
 - 15s - loss: -4.1451e+00 - val_loss: -4.1664e+00
Epoch 189/200
 - 15s - loss: -4.1452e+00 - val_loss: -4.1647e+00
Epoch 190/200
 - 15s - loss: -4.1452e+00 - val_loss: -4.1665e+00
Epoch 191/200
 - 15s - loss: -4.1454e+00 - val_loss: -4.1663e+00
Epoch 192/200
 - 15s - loss: -4.1452e+00 - val_loss: -4.1666e+00
Epoch 193/200
 - 15s - loss: -4.1455e+00 - val_loss: -4.1660e+00
Epoch 194/200
 - 15s - loss: -4.1453e+00 - val_loss: -4.1662e+00
Epoch 195/200
 - 15s - loss: -4.1451e+00 - val_loss: -4.1665e+00
Epoch 196/200
 - 15s - loss: -4.1454e+00 - val_loss: -4.1658e+00
Epoch 197/200
 - 14s - loss: -4.1454e+00 - val_loss: -4.1664e+00
Epoch 198/200
 - 15s - loss: -4.1454e+00 - val_loss: -4.1622e+00
Epoch 199/200
 - 15s - loss: -4.1452e+00 - val_loss: -4.1669e+00
Epoch 200/200
 - 15s - loss: -4.1453e+00 - val_loss: -4.1659e+00
2019-12-23 19:30:21,033 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-23 19:30:44,247 [INFO] Last epoch loss evaluation: train_loss = -4.165450, val_loss = -4.166895
2019-12-23 19:30:44,247 [INFO] Training autoencoder complete
2019-12-23 19:30:44,249 [INFO] Encoding data for supervised training
2019-12-23 19:31:05,883 [INFO] Encoding complete
2019-12-23 19:31:05,883 [INFO] Training neural network layers (after autoencoder)
Train on 848342 samples, validate on 565562 samples
Epoch 1/300
 - 7s - loss: 0.0220 - val_loss: 0.0098
 - val_f1: 0.9743
Epoch 2/300
 - 7s - loss: 0.0099 - val_loss: 0.0088
 - val_f1: 0.9776
Epoch 3/300
 - 7s - loss: 0.0087 - val_loss: 0.0085
 - val_f1: 0.9761
Epoch 4/300
 - 7s - loss: 0.0082 - val_loss: 0.0078
 - val_f1: 0.9797
Epoch 5/300
 - 7s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9794
Epoch 6/300
 - 7s - loss: 0.0074 - val_loss: 0.0097
 - val_f1: 0.9742
Epoch 7/300
 - 7s - loss: 0.0068 - val_loss: 0.0056
 - val_f1: 0.9869
Epoch 8/300
 - 7s - loss: 0.0065 - val_loss: 0.0112
 - val_f1: 0.9657
Epoch 9/300
 - 7s - loss: 0.0074 - val_loss: 0.0066
 - val_f1: 0.9841
Epoch 10/300
 - 7s - loss: 0.0074 - val_loss: 0.0065
 - val_f1: 0.9838
Epoch 11/300
 - 7s - loss: 0.0073 - val_loss: 0.0066
 - val_f1: 0.9830
Epoch 12/300
 - 7s - loss: 0.0072 - val_loss: 0.0062
 - val_f1: 0.9846
Epoch 13/300
 - 7s - loss: 0.0070 - val_loss: 0.0063
 - val_f1: 0.9888
Epoch 14/300
 - 7s - loss: 0.0067 - val_loss: 0.0056
 - val_f1: 0.9856
Epoch 15/300
 - 7s - loss: 0.0065 - val_loss: 0.0057
 - val_f1: 0.9859
Epoch 16/300
 - 7s - loss: 0.0063 - val_loss: 0.0054
 - val_f1: 0.9881
Epoch 17/300
 - 7s - loss: 0.0060 - val_loss: 0.0054
 - val_f1: 0.9884
Epoch 18/300
 - 7s - loss: 0.0061 - val_loss: 0.0054
 - val_f1: 0.9896
Epoch 19/300
 - 7s - loss: 0.0059 - val_loss: 0.0061
 - val_f1: 0.9853
Epoch 20/300
 - 7s - loss: 0.0056 - val_loss: 0.0042
 - val_f1: 0.9935
Epoch 21/300
 - 7s - loss: 0.0059 - val_loss: 0.0071
 - val_f1: 0.9846
Epoch 22/300
 - 7s - loss: 0.0063 - val_loss: 0.0068
 - val_f1: 0.9856
Epoch 23/300
 - 7s - loss: 0.0065 - val_loss: 0.0059
 - val_f1: 0.9873
Epoch 24/300
 - 7s - loss: 0.0063 - val_loss: 0.0061
 - val_f1: 0.9858
Epoch 25/300
 - 7s - loss: 0.0062 - val_loss: 0.0055
 - val_f1: 0.9896
Epoch 26/300
 - 7s - loss: 0.0060 - val_loss: 0.0050
 - val_f1: 0.9896
Epoch 27/300
 - 7s - loss: 0.0056 - val_loss: 0.0052
 - val_f1: 0.9893
Epoch 28/300
 - 7s - loss: 0.0053 - val_loss: 0.0042
 - val_f1: 0.9920
Epoch 29/300
 - 7s - loss: 0.0051 - val_loss: 0.0046
 - val_f1: 0.9906
Epoch 30/300
 - 7s - loss: 0.0056 - val_loss: 0.0060
 - val_f1: 0.9874
Epoch 31/300
 - 7s - loss: 0.0061 - val_loss: 0.0055
2019-12-23 19:39:05,707 [INFO] epoch = 30. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/ann_model_epoch_30.pickle
 - val_f1: 0.9871
Epoch 32/300
 - 7s - loss: 0.0059 - val_loss: 0.0043
 - val_f1: 0.9914
Epoch 33/300
 - 7s - loss: 0.0057 - val_loss: 0.0063
 - val_f1: 0.9809
Epoch 34/300
 - 7s - loss: 0.0055 - val_loss: 0.0040
 - val_f1: 0.9938
Epoch 35/300
 - 7s - loss: 0.0052 - val_loss: 0.0043
 - val_f1: 0.9913
Epoch 36/300
 - 7s - loss: 0.0050 - val_loss: 0.0040
 - val_f1: 0.9935
Epoch 37/300
 - 7s - loss: 0.0052 - val_loss: 0.0039
 - val_f1: 0.9921
Epoch 38/300
 - 7s - loss: 0.0050 - val_loss: 0.0042
 - val_f1: 0.9912
Epoch 39/300
 - 7s - loss: 0.0049 - val_loss: 0.0038
 - val_f1: 0.9920
Epoch 40/300
 - 7s - loss: 0.0050 - val_loss: 0.0036
 - val_f1: 0.9911
Epoch 41/300
 - 7s - loss: 0.0049 - val_loss: 0.0039
 - val_f1: 0.9928
Epoch 42/300
 - 7s - loss: 0.0048 - val_loss: 0.0044
 - val_f1: 0.9908
Epoch 43/300
 - 7s - loss: 0.0049 - val_loss: 0.0043
 - val_f1: 0.9905
Epoch 44/300
 - 7s - loss: 0.0048 - val_loss: 0.0045
 - val_f1: 0.9917
Epoch 45/300
 - 7s - loss: 0.0047 - val_loss: 0.0040
 - val_f1: 0.9919
Epoch 46/300
 - 7s - loss: 0.0046 - val_loss: 0.0037
 - val_f1: 0.9921
Epoch 47/300
 - 7s - loss: 0.0045 - val_loss: 0.0055
 - val_f1: 0.9889
Epoch 48/300
 - 7s - loss: 0.0044 - val_loss: 0.0035
 - val_f1: 0.9910
Epoch 49/300
 - 7s - loss: 0.0044 - val_loss: 0.0036
 - val_f1: 0.9931
Epoch 50/300
 - 7s - loss: 0.0045 - val_loss: 0.0044
 - val_f1: 0.9909
Epoch 51/300
 - 7s - loss: 0.0044 - val_loss: 0.0035
 - val_f1: 0.9931
Epoch 52/300
 - 7s - loss: 0.0045 - val_loss: 0.0034
 - val_f1: 0.9914
Epoch 53/300
 - 7s - loss: 0.0044 - val_loss: 0.0033
 - val_f1: 0.9942
Epoch 54/300
 - 7s - loss: 0.0044 - val_loss: 0.0039
 - val_f1: 0.9917
Epoch 55/300
 - 7s - loss: 0.0045 - val_loss: 0.0039
 - val_f1: 0.9914
Epoch 56/300
 - 7s - loss: 0.0045 - val_loss: 0.0038
 - val_f1: 0.9915
Epoch 57/300
 - 7s - loss: 0.0046 - val_loss: 0.0032
 - val_f1: 0.9938
Epoch 58/300
 - 7s - loss: 0.0045 - val_loss: 0.0037
 - val_f1: 0.9920
Epoch 59/300
 - 7s - loss: 0.0044 - val_loss: 0.0042
 - val_f1: 0.9917
Epoch 60/300
 - 7s - loss: 0.0043 - val_loss: 0.0034
 - val_f1: 0.9950
Epoch 61/300
 - 7s - loss: 0.0043 - val_loss: 0.0039
2019-12-23 19:46:54,615 [INFO] epoch = 60. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/ann_model_epoch_60.pickle
 - val_f1: 0.9931
Epoch 62/300
 - 7s - loss: 0.0044 - val_loss: 0.0033
 - val_f1: 0.9944
Epoch 63/300
 - 7s - loss: 0.0044 - val_loss: 0.0032
 - val_f1: 0.9938
Epoch 64/300
 - 7s - loss: 0.0043 - val_loss: 0.0033
 - val_f1: 0.9917
Epoch 65/300
 - 7s - loss: 0.0042 - val_loss: 0.0031
 - val_f1: 0.9946
Epoch 66/300
 - 7s - loss: 0.0042 - val_loss: 0.0067
 - val_f1: 0.9747
Epoch 67/300
 - 7s - loss: 0.0043 - val_loss: 0.0038
 - val_f1: 0.9906
Epoch 68/300
 - 7s - loss: 0.0041 - val_loss: 0.0052
 - val_f1: 0.9892
Epoch 69/300
 - 7s - loss: 0.0047 - val_loss: 0.0063
 - val_f1: 0.9839
Epoch 70/300
 - 7s - loss: 0.0060 - val_loss: 0.0052
 - val_f1: 0.9896
Epoch 71/300
 - 7s - loss: 0.0056 - val_loss: 0.0044
 - val_f1: 0.9921
Epoch 72/300
 - 7s - loss: 0.0047 - val_loss: 0.0036
 - val_f1: 0.9922
Epoch 73/300
 - 7s - loss: 0.0048 - val_loss: 0.0038
 - val_f1: 0.9902
Epoch 74/300
 - 7s - loss: 0.0053 - val_loss: 0.0049
 - val_f1: 0.9893
Epoch 75/300
 - 7s - loss: 0.0047 - val_loss: 0.0035
 - val_f1: 0.9929
Epoch 76/300
 - 7s - loss: 0.0044 - val_loss: 0.0036
 - val_f1: 0.9912
Epoch 77/300
 - 7s - loss: 0.0044 - val_loss: 0.0032
 - val_f1: 0.9930
Epoch 78/300
 - 7s - loss: 0.0043 - val_loss: 0.0033
 - val_f1: 0.9945
Epoch 79/300
 - 7s - loss: 0.0041 - val_loss: 0.0049
 - val_f1: 0.9878
Epoch 80/300
 - 7s - loss: 0.0042 - val_loss: 0.0035
 - val_f1: 0.9927
Epoch 81/300
 - 7s - loss: 0.0041 - val_loss: 0.0031
 - val_f1: 0.9945
Epoch 82/300
 - 7s - loss: 0.0042 - val_loss: 0.0033
 - val_f1: 0.9927
Epoch 83/300
 - 7s - loss: 0.0040 - val_loss: 0.0033
 - val_f1: 0.9918
Epoch 84/300
 - 7s - loss: 0.0046 - val_loss: 0.0036
 - val_f1: 0.9923
Epoch 85/300
 - 7s - loss: 0.0044 - val_loss: 0.0033
 - val_f1: 0.9921
Epoch 86/300
 - 7s - loss: 0.0047 - val_loss: 0.0036
 - val_f1: 0.9920
Epoch 87/300
 - 7s - loss: 0.0045 - val_loss: 0.0035
 - val_f1: 0.9917
Epoch 88/300
 - 7s - loss: 0.0044 - val_loss: 0.0034
 - val_f1: 0.9927
Epoch 89/300
 - 7s - loss: 0.0042 - val_loss: 0.0039
 - val_f1: 0.9909
Epoch 90/300
 - 7s - loss: 0.0042 - val_loss: 0.0033
 - val_f1: 0.9933
Epoch 91/300
 - 7s - loss: 0.0045 - val_loss: 0.0031
2019-12-23 19:54:43,769 [INFO] epoch = 90. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/ann_model_epoch_90.pickle
 - val_f1: 0.9928
Epoch 92/300
 - 7s - loss: 0.0045 - val_loss: 0.0030
 - val_f1: 0.9929
Epoch 93/300
 - 7s - loss: 0.0044 - val_loss: 0.0031
 - val_f1: 0.9925
Epoch 94/300
 - 7s - loss: 0.0043 - val_loss: 0.0031
 - val_f1: 0.9922
Epoch 95/300
 - 7s - loss: 0.0043 - val_loss: 0.0034
 - val_f1: 0.9939
Epoch 96/300
 - 7s - loss: 0.0043 - val_loss: 0.0042
 - val_f1: 0.9904
Epoch 97/300
 - 7s - loss: 0.0044 - val_loss: 0.0031
 - val_f1: 0.9920
Epoch 98/300
 - 7s - loss: 0.0042 - val_loss: 0.0043
 - val_f1: 0.9940
Epoch 99/300
 - 7s - loss: 0.0042 - val_loss: 0.0035
 - val_f1: 0.9937
Epoch 100/300
 - 7s - loss: 0.0040 - val_loss: 0.0033
 - val_f1: 0.9929
Epoch 101/300
 - 7s - loss: 0.0043 - val_loss: 0.0034
 - val_f1: 0.9927
Epoch 102/300
 - 7s - loss: 0.0042 - val_loss: 0.0031
 - val_f1: 0.9941
Epoch 103/300
 - 7s - loss: 0.0041 - val_loss: 0.0033
 - val_f1: 0.9913
Epoch 104/300
 - 7s - loss: 0.0043 - val_loss: 0.0033
 - val_f1: 0.9914
Epoch 105/300
 - 7s - loss: 0.0043 - val_loss: 0.0029
 - val_f1: 0.9946
Epoch 106/300
 - 7s - loss: 0.0042 - val_loss: 0.0031
 - val_f1: 0.9938
Epoch 107/300
 - 7s - loss: 0.0042 - val_loss: 0.0033
 - val_f1: 0.9947
Epoch 108/300
 - 7s - loss: 0.0042 - val_loss: 0.0035
 - val_f1: 0.9918
Epoch 109/300
 - 7s - loss: 0.0042 - val_loss: 0.0032
 - val_f1: 0.9929
Epoch 110/300
 - 7s - loss: 0.0042 - val_loss: 0.0046
 - val_f1: 0.9904
Epoch 111/300
 - 7s - loss: 0.0041 - val_loss: 0.0030
 - val_f1: 0.9951
Epoch 112/300
 - 7s - loss: 0.0042 - val_loss: 0.0036
 - val_f1: 0.9954
Epoch 113/300
 - 7s - loss: 0.0041 - val_loss: 0.0035
 - val_f1: 0.9918
Epoch 114/300
 - 7s - loss: 0.0042 - val_loss: 0.0035
 - val_f1: 0.9917
Epoch 115/300
 - 7s - loss: 0.0041 - val_loss: 0.0041
 - val_f1: 0.9936
Epoch 116/300
 - 7s - loss: 0.0040 - val_loss: 0.0028
 - val_f1: 0.9950
Epoch 117/300
 - 7s - loss: 0.0041 - val_loss: 0.0058
 - val_f1: 0.9820
Epoch 118/300
 - 7s - loss: 0.0042 - val_loss: 0.0032
 - val_f1: 0.9933
Epoch 119/300
 - 7s - loss: 0.0041 - val_loss: 0.0042
 - val_f1: 0.9901
Epoch 120/300
 - 7s - loss: 0.0041 - val_loss: 0.0037
 - val_f1: 0.9915
Epoch 121/300
 - 7s - loss: 0.0040 - val_loss: 0.0033
2019-12-23 20:02:32,627 [INFO] epoch = 120. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/ann_model_epoch_120.pickle
 - val_f1: 0.9915
Epoch 122/300
 - 7s - loss: 0.0040 - val_loss: 0.0036
 - val_f1: 0.9929
Epoch 123/300
 - 7s - loss: 0.0040 - val_loss: 0.0032
 - val_f1: 0.9922
Epoch 124/300
 - 7s - loss: 0.0040 - val_loss: 0.0034
 - val_f1: 0.9915
Epoch 125/300
 - 7s - loss: 0.0039 - val_loss: 0.0033
 - val_f1: 0.9928
Epoch 126/300
 - 7s - loss: 0.0040 - val_loss: 0.0036
 - val_f1: 0.9914
Epoch 127/300
 - 7s - loss: 0.0040 - val_loss: 0.0035
 - val_f1: 0.9921
Epoch 128/300
 - 7s - loss: 0.0040 - val_loss: 0.0033
 - val_f1: 0.9937
Epoch 129/300
 - 7s - loss: 0.0040 - val_loss: 0.0030
 - val_f1: 0.9951
Epoch 130/300
 - 7s - loss: 0.0041 - val_loss: 0.0033
 - val_f1: 0.9934
Epoch 131/300
 - 7s - loss: 0.0040 - val_loss: 0.0036
 - val_f1: 0.9926
Epoch 132/300
 - 7s - loss: 0.0040 - val_loss: 0.0040
 - val_f1: 0.9914
Epoch 133/300
 - 7s - loss: 0.0038 - val_loss: 0.0032
 - val_f1: 0.9916
Epoch 134/300
 - 7s - loss: 0.0039 - val_loss: 0.0034
 - val_f1: 0.9933
Epoch 135/300
 - 7s - loss: 0.0040 - val_loss: 0.0030
 - val_f1: 0.9935
Epoch 136/300
 - 7s - loss: 0.0038 - val_loss: 0.0030
 - val_f1: 0.9944
Epoch 137/300
 - 7s - loss: 0.0041 - val_loss: 0.0033
 - val_f1: 0.9927
Epoch 138/300
 - 7s - loss: 0.0041 - val_loss: 0.0032
 - val_f1: 0.9931
Epoch 139/300
 - 7s - loss: 0.0040 - val_loss: 0.0031
 - val_f1: 0.9943
Epoch 140/300
 - 7s - loss: 0.0039 - val_loss: 0.0029
 - val_f1: 0.9947
Epoch 141/300
 - 7s - loss: 0.0039 - val_loss: 0.0036
 - val_f1: 0.9936
Epoch 142/300
 - 7s - loss: 0.0039 - val_loss: 0.0032
 - val_f1: 0.9939
Epoch 143/300
 - 7s - loss: 0.0040 - val_loss: 0.0032
 - val_f1: 0.9927
Epoch 144/300
 - 7s - loss: 0.0040 - val_loss: 0.0028
 - val_f1: 0.9953
Epoch 145/300
 - 7s - loss: 0.0039 - val_loss: 0.0039
 - val_f1: 0.9913
Epoch 146/300
 - 7s - loss: 0.0039 - val_loss: 0.0033
 - val_f1: 0.9940
Epoch 147/300
 - 7s - loss: 0.0039 - val_loss: 0.0030
 - val_f1: 0.9921
Epoch 148/300
 - 7s - loss: 0.0040 - val_loss: 0.0032
 - val_f1: 0.9943
Epoch 149/300
 - 7s - loss: 0.0038 - val_loss: 0.0034
 - val_f1: 0.9928
Epoch 150/300
 - 7s - loss: 0.0040 - val_loss: 0.0063
 - val_f1: 0.9773
Epoch 151/300
 - 7s - loss: 0.0040 - val_loss: 0.0029
2019-12-23 20:10:21,301 [INFO] epoch = 150. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/ann_model_epoch_150.pickle
 - val_f1: 0.9946
Epoch 152/300
 - 7s - loss: 0.0041 - val_loss: 0.0031
 - val_f1: 0.9917
Epoch 153/300
 - 7s - loss: 0.0039 - val_loss: 0.0035
 - val_f1: 0.9917
Epoch 154/300
 - 7s - loss: 0.0040 - val_loss: 0.0030
 - val_f1: 0.9923
Epoch 155/300
 - 7s - loss: 0.0039 - val_loss: 0.0032
 - val_f1: 0.9925
Epoch 156/300
 - 7s - loss: 0.0038 - val_loss: 0.0029
 - val_f1: 0.9940
Epoch 157/300
 - 7s - loss: 0.0038 - val_loss: 0.0033
 - val_f1: 0.9952
Epoch 158/300
 - 7s - loss: 0.0039 - val_loss: 0.0036
 - val_f1: 0.9907
Epoch 159/300
 - 7s - loss: 0.0039 - val_loss: 0.0031
 - val_f1: 0.9944
Epoch 160/300
 - 7s - loss: 0.0040 - val_loss: 0.0030
 - val_f1: 0.9945
Epoch 161/300
 - 7s - loss: 0.0040 - val_loss: 0.0030
 - val_f1: 0.9936
Epoch 162/300
 - 7s - loss: 0.0041 - val_loss: 0.0030
 - val_f1: 0.9922
Epoch 163/300
 - 7s - loss: 0.0039 - val_loss: 0.0026
 - val_f1: 0.9954
Epoch 164/300
 - 7s - loss: 0.0038 - val_loss: 0.0028
 - val_f1: 0.9955
Epoch 165/300
 - 7s - loss: 0.0039 - val_loss: 0.0038
 - val_f1: 0.9937
Epoch 166/300
 - 7s - loss: 0.0038 - val_loss: 0.0027
 - val_f1: 0.9950
Epoch 167/300
 - 7s - loss: 0.0039 - val_loss: 0.0031
 - val_f1: 0.9921
Epoch 168/300
 - 7s - loss: 0.0038 - val_loss: 0.0031
 - val_f1: 0.9917
Epoch 169/300
 - 7s - loss: 0.0036 - val_loss: 0.0029
 - val_f1: 0.9925
Epoch 170/300
 - 7s - loss: 0.0038 - val_loss: 0.0035
 - val_f1: 0.9927
Epoch 171/300
 - 7s - loss: 0.0038 - val_loss: 0.0029
 - val_f1: 0.9931
Epoch 172/300
 - 7s - loss: 0.0039 - val_loss: 0.0029
 - val_f1: 0.9942
Epoch 173/300
 - 7s - loss: 0.0040 - val_loss: 0.0028
 - val_f1: 0.9949
Epoch 174/300
 - 7s - loss: 0.0039 - val_loss: 0.0031
 - val_f1: 0.9922
Epoch 175/300
 - 7s - loss: 0.0037 - val_loss: 0.0032
 - val_f1: 0.9940
Epoch 176/300
 - 7s - loss: 0.0040 - val_loss: 0.0029
 - val_f1: 0.9943
Epoch 177/300
 - 7s - loss: 0.0040 - val_loss: 0.0031
 - val_f1: 0.9948
Epoch 178/300
 - 7s - loss: 0.0037 - val_loss: 0.0035
 - val_f1: 0.9909
Epoch 179/300
 - 7s - loss: 0.0036 - val_loss: 0.0030
 - val_f1: 0.9943
Epoch 180/300
 - 7s - loss: 0.0038 - val_loss: 0.0035
 - val_f1: 0.9918
Epoch 181/300
 - 7s - loss: 0.0038 - val_loss: 0.0034
2019-12-23 20:18:10,384 [INFO] epoch = 180. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/ann_model_epoch_180.pickle
 - val_f1: 0.9916
Epoch 182/300
 - 7s - loss: 0.0038 - val_loss: 0.0038
 - val_f1: 0.9931
Epoch 183/300
 - 7s - loss: 0.0038 - val_loss: 0.0029
 - val_f1: 0.9920
Epoch 184/300
 - 7s - loss: 0.0037 - val_loss: 0.0036
 - val_f1: 0.9926
Epoch 185/300
 - 7s - loss: 0.0039 - val_loss: 0.0029
 - val_f1: 0.9921
Epoch 186/300
 - 7s - loss: 0.0041 - val_loss: 0.0031
 - val_f1: 0.9922
Epoch 187/300
 - 7s - loss: 0.0040 - val_loss: 0.0035
 - val_f1: 0.9915
Epoch 188/300
 - 7s - loss: 0.0038 - val_loss: 0.0031
 - val_f1: 0.9917
Epoch 189/300
 - 7s - loss: 0.0037 - val_loss: 0.0029
 - val_f1: 0.9945
Epoch 190/300
 - 7s - loss: 0.0037 - val_loss: 0.0037
 - val_f1: 0.9918
Epoch 191/300
 - 7s - loss: 0.0038 - val_loss: 0.0033
 - val_f1: 0.9922
Epoch 192/300
 - 7s - loss: 0.0038 - val_loss: 0.0036
 - val_f1: 0.9955
Epoch 193/300
 - 7s - loss: 0.0038 - val_loss: 0.0030
 - val_f1: 0.9916
Epoch 194/300
 - 7s - loss: 0.0036 - val_loss: 0.0031
 - val_f1: 0.9940
Epoch 195/300
 - 7s - loss: 0.0038 - val_loss: 0.0027
 - val_f1: 0.9946
Epoch 196/300
 - 7s - loss: 0.0038 - val_loss: 0.0029
 - val_f1: 0.9923
Epoch 197/300
 - 7s - loss: 0.0039 - val_loss: 0.0030
 - val_f1: 0.9929
Epoch 198/300
 - 7s - loss: 0.0037 - val_loss: 0.0027
 - val_f1: 0.9959
Epoch 199/300
 - 7s - loss: 0.0036 - val_loss: 0.0029
 - val_f1: 0.9939
Epoch 200/300
 - 7s - loss: 0.0037 - val_loss: 0.0030
 - val_f1: 0.9945
Epoch 201/300
 - 7s - loss: 0.0038 - val_loss: 0.0030
 - val_f1: 0.9934
Epoch 202/300
 - 7s - loss: 0.0036 - val_loss: 0.0032
 - val_f1: 0.9916
Epoch 203/300
 - 7s - loss: 0.0038 - val_loss: 0.0029
 - val_f1: 0.9951
Epoch 204/300
 - 7s - loss: 0.0037 - val_loss: 0.0033
 - val_f1: 0.9915
Epoch 205/300
 - 7s - loss: 0.0037 - val_loss: 0.0029
 - val_f1: 0.9952
Epoch 206/300
 - 7s - loss: 0.0038 - val_loss: 0.0030
 - val_f1: 0.9924
Epoch 207/300
 - 7s - loss: 0.0038 - val_loss: 0.0029
 - val_f1: 0.9951
Epoch 208/300
 - 7s - loss: 0.0037 - val_loss: 0.0034
 - val_f1: 0.9916
Epoch 209/300
 - 7s - loss: 0.0036 - val_loss: 0.0034
 - val_f1: 0.9923
Epoch 210/300
 - 7s - loss: 0.0039 - val_loss: 0.0034
 - val_f1: 0.9924
Epoch 211/300
 - 7s - loss: 0.0037 - val_loss: 0.0030
2019-12-23 20:25:58,841 [INFO] epoch = 210. Intermediate model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/ann_model_epoch_210.pickle
 - val_f1: 0.9938
Epoch 212/300
 - 7s - loss: 0.0038 - val_loss: 0.0045
 - val_f1: 0.9904
Epoch 213/300
 - 7s - loss: 0.0035 - val_loss: 0.0032
2019-12-23 20:26:38,675 [INFO] WeightRestorer::on_train_end(): restoring best weights
2019-12-23 20:27:02,087 [INFO] Last epoch loss evaluation: train_loss = 0.002516, val_loss = 0.002587
2019-12-23 20:27:02,129 [INFO] Training complete. time_to_train = 6329.75 sec, 105.50 min
2019-12-23 20:27:02,137 [INFO] Model saved to results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/best_model.pickle
2019-12-23 20:27:02,321 [INFO] Plot saved to: results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/training_error_history.png
2019-12-23 20:27:02,488 [INFO] Plot saved to: results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/training_f1_history.png
2019-12-23 20:27:02,488 [INFO] Making predictions on training, validation, testing data
2019-12-23 20:28:28,299 [INFO] Evaluating predictions (results)
/home/sunanda/test/ml_env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2019-12-23 20:28:38,450 [INFO] Dataset: Testing. Classification report below
2019-12-23 20:28:38,450 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       1.00      1.00      1.00    454265
                   Bot       0.97      0.39      0.56       391
                  DDoS       1.00      1.00      1.00     25605
         DoS GoldenEye       0.99      0.99      0.99      2058
              DoS Hulk       0.99      0.98      0.98     46025
      DoS Slowhttptest       0.89      0.98      0.93      1100
         DoS slowloris       0.99      0.97      0.98      1159
           FTP-Patator       0.99      1.00      0.99      1587
              PortScan       0.99      1.00      1.00     31761
           SSH-Patator       0.96      0.98      0.97      1179
Web Attack Brute Force       1.00      0.11      0.20       302
        Web Attack XSS       0.00      0.00      0.00       130

             micro avg       1.00      1.00      1.00    565562
             macro avg       0.90      0.78      0.80    565562
          weighted avg       0.99      1.00      0.99    565562

2019-12-23 20:28:38,450 [INFO] Overall accuracy (micro avg): 0.9951994653106114
/home/sunanda/test/ids_experiments/utility.py:307: RuntimeWarning: invalid value encountered in true_divide
  PPV = TP / (TP + FP)    # Precision or positive predictive value
/home/sunanda/test/ids_experiments/utility.py:311: RuntimeWarning: invalid value encountered in true_divide
  FDR = FP / (TP + FP)    # False discovery rate
2019-12-23 20:28:50,009 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9952         0.9952                       0.9952                0.0004                   0.0048  0.9952
1     Macro avg        0.9992         0.8973                       0.7825                0.0015                   0.2175  0.7998
2  Weighted avg        0.9960         0.9950                       0.9952                0.0127                   0.0048  0.9948
2019-12-23 20:29:00,354 [INFO] Dataset: Validation. Classification report below
2019-12-23 20:29:00,354 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       1.00      1.00      1.00    454264
                   Bot       0.96      0.36      0.52       391
                  DDoS       1.00      1.00      1.00     25605
         DoS GoldenEye       0.99      0.99      0.99      2059
              DoS Hulk       0.99      0.98      0.99     46025
      DoS Slowhttptest       0.89      0.98      0.93      1099
         DoS slowloris       0.98      0.97      0.98      1159
           FTP-Patator       0.99      1.00      0.99      1587
              PortScan       0.99      1.00      1.00     31761
           SSH-Patator       0.96      0.97      0.97      1180
Web Attack Brute Force       0.92      0.08      0.14       301
        Web Attack XSS       0.80      0.03      0.06       131

             micro avg       1.00      1.00      1.00    565562
             macro avg       0.96      0.78      0.80    565562
          weighted avg       1.00      1.00      1.00    565562

2019-12-23 20:29:00,354 [INFO] Overall accuracy (micro avg): 0.9954204844031247
2019-12-23 20:29:12,091 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9954         0.9954                       0.9954                0.0004                   0.0046  0.9954
1     Macro avg        0.9992         0.9564                       0.7791                0.0014                   0.2209  0.7968
2  Weighted avg        0.9962         0.9953                       0.9954                0.0122                   0.0046  0.9950
2019-12-23 20:29:46,172 [INFO] Dataset: Training. Classification report below
2019-12-23 20:29:46,172 [INFO] 
                        precision    recall  f1-score   support

                BENIGN       1.00      1.00      1.00   1362791
                   Bot       0.98      0.37      0.54      1174
                  DDoS       1.00      1.00      1.00     76815
         DoS GoldenEye       0.99      0.99      0.99      6176
              DoS Hulk       0.99      0.98      0.98    138074
      DoS Slowhttptest       0.90      0.98      0.94      3300
         DoS slowloris       0.99      0.98      0.98      3478
           FTP-Patator       0.99      0.99      0.99      4761
              PortScan       0.99      1.00      1.00     95282
           SSH-Patator       0.97      0.97      0.97      3538
Web Attack Brute Force       0.99      0.09      0.17       904
        Web Attack XSS       1.00      0.03      0.06       391

             micro avg       1.00      1.00      1.00   1696684
             macro avg       0.98      0.78      0.80   1696684
          weighted avg       1.00      1.00      1.00   1696684

2019-12-23 20:29:46,172 [INFO] Overall accuracy (micro avg): 0.9954222471597539
2019-12-23 20:30:24,876 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9954         0.9954                       0.9954                0.0004                   0.0046  0.9954
1     Macro avg        0.9992         0.9826                       0.7829                0.0014                   0.2171  0.8029
2  Weighted avg        0.9962         0.9954                       0.9954                0.0123                   0.0046  0.9950
2019-12-23 20:30:24,927 [INFO] Results saved to: results_additional_exps/semi_sup_perf_ids17_ae_ann_rep3/semi_sup_perf_ids17_ae_ann_rep3_results.xlsx
2019-12-23 20:30:24,933 [INFO] ================= Finished running experiment no. 3 ================= 

2019-12-23 20:30:25,000 [INFO] ================= Finished running 6 experiments ================= 

 - val_f1: 0.9934
Epoch 00213: early stopping
Using TensorFlow backend.
2020-01-11 12:31:23,788 [INFO] Initialized logging. log_filename = results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/run_log.log
2020-01-11 12:31:23,788 [INFO] ================= Running experiment no. 1  ================= 

2020-01-11 12:31:23,788 [INFO] Experiment parameters given below
2020-01-11 12:31:23,788 [INFO] 
{'experiment_num': 1, 'results_dir': 'results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1', 'model_type': 'classifier', 'model': 'ae_ann', 'normal_label': 'BENIGN', 'split_random_seed': 42, 'unsupervised_ratio': 0.5, 'ae_encoder_units': [64], 'ae_encoder_activations': ['relu'], 'ae_encoder_dropout_rates': [0.2], 'ae_encoder_l1_param': -1, 'ae_decoder_units': '', 'ae_decoder_activations': '', 'ae_decoder_dropout_rates': '', 'output_activation': 'sigmoid', 'loss_function': 'binary_crossentropy', 'ae_epochs': 200, 'ann_layer_units': [64], 'ann_layer_activations': ['relu'], 'ann_layer_dropout_rates': [0.2], 'ann_epochs': 200, 'early_stop_patience': 50, 'batch_size': 256, 'dataset_dir': '../Datasets/small_datasets/ids2018', 'description': 'semi_sup_perf_ids18_subset_ae_ann_rep1'}
2020-01-11 12:31:23,788 [INFO] Created tensorboard log directory: results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/tf_logs_run_2020_01_11-12_31_23
2020-01-11 12:31:23,788 [INFO] Loading datsets from: ../Datasets/small_datasets/ids2018
2020-01-11 12:31:23,789 [INFO] Reading X, y files
2020-01-11 12:31:23,789 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_train.h5
2020-01-11 12:31:28,268 [INFO] Reading complete. time_to_read=4.48 seconds
2020-01-11 12:31:28,268 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_val.h5
2020-01-11 12:31:29,800 [INFO] Reading complete. time_to_read=1.53 seconds
2020-01-11 12:31:29,800 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_test.h5
2020-01-11 12:31:31,328 [INFO] Reading complete. time_to_read=1.53 seconds
2020-01-11 12:31:31,328 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_train.h5
2020-01-11 12:31:31,614 [INFO] Reading complete. time_to_read=0.29 seconds
2020-01-11 12:31:31,614 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_val.h5
2020-01-11 12:31:31,699 [INFO] Reading complete. time_to_read=0.08 seconds
2020-01-11 12:31:31,699 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_test.h5
2020-01-11 12:31:31,784 [INFO] Reading complete. time_to_read=0.08 seconds
2020-01-11 12:31:35,694 [INFO] Initializing model
2020-01-11 12:31:35,694 [WARNING] From /home/hasitha/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

2020-01-11 12:31:35,704 [WARNING] From /home/hasitha/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

2020-01-11 12:31:35,705 [WARNING] From /home/hasitha/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

2020-01-11 12:31:35,763 [WARNING] From /home/hasitha/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

2020-01-11 12:31:35,778 [WARNING] From /home/hasitha/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
2020-01-11 12:31:35,799 [WARNING] From /home/hasitha/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

2020-01-11 12:31:35,813 [WARNING] From /home/hasitha/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.

2020-01-11 12:31:35,815 [WARNING] From /home/hasitha/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2020-01-11 12:31:35,826 [INFO] _________________________________________________________________
2020-01-11 12:31:35,826 [INFO] Layer (type)                 Output Shape              Param #   
2020-01-11 12:31:35,826 [INFO] =================================================================
2020-01-11 12:31:35,826 [INFO] dense_1 (Dense)              (None, 64)                4992      
2020-01-11 12:31:35,826 [INFO] _________________________________________________________________
2020-01-11 12:31:35,826 [INFO] batch_normalization_1 (Batch (None, 64)                256       
2020-01-11 12:31:35,826 [INFO] _________________________________________________________________
2020-01-11 12:31:35,826 [INFO] dropout_1 (Dropout)          (None, 64)                0         
2020-01-11 12:31:35,826 [INFO] _________________________________________________________________
2020-01-11 12:31:35,826 [INFO] dense_2 (Dense)              (None, 77)                5005      
2020-01-11 12:31:35,826 [INFO] =================================================================
2020-01-11 12:31:35,826 [INFO] Total params: 10,253
2020-01-11 12:31:35,827 [INFO] Trainable params: 10,125
2020-01-11 12:31:35,827 [INFO] Non-trainable params: 128
2020-01-11 12:31:35,827 [INFO] _________________________________________________________________
2020-01-11 12:31:35,936 [INFO] _________________________________________________________________
2020-01-11 12:31:35,936 [INFO] Layer (type)                 Output Shape              Param #   
2020-01-11 12:31:35,936 [INFO] =================================================================
2020-01-11 12:31:35,936 [INFO] dense_3 (Dense)              (None, 64)                4160      
2020-01-11 12:31:35,936 [INFO] _________________________________________________________________
2020-01-11 12:31:35,936 [INFO] batch_normalization_2 (Batch (None, 64)                256       
2020-01-11 12:31:35,936 [INFO] _________________________________________________________________
2020-01-11 12:31:35,936 [INFO] dropout_2 (Dropout)          (None, 64)                0         
2020-01-11 12:31:35,936 [INFO] _________________________________________________________________
2020-01-11 12:31:35,936 [INFO] dense_4 (Dense)              (None, 15)                975       
2020-01-11 12:31:35,937 [INFO] =================================================================
2020-01-11 12:31:35,937 [INFO] Total params: 5,391
2020-01-11 12:31:35,937 [INFO] Trainable params: 5,263
2020-01-11 12:31:35,937 [INFO] Non-trainable params: 128
2020-01-11 12:31:35,937 [INFO] _________________________________________________________________
2020-01-11 12:31:35,937 [INFO] Training model
2020-01-11 12:31:35,937 [INFO] Splitting train set into 2 sets (unsupervised, supervised), random_seed = 42
2020-01-11 12:31:59,124 [INFO] Split sizes (instances). total = 1936462, unsupervised = 968231, supervised = 968231, unsupervised dataset hash = 29de1dd377723229ed7420efecc6b6eb2708be48
2020-01-11 12:31:59,124 [INFO] Training autoencoder
2020-01-11 12:31:59.868457: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-01-11 12:31:59.888851: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2893425000 Hz
2020-01-11 12:31:59.889048: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56338326ea50 executing computations on platform Host. Devices:
2020-01-11 12:31:59.889072: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2020-01-11 12:32:00.036190: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2020-01-11 12:32:00,041 [WARNING] From /home/hasitha/anaconda3/lib/python3.7/site-packages/keras/callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

2020-01-11 12:32:00,041 [WARNING] From /home/hasitha/anaconda3/lib/python3.7/site-packages/keras/callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.

Train on 968231 samples, validate on 645487 samples
Epoch 1/200
 - 16s - loss: -3.2641e+00 - val_loss: -3.6478e+00
Epoch 2/200
 - 16s - loss: -3.6192e+00 - val_loss: -3.6371e+00
Epoch 3/200
 - 16s - loss: -3.6307e+00 - val_loss: -3.6570e+00
Epoch 4/200
 - 16s - loss: -3.6374e+00 - val_loss: -3.6679e+00
Epoch 5/200
 - 16s - loss: -3.6416e+00 - val_loss: -3.6701e+00
Epoch 6/200
 - 16s - loss: -3.6428e+00 - val_loss: -3.6702e+00
Epoch 7/200
 - 16s - loss: -3.6435e+00 - val_loss: -3.6545e+00
Epoch 8/200
 - 16s - loss: -3.6467e+00 - val_loss: -3.6718e+00
Epoch 9/200
 - 16s - loss: -3.6477e+00 - val_loss: -3.6724e+00
Epoch 10/200
 - 16s - loss: -3.6488e+00 - val_loss: -3.6699e+00
Epoch 11/200
 - 16s - loss: -3.6500e+00 - val_loss: -3.6307e+00
Epoch 12/200
 - 16s - loss: -3.6502e+00 - val_loss: -3.6735e+00
Epoch 13/200
 - 16s - loss: -3.6483e+00 - val_loss: -3.6743e+00
Epoch 14/200
 - 16s - loss: -3.6510e+00 - val_loss: -3.6329e+00
Epoch 15/200
 - 16s - loss: -3.6519e+00 - val_loss: -3.6728e+00
Epoch 16/200
 - 16s - loss: -3.6515e+00 - val_loss: -3.6280e+00
Epoch 17/200
 - 16s - loss: -3.6524e+00 - val_loss: -3.6741e+00
Epoch 18/200
 - 16s - loss: -3.6528e+00 - val_loss: -3.6582e+00
Epoch 19/200
 - 16s - loss: -3.6525e+00 - val_loss: -3.6739e+00
Epoch 20/200
 - 16s - loss: -3.6534e+00 - val_loss: -3.6737e+00
Epoch 21/200
 - 16s - loss: -3.6529e+00 - val_loss: -3.6579e+00
2020-01-11 12:37:31,483 [INFO] epoch = 20. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/ae_model_epoch_20.pickle
Epoch 22/200
 - 16s - loss: -3.6538e+00 - val_loss: -3.6734e+00
Epoch 23/200
 - 16s - loss: -3.6530e+00 - val_loss: -3.6735e+00
Epoch 24/200
 - 16s - loss: -3.6543e+00 - val_loss: -3.6726e+00
Epoch 25/200
 - 16s - loss: -3.6514e+00 - val_loss: -3.6562e+00
Epoch 26/200
 - 16s - loss: -3.6541e+00 - val_loss: -3.6711e+00
Epoch 27/200
 - 16s - loss: -3.6547e+00 - val_loss: -3.6742e+00
Epoch 28/200
 - 16s - loss: -3.6546e+00 - val_loss: -3.6739e+00
Epoch 29/200
 - 16s - loss: -3.6519e+00 - val_loss: -3.6740e+00
Epoch 30/200
 - 16s - loss: -3.6546e+00 - val_loss: -3.6732e+00
Epoch 31/200
 - 16s - loss: -3.6550e+00 - val_loss: -3.6656e+00
Epoch 32/200
 - 16s - loss: -3.6549e+00 - val_loss: -3.6746e+00
Epoch 33/200
 - 16s - loss: -3.6549e+00 - val_loss: -3.6215e+00
Epoch 34/200
 - 16s - loss: -3.6557e+00 - val_loss: -3.6746e+00
Epoch 35/200
 - 16s - loss: -3.6543e+00 - val_loss: -3.6715e+00
Epoch 36/200
 - 16s - loss: -3.6547e+00 - val_loss: -3.6721e+00
Epoch 37/200
 - 16s - loss: -3.6552e+00 - val_loss: -3.6737e+00
Epoch 38/200
 - 16s - loss: -3.6553e+00 - val_loss: -3.6507e+00
Epoch 39/200
 - 16s - loss: -3.6559e+00 - val_loss: -3.6627e+00
Epoch 40/200
 - 16s - loss: -3.6527e+00 - val_loss: -3.6721e+00
Epoch 41/200
 - 16s - loss: -3.6545e+00 - val_loss: -3.6743e+00
2020-01-11 12:42:54,213 [INFO] epoch = 40. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/ae_model_epoch_40.pickle
Epoch 42/200
 - 16s - loss: -3.6555e+00 - val_loss: -3.6750e+00
Epoch 43/200
 - 16s - loss: -3.6556e+00 - val_loss: -3.6742e+00
Epoch 44/200
 - 16s - loss: -3.6555e+00 - val_loss: -3.6691e+00
Epoch 45/200
 - 16s - loss: -3.6558e+00 - val_loss: -3.6738e+00
Epoch 46/200
 - 16s - loss: -3.6552e+00 - val_loss: -3.6524e+00
Epoch 47/200
 - 16s - loss: -3.6549e+00 - val_loss: -3.6753e+00
Epoch 48/200
 - 16s - loss: -3.6567e+00 - val_loss: -3.6752e+00
Epoch 49/200
 - 16s - loss: -3.6562e+00 - val_loss: -3.6474e+00
Epoch 50/200
 - 16s - loss: -3.6557e+00 - val_loss: -3.6751e+00
Epoch 51/200
 - 16s - loss: -3.6565e+00 - val_loss: -3.6740e+00
Epoch 52/200
 - 16s - loss: -3.6537e+00 - val_loss: -3.6752e+00
Epoch 53/200
 - 16s - loss: -3.6561e+00 - val_loss: -3.6754e+00
Epoch 54/200
 - 16s - loss: -3.6565e+00 - val_loss: -3.6736e+00
Epoch 55/200
 - 16s - loss: -3.6559e+00 - val_loss: -3.6756e+00
Epoch 56/200
 - 16s - loss: -3.6569e+00 - val_loss: -3.6737e+00
Epoch 57/200
 - 16s - loss: -3.6553e+00 - val_loss: -3.6755e+00
Epoch 58/200
 - 16s - loss: -3.6566e+00 - val_loss: -3.6753e+00
Epoch 59/200
 - 16s - loss: -3.6567e+00 - val_loss: -3.6756e+00
Epoch 60/200
 - 16s - loss: -3.6568e+00 - val_loss: -3.6687e+00
Epoch 61/200
 - 16s - loss: -3.6571e+00 - val_loss: -3.6654e+00
2020-01-11 12:48:16,813 [INFO] epoch = 60. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/ae_model_epoch_60.pickle
Epoch 62/200
 - 16s - loss: -3.6529e+00 - val_loss: -3.6755e+00
Epoch 63/200
 - 16s - loss: -3.6559e+00 - val_loss: -3.6766e+00
Epoch 64/200
 - 16s - loss: -3.6565e+00 - val_loss: -3.6769e+00
Epoch 65/200
 - 16s - loss: -3.6573e+00 - val_loss: -3.6758e+00
Epoch 66/200
 - 16s - loss: -3.6560e+00 - val_loss: -3.6756e+00
Epoch 67/200
 - 16s - loss: -3.6569e+00 - val_loss: -3.6757e+00
Epoch 68/200
 - 16s - loss: -3.6542e+00 - val_loss: -3.6762e+00
Epoch 69/200
 - 16s - loss: -3.6573e+00 - val_loss: -3.6765e+00
Epoch 70/200
 - 16s - loss: -3.6553e+00 - val_loss: -3.6613e+00
Epoch 71/200
 - 16s - loss: -3.6563e+00 - val_loss: -3.6691e+00
Epoch 72/200
 - 16s - loss: -3.6555e+00 - val_loss: -3.6620e+00
Epoch 73/200
 - 16s - loss: -3.6575e+00 - val_loss: -3.6761e+00
Epoch 74/200
 - 16s - loss: -3.6573e+00 - val_loss: -3.6751e+00
Epoch 75/200
 - 16s - loss: -3.6533e+00 - val_loss: -3.6588e+00
Epoch 76/200
 - 16s - loss: -3.6561e+00 - val_loss: -3.6755e+00
Epoch 77/200
 - 16s - loss: -3.6574e+00 - val_loss: -3.6754e+00
Epoch 78/200
 - 16s - loss: -3.6578e+00 - val_loss: -3.6755e+00
Epoch 79/200
 - 16s - loss: -3.6562e+00 - val_loss: -3.6753e+00
Epoch 80/200
 - 16s - loss: -3.6576e+00 - val_loss: -3.6758e+00
Epoch 81/200
 - 16s - loss: -3.6539e+00 - val_loss: -3.6633e+00
2020-01-11 12:53:39,647 [INFO] epoch = 80. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/ae_model_epoch_80.pickle
Epoch 82/200
 - 16s - loss: -3.6571e+00 - val_loss: -3.6758e+00
Epoch 83/200
 - 16s - loss: -3.6567e+00 - val_loss: -3.6534e+00
Epoch 84/200
 - 16s - loss: -3.6566e+00 - val_loss: -3.6753e+00
Epoch 85/200
 - 16s - loss: -3.6574e+00 - val_loss: -3.6678e+00
Epoch 86/200
 - 16s - loss: -3.6541e+00 - val_loss: -3.6758e+00
Epoch 87/200
 - 16s - loss: -3.6563e+00 - val_loss: -3.6344e+00
Epoch 88/200
 - 16s - loss: -3.6572e+00 - val_loss: -3.6760e+00
Epoch 89/200
 - 16s - loss: -3.6565e+00 - val_loss: -3.6734e+00
Epoch 90/200
 - 16s - loss: -3.6579e+00 - val_loss: -3.6768e+00
Epoch 91/200
 - 16s - loss: -3.6572e+00 - val_loss: -3.6755e+00
Epoch 92/200
 - 16s - loss: -3.6581e+00 - val_loss: -3.6660e+00
Epoch 93/200
 - 16s - loss: -3.6569e+00 - val_loss: -3.6518e+00
Epoch 94/200
 - 16s - loss: -3.6575e+00 - val_loss: -3.6769e+00
Epoch 95/200
 - 16s - loss: -3.6579e+00 - val_loss: -3.6761e+00
Epoch 96/200
 - 16s - loss: -3.6581e+00 - val_loss: -3.6760e+00
Epoch 97/200
 - 16s - loss: -3.6574e+00 - val_loss: -3.6760e+00
Epoch 98/200
 - 16s - loss: -3.6565e+00 - val_loss: -3.6566e+00
Epoch 99/200
 - 16s - loss: -3.6582e+00 - val_loss: -3.6775e+00
Epoch 100/200
 - 16s - loss: -3.6580e+00 - val_loss: -3.6760e+00
Epoch 101/200
 - 16s - loss: -3.6578e+00 - val_loss: -3.6773e+00
2020-01-11 12:59:03,998 [INFO] epoch = 100. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/ae_model_epoch_100.pickle
Epoch 102/200
 - 16s - loss: -3.6572e+00 - val_loss: -3.6775e+00
Epoch 103/200
 - 16s - loss: -3.6578e+00 - val_loss: -3.6761e+00
Epoch 104/200
 - 16s - loss: -3.6564e+00 - val_loss: -3.6747e+00
Epoch 105/200
 - 16s - loss: -3.6578e+00 - val_loss: -3.6757e+00
Epoch 106/200
 - 16s - loss: -3.6558e+00 - val_loss: -3.6655e+00
Epoch 107/200
 - 16s - loss: -3.6578e+00 - val_loss: -3.6462e+00
Epoch 108/200
 - 16s - loss: -3.6576e+00 - val_loss: -3.6576e+00
Epoch 109/200
 - 16s - loss: -3.6576e+00 - val_loss: -3.6769e+00
Epoch 110/200
 - 16s - loss: -3.6582e+00 - val_loss: -3.6763e+00
Epoch 111/200
 - 16s - loss: -3.6579e+00 - val_loss: -3.6764e+00
Epoch 112/200
 - 16s - loss: -3.6562e+00 - val_loss: -3.6662e+00
Epoch 113/200
 - 16s - loss: -3.6577e+00 - val_loss: -3.6771e+00
Epoch 114/200
 - 16s - loss: -3.6580e+00 - val_loss: -3.6655e+00
Epoch 115/200
 - 16s - loss: -3.6580e+00 - val_loss: -3.6742e+00
Epoch 116/200
 - 16s - loss: -3.6574e+00 - val_loss: -3.6568e+00
Epoch 117/200
 - 16s - loss: -3.6582e+00 - val_loss: -3.6762e+00
Epoch 118/200
 - 16s - loss: -3.6580e+00 - val_loss: -3.6774e+00
Epoch 119/200
 - 16s - loss: -3.6582e+00 - val_loss: -3.6769e+00
Epoch 120/200
 - 16s - loss: -3.6578e+00 - val_loss: -3.6449e+00
Epoch 121/200
 - 16s - loss: -3.6581e+00 - val_loss: -3.6712e+00
2020-01-11 13:04:25,712 [INFO] epoch = 120. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/ae_model_epoch_120.pickle
Epoch 122/200
 - 16s - loss: -3.6582e+00 - val_loss: -3.6770e+00
Epoch 123/200
 - 16s - loss: -3.6583e+00 - val_loss: -3.6753e+00
Epoch 124/200
 - 16s - loss: -3.6565e+00 - val_loss: -3.6763e+00
Epoch 125/200
 - 16s - loss: -3.6577e+00 - val_loss: -3.6558e+00
Epoch 126/200
 - 16s - loss: -3.6550e+00 - val_loss: -3.6693e+00
Epoch 127/200
 - 16s - loss: -3.6576e+00 - val_loss: -3.6510e+00
Epoch 128/200
 - 16s - loss: -3.6578e+00 - val_loss: -3.6765e+00
Epoch 129/200
 - 16s - loss: -3.6575e+00 - val_loss: -3.6758e+00
Epoch 130/200
 - 16s - loss: -3.6575e+00 - val_loss: -3.6765e+00
Epoch 131/200
 - 16s - loss: -3.6563e+00 - val_loss: -3.6770e+00
Epoch 132/200
 - 16s - loss: -3.6580e+00 - val_loss: -3.6763e+00
Epoch 133/200
 - 16s - loss: -3.6581e+00 - val_loss: -3.6745e+00
Epoch 134/200
 - 16s - loss: -3.6563e+00 - val_loss: -3.6514e+00
Epoch 135/200
 - 16s - loss: -3.6578e+00 - val_loss: -3.6762e+00
Epoch 136/200
 - 16s - loss: -3.6570e+00 - val_loss: -3.6517e+00
Epoch 137/200
 - 16s - loss: -3.6579e+00 - val_loss: -3.6765e+00
Epoch 138/200
 - 16s - loss: -3.6570e+00 - val_loss: -3.6600e+00
Epoch 139/200
 - 16s - loss: -3.6565e+00 - val_loss: -3.6523e+00
Epoch 140/200
 - 16s - loss: -3.6572e+00 - val_loss: -3.6771e+00
Epoch 141/200
 - 16s - loss: -3.6577e+00 - val_loss: -3.6763e+00
2020-01-11 13:09:51,349 [INFO] epoch = 140. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/ae_model_epoch_140.pickle
Epoch 142/200
 - 16s - loss: -3.6569e+00 - val_loss: -3.6774e+00
Epoch 143/200
 - 16s - loss: -3.6575e+00 - val_loss: -3.6772e+00
Epoch 144/200
 - 16s - loss: -3.6574e+00 - val_loss: -3.6769e+00
Epoch 145/200
 - 16s - loss: -3.6581e+00 - val_loss: -3.6767e+00
Epoch 146/200
 - 16s - loss: -3.6574e+00 - val_loss: -3.6755e+00
Epoch 147/200
 - 16s - loss: -3.6572e+00 - val_loss: -3.6749e+00
Epoch 148/200
 - 16s - loss: -3.6572e+00 - val_loss: -3.6766e+00
Epoch 149/200
 - 16s - loss: -3.6579e+00 - val_loss: -3.6767e+00
Epoch 150/200
 - 16s - loss: -3.6579e+00 - val_loss: -3.6768e+00
Epoch 151/200
 - 16s - loss: -3.6573e+00 - val_loss: -3.6577e+00
Epoch 152/200
 - 16s - loss: -3.6580e+00 - val_loss: -3.6697e+00
2020-01-11 13:12:49,249 [INFO] WeightRestorer::on_train_end(): restoring best weights
2020-01-11 13:13:13,316 [INFO] Last epoch loss evaluation: train_loss = -3.675556, val_loss = -3.677525
2020-01-11 13:13:13,316 [INFO] Training autoencoder complete
2020-01-11 13:13:13,316 [INFO] Encoding data for supervised training
2020-01-11 13:13:30,608 [INFO] Encoding complete
2020-01-11 13:13:30,608 [INFO] Training neural network layers (after autoencoder)
Epoch 00152: early stopping
Train on 968231 samples, validate on 645487 samples
Epoch 1/200
 - 8s - loss: 0.0173 - val_loss: 0.0086
 - val_f1: 0.9771
Epoch 2/200
 - 8s - loss: 0.0087 - val_loss: 0.0083
 - val_f1: 0.9776
Epoch 3/200
 - 8s - loss: 0.0085 - val_loss: 0.0081
 - val_f1: 0.9775
Epoch 4/200
 - 8s - loss: 0.0084 - val_loss: 0.0081
 - val_f1: 0.9775
Epoch 5/200
 - 8s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9775
Epoch 6/200
 - 8s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9777
Epoch 7/200
 - 8s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9778
Epoch 8/200
 - 8s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9785
Epoch 9/200
 - 8s - loss: 0.0081 - val_loss: 0.0081
 - val_f1: 0.9775
Epoch 10/200
 - 8s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9780
Epoch 11/200
 - 8s - loss: 0.0081 - val_loss: 0.0082
 - val_f1: 0.9775
Epoch 12/200
 - 8s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 13/200
 - 8s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 14/200
 - 8s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 15/200
 - 8s - loss: 0.0080 - val_loss: 0.0078
 - val_f1: 0.9779
Epoch 16/200
 - 8s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 17/200
 - 8s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 18/200
 - 8s - loss: 0.0080 - val_loss: 0.0078
 - val_f1: 0.9780
Epoch 19/200
 - 8s - loss: 0.0080 - val_loss: 0.0078
 - val_f1: 0.9779
Epoch 20/200
 - 8s - loss: 0.0080 - val_loss: 0.0078
 - val_f1: 0.9779
Epoch 21/200
 - 8s - loss: 0.0080 - val_loss: 0.0079
/home/hasitha/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2020-01-11 13:19:11,558 [INFO] epoch = 20. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/ann_model_epoch_20.pickle
 - val_f1: 0.9764
Epoch 22/200
 - 8s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9778
Epoch 23/200
 - 9s - loss: 0.0080 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 24/200
 - 9s - loss: 0.0080 - val_loss: 0.0078
 - val_f1: 0.9789
Epoch 25/200
 - 9s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9766
Epoch 26/200
 - 9s - loss: 0.0080 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 27/200
 - 9s - loss: 0.0080 - val_loss: 0.0078
 - val_f1: 0.9789
Epoch 28/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 29/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 30/200
 - 9s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 31/200
 - 9s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9784
Epoch 32/200
 - 9s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9766
Epoch 33/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9787
Epoch 34/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 35/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 36/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9788
Epoch 37/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9787
Epoch 38/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 39/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 40/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 41/200
 - 9s - loss: 0.0079 - val_loss: 0.0079
2020-01-11 13:24:49,256 [INFO] epoch = 40. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/ann_model_epoch_40.pickle
 - val_f1: 0.9784
Epoch 42/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 43/200
 - 9s - loss: 0.0079 - val_loss: 0.0077
 - val_f1: 0.9788
Epoch 44/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 45/200
 - 9s - loss: 0.0079 - val_loss: 0.0077
 - val_f1: 0.9783
Epoch 46/200
 - 9s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 47/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 48/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9794
Epoch 49/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 50/200
 - 9s - loss: 0.0079 - val_loss: 0.0077
 - val_f1: 0.9784
Epoch 51/200
 - 9s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9784
Epoch 52/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 53/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 54/200
 - 9s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 55/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 56/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9788
Epoch 57/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9789
Epoch 58/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 59/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9770
Epoch 60/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9783
Epoch 61/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
2020-01-11 13:30:27,382 [INFO] epoch = 60. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/ann_model_epoch_60.pickle
 - val_f1: 0.9785
Epoch 62/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 63/200
 - 8s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 64/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 65/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9780
Epoch 66/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 67/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 68/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 69/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 70/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9784
Epoch 71/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9787
Epoch 72/200
 - 8s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 73/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9782
Epoch 74/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9786
Epoch 75/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9783
Epoch 76/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9784
Epoch 77/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 78/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9784
Epoch 79/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 80/200
 - 9s - loss: 0.0078 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 81/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
2020-01-11 13:36:05,783 [INFO] epoch = 80. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/ann_model_epoch_80.pickle
 - val_f1: 0.9785
Epoch 82/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9785
Epoch 83/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9786
Epoch 84/200
 - 9s - loss: 0.0078 - val_loss: 0.0076
 - val_f1: 0.9788
Epoch 85/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9771
Epoch 86/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9790
Epoch 87/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9783
Epoch 88/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9787
Epoch 89/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 90/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9784
Epoch 91/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9783
Epoch 92/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9784
Epoch 93/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9785
Epoch 94/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9785
Epoch 95/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9769
Epoch 96/200
 - 9s - loss: 0.0078 - val_loss: 0.0076
 - val_f1: 0.9789
Epoch 97/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9784
Epoch 98/200
 - 9s - loss: 0.0078 - val_loss: 0.0076
 - val_f1: 0.9790
Epoch 99/200
 - 9s - loss: 0.0077 - val_loss: 0.0076
 - val_f1: 0.9788
Epoch 100/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9780
Epoch 101/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
2020-01-11 13:41:46,260 [INFO] epoch = 100. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/ann_model_epoch_100.pickle
 - val_f1: 0.9782
Epoch 102/200
 - 9s - loss: 0.0078 - val_loss: 0.0076
 - val_f1: 0.9769
Epoch 103/200
 - 9s - loss: 0.0077 - val_loss: 0.0076
 - val_f1: 0.9787
Epoch 104/200
 - 9s - loss: 0.0078 - val_loss: 0.0076
 - val_f1: 0.9788
Epoch 105/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9788
Epoch 106/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9793
Epoch 107/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9786
Epoch 108/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9784
Epoch 109/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9783
Epoch 110/200
 - 9s - loss: 0.0078 - val_loss: 0.0076
 - val_f1: 0.9788
Epoch 111/200
 - 9s - loss: 0.0078 - val_loss: 0.0076
 - val_f1: 0.9791
Epoch 112/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 113/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9787
Epoch 114/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9784
Epoch 115/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 116/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9765
Epoch 117/200
 - 9s - loss: 0.0077 - val_loss: 0.0076
 - val_f1: 0.9788
Epoch 118/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 119/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9788
Epoch 120/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9788
Epoch 121/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
2020-01-11 13:47:26,789 [INFO] epoch = 120. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/ann_model_epoch_120.pickle
 - val_f1: 0.9781
Epoch 122/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9784
Epoch 123/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9785
Epoch 124/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9787
Epoch 125/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9783
Epoch 126/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9783
Epoch 127/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9784
Epoch 128/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9787
Epoch 129/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9785
Epoch 130/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9781
Epoch 131/200
 - 9s - loss: 0.0078 - val_loss: 0.0076
 - val_f1: 0.9788
Epoch 132/200
 - 9s - loss: 0.0077 - val_loss: 0.0076
 - val_f1: 0.9787
Epoch 133/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9786
Epoch 134/200
 - 9s - loss: 0.0077 - val_loss: 0.0076
 - val_f1: 0.9786
Epoch 135/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9788
Epoch 136/200
 - 9s - loss: 0.0077 - val_loss: 0.0076
 - val_f1: 0.9789
Epoch 137/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9786
Epoch 138/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9783
Epoch 139/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9783
Epoch 140/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9787
Epoch 141/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
2020-01-11 13:53:08,392 [INFO] epoch = 140. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/ann_model_epoch_140.pickle
 - val_f1: 0.9783
Epoch 142/200
 - 9s - loss: 0.0077 - val_loss: 0.0076
 - val_f1: 0.9789
Epoch 143/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9783
Epoch 144/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9783
Epoch 145/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9785
Epoch 146/200
 - 9s - loss: 0.0077 - val_loss: 0.0076
 - val_f1: 0.9805
Epoch 147/200
 - 9s - loss: 0.0077 - val_loss: 0.0076
 - val_f1: 0.9788
Epoch 148/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 149/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9788
Epoch 150/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9787
Epoch 151/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9785
Epoch 152/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9787
Epoch 153/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9787
Epoch 154/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9791
Epoch 155/200
 - 9s - loss: 0.0077 - val_loss: 0.0076
 - val_f1: 0.9787
Epoch 156/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9788
Epoch 157/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9785
Epoch 158/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9782
Epoch 159/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9785
Epoch 160/200
 - 9s - loss: 0.0077 - val_loss: 0.0076
 - val_f1: 0.9788
Epoch 161/200
 - 9s - loss: 0.0077 - val_loss: 0.0076
2020-01-11 13:58:50,454 [INFO] epoch = 160. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/ann_model_epoch_160.pickle
 - val_f1: 0.9788
Epoch 162/200
 - 9s - loss: 0.0077 - val_loss: 0.0076
 - val_f1: 0.9789
Epoch 163/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9782
Epoch 164/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9788
Epoch 165/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9787
Epoch 166/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9789
Epoch 167/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9782
Epoch 168/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9783
Epoch 169/200
 - 9s - loss: 0.0077 - val_loss: 0.0076
 - val_f1: 0.9785
Epoch 170/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9786
Epoch 171/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9787
Epoch 172/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9783
Epoch 173/200
 - 9s - loss: 0.0077 - val_loss: 0.0076
 - val_f1: 0.9792
Epoch 174/200
 - 9s - loss: 0.0077 - val_loss: 0.0076
 - val_f1: 0.9789
Epoch 175/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9789
Epoch 176/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9790
Epoch 177/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9786
Epoch 178/200
 - 9s - loss: 0.0077 - val_loss: 0.0076
 - val_f1: 0.9792
Epoch 179/200
 - 9s - loss: 0.0077 - val_loss: 0.0076
 - val_f1: 0.9787
Epoch 180/200
 - 9s - loss: 0.0077 - val_loss: 0.0076
 - val_f1: 0.9786
Epoch 181/200
 - 9s - loss: 0.0077 - val_loss: 0.0076
2020-01-11 14:04:32,155 [INFO] epoch = 180. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/ann_model_epoch_180.pickle
 - val_f1: 0.9788
Epoch 182/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9785
Epoch 183/200
 - 9s - loss: 0.0077 - val_loss: 0.0076
 - val_f1: 0.9772
Epoch 184/200
 - 9s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 185/200
 - 9s - loss: 0.0077 - val_loss: 0.0076
 - val_f1: 0.9784
Epoch 186/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9790
Epoch 187/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9800
Epoch 188/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9787
Epoch 189/200
 - 9s - loss: 0.0077 - val_loss: 0.0076
 - val_f1: 0.9788
Epoch 190/200
 - 9s - loss: 0.0077 - val_loss: 0.0076
 - val_f1: 0.9793
Epoch 191/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9789
Epoch 192/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9787
Epoch 193/200
 - 9s - loss: 0.0077 - val_loss: 0.0076
 - val_f1: 0.9786
Epoch 194/200
 - 9s - loss: 0.0076 - val_loss: 0.0076
 - val_f1: 0.9786
Epoch 195/200
 - 9s - loss: 0.0077 - val_loss: 0.0076
 - val_f1: 0.9793
Epoch 196/200
 - 9s - loss: 0.0076 - val_loss: 0.0076
 - val_f1: 0.9788
Epoch 197/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9783
Epoch 198/200
 - 9s - loss: 0.0077 - val_loss: 0.0076
 - val_f1: 0.9791
Epoch 199/200
 - 9s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 200/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
2020-01-11 14:10:05,581 [INFO] WeightRestorer::on_train_end(): restoring best weights
2020-01-11 14:10:31,978 [INFO] Last epoch loss evaluation: train_loss = 0.007480, val_loss = 0.007567
2020-01-11 14:10:32,025 [INFO] Training complete. time_to_train = 5936.09 sec, 98.93 min
2020-01-11 14:10:32,032 [INFO] Model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/best_model.pickle
2020-01-11 14:10:32,035 [INFO] Training history saved to: results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/training_error_history.csv
2020-01-11 14:10:32,182 [INFO] Plot saved to: results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/training_error_history.png
2020-01-11 14:10:32,311 [INFO] Plot saved to: results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/training_f1_history.png
2020-01-11 14:10:32,311 [INFO] Making predictions on training, validation, testing data
2020-01-11 14:11:51,718 [INFO] Evaluating predictions (results)
2020-01-11 14:12:03,940 [INFO] Dataset: Testing. Classification report below
2020-01-11 14:12:03,941 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99    535650
                     Bot       1.00      1.00      1.00     11465
        Brute Force -Web       1.00      0.33      0.50        24
        Brute Force -XSS       0.75      0.33      0.46         9
        DDOS attack-HOIC       1.00      1.00      1.00     27447
    DDOS attack-LOIC-UDP       0.80      0.94      0.86        67
  DDoS attacks-LOIC-HTTP       1.00      1.00      1.00     23010
   DoS attacks-GoldenEye       0.99      1.00      1.00      1651
        DoS attacks-Hulk       1.00      1.00      1.00     18478
DoS attacks-SlowHTTPTest       0.74      0.46      0.57      5596
   DoS attacks-Slowloris       0.97      0.97      0.97       440
          FTP-BruteForce       0.69      0.88      0.78      7718
           Infilteration       0.46      0.01      0.03      6404
           SQL Injection       1.00      0.25      0.40         4
          SSH-Bruteforce       1.00      1.00      1.00      7525

               micro avg       0.98      0.98      0.98    645488
               macro avg       0.89      0.75      0.77    645488
            weighted avg       0.98      0.98      0.98    645488

2020-01-11 14:12:03,941 [INFO] Overall accuracy (micro avg): 0.9837208437647176
2020-01-11 14:12:17,822 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9837         0.9837                       0.9837                0.0012                   0.0163  0.9837
1     Macro avg        0.9978         0.8924                       0.7457                0.0043                   0.2543  0.7702
2  Weighted avg        0.9913         0.9788                       0.9837                0.0483                   0.0163  0.9787
2020-01-11 14:12:29,958 [INFO] Dataset: Validation. Classification report below
2020-01-11 14:12:29,958 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99    535650
                     Bot       1.00      1.00      1.00     11465
        Brute Force -Web       0.92      0.48      0.63        25
        Brute Force -XSS       1.00      0.67      0.80         9
        DDOS attack-HOIC       1.00      1.00      1.00     27447
    DDOS attack-LOIC-UDP       0.81      0.93      0.86        68
  DDoS attacks-LOIC-HTTP       1.00      1.00      1.00     23009
   DoS attacks-GoldenEye       0.99      1.00      1.00      1651
        DoS attacks-Hulk       1.00      1.00      1.00     18478
DoS attacks-SlowHTTPTest       0.75      0.46      0.57      5596
   DoS attacks-Slowloris       0.95      0.98      0.97       439
          FTP-BruteForce       0.69      0.89      0.78      7718
           Infilteration       0.39      0.01      0.02      6403
           SQL Injection       0.33      0.25      0.29         4
          SSH-Bruteforce       1.00      1.00      1.00      7525

               micro avg       0.98      0.98      0.98    645487
               macro avg       0.85      0.78      0.79    645487
            weighted avg       0.98      0.98      0.98    645487

2020-01-11 14:12:29,958 [INFO] Overall accuracy (micro avg): 0.9837332122877765
2020-01-11 14:12:43,756 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9837         0.9837                       0.9837                0.0012                   0.0163  0.9837
1     Macro avg        0.9978         0.8548                       0.7772                0.0043                   0.2228  0.7936
2  Weighted avg        0.9913         0.9781                       0.9837                0.0483                   0.0163  0.9787
2020-01-11 14:13:23,374 [INFO] Dataset: Training. Classification report below
2020-01-11 14:13:23,374 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99   1606949
                     Bot       1.00      1.00      1.00     34396
        Brute Force -Web       0.90      0.37      0.52        73
        Brute Force -XSS       0.93      0.50      0.65        26
        DDOS attack-HOIC       1.00      1.00      1.00     82341
    DDOS attack-LOIC-UDP       0.81      0.96      0.88       203
  DDoS attacks-LOIC-HTTP       1.00      1.00      1.00     69029
   DoS attacks-GoldenEye       0.99      1.00      1.00      4954
        DoS attacks-Hulk       1.00      1.00      1.00     55435
DoS attacks-SlowHTTPTest       0.74      0.46      0.57     16787
   DoS attacks-Slowloris       0.97      0.99      0.98      1318
          FTP-BruteForce       0.69      0.88      0.78     23153
           Infilteration       0.54      0.02      0.03     19210
           SQL Injection       0.57      0.33      0.42        12
          SSH-Bruteforce       1.00      1.00      1.00     22576

               micro avg       0.98      0.98      0.98   1936462
               macro avg       0.88      0.77      0.79   1936462
            weighted avg       0.98      0.98      0.98   1936462

2020-01-11 14:13:23,374 [INFO] Overall accuracy (micro avg): 0.9838235916842158
2020-01-11 14:14:08,380 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9838         0.9838                       0.9838                0.0012                   0.0162  0.9838
1     Macro avg        0.9978         0.8757                       0.7677                0.0043                   0.2323  0.7881
2  Weighted avg        0.9914         0.9797                       0.9838                0.0480                   0.0162  0.9789
2020-01-11 14:14:08,406 [INFO] Results saved to: results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep1/semi_sup_perf_ids18_subset_ae_ann_rep1_results.xlsx
2020-01-11 14:14:08,414 [INFO] ================= Finished running experiment no. 1 ================= 

2020-01-11 14:14:08,494 [INFO] Created directory: results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2
2020-01-11 14:14:08,494 [INFO] Initialized logging. log_filename = results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/run_log.log
2020-01-11 14:14:08,494 [INFO] ================= Running experiment no. 2  ================= 

2020-01-11 14:14:08,494 [INFO] Experiment parameters given below
2020-01-11 14:14:08,494 [INFO] 
{'experiment_num': 2, 'results_dir': 'results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2', 'model_type': 'classifier', 'model': 'ae_ann', 'normal_label': 'BENIGN', 'split_random_seed': 42, 'unsupervised_ratio': 0.5, 'ae_encoder_units': [64], 'ae_encoder_activations': ['relu'], 'ae_encoder_dropout_rates': [0.2], 'ae_encoder_l1_param': -1, 'ae_decoder_units': '', 'ae_decoder_activations': '', 'ae_decoder_dropout_rates': '', 'output_activation': 'sigmoid', 'loss_function': 'binary_crossentropy', 'ae_epochs': 200, 'ann_layer_units': [64], 'ann_layer_activations': ['relu'], 'ann_layer_dropout_rates': [0.2], 'ann_epochs': 200, 'early_stop_patience': 50, 'batch_size': 256, 'dataset_dir': '../Datasets/small_datasets/ids2018', 'description': 'semi_sup_perf_ids18_subset_ae_ann_rep2'}
2020-01-11 14:14:08,494 [INFO] Created tensorboard log directory: results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/tf_logs_run_2020_01_11-14_14_08
2020-01-11 14:14:08,494 [INFO] Loading datsets from: ../Datasets/small_datasets/ids2018
2020-01-11 14:14:08,494 [INFO] Reading X, y files
2020-01-11 14:14:08,494 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_train.h5
2020-01-11 14:14:12,929 [INFO] Reading complete. time_to_read=4.43 seconds
2020-01-11 14:14:12,929 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_val.h5
2020-01-11 14:14:14,463 [INFO] Reading complete. time_to_read=1.53 seconds
2020-01-11 14:14:14,464 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_test.h5
2020-01-11 14:14:15,994 [INFO] Reading complete. time_to_read=1.53 seconds
2020-01-11 14:14:15,994 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_train.h5
2020-01-11 14:14:16,251 [INFO] Reading complete. time_to_read=0.26 seconds
2020-01-11 14:14:16,251 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_val.h5
2020-01-11 14:14:16,336 [INFO] Reading complete. time_to_read=0.08 seconds
2020-01-11 14:14:16,336 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_test.h5
2020-01-11 14:14:16,422 [INFO] Reading complete. time_to_read=0.09 seconds
2020-01-11 14:14:20,390 [INFO] Initializing model
2020-01-11 14:14:20,510 [INFO] _________________________________________________________________
2020-01-11 14:14:20,510 [INFO] Layer (type)                 Output Shape              Param #   
2020-01-11 14:14:20,510 [INFO] =================================================================
2020-01-11 14:14:20,510 [INFO] dense_5 (Dense)              (None, 64)                4992      
2020-01-11 14:14:20,510 [INFO] _________________________________________________________________
2020-01-11 14:14:20,510 [INFO] batch_normalization_3 (Batch (None, 64)                256       
2020-01-11 14:14:20,511 [INFO] _________________________________________________________________
2020-01-11 14:14:20,511 [INFO] dropout_3 (Dropout)          (None, 64)                0         
2020-01-11 14:14:20,511 [INFO] _________________________________________________________________
2020-01-11 14:14:20,511 [INFO] dense_6 (Dense)              (None, 77)                5005      
2020-01-11 14:14:20,511 [INFO] =================================================================
2020-01-11 14:14:20,511 [INFO] Total params: 10,253
2020-01-11 14:14:20,511 [INFO] Trainable params: 10,125
2020-01-11 14:14:20,511 [INFO] Non-trainable params: 128
2020-01-11 14:14:20,511 [INFO] _________________________________________________________________
2020-01-11 14:14:20,622 [INFO] _________________________________________________________________
2020-01-11 14:14:20,622 [INFO] Layer (type)                 Output Shape              Param #   
2020-01-11 14:14:20,622 [INFO] =================================================================
2020-01-11 14:14:20,622 [INFO] dense_7 (Dense)              (None, 64)                4160      
2020-01-11 14:14:20,622 [INFO] _________________________________________________________________
2020-01-11 14:14:20,623 [INFO] batch_normalization_4 (Batch (None, 64)                256       
2020-01-11 14:14:20,623 [INFO] _________________________________________________________________
2020-01-11 14:14:20,623 [INFO] dropout_4 (Dropout)          (None, 64)                0         
2020-01-11 14:14:20,623 [INFO] _________________________________________________________________
2020-01-11 14:14:20,623 [INFO] dense_8 (Dense)              (None, 15)                975       
2020-01-11 14:14:20,623 [INFO] =================================================================
2020-01-11 14:14:20,623 [INFO] Total params: 5,391
2020-01-11 14:14:20,623 [INFO] Trainable params: 5,263
2020-01-11 14:14:20,623 [INFO] Non-trainable params: 128
2020-01-11 14:14:20,623 [INFO] _________________________________________________________________
2020-01-11 14:14:20,623 [INFO] Training model
2020-01-11 14:14:20,623 [INFO] Splitting train set into 2 sets (unsupervised, supervised), random_seed = 42
2020-01-11 14:14:43,643 [INFO] Split sizes (instances). total = 1936462, unsupervised = 968231, supervised = 968231, unsupervised dataset hash = df2add71ad9b72cb5233a111509b2b74ff33670a
2020-01-11 14:14:43,643 [INFO] Training autoencoder
 - val_f1: 0.9785
Train on 968231 samples, validate on 645487 samples
Epoch 1/200
 - 17s - loss: -3.2571e+00 - val_loss: -3.6518e+00
Epoch 2/200
 - 16s - loss: -3.6186e+00 - val_loss: -3.6606e+00
Epoch 3/200
 - 17s - loss: -3.6297e+00 - val_loss: -3.6601e+00
Epoch 4/200
 - 16s - loss: -3.6375e+00 - val_loss: -3.6669e+00
Epoch 5/200
 - 16s - loss: -3.6397e+00 - val_loss: -3.6702e+00
Epoch 6/200
 - 16s - loss: -3.6424e+00 - val_loss: -3.5082e+00
Epoch 7/200
 - 16s - loss: -3.6456e+00 - val_loss: -3.6702e+00
Epoch 8/200
 - 16s - loss: -3.6458e+00 - val_loss: -3.6708e+00
Epoch 9/200
 - 16s - loss: -3.6454e+00 - val_loss: -3.6705e+00
Epoch 10/200
 - 16s - loss: -3.6472e+00 - val_loss: -3.6691e+00
Epoch 11/200
 - 16s - loss: -3.6491e+00 - val_loss: -3.6720e+00
Epoch 12/200
 - 16s - loss: -3.6476e+00 - val_loss: -3.6685e+00
Epoch 13/200
 - 16s - loss: -3.6490e+00 - val_loss: -3.6736e+00
Epoch 14/200
 - 16s - loss: -3.6503e+00 - val_loss: -3.6735e+00
Epoch 15/200
 - 16s - loss: -3.6510e+00 - val_loss: -3.6742e+00
Epoch 16/200
 - 16s - loss: -3.6492e+00 - val_loss: -3.5936e+00
Epoch 17/200
 - 16s - loss: -3.6518e+00 - val_loss: -3.6753e+00
Epoch 18/200
 - 16s - loss: -3.6524e+00 - val_loss: -3.6759e+00
Epoch 19/200
 - 16s - loss: -3.6500e+00 - val_loss: -3.6753e+00
Epoch 20/200
 - 16s - loss: -3.6528e+00 - val_loss: -3.6762e+00
Epoch 21/200
 - 16s - loss: -3.6527e+00 - val_loss: -3.6749e+00
2020-01-11 14:20:30,078 [INFO] epoch = 20. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/ae_model_epoch_20.pickle
Epoch 22/200
 - 16s - loss: -3.6531e+00 - val_loss: -3.6750e+00
Epoch 23/200
 - 16s - loss: -3.6533e+00 - val_loss: -3.6738e+00
Epoch 24/200
 - 16s - loss: -3.6535e+00 - val_loss: -3.6749e+00
Epoch 25/200
 - 16s - loss: -3.6534e+00 - val_loss: -3.6756e+00
Epoch 26/200
 - 16s - loss: -3.6536e+00 - val_loss: -3.6754e+00
Epoch 27/200
 - 16s - loss: -3.6531e+00 - val_loss: -3.6751e+00
Epoch 28/200
 - 16s - loss: -3.6541e+00 - val_loss: -3.6759e+00
Epoch 29/200
 - 16s - loss: -3.6536e+00 - val_loss: -3.6670e+00
Epoch 30/200
 - 16s - loss: -3.6543e+00 - val_loss: -3.6701e+00
Epoch 31/200
 - 16s - loss: -3.6536e+00 - val_loss: -3.6765e+00
Epoch 32/200
 - 16s - loss: -3.6541e+00 - val_loss: -3.6619e+00
Epoch 33/200
 - 16s - loss: -3.6546e+00 - val_loss: -3.6765e+00
Epoch 34/200
 - 16s - loss: -3.6541e+00 - val_loss: -3.6104e+00
Epoch 35/200
 - 16s - loss: -3.6531e+00 - val_loss: -3.6298e+00
Epoch 36/200
 - 16s - loss: -3.6552e+00 - val_loss: -3.6434e+00
Epoch 37/200
 - 16s - loss: -3.6553e+00 - val_loss: -3.6772e+00
Epoch 38/200
 - 16s - loss: -3.6533e+00 - val_loss: -3.6501e+00
Epoch 39/200
 - 16s - loss: -3.6536e+00 - val_loss: -3.6767e+00
Epoch 40/200
 - 16s - loss: -3.6554e+00 - val_loss: -3.6768e+00
Epoch 41/200
 - 16s - loss: -3.6543e+00 - val_loss: -3.6545e+00
2020-01-11 14:25:57,432 [INFO] epoch = 40. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/ae_model_epoch_40.pickle
Epoch 42/200
 - 16s - loss: -3.6556e+00 - val_loss: -3.6220e+00
Epoch 43/200
 - 16s - loss: -3.6553e+00 - val_loss: -3.6401e+00
Epoch 44/200
 - 16s - loss: -3.6545e+00 - val_loss: -3.6207e+00
Epoch 45/200
 - 16s - loss: -3.6551e+00 - val_loss: -3.6719e+00
Epoch 46/200
 - 16s - loss: -3.6556e+00 - val_loss: -3.6661e+00
Epoch 47/200
 - 16s - loss: -3.6562e+00 - val_loss: -3.6743e+00
Epoch 48/200
 - 16s - loss: -3.6549e+00 - val_loss: -3.6777e+00
Epoch 49/200
 - 16s - loss: -3.6551e+00 - val_loss: -3.6760e+00
Epoch 50/200
 - 16s - loss: -3.6548e+00 - val_loss: -3.6673e+00
Epoch 51/200
 - 16s - loss: -3.6557e+00 - val_loss: -3.6718e+00
Epoch 52/200
 - 16s - loss: -3.6551e+00 - val_loss: -3.6776e+00
Epoch 53/200
 - 16s - loss: -3.6549e+00 - val_loss: -3.6773e+00
Epoch 54/200
 - 16s - loss: -3.6554e+00 - val_loss: -3.6772e+00
Epoch 55/200
 - 16s - loss: -3.6563e+00 - val_loss: -3.6779e+00
Epoch 56/200
 - 16s - loss: -3.6561e+00 - val_loss: -3.6764e+00
Epoch 57/200
 - 16s - loss: -3.6556e+00 - val_loss: -3.5996e+00
Epoch 58/200
 - 16s - loss: -3.6567e+00 - val_loss: -3.6742e+00
Epoch 59/200
 - 16s - loss: -3.6537e+00 - val_loss: -3.6769e+00
Epoch 60/200
 - 16s - loss: -3.6554e+00 - val_loss: -3.6695e+00
Epoch 61/200
 - 16s - loss: -3.6566e+00 - val_loss: -3.6615e+00
2020-01-11 14:31:24,230 [INFO] epoch = 60. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/ae_model_epoch_60.pickle
Epoch 62/200
 - 16s - loss: -3.6555e+00 - val_loss: -3.6779e+00
Epoch 63/200
 - 16s - loss: -3.6553e+00 - val_loss: -3.6772e+00
Epoch 64/200
 - 16s - loss: -3.6546e+00 - val_loss: -3.6759e+00
Epoch 65/200
 - 16s - loss: -3.6565e+00 - val_loss: -3.6269e+00
Epoch 66/200
 - 16s - loss: -3.6569e+00 - val_loss: -3.6775e+00
Epoch 67/200
 - 16s - loss: -3.6561e+00 - val_loss: -3.6766e+00
Epoch 68/200
 - 16s - loss: -3.6567e+00 - val_loss: -3.6777e+00
Epoch 69/200
 - 16s - loss: -3.6567e+00 - val_loss: -3.6755e+00
Epoch 70/200
 - 16s - loss: -3.6542e+00 - val_loss: -3.6774e+00
Epoch 71/200
 - 16s - loss: -3.6558e+00 - val_loss: -3.6773e+00
Epoch 72/200
 - 16s - loss: -3.6566e+00 - val_loss: -3.6731e+00
Epoch 73/200
 - 16s - loss: -3.6570e+00 - val_loss: -3.6779e+00
Epoch 74/200
 - 16s - loss: -3.6569e+00 - val_loss: -3.6779e+00
Epoch 75/200
 - 16s - loss: -3.6551e+00 - val_loss: -3.6781e+00
Epoch 76/200
 - 16s - loss: -3.6566e+00 - val_loss: -3.6783e+00
Epoch 77/200
 - 16s - loss: -3.6561e+00 - val_loss: -3.6750e+00
Epoch 78/200
 - 16s - loss: -3.6567e+00 - val_loss: -3.6286e+00
Epoch 79/200
 - 16s - loss: -3.6572e+00 - val_loss: -3.6745e+00
Epoch 80/200
 - 16s - loss: -3.6565e+00 - val_loss: -3.6560e+00
Epoch 81/200
 - 16s - loss: -3.6564e+00 - val_loss: -3.6470e+00
2020-01-11 14:36:50,628 [INFO] epoch = 80. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/ae_model_epoch_80.pickle
Epoch 82/200
 - 16s - loss: -3.6562e+00 - val_loss: -3.6784e+00
Epoch 83/200
 - 16s - loss: -3.6569e+00 - val_loss: -3.6779e+00
Epoch 84/200
 - 16s - loss: -3.6571e+00 - val_loss: -3.6753e+00
Epoch 85/200
 - 16s - loss: -3.6543e+00 - val_loss: -3.6445e+00
Epoch 86/200
 - 16s - loss: -3.6558e+00 - val_loss: -3.6397e+00
Epoch 87/200
 - 16s - loss: -3.6564e+00 - val_loss: -3.6746e+00
Epoch 88/200
 - 16s - loss: -3.6570e+00 - val_loss: -3.6663e+00
Epoch 89/200
 - 16s - loss: -3.6572e+00 - val_loss: -3.6785e+00
Epoch 90/200
 - 16s - loss: -3.6569e+00 - val_loss: -3.6732e+00
Epoch 91/200
 - 16s - loss: -3.6561e+00 - val_loss: -3.6268e+00
Epoch 92/200
 - 16s - loss: -3.6565e+00 - val_loss: -3.6772e+00
Epoch 93/200
 - 16s - loss: -3.6552e+00 - val_loss: -3.6785e+00
Epoch 94/200
 - 16s - loss: -3.6565e+00 - val_loss: -3.6494e+00
Epoch 95/200
 - 16s - loss: -3.6563e+00 - val_loss: -3.6691e+00
Epoch 96/200
 - 16s - loss: -3.6571e+00 - val_loss: -3.6784e+00
Epoch 97/200
 - 16s - loss: -3.6575e+00 - val_loss: -3.6771e+00
Epoch 98/200
 - 16s - loss: -3.6566e+00 - val_loss: -3.6782e+00
Epoch 99/200
 - 16s - loss: -3.6575e+00 - val_loss: -3.6784e+00
Epoch 100/200
 - 16s - loss: -3.6555e+00 - val_loss: -3.6600e+00
Epoch 101/200
 - 16s - loss: -3.6572e+00 - val_loss: -3.6776e+00
2020-01-11 14:42:16,996 [INFO] epoch = 100. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/ae_model_epoch_100.pickle
Epoch 102/200
 - 16s - loss: -3.6572e+00 - val_loss: -3.6761e+00
Epoch 103/200
 - 16s - loss: -3.6547e+00 - val_loss: -3.6775e+00
Epoch 104/200
 - 16s - loss: -3.6560e+00 - val_loss: -3.6173e+00
Epoch 105/200
 - 16s - loss: -3.6569e+00 - val_loss: -3.6597e+00
Epoch 106/200
 - 16s - loss: -3.6569e+00 - val_loss: -3.6764e+00
Epoch 107/200
 - 16s - loss: -3.6559e+00 - val_loss: -3.6764e+00
Epoch 108/200
 - 16s - loss: -3.6570e+00 - val_loss: -3.6512e+00
Epoch 109/200
 - 16s - loss: -3.6566e+00 - val_loss: -3.6779e+00
Epoch 110/200
 - 16s - loss: -3.6546e+00 - val_loss: -3.6778e+00
Epoch 111/200
 - 16s - loss: -3.6570e+00 - val_loss: -3.6769e+00
Epoch 112/200
 - 16s - loss: -3.6579e+00 - val_loss: -3.6645e+00
Epoch 113/200
 - 16s - loss: -3.6559e+00 - val_loss: -3.6279e+00
Epoch 114/200
 - 16s - loss: -3.6570e+00 - val_loss: -3.6787e+00
Epoch 115/200
 - 16s - loss: -3.6575e+00 - val_loss: -3.6783e+00
Epoch 116/200
 - 16s - loss: -3.6567e+00 - val_loss: -3.6717e+00
Epoch 117/200
 - 16s - loss: -3.6577e+00 - val_loss: -3.6780e+00
Epoch 118/200
 - 16s - loss: -3.6578e+00 - val_loss: -3.6777e+00
Epoch 119/200
 - 16s - loss: -3.6571e+00 - val_loss: -3.6786e+00
Epoch 120/200
 - 16s - loss: -3.6575e+00 - val_loss: -3.6787e+00
Epoch 121/200
 - 16s - loss: -3.6573e+00 - val_loss: -3.6778e+00
2020-01-11 14:47:43,333 [INFO] epoch = 120. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/ae_model_epoch_120.pickle
Epoch 122/200
 - 16s - loss: -3.6573e+00 - val_loss: -3.6779e+00
Epoch 123/200
 - 16s - loss: -3.6578e+00 - val_loss: -3.6779e+00
Epoch 124/200
 - 16s - loss: -3.6552e+00 - val_loss: -3.6640e+00
Epoch 125/200
 - 16s - loss: -3.6574e+00 - val_loss: -3.6784e+00
Epoch 126/200
 - 16s - loss: -3.6577e+00 - val_loss: -3.6786e+00
Epoch 127/200
 - 16s - loss: -3.6564e+00 - val_loss: -3.6782e+00
Epoch 128/200
 - 16s - loss: -3.6576e+00 - val_loss: -3.6784e+00
Epoch 129/200
 - 16s - loss: -3.6574e+00 - val_loss: -3.6788e+00
Epoch 130/200
 - 16s - loss: -3.6576e+00 - val_loss: -3.6786e+00
Epoch 131/200
 - 16s - loss: -3.6577e+00 - val_loss: -3.6775e+00
Epoch 132/200
 - 16s - loss: -3.6575e+00 - val_loss: -3.6784e+00
Epoch 133/200
 - 16s - loss: -3.6570e+00 - val_loss: -3.6786e+00
Epoch 134/200
 - 16s - loss: -3.6567e+00 - val_loss: -3.6528e+00
Epoch 135/200
 - 16s - loss: -3.6582e+00 - val_loss: -3.6783e+00
Epoch 136/200
 - 16s - loss: -3.6578e+00 - val_loss: -3.6583e+00
Epoch 137/200
 - 16s - loss: -3.6576e+00 - val_loss: -3.6650e+00
Epoch 138/200
 - 16s - loss: -3.6577e+00 - val_loss: -3.6785e+00
Epoch 139/200
 - 16s - loss: -3.6568e+00 - val_loss: -3.6787e+00
Epoch 140/200
 - 16s - loss: -3.6568e+00 - val_loss: -3.6447e+00
Epoch 141/200
 - 16s - loss: -3.6569e+00 - val_loss: -3.6786e+00
2020-01-11 14:53:09,713 [INFO] epoch = 140. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/ae_model_epoch_140.pickle
Epoch 142/200
 - 16s - loss: -3.6572e+00 - val_loss: -3.6532e+00
Epoch 143/200
 - 16s - loss: -3.6579e+00 - val_loss: -3.6789e+00
Epoch 144/200
 - 16s - loss: -3.6562e+00 - val_loss: -3.6790e+00
Epoch 145/200
 - 16s - loss: -3.6570e+00 - val_loss: -3.6427e+00
Epoch 146/200
 - 16s - loss: -3.6573e+00 - val_loss: -3.6330e+00
Epoch 147/200
 - 16s - loss: -3.6580e+00 - val_loss: -3.6589e+00
Epoch 148/200
 - 16s - loss: -3.6576e+00 - val_loss: -3.6783e+00
Epoch 149/200
 - 16s - loss: -3.6582e+00 - val_loss: -3.6789e+00
Epoch 150/200
 - 16s - loss: -3.6579e+00 - val_loss: -3.6789e+00
Epoch 151/200
 - 16s - loss: -3.6562e+00 - val_loss: -3.6784e+00
Epoch 152/200
 - 16s - loss: -3.6574e+00 - val_loss: -3.6781e+00
Epoch 153/200
 - 16s - loss: -3.6566e+00 - val_loss: -3.6412e+00
Epoch 154/200
 - 16s - loss: -3.6583e+00 - val_loss: -3.6625e+00
Epoch 155/200
 - 16s - loss: -3.6567e+00 - val_loss: -3.6784e+00
Epoch 156/200
 - 16s - loss: -3.6579e+00 - val_loss: -3.6779e+00
Epoch 157/200
 - 16s - loss: -3.6571e+00 - val_loss: -3.6372e+00
Epoch 158/200
 - 16s - loss: -3.6563e+00 - val_loss: -3.6621e+00
Epoch 159/200
 - 16s - loss: -3.6577e+00 - val_loss: -3.6333e+00
Epoch 160/200
 - 16s - loss: -3.6580e+00 - val_loss: -3.6748e+00
Epoch 161/200
 - 16s - loss: -3.6577e+00 - val_loss: -3.6784e+00
2020-01-11 14:58:36,406 [INFO] epoch = 160. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/ae_model_epoch_160.pickle
Epoch 162/200
 - 16s - loss: -3.6573e+00 - val_loss: -3.6725e+00
Epoch 163/200
 - 16s - loss: -3.6576e+00 - val_loss: -3.6735e+00
Epoch 164/200
 - 16s - loss: -3.6584e+00 - val_loss: -3.6785e+00
Epoch 165/200
 - 16s - loss: -3.6573e+00 - val_loss: -3.6780e+00
Epoch 166/200
 - 16s - loss: -3.6576e+00 - val_loss: -3.6786e+00
Epoch 167/200
 - 16s - loss: -3.6570e+00 - val_loss: -3.6606e+00
Epoch 168/200
 - 16s - loss: -3.6585e+00 - val_loss: -3.6791e+00
Epoch 169/200
 - 16s - loss: -3.6583e+00 - val_loss: -3.6513e+00
Epoch 170/200
 - 16s - loss: -3.6574e+00 - val_loss: -3.6414e+00
Epoch 171/200
 - 16s - loss: -3.6575e+00 - val_loss: -3.6785e+00
Epoch 172/200
 - 16s - loss: -3.6572e+00 - val_loss: -3.6784e+00
Epoch 173/200
 - 16s - loss: -3.6572e+00 - val_loss: -3.6742e+00
Epoch 174/200
 - 16s - loss: -3.6585e+00 - val_loss: -3.6672e+00
Epoch 175/200
 - 16s - loss: -3.6581e+00 - val_loss: -3.6789e+00
Epoch 176/200
 - 16s - loss: -3.6567e+00 - val_loss: -3.6775e+00
Epoch 177/200
 - 16s - loss: -3.6580e+00 - val_loss: -3.6651e+00
Epoch 178/200
 - 16s - loss: -3.6572e+00 - val_loss: -3.6743e+00
Epoch 179/200
 - 16s - loss: -3.6579e+00 - val_loss: -3.6323e+00
Epoch 180/200
 - 16s - loss: -3.6578e+00 - val_loss: -3.6788e+00
Epoch 181/200
 - 16s - loss: -3.6569e+00 - val_loss: -3.6044e+00
2020-01-11 15:04:03,052 [INFO] epoch = 180. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/ae_model_epoch_180.pickle
Epoch 182/200
 - 16s - loss: -3.6583e+00 - val_loss: -3.6749e+00
Epoch 183/200
 - 16s - loss: -3.6580e+00 - val_loss: -3.6715e+00
Epoch 184/200
 - 16s - loss: -3.6585e+00 - val_loss: -3.6612e+00
Epoch 185/200
 - 16s - loss: -3.6574e+00 - val_loss: -3.6781e+00
Epoch 186/200
 - 16s - loss: -3.6576e+00 - val_loss: -3.6293e+00
Epoch 187/200
 - 16s - loss: -3.6583e+00 - val_loss: -3.6558e+00
Epoch 188/200
 - 16s - loss: -3.6582e+00 - val_loss: -3.6775e+00
Epoch 189/200
 - 16s - loss: -3.6571e+00 - val_loss: -3.6712e+00
Epoch 190/200
 - 16s - loss: -3.6571e+00 - val_loss: -3.6792e+00
Epoch 191/200
 - 16s - loss: -3.6583e+00 - val_loss: -3.6192e+00
Epoch 192/200
 - 16s - loss: -3.6579e+00 - val_loss: -3.6191e+00
Epoch 193/200
 - 16s - loss: -3.6570e+00 - val_loss: -3.6784e+00
Epoch 194/200
 - 16s - loss: -3.6584e+00 - val_loss: -3.6789e+00
Epoch 195/200
 - 16s - loss: -3.6580e+00 - val_loss: -3.6615e+00
Epoch 196/200
 - 16s - loss: -3.6581e+00 - val_loss: -3.6788e+00
Epoch 197/200
 - 16s - loss: -3.6581e+00 - val_loss: -3.6789e+00
Epoch 198/200
 - 16s - loss: -3.6573e+00 - val_loss: -3.6779e+00
Epoch 199/200
 - 16s - loss: -3.6565e+00 - val_loss: -3.6268e+00
Epoch 200/200
 - 16s - loss: -3.6568e+00 - val_loss: -3.6788e+00
2020-01-11 15:09:14,470 [INFO] WeightRestorer::on_train_end(): restoring best weights
2020-01-11 15:09:41,193 [INFO] Last epoch loss evaluation: train_loss = -3.676274, val_loss = -3.679179
2020-01-11 15:09:41,193 [INFO] Training autoencoder complete
2020-01-11 15:09:41,194 [INFO] Encoding data for supervised training
2020-01-11 15:10:03,288 [INFO] Encoding complete
2020-01-11 15:10:03,289 [INFO] Training neural network layers (after autoencoder)
Train on 968231 samples, validate on 645487 samples
Epoch 1/200
 - 8s - loss: 0.0174 - val_loss: 0.0084
 - val_f1: 0.9774
Epoch 2/200
 - 8s - loss: 0.0087 - val_loss: 0.0084
 - val_f1: 0.9768
Epoch 3/200
 - 9s - loss: 0.0085 - val_loss: 0.0081
 - val_f1: 0.9777
Epoch 4/200
 - 9s - loss: 0.0084 - val_loss: 0.0081
 - val_f1: 0.9775
Epoch 5/200
 - 9s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9762
Epoch 6/200
 - 9s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9778
Epoch 7/200
 - 9s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 8/200
 - 9s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9780
Epoch 9/200
 - 9s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9778
Epoch 10/200
 - 9s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9777
Epoch 11/200
 - 9s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9776
Epoch 12/200
 - 9s - loss: 0.0081 - val_loss: 0.0082
 - val_f1: 0.9766
Epoch 13/200
 - 9s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 14/200
 - 9s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 15/200
 - 9s - loss: 0.0081 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 16/200
 - 9s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9765
Epoch 17/200
 - 9s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9778
Epoch 18/200
 - 9s - loss: 0.0080 - val_loss: 0.0078
 - val_f1: 0.9779
Epoch 19/200
 - 9s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 20/200
 - 9s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 21/200
 - 9s - loss: 0.0080 - val_loss: 0.0080
/home/hasitha/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2020-01-11 15:16:12,690 [INFO] epoch = 20. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/ann_model_epoch_20.pickle
 - val_f1: 0.9758
Epoch 22/200
 - 9s - loss: 0.0080 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 23/200
 - 9s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 24/200
 - 9s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 25/200
 - 9s - loss: 0.0080 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 26/200
 - 9s - loss: 0.0080 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 27/200
 - 9s - loss: 0.0080 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 28/200
 - 9s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 29/200
 - 9s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 30/200
 - 9s - loss: 0.0080 - val_loss: 0.0078
 - val_f1: 0.9780
Epoch 31/200
 - 9s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 32/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9787
Epoch 33/200
 - 9s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 34/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 35/200
 - 9s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 36/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 37/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 38/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 39/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 40/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 41/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
2020-01-11 15:22:18,369 [INFO] epoch = 40. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/ann_model_epoch_40.pickle
 - val_f1: 0.9784
Epoch 42/200
 - 9s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9777
Epoch 43/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 44/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9780
Epoch 45/200
 - 9s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 46/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 47/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 48/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 49/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 50/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 51/200
 - 9s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 52/200
 - 9s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 53/200
 - 9s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 54/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 55/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9787
Epoch 56/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 57/200
 - 9s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 58/200
 - 9s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9784
Epoch 59/200
 - 9s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9784
Epoch 60/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 61/200
 - 9s - loss: 0.0079 - val_loss: 0.0079
2020-01-11 15:28:24,713 [INFO] epoch = 60. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/ann_model_epoch_60.pickle
 - val_f1: 0.9765
Epoch 62/200
 - 9s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9787
Epoch 63/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 64/200
 - 9s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9785
Epoch 65/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 66/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9779
Epoch 67/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 68/200
 - 9s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 69/200
 - 9s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9777
Epoch 70/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 71/200
 - 9s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9786
Epoch 72/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9788
Epoch 73/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9780
Epoch 74/200
 - 9s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 75/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9787
Epoch 76/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 77/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 78/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9787
Epoch 79/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 80/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9788
Epoch 81/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
2020-01-11 15:34:30,335 [INFO] epoch = 80. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/ann_model_epoch_80.pickle
 - val_f1: 0.9779
Epoch 82/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9781
Epoch 83/200
 - 9s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 84/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9780
Epoch 85/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 86/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9787
Epoch 87/200
 - 9s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9783
Epoch 88/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 89/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 90/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 91/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 92/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 93/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9787
Epoch 94/200
 - 9s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 95/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 96/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 97/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 98/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 99/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 100/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 101/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
2020-01-11 15:40:36,446 [INFO] epoch = 100. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/ann_model_epoch_100.pickle
 - val_f1: 0.9781
Epoch 102/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9788
Epoch 103/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9782
Epoch 104/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9788
Epoch 105/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9788
Epoch 106/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 107/200
 - 9s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 108/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 109/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9780
Epoch 110/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 111/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 112/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9782
Epoch 113/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9785
Epoch 114/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9780
Epoch 115/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 116/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 117/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 118/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 119/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9784
Epoch 120/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 121/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
2020-01-11 15:46:42,850 [INFO] epoch = 120. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/ann_model_epoch_120.pickle
 - val_f1: 0.9789
Epoch 122/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 123/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9780
Epoch 124/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 125/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 126/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9781
Epoch 127/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9786
Epoch 128/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 129/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9800
Epoch 130/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9787
Epoch 131/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 132/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9786
Epoch 133/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9786
Epoch 134/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 135/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9785
Epoch 136/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9788
Epoch 137/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9786
Epoch 138/200
 - 9s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 139/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9788
Epoch 140/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9788
Epoch 141/200
 - 9s - loss: 0.0078 - val_loss: 0.0079
2020-01-11 15:52:48,789 [INFO] epoch = 140. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/ann_model_epoch_140.pickle
 - val_f1: 0.9784
Epoch 142/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 143/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9787
Epoch 144/200
 - 9s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9763
Epoch 145/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9781
Epoch 146/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9785
Epoch 147/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9784
Epoch 148/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9785
Epoch 149/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 150/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 151/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9787
Epoch 152/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9781
Epoch 153/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9790
Epoch 154/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 155/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9788
Epoch 156/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9788
Epoch 157/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9788
Epoch 158/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9789
Epoch 159/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9786
Epoch 160/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9767
Epoch 161/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
2020-01-11 15:58:54,910 [INFO] epoch = 160. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/ann_model_epoch_160.pickle
 - val_f1: 0.9781
Epoch 162/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9781
Epoch 163/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9785
Epoch 164/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9789
Epoch 165/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9780
Epoch 166/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9781
Epoch 167/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9786
Epoch 168/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 169/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 170/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9787
Epoch 171/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9786
Epoch 172/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9781
Epoch 173/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9787
Epoch 174/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9782
Epoch 175/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9782
Epoch 176/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9789
Epoch 177/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9786
Epoch 178/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9785
Epoch 179/200
 - 9s - loss: 0.0078 - val_loss: 0.0079
 - val_f1: 0.9784
Epoch 180/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9786
Epoch 181/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
2020-01-11 16:05:02,924 [INFO] epoch = 180. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/ann_model_epoch_180.pickle
 - val_f1: 0.9787
Epoch 182/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9784
Epoch 183/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 184/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9787
Epoch 185/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9786
Epoch 186/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 187/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9787
Epoch 188/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9788
Epoch 189/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9785
Epoch 190/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9788
Epoch 191/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9783
Epoch 192/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9781
Epoch 193/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9783
Epoch 194/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9785
Epoch 195/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 196/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9784
Epoch 197/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9786
Epoch 198/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9785
Epoch 199/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9788
Epoch 200/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
2020-01-11 16:11:02,331 [INFO] WeightRestorer::on_train_end(): restoring best weights
2020-01-11 16:11:30,768 [INFO] Last epoch loss evaluation: train_loss = 0.007586, val_loss = 0.007680
2020-01-11 16:11:30,816 [INFO] Training complete. time_to_train = 7030.19 sec, 117.17 min
2020-01-11 16:11:30,824 [INFO] Model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/best_model.pickle
2020-01-11 16:11:30,826 [INFO] Training history saved to: results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/training_error_history.csv
2020-01-11 16:11:30,963 [INFO] Plot saved to: results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/training_error_history.png
2020-01-11 16:11:31,094 [INFO] Plot saved to: results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/training_f1_history.png
2020-01-11 16:11:31,094 [INFO] Making predictions on training, validation, testing data
2020-01-11 16:13:02,719 [INFO] Evaluating predictions (results)
2020-01-11 16:13:14,873 [INFO] Dataset: Testing. Classification report below
2020-01-11 16:13:14,873 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99    535650
                     Bot       1.00      1.00      1.00     11465
        Brute Force -Web       0.83      0.21      0.33        24
        Brute Force -XSS       1.00      0.33      0.50         9
        DDOS attack-HOIC       1.00      1.00      1.00     27447
    DDOS attack-LOIC-UDP       0.74      0.97      0.84        67
  DDoS attacks-LOIC-HTTP       0.99      0.99      0.99     23010
   DoS attacks-GoldenEye       0.99      1.00      1.00      1651
        DoS attacks-Hulk       1.00      1.00      1.00     18478
DoS attacks-SlowHTTPTest       0.75      0.51      0.61      5596
   DoS attacks-Slowloris       0.96      0.97      0.97       440
          FTP-BruteForce       0.71      0.88      0.79      7718
           Infilteration       0.41      0.01      0.02      6404
           SQL Injection       0.67      0.50      0.57         4
          SSH-Bruteforce       1.00      1.00      1.00      7525

               micro avg       0.98      0.98      0.98    645488
               macro avg       0.87      0.76      0.77    645488
            weighted avg       0.98      0.98      0.98    645488

2020-01-11 16:13:14,873 [INFO] Overall accuracy (micro avg): 0.9837254914111494
2020-01-11 16:13:28,697 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9837         0.9837                       0.9837                0.0012                   0.0163  0.9837
1     Macro avg        0.9978         0.8695                       0.7584                0.0044                   0.2416  0.7739
2  Weighted avg        0.9910         0.9782                       0.9837                0.0493                   0.0163  0.9788
2020-01-11 16:13:40,850 [INFO] Dataset: Validation. Classification report below
2020-01-11 16:13:40,850 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99    535650
                     Bot       1.00      1.00      1.00     11465
        Brute Force -Web       0.88      0.28      0.42        25
        Brute Force -XSS       0.86      0.67      0.75         9
        DDOS attack-HOIC       1.00      1.00      1.00     27447
    DDOS attack-LOIC-UDP       0.78      0.99      0.87        68
  DDoS attacks-LOIC-HTTP       0.99      0.99      0.99     23009
   DoS attacks-GoldenEye       1.00      1.00      1.00      1651
        DoS attacks-Hulk       1.00      1.00      1.00     18478
DoS attacks-SlowHTTPTest       0.76      0.51      0.61      5596
   DoS attacks-Slowloris       0.95      0.98      0.97       439
          FTP-BruteForce       0.71      0.88      0.79      7718
           Infilteration       0.39      0.01      0.02      6403
           SQL Injection       1.00      0.25      0.40         4
          SSH-Bruteforce       1.00      1.00      1.00      7525

               micro avg       0.98      0.98      0.98    645487
               macro avg       0.89      0.77      0.79    645487
            weighted avg       0.98      0.98      0.98    645487

2020-01-11 16:13:40,850 [INFO] Overall accuracy (micro avg): 0.9838013778743802
2020-01-11 16:13:54,659 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9838         0.9838                       0.9838                0.0012                   0.0162  0.9838
1     Macro avg        0.9978         0.8867                       0.7704                0.0044                   0.2296  0.7873
2  Weighted avg        0.9910         0.9782                       0.9838                0.0493                   0.0162  0.9788
2020-01-11 16:14:34,303 [INFO] Dataset: Training. Classification report below
2020-01-11 16:14:34,303 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99   1606949
                     Bot       1.00      1.00      1.00     34396
        Brute Force -Web       0.95      0.26      0.41        73
        Brute Force -XSS       1.00      0.50      0.67        26
        DDOS attack-HOIC       1.00      1.00      1.00     82341
    DDOS attack-LOIC-UDP       0.76      0.97      0.85       203
  DDoS attacks-LOIC-HTTP       0.99      0.99      0.99     69029
   DoS attacks-GoldenEye       1.00      1.00      1.00      4954
        DoS attacks-Hulk       1.00      1.00      1.00     55435
DoS attacks-SlowHTTPTest       0.76      0.51      0.61     16787
   DoS attacks-Slowloris       0.96      0.99      0.98      1318
          FTP-BruteForce       0.71      0.88      0.79     23153
           Infilteration       0.47      0.01      0.02     19210
           SQL Injection       1.00      0.33      0.50        12
          SSH-Bruteforce       1.00      1.00      1.00     22576

               micro avg       0.98      0.98      0.98   1936462
               macro avg       0.91      0.76      0.79   1936462
            weighted avg       0.98      0.98      0.98   1936462

2020-01-11 16:14:34,303 [INFO] Overall accuracy (micro avg): 0.983808615919135
2020-01-11 16:15:19,333 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9838         0.9838                       0.9838                0.0012                   0.0162  0.9838
1     Macro avg        0.9978         0.9059                       0.7635                0.0044                   0.2365  0.7873
2  Weighted avg        0.9911         0.9789                       0.9838                0.0491                   0.0162  0.9789
2020-01-11 16:15:19,359 [INFO] Results saved to: results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep2/semi_sup_perf_ids18_subset_ae_ann_rep2_results.xlsx
2020-01-11 16:15:19,364 [INFO] ================= Finished running experiment no. 2 ================= 

2020-01-11 16:15:19,443 [INFO] Created directory: results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3
2020-01-11 16:15:19,443 [INFO] Initialized logging. log_filename = results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/run_log.log
2020-01-11 16:15:19,443 [INFO] ================= Running experiment no. 3  ================= 

2020-01-11 16:15:19,443 [INFO] Experiment parameters given below
2020-01-11 16:15:19,443 [INFO] 
{'experiment_num': 3, 'results_dir': 'results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3', 'model_type': 'classifier', 'model': 'ae_ann', 'normal_label': 'BENIGN', 'split_random_seed': 42, 'unsupervised_ratio': 0.5, 'ae_encoder_units': [64], 'ae_encoder_activations': ['relu'], 'ae_encoder_dropout_rates': [0.2], 'ae_encoder_l1_param': -1, 'ae_decoder_units': '', 'ae_decoder_activations': '', 'ae_decoder_dropout_rates': '', 'output_activation': 'sigmoid', 'loss_function': 'binary_crossentropy', 'ae_epochs': 200, 'ann_layer_units': [64], 'ann_layer_activations': ['relu'], 'ann_layer_dropout_rates': [0.2], 'ann_epochs': 200, 'early_stop_patience': 50, 'batch_size': 256, 'dataset_dir': '../Datasets/small_datasets/ids2018', 'description': 'semi_sup_perf_ids18_subset_ae_ann_rep3'}
2020-01-11 16:15:19,443 [INFO] Created tensorboard log directory: results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/tf_logs_run_2020_01_11-16_15_19
2020-01-11 16:15:19,443 [INFO] Loading datsets from: ../Datasets/small_datasets/ids2018
2020-01-11 16:15:19,443 [INFO] Reading X, y files
2020-01-11 16:15:19,443 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_train.h5
2020-01-11 16:15:23,896 [INFO] Reading complete. time_to_read=4.45 seconds
2020-01-11 16:15:23,896 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_val.h5
2020-01-11 16:15:25,432 [INFO] Reading complete. time_to_read=1.54 seconds
2020-01-11 16:15:25,432 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/X_test.h5
2020-01-11 16:15:26,968 [INFO] Reading complete. time_to_read=1.54 seconds
2020-01-11 16:15:26,969 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_train.h5
2020-01-11 16:15:27,216 [INFO] Reading complete. time_to_read=0.25 seconds
2020-01-11 16:15:27,216 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_val.h5
2020-01-11 16:15:27,300 [INFO] Reading complete. time_to_read=0.08 seconds
2020-01-11 16:15:27,301 [INFO] Reading HDF dataset ../Datasets/small_datasets/ids2018/y_test.h5
2020-01-11 16:15:27,386 [INFO] Reading complete. time_to_read=0.09 seconds
2020-01-11 16:15:31,282 [INFO] Initializing model
2020-01-11 16:15:31,404 [INFO] _________________________________________________________________
2020-01-11 16:15:31,404 [INFO] Layer (type)                 Output Shape              Param #   
2020-01-11 16:15:31,404 [INFO] =================================================================
2020-01-11 16:15:31,404 [INFO] dense_9 (Dense)              (None, 64)                4992      
2020-01-11 16:15:31,404 [INFO] _________________________________________________________________
2020-01-11 16:15:31,404 [INFO] batch_normalization_5 (Batch (None, 64)                256       
2020-01-11 16:15:31,404 [INFO] _________________________________________________________________
2020-01-11 16:15:31,404 [INFO] dropout_5 (Dropout)          (None, 64)                0         
2020-01-11 16:15:31,404 [INFO] _________________________________________________________________
2020-01-11 16:15:31,404 [INFO] dense_10 (Dense)             (None, 77)                5005      
2020-01-11 16:15:31,404 [INFO] =================================================================
2020-01-11 16:15:31,405 [INFO] Total params: 10,253
2020-01-11 16:15:31,405 [INFO] Trainable params: 10,125
2020-01-11 16:15:31,405 [INFO] Non-trainable params: 128
2020-01-11 16:15:31,405 [INFO] _________________________________________________________________
2020-01-11 16:15:31,516 [INFO] _________________________________________________________________
2020-01-11 16:15:31,516 [INFO] Layer (type)                 Output Shape              Param #   
2020-01-11 16:15:31,516 [INFO] =================================================================
2020-01-11 16:15:31,516 [INFO] dense_11 (Dense)             (None, 64)                4160      
2020-01-11 16:15:31,516 [INFO] _________________________________________________________________
2020-01-11 16:15:31,516 [INFO] batch_normalization_6 (Batch (None, 64)                256       
2020-01-11 16:15:31,516 [INFO] _________________________________________________________________
2020-01-11 16:15:31,516 [INFO] dropout_6 (Dropout)          (None, 64)                0         
2020-01-11 16:15:31,516 [INFO] _________________________________________________________________
2020-01-11 16:15:31,516 [INFO] dense_12 (Dense)             (None, 15)                975       
2020-01-11 16:15:31,517 [INFO] =================================================================
2020-01-11 16:15:31,517 [INFO] Total params: 5,391
2020-01-11 16:15:31,517 [INFO] Trainable params: 5,263
2020-01-11 16:15:31,517 [INFO] Non-trainable params: 128
2020-01-11 16:15:31,517 [INFO] _________________________________________________________________
2020-01-11 16:15:31,517 [INFO] Training model
2020-01-11 16:15:31,517 [INFO] Splitting train set into 2 sets (unsupervised, supervised), random_seed = 42
2020-01-11 16:15:56,082 [INFO] Split sizes (instances). total = 1936462, unsupervised = 968231, supervised = 968231, unsupervised dataset hash = df2add71ad9b72cb5233a111509b2b74ff33670a
2020-01-11 16:15:56,082 [INFO] Training autoencoder
 - val_f1: 0.9789
Train on 968231 samples, validate on 645487 samples
Epoch 1/200
 - 17s - loss: -3.2590e+00 - val_loss: -3.6546e+00
Epoch 2/200
 - 17s - loss: -3.6169e+00 - val_loss: -3.6622e+00
Epoch 3/200
 - 17s - loss: -3.6289e+00 - val_loss: -3.4918e+00
Epoch 4/200
 - 17s - loss: -3.6349e+00 - val_loss: -3.6657e+00
Epoch 5/200
 - 17s - loss: -3.6385e+00 - val_loss: -3.6674e+00
Epoch 6/200
 - 17s - loss: -3.6411e+00 - val_loss: -3.6578e+00
Epoch 7/200
 - 17s - loss: -3.6422e+00 - val_loss: -3.6690e+00
Epoch 8/200
 - 17s - loss: -3.6426e+00 - val_loss: -3.6701e+00
Epoch 9/200
 - 17s - loss: -3.6452e+00 - val_loss: -3.6697e+00
Epoch 10/200
 - 17s - loss: -3.6457e+00 - val_loss: -3.6697e+00
Epoch 11/200
 - 17s - loss: -3.6466e+00 - val_loss: -3.6705e+00
Epoch 12/200
 - 17s - loss: -3.6474e+00 - val_loss: -3.6690e+00
Epoch 13/200
 - 17s - loss: -3.6466e+00 - val_loss: -3.6627e+00
Epoch 14/200
 - 17s - loss: -3.6484e+00 - val_loss: -3.6719e+00
Epoch 15/200
 - 17s - loss: -3.6483e+00 - val_loss: -3.6690e+00
Epoch 16/200
 - 17s - loss: -3.6512e+00 - val_loss: -3.6733e+00
Epoch 17/200
 - 17s - loss: -3.6515e+00 - val_loss: -3.6729e+00
Epoch 18/200
 - 17s - loss: -3.6505e+00 - val_loss: -3.6732e+00
Epoch 19/200
 - 17s - loss: -3.6521e+00 - val_loss: -3.6742e+00
Epoch 20/200
 - 17s - loss: -3.6515e+00 - val_loss: -3.6599e+00
Epoch 21/200
 - 17s - loss: -3.6528e+00 - val_loss: -3.6629e+00
2020-01-11 16:21:49,367 [INFO] epoch = 20. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/ae_model_epoch_20.pickle
Epoch 22/200
 - 17s - loss: -3.6513e+00 - val_loss: -3.6756e+00
Epoch 23/200
 - 17s - loss: -3.6529e+00 - val_loss: -3.6740e+00
Epoch 24/200
 - 17s - loss: -3.6532e+00 - val_loss: -3.5801e+00
Epoch 25/200
 - 17s - loss: -3.6539e+00 - val_loss: -3.6747e+00
Epoch 26/200
 - 17s - loss: -3.6541e+00 - val_loss: -3.6758e+00
Epoch 27/200
 - 17s - loss: -3.6548e+00 - val_loss: -3.6104e+00
Epoch 28/200
 - 17s - loss: -3.6518e+00 - val_loss: -3.5885e+00
Epoch 29/200
 - 17s - loss: -3.6540e+00 - val_loss: -3.6761e+00
Epoch 30/200
 - 17s - loss: -3.6542e+00 - val_loss: -3.6755e+00
Epoch 31/200
 - 17s - loss: -3.6540e+00 - val_loss: -3.6757e+00
Epoch 32/200
 - 17s - loss: -3.6542e+00 - val_loss: -3.6757e+00
Epoch 33/200
 - 17s - loss: -3.6549e+00 - val_loss: -3.5984e+00
Epoch 34/200
 - 17s - loss: -3.6546e+00 - val_loss: -3.6759e+00
Epoch 35/200
 - 17s - loss: -3.6547e+00 - val_loss: -3.6759e+00
Epoch 36/200
 - 17s - loss: -3.6556e+00 - val_loss: -3.6753e+00
Epoch 37/200
 - 17s - loss: -3.6554e+00 - val_loss: -3.6745e+00
Epoch 38/200
 - 17s - loss: -3.6520e+00 - val_loss: -3.6198e+00
Epoch 39/200
 - 17s - loss: -3.6539e+00 - val_loss: -3.6757e+00
Epoch 40/200
 - 17s - loss: -3.6548e+00 - val_loss: -3.6757e+00
Epoch 41/200
 - 17s - loss: -3.6545e+00 - val_loss: -3.6762e+00
2020-01-11 16:27:27,855 [INFO] epoch = 40. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/ae_model_epoch_40.pickle
Epoch 42/200
 - 17s - loss: -3.6551e+00 - val_loss: -3.6760e+00
Epoch 43/200
 - 17s - loss: -3.6546e+00 - val_loss: -3.5932e+00
Epoch 44/200
 - 17s - loss: -3.6544e+00 - val_loss: -3.6759e+00
Epoch 45/200
 - 17s - loss: -3.6553e+00 - val_loss: -3.6752e+00
Epoch 46/200
 - 17s - loss: -3.6549e+00 - val_loss: -3.6606e+00
Epoch 47/200
 - 17s - loss: -3.6559e+00 - val_loss: -3.6031e+00
Epoch 48/200
 - 17s - loss: -3.6556e+00 - val_loss: -3.6754e+00
Epoch 49/200
 - 17s - loss: -3.6557e+00 - val_loss: -3.6761e+00
Epoch 50/200
 - 17s - loss: -3.6558e+00 - val_loss: -3.6756e+00
Epoch 51/200
 - 17s - loss: -3.6559e+00 - val_loss: -3.6754e+00
Epoch 52/200
 - 17s - loss: -3.6560e+00 - val_loss: -3.6750e+00
Epoch 53/200
 - 17s - loss: -3.6563e+00 - val_loss: -3.6746e+00
Epoch 54/200
 - 17s - loss: -3.6551e+00 - val_loss: -3.6761e+00
Epoch 55/200
 - 17s - loss: -3.6570e+00 - val_loss: -3.5928e+00
Epoch 56/200
 - 17s - loss: -3.6564e+00 - val_loss: -3.6733e+00
Epoch 57/200
 - 17s - loss: -3.6562e+00 - val_loss: -3.6737e+00
Epoch 58/200
 - 17s - loss: -3.6553e+00 - val_loss: -3.6714e+00
Epoch 59/200
 - 17s - loss: -3.6528e+00 - val_loss: -3.6623e+00
Epoch 60/200
 - 17s - loss: -3.6568e+00 - val_loss: -3.6747e+00
Epoch 61/200
 - 17s - loss: -3.6555e+00 - val_loss: -3.6754e+00
2020-01-11 16:33:06,271 [INFO] epoch = 60. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/ae_model_epoch_60.pickle
Epoch 62/200
 - 17s - loss: -3.6559e+00 - val_loss: -3.6756e+00
Epoch 63/200
 - 17s - loss: -3.6564e+00 - val_loss: -3.6758e+00
Epoch 64/200
 - 17s - loss: -3.6549e+00 - val_loss: -3.6760e+00
Epoch 65/200
 - 17s - loss: -3.6560e+00 - val_loss: -3.6740e+00
Epoch 66/200
 - 17s - loss: -3.6563e+00 - val_loss: -3.6756e+00
Epoch 67/200
 - 17s - loss: -3.6567e+00 - val_loss: -3.6758e+00
Epoch 68/200
 - 17s - loss: -3.6567e+00 - val_loss: -3.6769e+00
Epoch 69/200
 - 17s - loss: -3.6572e+00 - val_loss: -3.6747e+00
Epoch 70/200
 - 17s - loss: -3.6565e+00 - val_loss: -3.6721e+00
Epoch 71/200
 - 17s - loss: -3.6556e+00 - val_loss: -3.6762e+00
Epoch 72/200
 - 17s - loss: -3.6570e+00 - val_loss: -3.6758e+00
Epoch 73/200
 - 17s - loss: -3.6563e+00 - val_loss: -3.6774e+00
Epoch 74/200
 - 17s - loss: -3.6568e+00 - val_loss: -3.6288e+00
Epoch 75/200
 - 17s - loss: -3.6534e+00 - val_loss: -3.6765e+00
Epoch 76/200
 - 17s - loss: -3.6563e+00 - val_loss: -3.5513e+00
Epoch 77/200
 - 17s - loss: -3.6572e+00 - val_loss: -3.6742e+00
Epoch 78/200
 - 17s - loss: -3.6567e+00 - val_loss: -3.6768e+00
Epoch 79/200
 - 17s - loss: -3.6570e+00 - val_loss: -3.6771e+00
Epoch 80/200
 - 17s - loss: -3.6575e+00 - val_loss: -3.6752e+00
Epoch 81/200
 - 17s - loss: -3.6563e+00 - val_loss: -3.6186e+00
2020-01-11 16:38:44,353 [INFO] epoch = 80. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/ae_model_epoch_80.pickle
Epoch 82/200
 - 17s - loss: -3.6566e+00 - val_loss: -3.6766e+00
Epoch 83/200
 - 17s - loss: -3.6573e+00 - val_loss: -3.6764e+00
Epoch 84/200
 - 17s - loss: -3.6573e+00 - val_loss: -3.5876e+00
Epoch 85/200
 - 17s - loss: -3.6560e+00 - val_loss: -3.6771e+00
Epoch 86/200
 - 17s - loss: -3.6573e+00 - val_loss: -3.6765e+00
Epoch 87/200
 - 17s - loss: -3.6577e+00 - val_loss: -3.6757e+00
Epoch 88/200
 - 17s - loss: -3.6567e+00 - val_loss: -3.6768e+00
Epoch 89/200
 - 17s - loss: -3.6570e+00 - val_loss: -3.6775e+00
Epoch 90/200
 - 17s - loss: -3.6576e+00 - val_loss: -3.6769e+00
Epoch 91/200
 - 17s - loss: -3.6569e+00 - val_loss: -3.6763e+00
Epoch 92/200
 - 17s - loss: -3.6534e+00 - val_loss: -3.6760e+00
Epoch 93/200
 - 17s - loss: -3.6536e+00 - val_loss: -3.6745e+00
Epoch 94/200
 - 17s - loss: -3.6563e+00 - val_loss: -3.6698e+00
Epoch 95/200
 - 17s - loss: -3.6568e+00 - val_loss: -3.6777e+00
Epoch 96/200
 - 17s - loss: -3.6576e+00 - val_loss: -3.6776e+00
Epoch 97/200
 - 17s - loss: -3.6576e+00 - val_loss: -3.6766e+00
Epoch 98/200
 - 17s - loss: -3.6575e+00 - val_loss: -3.6751e+00
Epoch 99/200
 - 17s - loss: -3.6570e+00 - val_loss: -3.6774e+00
Epoch 100/200
 - 17s - loss: -3.6580e+00 - val_loss: -3.6765e+00
Epoch 101/200
 - 17s - loss: -3.6565e+00 - val_loss: -3.5647e+00
2020-01-11 16:44:23,056 [INFO] epoch = 100. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/ae_model_epoch_100.pickle
Epoch 102/200
 - 17s - loss: -3.6566e+00 - val_loss: -3.6692e+00
Epoch 103/200
 - 17s - loss: -3.6576e+00 - val_loss: -3.6761e+00
Epoch 104/200
 - 17s - loss: -3.6579e+00 - val_loss: -3.6740e+00
Epoch 105/200
 - 17s - loss: -3.6566e+00 - val_loss: -3.6774e+00
Epoch 106/200
 - 17s - loss: -3.6575e+00 - val_loss: -3.6762e+00
Epoch 107/200
 - 17s - loss: -3.6573e+00 - val_loss: -3.5970e+00
Epoch 108/200
 - 17s - loss: -3.6581e+00 - val_loss: -3.6648e+00
Epoch 109/200
 - 17s - loss: -3.6577e+00 - val_loss: -3.5665e+00
Epoch 110/200
 - 17s - loss: -3.6575e+00 - val_loss: -3.5415e+00
Epoch 111/200
 - 17s - loss: -3.6576e+00 - val_loss: -3.6738e+00
Epoch 112/200
 - 17s - loss: -3.6579e+00 - val_loss: -3.6701e+00
Epoch 113/200
 - 17s - loss: -3.6558e+00 - val_loss: -3.6749e+00
Epoch 114/200
 - 17s - loss: -3.6575e+00 - val_loss: -3.6582e+00
Epoch 115/200
 - 17s - loss: -3.6580e+00 - val_loss: -3.6764e+00
Epoch 116/200
 - 17s - loss: -3.6580e+00 - val_loss: -3.6761e+00
Epoch 117/200
 - 17s - loss: -3.6581e+00 - val_loss: -3.6770e+00
Epoch 118/200
 - 17s - loss: -3.6539e+00 - val_loss: -3.6689e+00
Epoch 119/200
 - 17s - loss: -3.6562e+00 - val_loss: -3.6767e+00
Epoch 120/200
 - 17s - loss: -3.6577e+00 - val_loss: -3.6767e+00
Epoch 121/200
 - 17s - loss: -3.6579e+00 - val_loss: -3.6765e+00
2020-01-11 16:50:02,395 [INFO] epoch = 120. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/ae_model_epoch_120.pickle
Epoch 122/200
 - 17s - loss: -3.6570e+00 - val_loss: -3.6611e+00
Epoch 123/200
 - 17s - loss: -3.6581e+00 - val_loss: -3.6753e+00
Epoch 124/200
 - 17s - loss: -3.6577e+00 - val_loss: -3.5728e+00
Epoch 125/200
 - 17s - loss: -3.6577e+00 - val_loss: -3.6766e+00
Epoch 126/200
 - 17s - loss: -3.6566e+00 - val_loss: -3.6697e+00
Epoch 127/200
 - 17s - loss: -3.6582e+00 - val_loss: -3.6763e+00
Epoch 128/200
 - 17s - loss: -3.6578e+00 - val_loss: -3.6767e+00
Epoch 129/200
 - 17s - loss: -3.6576e+00 - val_loss: -3.6772e+00
Epoch 130/200
 - 17s - loss: -3.6580e+00 - val_loss: -3.6778e+00
Epoch 131/200
 - 17s - loss: -3.6581e+00 - val_loss: -3.6770e+00
Epoch 132/200
 - 17s - loss: -3.6579e+00 - val_loss: -3.6759e+00
Epoch 133/200
 - 17s - loss: -3.6583e+00 - val_loss: -3.6752e+00
Epoch 134/200
 - 17s - loss: -3.6580e+00 - val_loss: -3.6735e+00
Epoch 135/200
 - 17s - loss: -3.6536e+00 - val_loss: -3.6759e+00
Epoch 136/200
 - 17s - loss: -3.6561e+00 - val_loss: -3.6767e+00
Epoch 137/200
 - 17s - loss: -3.6574e+00 - val_loss: -3.6759e+00
Epoch 138/200
 - 17s - loss: -3.6574e+00 - val_loss: -3.5627e+00
Epoch 139/200
 - 17s - loss: -3.6544e+00 - val_loss: -3.6768e+00
Epoch 140/200
 - 17s - loss: -3.6542e+00 - val_loss: -3.6766e+00
Epoch 141/200
 - 17s - loss: -3.6568e+00 - val_loss: -3.6756e+00
2020-01-11 16:55:41,441 [INFO] epoch = 140. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/ae_model_epoch_140.pickle
Epoch 142/200
 - 17s - loss: -3.6573e+00 - val_loss: -3.6724e+00
Epoch 143/200
 - 17s - loss: -3.6584e+00 - val_loss: -3.6715e+00
Epoch 144/200
 - 17s - loss: -3.6580e+00 - val_loss: -3.6735e+00
Epoch 145/200
 - 17s - loss: -3.6573e+00 - val_loss: -3.5779e+00
Epoch 146/200
 - 17s - loss: -3.6576e+00 - val_loss: -3.6779e+00
Epoch 147/200
 - 17s - loss: -3.6578e+00 - val_loss: -3.6780e+00
Epoch 148/200
 - 17s - loss: -3.6577e+00 - val_loss: -3.6764e+00
Epoch 149/200
 - 17s - loss: -3.6580e+00 - val_loss: -3.6766e+00
Epoch 150/200
 - 17s - loss: -3.6578e+00 - val_loss: -3.6743e+00
Epoch 151/200
 - 17s - loss: -3.6545e+00 - val_loss: -3.5844e+00
Epoch 152/200
 - 17s - loss: -3.6565e+00 - val_loss: -3.6763e+00
Epoch 153/200
 - 17s - loss: -3.6583e+00 - val_loss: -3.6768e+00
Epoch 154/200
 - 17s - loss: -3.6576e+00 - val_loss: -3.6780e+00
Epoch 155/200
 - 17s - loss: -3.6574e+00 - val_loss: -3.6768e+00
Epoch 156/200
 - 17s - loss: -3.6543e+00 - val_loss: -3.6316e+00
Epoch 157/200
 - 17s - loss: -3.6581e+00 - val_loss: -3.6764e+00
Epoch 158/200
 - 17s - loss: -3.6588e+00 - val_loss: -3.6758e+00
Epoch 159/200
 - 17s - loss: -3.6573e+00 - val_loss: -3.6760e+00
Epoch 160/200
 - 17s - loss: -3.6579e+00 - val_loss: -3.6748e+00
Epoch 161/200
 - 17s - loss: -3.6583e+00 - val_loss: -3.6736e+00
2020-01-11 17:01:20,510 [INFO] epoch = 160. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/ae_model_epoch_160.pickle
Epoch 162/200
 - 17s - loss: -3.6570e+00 - val_loss: -3.6734e+00
Epoch 163/200
 - 17s - loss: -3.6556e+00 - val_loss: -3.5880e+00
Epoch 164/200
 - 17s - loss: -3.6573e+00 - val_loss: -3.6773e+00
Epoch 165/200
 - 17s - loss: -3.6583e+00 - val_loss: -3.6777e+00
Epoch 166/200
 - 17s - loss: -3.6582e+00 - val_loss: -3.6753e+00
Epoch 167/200
 - 17s - loss: -3.6580e+00 - val_loss: -3.6765e+00
Epoch 168/200
 - 17s - loss: -3.6581e+00 - val_loss: -3.6779e+00
Epoch 169/200
 - 17s - loss: -3.6576e+00 - val_loss: -3.6778e+00
Epoch 170/200
 - 17s - loss: -3.6580e+00 - val_loss: -3.5724e+00
Epoch 171/200
 - 17s - loss: -3.6585e+00 - val_loss: -3.6760e+00
Epoch 172/200
 - 17s - loss: -3.6577e+00 - val_loss: -3.6747e+00
Epoch 173/200
 - 17s - loss: -3.6583e+00 - val_loss: -3.6779e+00
Epoch 174/200
 - 17s - loss: -3.6585e+00 - val_loss: -3.5901e+00
Epoch 175/200
 - 17s - loss: -3.6577e+00 - val_loss: -3.6779e+00
Epoch 176/200
 - 17s - loss: -3.6585e+00 - val_loss: -3.6735e+00
Epoch 177/200
 - 17s - loss: -3.6572e+00 - val_loss: -3.5610e+00
Epoch 178/200
 - 17s - loss: -3.6589e+00 - val_loss: -3.6758e+00
Epoch 179/200
 - 17s - loss: -3.6583e+00 - val_loss: -3.6758e+00
Epoch 180/200
 - 17s - loss: -3.6556e+00 - val_loss: -3.5893e+00
Epoch 181/200
 - 17s - loss: -3.6575e+00 - val_loss: -3.6773e+00
2020-01-11 17:06:59,669 [INFO] epoch = 180. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/ae_model_epoch_180.pickle
Epoch 182/200
 - 17s - loss: -3.6585e+00 - val_loss: -3.6769e+00
Epoch 183/200
 - 17s - loss: -3.6569e+00 - val_loss: -3.6780e+00
Epoch 184/200
 - 17s - loss: -3.6548e+00 - val_loss: -3.6781e+00
Epoch 185/200
 - 17s - loss: -3.6588e+00 - val_loss: -3.6733e+00
Epoch 186/200
 - 17s - loss: -3.6574e+00 - val_loss: -3.6778e+00
Epoch 187/200
 - 17s - loss: -3.6581e+00 - val_loss: -3.6779e+00
Epoch 188/200
 - 17s - loss: -3.6583e+00 - val_loss: -3.6777e+00
Epoch 189/200
 - 17s - loss: -3.6583e+00 - val_loss: -3.5781e+00
Epoch 190/200
 - 17s - loss: -3.6574e+00 - val_loss: -3.6778e+00
Epoch 191/200
 - 17s - loss: -3.6581e+00 - val_loss: -3.6779e+00
Epoch 192/200
 - 17s - loss: -3.6578e+00 - val_loss: -3.6397e+00
Epoch 193/200
 - 17s - loss: -3.6586e+00 - val_loss: -3.6779e+00
Epoch 194/200
 - 17s - loss: -3.6588e+00 - val_loss: -3.6777e+00
Epoch 195/200
 - 17s - loss: -3.6582e+00 - val_loss: -3.6709e+00
Epoch 196/200
 - 17s - loss: -3.6585e+00 - val_loss: -3.6694e+00
Epoch 197/200
 - 17s - loss: -3.6548e+00 - val_loss: -3.6759e+00
Epoch 198/200
 - 17s - loss: -3.6579e+00 - val_loss: -3.6768e+00
Epoch 199/200
 - 17s - loss: -3.6545e+00 - val_loss: -3.6773e+00
Epoch 200/200
 - 17s - loss: -3.6578e+00 - val_loss: -3.6783e+00
2020-01-11 17:12:22,131 [INFO] WeightRestorer::on_train_end(): restoring best weights
2020-01-11 17:12:50,608 [INFO] Last epoch loss evaluation: train_loss = -3.676289, val_loss = -3.678330
2020-01-11 17:12:50,609 [INFO] Training autoencoder complete
2020-01-11 17:12:50,609 [INFO] Encoding data for supervised training
2020-01-11 17:13:14,234 [INFO] Encoding complete
2020-01-11 17:13:14,234 [INFO] Training neural network layers (after autoencoder)
Train on 968231 samples, validate on 645487 samples
Epoch 1/200
 - 9s - loss: 0.0171 - val_loss: 0.0085
 - val_f1: 0.9768
Epoch 2/200
 - 9s - loss: 0.0088 - val_loss: 0.0082
 - val_f1: 0.9774
Epoch 3/200
 - 9s - loss: 0.0085 - val_loss: 0.0080
 - val_f1: 0.9777
Epoch 4/200
 - 9s - loss: 0.0084 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 5/200
 - 9s - loss: 0.0083 - val_loss: 0.0081
 - val_f1: 0.9754
Epoch 6/200
 - 9s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 7/200
 - 9s - loss: 0.0082 - val_loss: 0.0080
 - val_f1: 0.9777
Epoch 8/200
 - 9s - loss: 0.0082 - val_loss: 0.0081
 - val_f1: 0.9780
Epoch 9/200
 - 9s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9756
Epoch 10/200
 - 9s - loss: 0.0081 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 11/200
 - 9s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 12/200
 - 9s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 13/200
 - 9s - loss: 0.0081 - val_loss: 0.0080
 - val_f1: 0.9780
Epoch 14/200
 - 9s - loss: 0.0080 - val_loss: 0.0081
 - val_f1: 0.9778
Epoch 15/200
 - 9s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 16/200
 - 9s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9780
Epoch 17/200
 - 9s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9781
Epoch 18/200
 - 9s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9781
Epoch 19/200
 - 9s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 20/200
 - 9s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 21/200
 - 9s - loss: 0.0080 - val_loss: 0.0079
/home/hasitha/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
2020-01-11 17:19:43,180 [INFO] epoch = 20. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/ann_model_epoch_20.pickle
 - val_f1: 0.9780
Epoch 22/200
 - 9s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 23/200
 - 9s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 24/200
 - 9s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9781
Epoch 25/200
 - 9s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 26/200
 - 9s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9778
Epoch 27/200
 - 9s - loss: 0.0080 - val_loss: 0.0079
 - val_f1: 0.9780
Epoch 28/200
 - 9s - loss: 0.0080 - val_loss: 0.0080
 - val_f1: 0.9779
Epoch 29/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 30/200
 - 9s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 31/200
 - 9s - loss: 0.0080 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 32/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 33/200
 - 9s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9793
Epoch 34/200
 - 9s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9781
Epoch 35/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 36/200
 - 9s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 37/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 38/200
 - 9s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9777
Epoch 39/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 40/200
 - 9s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9781
Epoch 41/200
 - 9s - loss: 0.0079 - val_loss: 0.0079
2020-01-11 17:26:06,265 [INFO] epoch = 40. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/ann_model_epoch_40.pickle
 - val_f1: 0.9780
Epoch 42/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 43/200
 - 9s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9782
Epoch 44/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 45/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9767
Epoch 46/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 47/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 48/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 49/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 50/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 51/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9787
Epoch 52/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9769
Epoch 53/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 54/200
 - 9s - loss: 0.0079 - val_loss: 0.0079
 - val_f1: 0.9779
Epoch 55/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 56/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 57/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 58/200
 - 9s - loss: 0.0079 - val_loss: 0.0077
 - val_f1: 0.9785
Epoch 59/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 60/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9780
Epoch 61/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
2020-01-11 17:32:33,306 [INFO] epoch = 60. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/ann_model_epoch_60.pickle
 - val_f1: 0.9783
Epoch 62/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 63/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 64/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 65/200
 - 9s - loss: 0.0079 - val_loss: 0.0077
 - val_f1: 0.9783
Epoch 66/200
 - 9s - loss: 0.0079 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 67/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9765
Epoch 68/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 69/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9779
Epoch 70/200
 - 9s - loss: 0.0079 - val_loss: 0.0080
 - val_f1: 0.9776
Epoch 71/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9787
Epoch 72/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9787
Epoch 73/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9787
Epoch 74/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 75/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9785
Epoch 76/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9781
Epoch 77/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9787
Epoch 78/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9787
Epoch 79/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 80/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9785
Epoch 81/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
2020-01-11 17:39:03,127 [INFO] epoch = 80. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/ann_model_epoch_80.pickle
 - val_f1: 0.9787
Epoch 82/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 83/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 84/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9788
Epoch 85/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9785
Epoch 86/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9783
Epoch 87/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 88/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 89/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9785
Epoch 90/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9781
Epoch 91/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9797
Epoch 92/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9782
Epoch 93/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9787
Epoch 94/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9780
Epoch 95/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9781
Epoch 96/200
 - 9s - loss: 0.0078 - val_loss: 0.0080
 - val_f1: 0.9775
Epoch 97/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9782
Epoch 98/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9787
Epoch 99/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 100/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9788
Epoch 101/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
2020-01-11 17:45:33,271 [INFO] epoch = 100. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/ann_model_epoch_100.pickle
 - val_f1: 0.9782
Epoch 102/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9783
Epoch 103/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 104/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9782
Epoch 105/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9786
Epoch 106/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9783
Epoch 107/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9782
Epoch 108/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9780
Epoch 109/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 110/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9787
Epoch 111/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9786
Epoch 112/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9786
Epoch 113/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9784
Epoch 114/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 115/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 116/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9780
Epoch 117/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9787
Epoch 118/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9785
Epoch 119/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9786
Epoch 120/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9784
Epoch 121/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
2020-01-11 17:52:03,347 [INFO] epoch = 120. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/ann_model_epoch_120.pickle
 - val_f1: 0.9785
Epoch 122/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9786
Epoch 123/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 124/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9766
Epoch 125/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9788
Epoch 126/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 127/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9789
Epoch 128/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9787
Epoch 129/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9786
Epoch 130/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 131/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9785
Epoch 132/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9779
Epoch 133/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9784
Epoch 134/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9785
Epoch 135/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9789
Epoch 136/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9788
Epoch 137/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9798
Epoch 138/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9787
Epoch 139/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9784
Epoch 140/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9787
Epoch 141/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
2020-01-11 17:58:33,421 [INFO] epoch = 140. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/ann_model_epoch_140.pickle
 - val_f1: 0.9786
Epoch 142/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9784
Epoch 143/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9783
Epoch 144/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9785
Epoch 145/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9782
Epoch 146/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9780
Epoch 147/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9780
Epoch 148/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9781
Epoch 149/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9786
Epoch 150/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9786
Epoch 151/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9783
Epoch 152/200
 - 9s - loss: 0.0078 - val_loss: 0.0078
 - val_f1: 0.9784
Epoch 153/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9787
Epoch 154/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9790
Epoch 155/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9783
Epoch 156/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9784
Epoch 157/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9785
Epoch 158/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9784
Epoch 159/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9781
Epoch 160/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9782
Epoch 161/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
2020-01-11 18:05:03,711 [INFO] epoch = 160. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/ann_model_epoch_160.pickle
 - val_f1: 0.9782
Epoch 162/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9782
Epoch 163/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9783
Epoch 164/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9781
Epoch 165/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9787
Epoch 166/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9769
Epoch 167/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9783
Epoch 168/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9785
Epoch 169/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9789
Epoch 170/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9787
Epoch 171/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9785
Epoch 172/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9787
Epoch 173/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9779
Epoch 174/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9788
Epoch 175/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9789
Epoch 176/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9787
Epoch 177/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9783
Epoch 178/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9784
Epoch 179/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9785
Epoch 180/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9788
Epoch 181/200
 - 9s - loss: 0.0077 - val_loss: 0.0078
2020-01-11 18:11:33,993 [INFO] epoch = 180. Intermediate model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/ann_model_epoch_180.pickle
 - val_f1: 0.9782
Epoch 182/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9783
Epoch 183/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9780
Epoch 184/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9784
Epoch 185/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9789
Epoch 186/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9784
Epoch 187/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9789
Epoch 188/200
 - 9s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9783
Epoch 189/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9786
Epoch 190/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9785
Epoch 191/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9783
Epoch 192/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9784
Epoch 193/200
 - 9s - loss: 0.0078 - val_loss: 0.0077
 - val_f1: 0.9780
Epoch 194/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9790
Epoch 195/200
 - 9s - loss: 0.0077 - val_loss: 0.0076
 - val_f1: 0.9788
Epoch 196/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9787
Epoch 197/200
 - 9s - loss: 0.0077 - val_loss: 0.0078
 - val_f1: 0.9782
Epoch 198/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9785
Epoch 199/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
 - val_f1: 0.9789
Epoch 200/200
 - 9s - loss: 0.0077 - val_loss: 0.0077
2020-01-11 18:17:55,453 [INFO] WeightRestorer::on_train_end(): restoring best weights
2020-01-11 18:18:25,221 [INFO] Last epoch loss evaluation: train_loss = 0.007546, val_loss = 0.007650
2020-01-11 18:18:25,269 [INFO] Training complete. time_to_train = 7373.75 sec, 122.90 min
2020-01-11 18:18:25,276 [INFO] Model saved to results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/best_model.pickle
2020-01-11 18:18:25,279 [INFO] Training history saved to: results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/training_error_history.csv
2020-01-11 18:18:25,413 [INFO] Plot saved to: results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/training_error_history.png
2020-01-11 18:18:25,545 [INFO] Plot saved to: results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/training_f1_history.png
2020-01-11 18:18:25,546 [INFO] Making predictions on training, validation, testing data
2020-01-11 18:20:02,385 [INFO] Evaluating predictions (results)
2020-01-11 18:20:14,545 [INFO] Dataset: Testing. Classification report below
2020-01-11 18:20:14,545 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99    535650
                     Bot       1.00      1.00      1.00     11465
        Brute Force -Web       0.80      0.33      0.47        24
        Brute Force -XSS       1.00      0.33      0.50         9
        DDOS attack-HOIC       1.00      1.00      1.00     27447
    DDOS attack-LOIC-UDP       0.71      0.99      0.83        67
  DDoS attacks-LOIC-HTTP       0.99      1.00      1.00     23010
   DoS attacks-GoldenEye       0.99      1.00      1.00      1651
        DoS attacks-Hulk       1.00      1.00      1.00     18478
DoS attacks-SlowHTTPTest       0.75      0.49      0.59      5596
   DoS attacks-Slowloris       0.93      0.97      0.95       440
          FTP-BruteForce       0.70      0.88      0.78      7718
           Infilteration       0.46      0.01      0.02      6404
           SQL Injection       0.25      0.25      0.25         4
          SSH-Bruteforce       1.00      1.00      1.00      7525

               micro avg       0.98      0.98      0.98    645488
               macro avg       0.84      0.75      0.76    645488
            weighted avg       0.98      0.98      0.98    645488

2020-01-11 18:20:14,545 [INFO] Overall accuracy (micro avg): 0.983657325930149
2020-01-11 18:20:28,370 [INFO] Average metrics for Testing dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9837         0.9837                       0.9837                0.0012                   0.0163  0.9837
1     Macro avg        0.9978         0.8383                       0.7498                0.0043                   0.2502  0.7584
2  Weighted avg        0.9911         0.9787                       0.9837                0.0487                   0.0163  0.9787
2020-01-11 18:20:40,504 [INFO] Dataset: Validation. Classification report below
2020-01-11 18:20:40,504 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99    535650
                     Bot       1.00      1.00      1.00     11465
        Brute Force -Web       1.00      0.48      0.65        25
        Brute Force -XSS       0.86      0.67      0.75         9
        DDOS attack-HOIC       1.00      1.00      1.00     27447
    DDOS attack-LOIC-UDP       0.76      0.99      0.86        68
  DDoS attacks-LOIC-HTTP       0.99      1.00      1.00     23009
   DoS attacks-GoldenEye       1.00      1.00      1.00      1651
        DoS attacks-Hulk       1.00      1.00      1.00     18478
DoS attacks-SlowHTTPTest       0.76      0.48      0.59      5596
   DoS attacks-Slowloris       0.93      0.99      0.96       439
          FTP-BruteForce       0.70      0.89      0.78      7718
           Infilteration       0.51      0.01      0.03      6403
           SQL Injection       0.50      0.25      0.33         4
          SSH-Bruteforce       1.00      1.00      1.00      7525

               micro avg       0.98      0.98      0.98    645487
               macro avg       0.87      0.78      0.80    645487
            weighted avg       0.98      0.98      0.98    645487

2020-01-11 18:20:40,504 [INFO] Overall accuracy (micro avg): 0.9837533521201821
2020-01-11 18:20:54,300 [INFO] Average metrics for Validation dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9838         0.9838                       0.9838                0.0012                   0.0162  0.9838
1     Macro avg        0.9978         0.8659                       0.7830                0.0043                   0.2170  0.7954
2  Weighted avg        0.9912         0.9793                       0.9838                0.0484                   0.0162  0.9788
2020-01-11 18:21:33,934 [INFO] Dataset: Training. Classification report below
2020-01-11 18:21:33,934 [INFO] 
                          precision    recall  f1-score   support

                  Benign       0.99      1.00      0.99   1606949
                     Bot       1.00      1.00      1.00     34396
        Brute Force -Web       0.96      0.37      0.53        73
        Brute Force -XSS       0.93      0.50      0.65        26
        DDOS attack-HOIC       1.00      1.00      1.00     82341
    DDOS attack-LOIC-UDP       0.75      0.99      0.85       203
  DDoS attacks-LOIC-HTTP       0.99      1.00      1.00     69029
   DoS attacks-GoldenEye       1.00      1.00      1.00      4954
        DoS attacks-Hulk       1.00      1.00      1.00     55435
DoS attacks-SlowHTTPTest       0.75      0.48      0.58     16787
   DoS attacks-Slowloris       0.94      0.99      0.96      1318
          FTP-BruteForce       0.70      0.88      0.78     23153
           Infilteration       0.55      0.02      0.03     19210
           SQL Injection       0.67      0.33      0.44        12
          SSH-Bruteforce       1.00      1.00      1.00     22576

               micro avg       0.98      0.98      0.98   1936462
               macro avg       0.88      0.77      0.79   1936462
            weighted avg       0.98      0.98      0.98   1936462

2020-01-11 18:21:33,934 [INFO] Overall accuracy (micro avg): 0.9837197941400347
2020-01-11 18:22:18,949 [INFO] Average metrics for Training dataset below
   average type  avg accuracy  avg precision  avg detection rate (recall)  avg false alarm rate  avg false negative rate  avg f1
0     Micro avg        0.9837         0.9837                       0.9837                0.0012                   0.0163  0.9837
1     Macro avg        0.9978         0.8812                       0.7704                0.0043                   0.2296  0.7884
2  Weighted avg        0.9912         0.9796                       0.9837                0.0483                   0.0163  0.9788
2020-01-11 18:22:18,975 [INFO] Results saved to: results_additional_exps/semi_sup_perf_ids18_subset_ae_ann_rep3/semi_sup_perf_ids18_subset_ae_ann_rep3_results.xlsx
2020-01-11 18:22:18,983 [INFO] ================= Finished running experiment no. 3 ================= 

2020-01-11 18:22:19,059 [INFO] ================= Finished running 3 experiments ================= 

 - val_f1: 0.9770
